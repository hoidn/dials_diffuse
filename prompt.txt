<prompt>
## **Context Preparation Protocol**

You are an expert context preparation agent. Your primary function is to analyze a codebase and a planned task, then produce a comprehensive context package in a specific JSON format. Your analysis must be entirely focused on serving the **`<planned task>`**.

### **Core Philosophy: Erring on the Side of Inclusion**

Your primary goal is to prevent missing **any potentially useful file**. It is better to include a file that is only peripherally related than to omit a file that provides crucial context. Think of your role as maximizing *recall* over *precision*. You must actively look for indirect relationships. For any given file, ask:

*   Does it define a data structure used by a relevant file?
*   Is it a configuration file that a relevant file might read?
*   Is it a parent component that uses a relevant component?
*   Is it a shared utility, helper, or service called by a relevant file?
*   Is it a test for a relevant file?
*   Does it establish a style, pattern, or architectural standard that should be followed?

If the answer to any of these is "yes," the file is likely relevant.

### **CRITICAL INCLUSION RULES**

These rules are mandatory and **override** your normal relevance assessment. You MUST follow them.

1.  **Critical Files Mandate:** You **MUST** read the list of files provided in the **`<critical>`** section of this prompt. For **every file** listed in that section, you **MUST**:
    *   Classify the file as "Relevant".
    *   Assign it a **very high score (e.g., 9.0 or higher)**.
    *   Use the justification: "Included as a mandatory core project document as specified in the <critical> context section."

2.  **Test File Mandate:** If the `<planned task>` involves modifying an existing file (e.g., `src/utils/parser.py`), you **MUST** find its corresponding test file (e.g., `tests/test_parser.py`). You **MUST** classify this test file as "Relevant" and assign it a **high score (e.g., 8.0 or higher)**. Its justification should state: "Mandatory inclusion of the test file for the modified source code."

3.  **Checklist Example Mandate:** If the `<planned task>` involves drafting, creating, or modifying a checklist, you **MUST** find the best example of an existing checklist (see checklists/) in the codebase. You **MUST** classify this file as "Relevant" and assign it a **high score (e.g., 8.0 or higher)**. Its justification should state: "Included as a mandatory style and format reference for the checklist creation task."

4.  **Plan Example Mandate:** If the `<planned task>` involves drafting a plan, you **MUST** find the best example of an existing plan document. You **MUST** classify this file as "Relevant" and assign it a **high score (e.g., 8.0 or higher)**. Its justification should state: "Included as a mandatory style and format reference for the planning task."

### **Instructions**

**Step 1: High-Level Analysis**
First, deeply analyze the **`<planned task>`** description and the overall codebase. From this analysis, generate the initial high-level fields for the JSON output:
*   `planned_task`: Write a concise, one-sentence summary of the task.
*   `task_requirements`: Create a list of specific, actionable requirements derived from the task description.
*   `codebase_summary`: Write a brief summary of the codebase's architecture, specifically focusing on the parts relevant to the planned task.

**Step 2: File-by-File Analysis & Classification**
Iterate through every file provided in the code context, applying the **Core Philosophy** at all times. This analysis will populate the `file_classification` object. For each file, perform the following:
1.  **Classify its Relevance:** Categorize the file as "Relevant", "Maybe Relevant", or "Irrelevant". **When in doubt, classify as "Maybe Relevant" instead of "Irrelevant".**
2.  **Score its Importance:** For all "Relevant" and "Maybe Relevant" files, assign an importance score from 1.0 to 10.0, keeping the **CRITICAL INCLUSION RULES** in mind. Use the following revised guide:
    *   **10.0: Critically Important.** The file is being directly created, edited, or is the primary subject of the task.
    *   **7.0 - 9.9: Highly Relevant.** Provides core logic, data models, direct dependencies (imports/exports), is a mandatory test file, or is a core project document from the `<critical>` section.
    *   **4.0 - 6.9: Contextually Relevant.** Provides useful surrounding context, shared utilities, configuration, or is a parent/child component that is not directly modified but is affected.
    *   **1.0 - 3.9: Potentially Relevant.** Included for broader context or as a style/pattern example. Useful for understanding the "bigger picture" or related conventions.
3.  **Justify the Classification:** For every file, write a `description` of its purpose and a `justification` for its classification and score. The justification must explain *why* the file is or isn't relevant, referencing your analysis (e.g., "This file defines the data model consumed by `relevant_file.py`," or "This config file sets variables used in the target module.").

**Step 3: Content Inclusion**
Prepare the file content for the final package. This step populates the `included_files` and `documentation_excerpts` arrays.
*   **For "Relevant" and "Maybe Relevant" code files:** Add an entry to the `included_files` array. Use the Jinja-style template syntax `{file_path}` in the `content` field.
*   **For large documentation files (e.g., under `libdocs/`):** Do not include the full file. Instead, identify the most relevant sections, create a targeted excerpt, and add an entry to the `documentation_excerpts` array.

**Step 4: Final Validation and Assembly**
Before generating the final JSON, perform a final review of your work. **Verify the following:**
- **Have you fully embraced the Core Philosophy of including all potentially useful files?**
- **Have you obeyed all CRITICAL INCLUSION RULES? Specifically:**
    - **Have you included all files from the `<critical>` section with a high score?**
    - Have you included the required test files, checklist examples, or plan examples if the task dictated it?
- Is the JSON structure perfectly valid?
- Are all file lists sorted by `score` in descending order?

If any revisions are necessary, write a non-json revision preamble section where you describe the adjustments.

Assemble all information into a single, valid JSON object according to the format below.

### **Final JSON Output Format**

```json
{
  "context_package": {
    "planned_task": "A one-sentence summary of the planned task.",
    "task_requirements": [
      "A specific, actionable requirement derived from the task.",
      "Another specific requirement."
    ],
    "codebase_summary": "A brief summary of the codebase architecture relevant to the task.",
    "file_classification": {
      "relevant_files": [
        {
          "path": "path/to/relevant/file.py",
          "description": "Brief description of the file's purpose.",
          "justification": "Why this file is essential for the planned task.",
          "score": 9.5
        }
      ],
      "maybe_relevant_files": [
        {
          "path": "path/to/maybe/relevant/file.py",
          "description": "Brief description of the file's purpose.",
          "justification": "Why this file provides useful context for the task.",
          "score": 6.0
        }
      ],
      "irrelevant_files": [
        {
          "path": "path/to/irrelevant/file.js",
          "description": "Brief description of the file's purpose.",
          "justification": "Why this file has no bearing on the planned task."
        }
      ]
    },
    "included_files": [
      {
        "score": 9.5,
        "path": "path/to/relevant/file.py",
        "description": "Brief description of the file's purpose.",
        "content": "{path/to/relevant/file.py}"
      }
    ],
    "documentation_excerpts": [
      {
        "score": 7.5,
        "source": "libdocs/api_reference.md",
        "title": "API Reference - Relevant Section Title",
        "content": "A short, relevant excerpt from the documentation file that is specific to the planned task..."
      }
    ]
  }
}
```
</prompt>
<planned task>
We will be improving the phase 3 visual diagnostic check, which currently incorrectly uses mocks. We will also update the proj conventions to prevent similar errors in future development. 
relevant context might include:
- code
- docs 
- phase 2 e2e visual check script (as an example)
</planned task>
<critical>
the following project files are 'critical' and should always be included as context, regardless of the specific task. They provide the fundamental "rules of the road," project goals, and architectural principles.

These files can be categorized into three groups: **Core Plans**, **Project Rules & Conventions**, and **Essential Guides**.

### 1. Core Plans & High-Level Strategy

These files define the "what" and "why" of the project. An agent cannot understand the purpose of its task without them.

*   `plan.md`: **The Master Plan.** This is the highest-authority technical specification for the entire processing pipeline. It is the single most critical document for understanding any task's place in the larger system.
*   `plan_adaptation.md`: **Critical Modifications to the Master Plan.** This document details the crucial pivot to support both stills and sequence data, a fundamental architectural decision that impacts how the pipeline is orchestrated. It's an essential addendum to `plan.md`.
*   `CLAUDE.md`: **Project-Specific AI Guidance & High-Level Log.** This file appears to be a working log of high-level decisions, goals, and technical achievements. For an AI agent, this provides invaluable context on recent progress, proven strategies, and the project's "state of mind."
*   `README.md`: **The Project Entry Point.** Provides the highest-level overview of the project's purpose and features.

### 2. Project Rules & Conventions

These files define the "how" of the project. They ensure that any work done by the agent is consistent with the established development process, coding standards, and architectural patterns.

*   `docs/00_START_HERE.md`: **Developer Orientation.** The primary onboarding document that explains the project's core philosophy, including the critical role of IDL files as specifications.
*   `docs/01_IDL_GUIDELINES.md`: **The "Language" of the Project.** This defines the syntax and semantics of the Interface Definition Language (IDL) used to specify every component. Without this, an agent cannot correctly interpret or create component contracts.
*   `docs/02_IMPLEMENTATION_RULES.md`: **Coding and Testing Standards.** This dictates how IDLs are translated into code, how testing should be approached (emphasizing integration over unit tests), and other critical implementation patterns.
*   `docs/03_PROJECT_RULES.md`: **Project Organization.** Defines the directory structure, version control workflow, and conventions for file organization. Essential for any task that involves creating or moving files.
*   `src/diffusepipe/types/types_IDL.md` (and `docs/ARCHITECTURE/types.md`): **Shared Data Contracts.** Defines the core data structures (`OperationOutcome`, configuration objects, etc.) that are used across the entire pipeline. Understanding these is fundamental to any component interaction.

### 3. Essential Guides & Distilled Knowledge

These documents contain "hard-won" knowledge and troubleshooting information that can prevent common errors and significantly accelerate development.

*   `docs/LESSONS_LEARNED.md`: **The "Wisdom" of the Project.** This file consolidates critical insights from past development challenges, such as the "Test Failure Crisis" and "Safe Refactoring" incident. It's a high-leverage document that helps avoid repeating past mistakes.
*   `docs/06_DIALS_DEBUGGING_GUIDE.md`: **The DIALS Bible.** DIALS is the most critical and complex external dependency. This guide is essential for debugging any task involving DIALS processing, which is a core part of the pipeline.
*   `docs/VISUAL_DIAGNOSTICS_GUIDE.md`: **Validation and Verification Guide.** This documents the tools used to visually verify the correctness of the pipeline's outputs. It's crucial for any task involving pipeline development or debugging.
</critical>
<code context>
This file is a merged representation of a subset of the codebase, containing specifically included files and files not matching ignore patterns, combined into a single document by Repomix.

<file_summary>
This section contains a summary of this file.

<purpose>
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.
</purpose>

<file_format>
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Repository files (if enabled)
5. Multiple file entries, each consisting of:
  - File path as an attribute
  - Full contents of the file
</file_format>

<usage_guidelines>
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.
</usage_guidelines>

<notes>
- Some files may have been excluded based on .gitignore rules and Repomix's configuration
- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files
- Only files matching these patterns are included: **/*.md, **/*.py, **/*.phil
- Files matching these patterns are excluded: .aider.chat.history.md
- Files matching patterns in .gitignore are excluded
- Files matching default ignore patterns are excluded
- Files are sorted by Git change count (files with more changes are at the bottom)
</notes>

</file_summary>

<directory_structure>
.claude/
  commands/
    cleanup_codebase.md
    context.md
    dpayload.md
    mem_to_docs.md
    sync_docs.md
    update_docs.md
    update_IDLs.md
agent/
  writing_checklists.md
checklists/
  phase0.md
  phase1.md
  phase2.md
  phase3.md
dev_scripts/
  debug_q_vector_suite/
    debug_q_fix.py
    debug_q_validation.py
    debug_simple_q_check.py
    simple_debug_test.py
    test_coordinate_matrix.py
    test_debug_logging.py
    test_q_fix.py
  check_reflection_columns.py
  compare_coordinates.py
  debug_manual_vs_stills.py
docs/
  ARCHITECTURE/
    adr/
      ADR_TEMPLATE.md
      README.md
    types.md
  LIBRARY_INTEGRATION/
    README.md
  TEMPLATES/
    IDL_IMPLEMENTATION_CHECKLIST.md
    TASK_INSTRUCTION_TEMPLATE.md
    WORKING_MEMORY_LOG.md
  WORKFLOWS/
    TASK_PREPARATION_WORKFLOW.md
  00_START_HERE.md
  01_IDL_GUIDELINES.md
  02_IMPLEMENTATION_RULES.md
  03_PROJECT_RULES.md
  04_REFACTORING_GUIDE.md
  05_DOCUMENTATION_GUIDE.md
  06_DIALS_DEBUGGING_GUIDE.md
  LESSONS_LEARNED.md
  VISUAL_DIAGNOSTICS_GUIDE.md
libdocs/
  dials/
    crystallographic_calculations.md
    dials_conventions.md
    dials_file_io.md
    dials_programs.md
    dials_scaling.md
    dxtbx_models.md
    flex_arrays.md
    README.md
scripts/
  dev_workflows/
    README.md
    run_phase2_e2e_visual_check.py
    run_phase3_e2e_visual_check.py
  visual_diagnostics/
    __init__.py
    check_dials_processing.py
    check_diffuse_extraction.py
    check_phase3_outputs.py
    check_pixel_masks.py
    check_total_mask.py
    demo_visual_checks.py
    plot_utils.py
    README.md
    test_imports.py
  demo_phase1_processing.py
src/
  diffusepipe/
    adapters/
      __init__.py
      dials_generate_mask_adapter_IDL.md
      dials_generate_mask_adapter.py
      dials_sequence_process_adapter_IDL.md
      dials_sequence_process_adapter.py
      dials_stills_process_adapter_IDL.md
      dials_stills_process_adapter.py
      dxtbx_io_adapter_IDL.md
      dxtbx_io_adapter.py
    config/
      find_spots.phil
      refine_detector.phil
      sequence_find_spots_default.phil
      sequence_import_default.phil
      sequence_index_default.phil
      sequence_integrate_default.phil
    crystallography/
      __init__.py
      model_validator_IDL.md
      q_consistency_checker_IDL.md
      q_consistency_checker.py
      still_processing_and_validation.py
      still_processor_and_validator_IDL.md
    diagnostics/
      __init__.py
      q_calculator_IDL.md
      q_calculator.py
      q_consistency_checker.py
    extraction/
      __init__.py
      data_extractor_IDL.md
      data_extractor.py
    masking/
      __init__.py
      bragg_mask_generator_IDL.md
      bragg_mask_generator.py
      pixel_mask_generator_IDL.md
      pixel_mask_generator.py
    merging/
      __init__.py
      merger_IDL.md
      merger.py
    orchestration/
      __init__.py
      pipeline_orchestrator_IDL.md
      stills_pipeline_orchestrator_IDL.md
      stills_pipeline_orchestrator.py
    scaling/
      components/
        __init__.py
        per_still_multiplier.py
        resolution_smoother.py
      __init__.py
      diffuse_scaling_model_IDL.md
      diffuse_scaling_model.py
    types/
      __init__.py
      types_IDL.md
      types_IDL.py
    utils/
      __init__.py
      cbf_utils_IDL.md
      cbf_utils.py
    voxelization/
      __init__.py
      global_voxel_grid_IDL.md
      global_voxel_grid.py
      voxel_accumulator_IDL.md
      voxel_accumulator.py
    __init__.py
    constants.py
    corrections_IDL.md
    corrections.py
    exceptions.py
    logging_config.py
tests/
  adapters/
    test_dials_generate_mask_adapter.py
    test_dials_stills_process_adapter.py
    test_dxtbx_io_adapter.py
  crystallography/
    __init__.py
    test_still_processing_and_validation.py
    test_still_processor.py
  diagnostics/
    test_q_calculator.py
  extraction/
    test_data_extractor_phase2.py
  integration/
    __init__.py
    test_phase1_workflow.py
    test_phase3_workflow.py
    test_sequence_adapter_integration.py
  masking/
    __init__.py
    test_bragg_mask_generator.py
    test_pixel_mask_generator.py
  merging/
    __init__.py
    test_merger.py
  orchestration/
    __init__.py
    test_stills_pipeline_orchestrator.py
  scaling/
    __init__.py
    test_diffuse_scaling_model.py
  utils/
    test_cbf_utils.py
  visual_diagnostics/
    test_check_phase3_outputs.py
  voxelization/
    __init__.py
    test_global_voxel_grid.py
    test_voxel_accumulator.py
  __init__.py
  conftest.py
  test_corrections_regression.py
  test_phase2_core_functionality.py
CLAUDE.md
critical.md
dp.md
meisburger.md
plan_adaptation.md
plan.md
plan2.md
README.md
</directory_structure>

<files>
This section contains the contents of the repository's files.

<file path=".claude/commands/cleanup_codebase.md">
# Codebase Cleanup Command

This command provides a systematic approach to cleaning up and organizing a codebase by removing obsolete files, archiving superseded documents, and organizing development utilities.

## Cleanup Categories

### 1. Archive Superseded Documentation
- Move outdated `.md` files to `archive/` directory
- Add `.ARCHIVED` suffix for clarity
- Preserve git history while removing clutter

### 2. Organize Root-Level Scripts
- Relocate ad-hoc debugging scripts to organized directories
- Move valuable integration tests to formal test suite
- Consolidate related utilities into themed subdirectories

### 3. Remove Dead Code
- Identify and remove unused functions
- Clean up imports and dead imports
- Remove commented-out code blocks

### 4. Code Quality Maintenance
- Apply consistent formatting (Black)
- Run linters and fix issues
- Ensure all changes are properly committed

## Implementation Checklist

### Phase 1: Setup and Planning
- [ ] Create todo list to track progress
- [ ] Identify files for archival based on content analysis
- [ ] Identify root-level scripts for relocation
- [ ] Identify dead code for removal

### Phase 2: Archive Superseded Files
- [ ] Create `archive/` directory if it doesn't exist
- [ ] Move superseded files with `.ARCHIVED` suffix:
  - `plans/validationfix.md` → `archive/validationfix.md.ARCHIVED`
  - `refactorfix.md` → `archive/refactorfix.md.ARCHIVED`
  - Any other resolved/obsolete documentation
- [ ] Commit archival changes: `git commit -m "ARCHIVE: Superseded documentation"`

### Phase 3: Organize Development Scripts
- [ ] Create `dev_scripts/` directory structure
- [ ] Create temporary sorting directory `_root_scripts_to_sort/`
- [ ] Move all root-level development scripts to sorting directory
- [ ] Evaluate and relocate each script:

#### Integration Tests
- [ ] Move to `tests/integration/` with appropriate naming
- [ ] Example: `test_sequence_adapter.py` → `tests/integration/test_sequence_adapter_integration.py`

#### Development Utilities
- [ ] Move to `dev_scripts/` for general utilities:
  - `debug_manual_vs_stills.py`
  - `check_reflection_columns.py`
  - `compare_coordinates.py`

#### Themed Script Collections
- [ ] Create themed subdirectories for related scripts:
  - `dev_scripts/debug_q_vector_suite/` for Q-vector debugging
  - Move related scripts: `debug_q_*.py`, `test_q_*.py`, etc.

- [ ] Remove temporary sorting directory
- [ ] Commit script organization: `git commit -m "CLEANUP: Relocate/organize root-level scripts"`

### Phase 4: Remove Dead Code
- [ ] Identify unused functions in core modules
- [ ] Example: Remove `get_q_bragg_from_refl_data` from `extractor.py`
- [ ] Verify removal doesn't break functionality
- [ ] Commit dead code removal: `git commit -m "REFACTOR: Remove unused functions"`

### Phase 5: Code Quality and Verification
- [ ] Apply code formatting: `python -m black .`
- [ ] Run linters: `python -m ruff check --fix`
- [ ] Run tests to verify no functionality broken: `python -m pytest tests/ -v`
- [ ] Manual verification of core functionality
- [ ] Final project root inspection for remaining stray files
- [ ] Mark all cleanup tasks as completed

## Directory Structure After Cleanup

```
project/
├── archive/                          # Superseded documentation
│   ├── validationfix.md.ARCHIVED
│   └── refactorfix.md.ARCHIVED
├── dev_scripts/                      # Development utilities
│   ├── debug_manual_vs_stills.py
│   ├── check_reflection_columns.py
│   ├── compare_coordinates.py
│   └── debug_q_vector_suite/         # Themed collections
│       ├── debug_q_fix.py
│       ├── debug_q_validation.py
│       └── test_q_fix.py
├── tests/
│   └── integration/                  # Formal integration tests
│       └── test_sequence_adapter_integration.py
└── src/                              # Clean core codebase
    └── diffusepipe/
        └── extraction/
            └── extractor.py          # Dead code removed
```

## Success Criteria

- ✅ All superseded documentation archived with clear naming
- ✅ Root directory contains only active development files
- ✅ Development utilities organized in logical directories
- ✅ Valuable tests integrated into formal test suite
- ✅ Dead code removed without breaking functionality
- ✅ Code formatting and linting applied consistently
- ✅ All changes committed with clear version control messages

## Best Practices

1. **Preserve History**: Use `git mv` for relocations to maintain file history
2. **Clear Naming**: Add `.ARCHIVED` suffix to archived files for clarity
3. **Logical Grouping**: Create themed directories for related utilities
4. **Incremental Commits**: Commit each major phase separately with descriptive messages
5. **Verification**: Test functionality after each major change
6. **Documentation**: Update any documentation that references moved files

## Common File Types to Address

### Archive Candidates
- Resolved issue reports (`*fix.md`)
- Superseded plans (`plans/obsolete_*.md`)
- Incident reports for resolved problems
- Outdated documentation

### Root-Level Script Categories
- Debug utilities (`debug_*.py`)
- Test scripts not in formal test suite (`test_*.py`)
- Comparison/analysis scripts (`compare_*.py`, `check_*.py`)
- Ad-hoc investigation scripts

### Dead Code Patterns
- Functions with `NotImplementedError` and no usage
- Commented-out code blocks
- Unused imports
- Obsolete utility functions

## Integration with Development Workflow

This cleanup process should be run:
- Before major releases
- After resolving significant technical debt
- When onboarding new team members
- As part of regular maintenance cycles

The organized structure makes it easier for developers to:
- Find relevant debugging utilities
- Understand project history through archived documentation
- Maintain clean, focused core codebase
- Add new development tools in appropriate locations
</file>

<file path=".claude/commands/mem_to_docs.md">
**Agent Task: Integrate Actionable Insights from Working Memory Logs into Core Project Documentation**

**Overall Goal:** To identify durable, valuable insights, solutions to significant problems, critical discoveries, proven strategies, and API usage patterns documented in raw working memory logs (e.g., the developer's current `WORKING_MEMORY_LOG.md`) and integrate this knowledge *directly* into the most relevant, authoritative sections of the main project documentation (e.g., `plan.md`, `00_START_HERE.md`, `02_IMPLEMENTATION_RULES.md`, `06_DIALS_DEBUGGING_GUIDE.md`, specific component IDLs, or DIALS library documentation within `libdocs/dials/`). Updates should only be made when an insight from a log represents a stable, confirmed piece of knowledge that enhances the permanent documentation.

**I. Context Priming & Preparation:**

1.  **Identify Source "Working Memory Log" Documents:**
    *   Primary sources for fresh insights:
        *   The developer's most current `WORKING_MEMORY_LOG.md` (or its equivalent filename).
    *   *Optionally, review recent, detailed commit messages if they capture insights not yet in logs.*

2.  **Map Core Documentation Areas (Authoritative Documents):**
    *   Familiarize yourself with the purpose and content of the primary documentation files where insights should be integrated:
        *   `plan.md` & `plan_adaptation.md`: For insights affecting technical design, module responsibilities, DIALS workflow strategies, or planned features.
        *   `docs/00_START_HERE.md`: For fundamental project philosophies, high-level structural understanding, or onboarding clarifications.
        *   `docs/01_IDL_GUIDELINES.md`: For clarifications or best practices related to IDL definition arising from experience.
        *   `docs/02_IMPLEMENTATION_RULES.md`: For refined coding standards, testing strategies, data handling patterns, or external service interaction best practices.
        *   `docs/03_PROJECT_RULES.md`: For evolved project workflows, directory structures, or configuration management practices.
        *   `docs/06_DIALS_DEBUGGING_GUIDE.md`: For new DIALS-specific troubleshooting tips, common error patterns, or effective diagnostic steps.
        *   `docs/VISUAL_DIAGNOSTICS_GUIDE.md`: For updates on using or interpreting visual checks based on new findings.
        *   `libdocs/dials/*.md` (e.g., `DIALS_Python_API_Reference.md`, `dxtbx_models.md`): For specific DIALS/DXTBX API usage nuances, "gotchas," or clarifications discovered through use.
        *   Individual component IDL files (`src/diffusepipe/**/*_IDL.md`): For updates to a component's contract, behavioral notes, preconditions, or error conditions based on implementation experience.
        *   `README.md` (root): For high-level project changes or critical setup notes.

3.  **Define Criteria for Integrating an Insight from a Working Log:**
    An insight from a working log is suitable for integration into core documentation if it:
    *   Represents a **confirmed and stable** solution to a non-trivial technical challenge, bug, or performance issue. (Avoid documenting solutions that are still experimental).
    *   Is a **critical discovery** about system behavior, an external library (e.g., DIALS), or a development process that has been validated and is now considered standard practice.
    *   Is a **proven strategy, best practice, or common pitfall** identified and confirmed through recent experience.
    *   Explains the **rationale behind a recently solidified design choice** or implementation rule that needs to be formally captured.
    *   Documents a **specific API usage pattern (e.g., for DIALS) or "gotcha"** that has been verified and is important for other developers.
    *   Adds a **crucial, now-standard step or consideration** to an existing workflow or guideline.
    *   Is **generalizable** enough to be useful beyond the immediate context in which it was logged.
    *   **Crucially, it should not be transient project status information or highly specific details of a temporary investigation unless those details reveal a generalizable lesson.**

4.  **Integration Strategy:**
    *   **Direct Update:** Modify the relevant sentence, paragraph, or section in the target core document.
    *   **New Subsection/Bullet Point:** If the insight is substantial but fits an existing document, add a new, clearly titled H3/H4 subsection or an explanatory bullet point.
    *   **Cross-Reference:** If the insight is best detailed in one primary document (e.g., a specific fix strategy in `06_DIALS_DEBUGGING_GUIDE.md`), ensure other briefly related documents (e.g., `plan.md` if it impacts a DIALS adapter strategy) succinctly reference the detailed explanation.
    *   **IDL Enhancement:** For insights directly impacting a component's contract, behavior, or usage notes, update its `*_IDL.md` file (e.g., in the `Behavior:` block, add a note, or refine `Preconditions:`).

**II. Insight Extraction, Mapping, and Integration Checklist:**

For each relevant entry or section in the source working memory log(s) (Phase I.1):

| Item ID    | Task Description                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     | Status | Details/Notes/Path (Source: `[LogFile/Section]`, Target: `[CoreDoc/Section]`, Insight: `[Brief Summary]`) |
| :--------- | :--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | :----- | :---------------------------------------------------------------------------------------------------------- |
| **1.0**    | **Scan Logs for Actionable Insights:**                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 |        |                                                                                                             |
| 1.1        | Read through the working memory log(s), focusing on entries marked as "Key Technical Achievements," "Decisions Made," "Goal Achieved," detailed problem resolutions, or summaries of significant debugging efforts. Filter out purely status-update or transient content.                                                                                                                                                                                                                                                                              | `[ ]`  |                                                                                                             |
| 1.2        | For each potential insight that meets the criteria in (I.3):                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         |        |                                                                                                             |
| 1.2.a      |   Clearly extract the core, durable information: <br>   - Problem/Context (if relevant to the lesson) <br>   - The Solution / Discovery / Best Practice / Pitfall <br>   - Key Rationale / Impact <br>   - Essential details (e.g., specific DIALS parameters, critical code snippet pattern, error signature).                                                                                                                                                                                                                             | `[ ]`  |                                                                                                             |
| 1.2.b      |   **Verify Stability:** Is this insight now considered a stable part of the project's knowledge or practice? (e.g., a fix that's been merged and tested, a decision that's been adopted).                                                                                                                                                                                                                                                                                                                                                          | `[ ]`  | If "No", defer integration.                                                                                 |
| **2.0**    | **Identify Target Core Document(s) & Section(s):**                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     |        |                                                                                                             |
| 2.1        |   Based on the nature of the insight, determine the *most appropriate and authoritative primary document(s)* from list (I.2) where this knowledge should permanently reside. <br> *Example: A newly confirmed critical PHIL parameter for DIALS sequence processing (from `CLAUDE.md`'s "Key Technical Achievements") should be documented in `plan_adaptation.md` or `plan.md` under the `DIALSSequenceProcessAdapter` section, and also added to `06_DIALS_DEBUGGING_GUIDE.md`.* | `[ ]`  |                                                                                                             |
| 2.2        |   Pinpoint the specific section(s), paragraph(s), or list(s) within the target document(s) that this insight should logically modify or be added to.                                                                                                                                                                                                                                                                                                                                                                                                 | `[ ]`  |                                                                                                             |
| **3.0**    | **Integrate Insight into Target Core Document(s):**                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    |        |                                                                                                             |
| 3.1        |   **Draft the Update:** Rephrase the insight from the log into clear, concise, and professional language suitable for permanent documentation. Ensure it fits the style and tone of the target document.                                                                                                                                                                                                                                                                                                                                              | `[ ]`  |                                                                                                             |
| 3.2        |   **Perform the Update:**                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              |        |                                                                                                             |
| 3.2.a      |     If modifying existing text, ensure the new information integrates smoothly, improves clarity, and doesn't dilute the existing message unnecessarily.                                                                                                                                                                                                                                                                                                                                                                                             | `[ ]`  |                                                                                                             |
| 3.2.b      |     If adding a new subsection, bullet point, or note, ensure it's placed logically and titled clearly.                                                                                                                                                                                                                                                                                                                                                                                                                                                | `[ ]`  |                                                                                                             |
| 3.2.c      |     For DIALS/DXTBX API usage patterns or "gotchas" discovered (e.g., from a `WORKING_MEMORY_LOG.md` entry about fixing a test mock), update the relevant `.md` file in `libdocs/dials/` or `docs/06_DIALS_DEBUGGING_GUIDE.md`. Ensure the advice is actionable (e.g., "Prefer `MagicMock` over `Mock` for DIALS objects requiring `__getitem__` because..."). | `[ ]`  |                                                                                                             |
| 3.2.d      |     If an insight clarifies or modifies a component's contractual behavior, preconditions, error handling, or important usage notes, update its `*_IDL.md` file directly (e.g., in the `Behavior:` block, or by adding/refining an `@raises_error` annotation).                                                                                                                                                                                                 | `[ ]`  |                                                                                                             |
| 3.3        |   **Verify Consistency and Accuracy:** After integration, re-read the updated section and its surroundings. Does it make sense? Is it accurate? Does it introduce any contradictions or ambiguities with other parts of the *same document* or closely related core documents? Resolve any such issues.                                                                                                                                                                   | `[ ]`  |                                                                                                             |
| 3.4        |   **Consider Deprecating Log Entry:** If the insight from a working log is now fully and authoritatively captured in the core documentation, consider if the original log entry (or section in `CLAUDE.md`, etc.) can be marked as "Integrated into [Core Doc Path]" or simplified to avoid future confusion or becoming a source of outdated information. (This is a soft suggestion, main focus is updating core docs). | `[ ]`  |                                                                                                             |

**III. Final Review & Commit:**

| Item ID    | Task Description                                                                                                                                                                                                  | Status | Details/Notes/Path                                                                                                                                                                                                                            |
| :--------- | :---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | :----- | :-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| 4.1        | Review all modified core documentation files. Check for overall coherence, accuracy, and ensure that the integrated information enhances the documents without making them overly verbose or disorganized.          | `[ ]`  |                                                                                                                                                                                                                                               |
| 4.2        | Ensure formatting (Markdown) is correct and consistent in all updated files.                                                                                                                                        | `[ ]`  |                                                                                                                                                                                                                                               |
| 4.3        | Update your *own* `WORKING_MEMORY_LOG.md` for *this specific task* of updating documentation.                                                                                                                       | `[ ]`  |                                                                                                                                                                                                                                               |
| 4.4        | Commit the changes to the core documentation files with clear, specific commit messages detailing which documents were updated and the nature of the insights integrated. Group related changes into logical commits. | `[ ]`  | Example: `DOCS: Integrate DIALS sequence PHIL params from logs into plan_adaptation.md` or `DOCS: Clarify MagicMock usage for DIALS objects in 02_IMPLEMENTATION_RULES.md based on recent test fixes.` |

---

**Agent Execution Notes:**

*   **Selectivity is Key:** Not every entry in a working log is a "lesson" suitable for core documentation. Focus on stable, verified, and broadly applicable knowledge.
*   **Atomicity of Updates:** Try to integrate one distinct insight at a time into its most relevant document section.
*   **Maintain Authoritative Sources:** The goal is to strengthen the existing authoritative documents, making them the single source of truth for their respective topics.
*   **Iterative Process:** This might be an ongoing task. As new significant insights emerge in working logs and become stable, they can be migrated.
</file>

<file path=".claude/commands/sync_docs.md">
# Initial Context

  First, read these core documents in order:
  1. docs/00_START_HERE.md - Developer orientation and project philosophy
  2. plan.md - Technical implementation plan for diffuse scattering processing
  3. CLAUDE.md - Project-specific guidance

  # Documentation Hierarchy

  Apply this precedence when resolving conflicts:
  1. plan.md (highest authority) - The technical specification
  2. IDL files (*_IDL.md) - Component contracts/interfaces
  3. Other .md files (e.g. start_here.md, project_rules.md - these must also match the project's dir structure)
  4. Implementation files (.py) - Actual code

  When inconsistencies are found, generally update lower-precedence documents to match higher ones (e.g., update IDL to match plan.md, not vice versa).

  # Specific Review Tasks

  1. Cross-Reference Validation

  - Verify that all modules/components mentioned in plan.md have corresponding IDL files
  - Check that IDL interfaces match the specifications in plan.md for:
    - Input/output data structures
    - Module responsibilities and boundaries
    - Processing workflows and dependencies
    - Error handling strategies

  2. Technical Consistency

  - Ensure DIALS integration approach is consistent across all docs (stills vs sequences processing)
  - Verify PHIL parameter specifications match between plan.md and CLAUDE.md
  - Check that correction factor formulas and conventions (especially multiplicative vs divisive) are consistent
  - Validate that the testing strategies align between 00_START_HERE.md and plan.md

  3. Implementation Alignment

  - For each IDL file, verify it accurately reflects any existing implementation
  - Check that type definitions in types_IDL.md match usage in other IDLs and plan.md
  - Ensure adapter patterns described in docs match actual adapter implementations

  4. Update Requirements

  For each inconsistency found:
  1. Document the discrepancy (file, line number, nature of conflict)
  2. Determine the correct resolution based on precedence rules
  3. Make the necessary update
  4. Record the change in a structured format

  # Output Format

  Create a summary report with:
  - List of all inconsistencies found
  - Changes made to resolve them
  - Any unresolved issues requiring human decision
  - Recommendations for documentation improvements

  # Areas of Special Focus

  - Consistency with api documentation
  - DIALS workflow (stills vs sequences)
  - Correction factor conventions
  - Memory management strategies
  - Testing approaches (integration vs unit)
</file>

<file path=".claude/commands/update_docs.md">
1. read start_here.md, claude.md and plan.md. read project_rules.md and all the IDL docs. 
2. review all other .md documentation files in this project and establish a precedence order between all of them (including those named in step 1.). in general plan.md is the highest source of truth, followed by idl docs, but for some of the .md files you'll have to use your judgement
3. write a shortlist of the .md doc files that are 'important' enough that they will be referred to repeatedly and need to be kept up to date
4. review the shortlist .md files for self-consistency. which sections are deprecated and can be removed? which are out of date and need to be updated? which have inconsistencies that need to be resolved?
5. based on the findings from 4., think of a documentation update approach
6. generate a todo checklist based on the approach you formulated
7. implement the checklist
</file>

<file path=".claude/commands/update_IDLs.md">
**Agent Task: Synchronize and Update IDL Specifications**

**Overall Goal:** To ensure all Interface Definition Language (IDL) files (`*_IDL.md`) accurately reflect the current Python implementation and are consistent with higher-precedence project documentation (primarily `plan.md` and `plan_adaptation.md`). This involves creating missing IDLs, updating existing IDLs to match correct implementations, and aligning IDLs with the overarching project plan.

**I. Context Priming & Preparation:**

1.  **Review Core Documentation:**
    *   `plan.md`: The primary technical specification for pipeline modules and their behavior.
    *   `plan_adaptation.md`: Critical modifications to `plan.md`, especially regarding DIALS processing modes.
    *   `docs/00_START_HERE.md`: For project philosophy and IDL context.
    *   `docs/01_IDL_GUIDELINES.md`: For IDL syntax, structure, and conventions.
    *   `docs/02_IMPLEMENTATION_RULES.md`: For how IDLs translate to code.
    *   `docs/ARCHITECTURE/types.md` and `src/diffusepipe/types/types_IDL.md`: For shared type definitions.
    *   `CLAUDE.md`: For project-specific DIALS integration details and conventions.

2.  **Documentation Precedence Rules (Strict Adherence Required):**
    1.  `plan.md` and `plan_adaptation.md` (Highest authority for functional requirements and module responsibilities).
    2.  IDL files (`*_IDL.md`) (Define the contract for components).
    3.  Other `.md` documentation files (e.g., `00_START_HERE.md`, `CLAUDE.md`, architectural docs – must also align with project structure).
    4.  Python implementation files (`.py`) (The actual code, which should implement an IDL that aligns with the plan).

3.  **Scope:** All `.py` files within `src/diffusepipe/` and their corresponding `*_IDL.md` files. Focus on adapters, crystallography components, diagnostics, extraction, and masking modules first.

**II. IDL Synchronization Checklist:**

For each Python module/class within `src/diffusepipe/` that represents a significant component or has a public interface:

| Item ID    | Task Description                                                                                                                                                                                                                               | Status | Details/Notes/Path (e.g., file path, specific inconsistency) |
| :--------- | :--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | :----- | :----------------------------------------------------------- |
| **1.0**    | **Identify Component & Locate/Create IDL:**                                                                                                                                                                                                    |        |                                                              |
| 1.1        | For `[src/diffusepipe/path/to/module.py]`: Does a corresponding `[src/diffusepipe/path/to/module_IDL.md]` exist?                                                                                                                                 | `[ ]`  |                                                              |
| 1.2        | **If NO IDL exists (for an implemented component that should have one):**                                                                                                                                                                      |        |                                                              |
| 1.2.a      |   Determine if an IDL is appropriate based on `01_IDL_GUIDELINES.md` (generally, yes for components with defined interfaces and responsibilities).                                                                                             | `[ ]`  |                                                              |
| 1.2.b      |   Consult `plan.md`/`plan_adaptation.md` to understand the intended role and contract of this component.                                                                                                                                         | `[ ]`  |                                                              |
| 1.2.c      |   Generate a new IDL specification for the component based on its current implementation and the plan. Follow all rules in `01_IDL_GUIDELINES.md` (Code-to-IDL reduction, dependency declaration, behavioral specs, error conditions, etc.). | `[ ]`  | Path: `[Path to new _IDL.md]`                                |
| **2.0**    | **Consistency Check: IDL vs. Implementation (if IDL exists):**                                                                                                                                                                                 |        | Path to IDL: `[Path to existing _IDL.md]`                      |
| 2.1        |   **Interface/Class Name:** Does the IDL interface/module name match the Python class/module name?                                                                                                                                               | `[ ]`  |                                                              |
| 2.2        |   **Method/Function Signatures:** For each public method/function in the Python code:                                                                                                                                                            |        |                                                              |
| 2.2.a      |     Is it defined in the IDL? If not, should it be (is it part of the public contract)?                                                                                                                                                          | `[ ]`  |                                                              |
| 2.2.b      |     Do parameter names, order, and types (Python type hints vs. IDL types) match?                                                                                                                                                              | `[ ]`  |                                                              |
| 2.2.c      |     Does the return type match?                                                                                                                                                                                                                  | `[ ]`  |                                                              |
| 2.3        |   **Data Structures:** If methods use complex dictionary parameters/returns:                                                                                                                                                                     |        |                                                              |
| 2.3.a      |     Does the IDL document the `Expected Data Format` or reference a `struct`?                                                                                                                                                                  | `[ ]`  |                                                              |
| 2.3.b      |     Does the Python implementation (e.g., Pydantic models in `types_IDL.py`, or internal data handling) align with this documented structure?                                                                                                   | `[ ]`  |                                                              |
| 2.4        |   **Behavioral Specification:**                                                                                                                                                                                                                |        |                                                              |
| 2.4.a      |     Does the implemented behavior of the Python code align with the `Behavior:`, `Preconditions:`, and `Postconditions:` described in the IDL?                                                                                                  | `[ ]`  |                                                              |
| 2.5        |   **Error Handling:** Does the Python code raise/handle errors consistent with `@raises_error` conditions or the error handling philosophy described in the IDL and `02_IMPLEMENTATION_RULES.md`?                                                  | `[ ]`  |                                                              |
| 2.6        |   **Dependencies:**                                                                                                                                                                                                                            |        |                                                              |
| 2.6.a      |     Are dependencies used in the Python code correctly declared in the IDL (`@depends_on`, `@depends_on_resource`)?                                                                                                                            | `[ ]`  |                                                              |
| 2.6.b      |     Are there dependencies in the IDL not reflected in the code (or vice-versa)?                                                                                                                                                                 | `[ ]`  |                                                              |
| **3.0**    | **Update IDL (if inconsistencies found in 2.0 AND implementation is correct w.r.t. plan):**                                                                                                                                                    |        |                                                              |
| 3.1        |   **CRITICAL DECISION POINT:** Before updating the IDL to match the implementation:                                                                                                                                                              |        |                                                              |
| 3.1.a      |     Verify if the *current Python implementation* aligns with the specifications in `plan.md` (and `plan_adaptation.md`).                                                                                                                       | `[ ]`  |                                                              |
| 3.1.b      |     **If YES (implementation IS ALIGNED with plan.md):** Proceed to update the IDL (sections 3.2 - 3.6) to match the correct implementation.                                                                                                  | `[ ]`  |                                                              |
| 3.1.c      |     **If NO (implementation IS NOT ALIGNED with plan.md):** **DO NOT update the IDL to match the incorrect implementation.** Instead, flag this as a "Code Divergence from Plan" issue. The *code* needs to be fixed, not the IDL (assuming the IDL *was* aligned with the plan). If the IDL was also not aligned with the plan, that's a separate issue for step 4. | `[ ]`  | Note: `[File path of divergent code]`, `[Specific plan section violated]` |
| 3.2        |   Update method signatures, parameter names/types, return types in IDL to match Python.                                                                                                                                                          | `[ ]`  |                                                              |
| 3.3        |   Update/add `Expected Data Format` or `struct` references in IDL for complex data.                                                                                                                                                             | `[ ]`  |                                                              |
| 3.4        |   Refine `Behavior:`, `Preconditions:`, `Postconditions:` in IDL to accurately reflect the (correct) implemented logic.                                                                                                                       | `[ ]`  |                                                              |
| 3.5        |   Update `@raises_error` annotations and error descriptions in IDL.                                                                                                                                                                            | `[ ]`  |                                                              |
| 3.6        |   Update dependency declarations (`@depends_on`, `@depends_on_resource`) in IDL.                                                                                                                                                               | `[ ]`  |                                                              |
| **4.0**    | **Consistency Check: IDL vs. Higher-Precedence Docs (plan.md, plan_adaptation.md):**                                                                                                                                                           |        | Path to IDL: `[Path to existing/updated _IDL.md]`              |
| 4.1        |   Does the IDL's defined purpose, behavior, and component responsibilities align with what is specified for that component/module in `plan.md` (and `plan_adaptation.md`)?                                                                       | `[ ]`  | Example: Does `DataExtractor_IDL.md` match Module 2.S.1 & 2.S.2 in `plan.md`? |
| 4.2        |   Are input/output data structures (conceptually, e.g., "Reflections\_dials\_i") consistent with those named in `plan.md`?                                                                                                                    | `[ ]`  |                                                              |
| 4.3        |   Are critical DIALS parameters or specific algorithmic choices mentioned in `plan.md` (e.g., for adapters) reflected or compatible with the IDL's behavioral description?                                                                       | `[ ]`  | e.g., critical PHILs for sequence processing if IDL is for an adapter. |
| 4.4        |   If inconsistencies are found here, **update the IDL to align with `plan.md`/`plan_adaptation.md`**. This may subsequently require code changes (which is a separate task).                                                                    | `[ ]`  |                                                              |
| **5.0**    | **Final IDL Review & Cleanup:**                                                                                                                                                                                                                |        |                                                              |
| 5.1        |   Ensure all IDL conventions from `01_IDL_GUIDELINES.md` are met (formatting, comments, annotations).                                                                                                                                             | `[ ]`  |                                                              |
| 5.2        |   Check for clarity, completeness, and lack of ambiguity.                                                                                                                                                                                        | `[ ]`  |                                                              |

**III. Output & Reporting:**

1.  **Commit Changes:** Commit all IDL file changes with clear messages (e.g., "SYNC: Update DataExtractor\_IDL.md to match implementation and plan").
2.  **Summary Report:** Provide a summary document listing:
    *   Newly created IDL files.
    *   IDL files that were updated and a brief note on the nature of changes (e.g., "signature update", "behavior clarification", "alignment with plan.md").
    *   Any instances where Python code was found to be divergent from `plan.md` (from step 3.1.c) – these require separate code fixes.
    *   Any sections of `plan.md` that seem to be missing corresponding component implementations or IDLs.

---

**Agent Execution Notes:**

*   This is a meticulous, file-by-file task. Proceed systematically through the `src/diffusepipe/` directory.
*   When comparing IDL to implementation, focus on the *public contract* and *essential behavior*. Internal helper methods or private attributes in Python do not typically need to be reflected in the IDL unless they impact the observable behavior relevant to the contract.
*   If `plan.md` is ambiguous or seems to conflict with a sensible implementation, flag this for human review. The plan is high-precedence, but clarifications might be needed.
*   Pay special attention to adapters (`src/diffusepipe/adapters/`) as their IDLs must accurately reflect how they interact with external DIALS tools/APIs, and this interaction must align with the strategies in `plan.md` and `plan_adaptation.md`.
</file>

<file path="checklists/phase2.md">
**Agent Task: Implement Phase 2 of `plan.md` - Per-Still Diffuse Intensity Extraction & Correction**

**Overall Goal for Agent (Phase 2):** To complete the `DataExtractor` component by implementing the pixel-based correction factor application (Module 2.S.2), ensuring efficient vectorized processing, and aligning its inputs/outputs with the overall pipeline design.

**Checklist Usage Instructions for Agent:**

1.  **Copy this entire checklist into your working memory or a dedicated scratchpad area.**
2.  **Context Priming:** Before starting a new major section (e.g., Module), carefully read all "Context Priming" items.
3.  **Sequential Execution:** Address checklist items in the order presented, unless an item explicitly states it can be done in parallel.
4.  **Update State:** As you work on an item, change its state field:
    *   `[ ] Open` -> `[P] In Progress` when you start.
    *   `[P] In Progress` -> `[D] Done` when completed successfully.
    *   `[P] In Progress` -> `[B] Blocked` if you encounter a blocker. Add a note explaining the blocker in the "Details/Notes/Path" column.
5.  **Record Details (in the "Details/Notes/Path" column):**
    *   If a step requires creating or modifying a file, add the **full relative path** to that file (e.g., `src/diffusepipe/extraction/data_extractor.py`).
    *   If a significant design decision or clarification is made, note it briefly.
    *   If a task is broken down, add indented sub-tasks.
6.  **Iterative Review:** Periodically re-read completed sections and notes.
7.  **Save State:** Ensure this checklist with current states and notes is saved if pausing.

---

**Phase 2: `DataExtractor` Implementation - Modules 2.S.1 (Refinement) & 2.S.2 (Core Implementation)**

| Item ID     | Task Description                                                                                                                              | State | Details/Notes/Path                                                                                                                                                                                                                                                             |
| :---------- | :-------------------------------------------------------------------------------------------------------------------------------------------- | :---- | :----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| **2.A**     | **Context Priming (Phase 2)**                                                                                                                 | `[ ]` |                                                                                                                                                                                                                                                                                |
| 2.A.1       | Re-read `plan.md` Module 2.S.1: Pixel-Based Diffuse Data Extraction & Q-Calculation (focus on inputs/outputs & vectorized processing note).      | `[ ]` |                                                                                                                                                                                                                                                                                |
| 2.A.2       | Re-read `plan.md` Module 2.S.2: Pixel-Based Correction Factor Application (this is the primary focus).                                          | `[ ]` |                                                                                                                                                                                                                                                                                |
| 2.A.3       | Re-read `plan.md` "Implementation Note on Efficient Pixel Data Handling".                                                                     | `[ ]` | Focus: Vectorized operations, per-still array outputs.                                                                                                                                                                                                                         |
| 2.A.4       | Review `src/diffusepipe/extraction/data_extractor_IDL.md`.                                                                                    | `[ ]` | Compare with `plan.md` for consistency.                                                                                                                                                                                                                                        |
| 2.A.5       | Review `src/diffusepipe/types/types_IDL.md` (for `ExtractionConfig`).                                                                         | `[ ]` |                                                                                                                                                                                                                                                                                |
| 2.A.6       | Review `docs/02_IMPLEMENTATION_RULES.md` (esp. NumPy/vectorization, DIALS API usage, error handling).                                        | `[ ]` |                                                                                                                                                                                                                                                                                |
| 2.A.7       | Review `libdocs/dials/DIALS_Python_API_Reference.md` Section C.4 (Geometric Corrections) for DIALS `Corrections` API.                           | `[ ]` | Focus: `Corrections` object instantiation, `lp()` and `qe()` methods, and return value conventions (divisor vs. multiplier).                                                                                                                                             |
| 2.A.8       | Confirm understanding: All correction factors are to be combined as *multipliers*.                                                              | `[ ]` | Divisors from APIs must be inverted.                                                                                                                                                                                                                                           |
| 2.A.9       | Understand Goal: Implement the full correction pipeline within `DataExtractor.py`, optimize for vectorized operations, and ensure robust testing. | `[ ]` |                                                                                                                                                                                                                                                                                |
|             |                                                                                                                                               |       |                                                                                                                                                                                                                                                                                |
| **2.B**     | **Refine `DataExtractor` Input Handling & Structure (Alignment with `plan.md`)**                                                                | `[ ]` | Path: `src/diffusepipe/extraction/data_extractor.py`                                                                                                                                                                                                                             |
| **2.B.idl** | **Review `DataExtractor_IDL.md`: `extract_from_still` method.**                                                                               | `[ ]` | **Purpose:** Ensure IDL reflects planned input changes for `Mask_total_2D_i`. <br>Input: `inputs: ComponentInputFiles` (check if it needs a field for in-memory mask, or if mask is passed differently), `config: ExtractionConfig`, `output_npz_path: str`. <br>Output: `OperationOutcome`. |
| 2.B.1       | Modify `DataExtractor.extract_from_still` to accept `Mask_total_2D_i` (e.g., as a tuple of `flex.bool` arrays) as an in-memory parameter.        | `[ ]` | Remove loading of `inputs.bragg_mask_path`. The orchestrator will now compute and pass `Mask_total_2D_i`.                                                                                                                                                                     |
| 2.B.2       | Update `DataExtractor._validate_inputs` to reflect the change in mask input if necessary.                                                       | `[ ]` |                                                                                                                                                                                                                                                                                |
| 2.B.3       | Ensure `DataExtractor._process_pixels` uses the passed `Mask_total_2D_i` for selecting pixels, not just a Bragg mask.                         | `[ ]` | Logic: `if Mask_total_2D_i[panel_idx](py, px): ...`                                                                                                                                                                                                                                |
|             |                                                                                                                                               |       |                                                                                                                                                                                                                                                                                |
| **2.C**     | **Implement Pixel Correction Factors (Module 2.S.2 within `DataExtractor._process_pixels`)**                                                      | `[ ]` | Path: `src/diffusepipe/extraction/data_extractor.py`                                                                                                                                                                                                                             |
| 2.C.1       | **Lorentz-Polarization (LP) Correction:**                                                                                                     | `[ ]` |                                                                                                                                                                                                                                                                                |
| 2.C.1.a     |    Instantiate DIALS `Corrections` object using `Experiment_dials_i`.                                                                           | `[ ]` | Potentially wrap this in a small adapter if preferred, or use directly.                                                                                                                                                                                                        |
| 2.C.1.b     |    Call `corrections_obj.lp(s1_flex_array)` for accepted diffuse pixel `s1` vectors.                                                            | `[ ]` | This returns *divisors*.                                                                                                                                                                                                                                                         |
| 2.C.1.c     |    Invert to get `LP_mult_array = 1.0 / LP_divisors_array`.                                                                                     | `[ ]` |                                                                                                                                                                                                                                                                                |
| 2.C.2       | **Detector Quantum Efficiency (QE) Correction:**                                                                                              | `[ ]` |                                                                                                                                                                                                                                                                                |
| 2.C.2.a     |    Use the same `Corrections` object.                                                                                                           | `[ ]` |                                                                                                                                                                                                                                                                                |
| 2.C.2.b     |    Call `corrections_obj.qe(s1_flex_array, panel_indices_flex_array)`.                                                                          | `[ ]` | This returns *multipliers*. `QE_mult_array = QE_multipliers_array`.                                                                                                                                                                                                            |
| 2.C.3       | **Solid Angle (SA) Correction (Custom):**                                                                                                     | `[ ]` |                                                                                                                                                                                                                                                                                |
| 2.C.3.a     |    Implement calculation: `Ω = (A_pixel × cos θ_normal_to_s1) / r²`.                                                                            | `[ ]` | Use `panel.get_pixel_lab_coord`, `panel.get_pixel_size`, `panel.get_fast_axis/slow_axis`.                                                                                                                                                                                        |
| 2.C.3.b     |    Invert to get `SA_mult_array = 1.0 / SolidAngle_divisor_array`.                                                                              | `[ ]` |                                                                                                                                                                                                                                                                                |
| 2.C.4       | **Air Attenuation (AA) Correction (Custom):**                                                                                                 | `[D]` | **COMPLETED** - Enhanced with NIST-based scientific accuracy                                                                                                                                                                                                                                                         |
| 2.C.4.a     |    Determine `path_length` (sample to pixel).                                                                                                   | `[D]` | Implemented in `_calculate_air_attenuation_correction`                                                                                                                                                                                                                                                                |
| 2.C.4.b     |    Determine X-ray wavelength/energy from `Experiment_dials_i.beam`.                                                                            | `[D]` | Energy conversion: `energy_ev = 12398.4 / wavelength`                                                                                                                                                                                                                                                                |
| 2.C.4.c     |    Implement `μ_air(E)` calculation (sum of mass attenuation coeffs for N, O, Ar × densities).                                                  | `[D]` | **Decision made:** NIST tabulated data implemented in `_get_mass_attenuation_coefficient`. Standard air composition with configurable T/P. Path: `src/diffusepipe/extraction/data_extractor.py` lines 834-891 |
| 2.C.4.d     |    Apply Beer-Lambert law: `Attenuation_divisor_array = exp(-μ_air_array * path_length_array)`.                                                 | `[D]` | Implemented: `attenuation = np.exp(-mu_air * path_length)`                                                                                                                                                                                                                                                           |
| 2.C.4.e     |    Invert to get `Air_mult_array = 1.0 / Attenuation_divisor_array`.                                                                            | `[D]` | Implemented: `air_mult = 1.0 / attenuation`                                                                                                                                                                                                                                                                          |
| 2.C.5       | **Total Correction Factor Application:**                                                                                                      | `[ ]` |                                                                                                                                                                                                                                                                                |
| 2.C.5.a     |    Combine all multiplicative factors: `TotalCorrection_mult_array = LP_mult_array * QE_mult_array * SA_mult_array * Air_mult_array`.           | `[ ]` |                                                                                                                                                                                                                                                                                |
| 2.C.5.b     |    Apply to intensity: `I_corrected_val_array = I_processed_per_sec_array * TotalCorrection_mult_array`.                                        | `[ ]` | (After background subtraction and exposure time normalization from existing code).                                                                                                                                                                                             |
|             |                                                                                                                                               |       |                                                                                                                                                                                                                                                                                |
| **2.D**     | **Implement Error Propagation for Corrections**                                                                                               | `[ ]` | Path: `src/diffusepipe/extraction/data_extractor.py` (within `_process_pixels`)                                                                                                                                                                                                |
| 2.D.1       |    `Var_photon_initial_array = I_raw_array / gain_array`.                                                                                       | `[ ]` |                                                                                                                                                                                                                                                                                |
| 2.D.2       |    Retrieve `Var_bkg_array` (from background map or 0 if constant background).                                                                  | `[ ]` |                                                                                                                                                                                                                                                                                |
| 2.D.3       |    `Var_processed_per_sec_array = (Var_photon_initial_array + Var_bkg_array) / (t_exp_array)^2`.                                                | `[ ]` |                                                                                                                                                                                                                                                                                |
| 2.D.4       |    `Var_corrected_val_array = Var_processed_per_sec_array * (TotalCorrection_mult_array)^2`.                                                    | `[ ]` |                                                                                                                                                                                                                                                                                |
| 2.D.5       |    Add comment documenting assumption: `TotalCorrection_mult_array` has negligible uncertainty.                                                 | `[ ]` |                                                                                                                                                                                                                                                                                |
| 2.D.6       |    `Sigma_corrected_val_array = sqrt(Var_corrected_val_array)`.                                                                                 | `[ ]` |                                                                                                                                                                                                                                                                                |
|             |                                                                                                                                               |       |                                                                                                                                                                                                                                                                                |
| **2.E**     | **Vectorization and Performance Optimization**                                                                                                | `[D]` | **COMPLETED** - 2.4x performance improvement achieved. Path: `src/diffusepipe/extraction/data_extractor.py`                                                                                                                                                                    |
| 2.E.1       | Refactor `_process_pixels` to use vectorized NumPy/flex array operations instead of per-pixel Python loops for all calculations.              | `[D]` | Implemented `_process_pixels_vectorized` with vectorized: coordinate extraction, q-vector calc, corrections, filtering. Original kept as `_process_pixels_iterative` for comparison. |
| 2.E.2       | Ensure outputs (`q_vectors`, `intensities`, `sigmas`) are stored as contiguous NumPy/flex arrays as per `plan.md` note.                       | `[D]` | All outputs are NumPy arrays. Profiling shows 2.4x speedup (4.0s → 1.7s for similar workload).                                                                                                                                                                               |
|             |                                                                                                                                               |       |                                                                                                                                                                                                                                                                                |
| **2.F**     | **Configuration Handling**                                                                                                                    | `[ ]` | Path: `src/diffusepipe/extraction/data_extractor.py`, `src/diffusepipe/types/types_IDL.py`                                                                                                                                                                                     |
| 2.F.1       | Ensure `DataExtractor` uses `lp_correction_enabled` from `ExtractionConfig`.                                                                  | `[ ]` |                                                                                                                                                                                                                                                                                |
| 2.F.2       | Add any new necessary parameters to `ExtractionConfig` (e.g., for air attenuation like air_temp_K, air_pressure_atm if not assuming STP).    | `[ ]` | And update `types_IDL.md` and `types_IDL.py` accordingly.                                                                                                                                                                                                                            |
|             |                                                                                                                                               |       |                                                                                                                                                                                                                                                                                |
| **2.G**     | **Testing (Phase 2)**                                                                                                                         | `[ ]` | Path: `tests/extraction/test_data_extractor.py` (new or extend existing)                                                                                                                                                                                                           |
| 2.G.T.1     | Unit Test: LP correction retrieval and application.                                                                                           | `[ ]` | Mock DIALS `Corrections` API or use with synthetic `Experiment`.                                                                                                                                                                                                                   |
| 2.G.T.2     | Unit Test: QE correction retrieval and application.                                                                                           | `[ ]` | Mock DIALS `Corrections` API or use with synthetic `Experiment`.                                                                                                                                                                                                                   |
| 2.G.T.3     | Unit Test: Solid Angle (SA) custom calculation logic for various pixel positions.                                                             | `[ ]` |                                                                                                                                                                                                                                                                                |
| 2.G.T.4     | Unit Test: Air Attenuation (AA) custom calculation logic.                                                                                     | `[ ]` |                                                                                                                                                                                                                                                                                |
| 2.G.T.4.a   |    Sub-Test: `μ_air(E)` calculation with different energies/sources.                                                                            | `[ ]` |                                                                                                                                                                                                                                                                                |
| 2.G.T.5     | Unit Test: Combination of all correction factors (`TotalCorrection_mult`).                                                                    | `[ ]` |                                                                                                                                                                                                                                                                                |
| 2.G.T.6     | Unit Test: Error propagation formulas for each correction step and overall.                                                                   | `[ ]` |                                                                                                                                                                                                                                                                                |
| 2.G.T.7     | Integration Test: `DataExtractor.extract_from_still` with mocked DIALS outputs and a small synthetic image, verifying corrected intensities.    | `[ ]` | Use known inputs, apply corrections manually/analytically, compare with component output.                                                                                                                                                                                        |
| 2.G.T.8     | Test: `DataExtractor` correctly uses `Mask_total_2D_i` (when passed).                                                                         | `[ ]` |                                                                                                                                                                                                                                                                                |
| 2.G.T.9     | Regression Test: `apply_corrections()` helper as per `plan.md` (Module 0.7, 2.S.2).                                                           | `[ ]` | This helper should reside in `src/diffusepipe/corrections.py` (create if not exists). Compares synthetic pixel at 45° with analytic values.                                                                                                                                    |
|             |                                                                                                                                               |       |                                                                                                                                                                                                                                                                                |
| **2.H**     | **IDL and Documentation Review**                                                                                                              | `[ ]` |                                                                                                                                                                                                                                                                                |
| 2.H.1       | Review and update `src/diffusepipe/extraction/data_extractor_IDL.md` to reflect final implementation details, especially input mask type.      | `[ ]` |                                                                                                                                                                                                                                                                                |
| 2.H.2       | Document assumptions made (e.g., `TotalCorrection_mult` uncertainty, source of `μ_air`).                                                        | `[ ]` | In code comments and/or relevant `.md` files.                                                                                                                                                                                                                                  |
|             |                                                                                                                                               |       |                                                                                                                                                                                                                                                                                |
| **2.Z**     | **Phase 2 Review & Next Steps**                                                                                                               | `[ ]` |                                                                                                                                                                                                                                                                                |
| 2.Z.1       | Self-Review: All Phase 2 items addressed? All TODOs in `DataExtractor.py` related to corrections resolved?                                    | `[ ]` |                                                                                                                                                                                                                                                                                |
| 2.Z.2       | Code formatting (`black .`) and linting (`ruff check --fix .`) pass.                                                                            | `[ ]` |                                                                                                                                                                                                                                                                                |
| 2.Z.3       | All tests (Phase 0, 1, 2) pass.                                                                                                                 | `[ ]` |                                                                                                                                                                                                                                                                                |
| 2.Z.4       | Context Refresh: Re-read `plan.md` sections for Phase 3 (Voxelization, Relative Scaling).                                                     | `[ ]` |                                                                                                                                                                                                                                                                                |
| 2.Z.5       | Decision: Proceed to Phase 3 Checklist.                                                                                                       | `[ ]` |                                                                                                                                                                                                                                                                                |
</file>

<file path="checklists/phase3.md">
Agent Task: Implement Phase 3 of plan.md - Voxelization, Relative Scaling, and Merging
  Overall Goal for Agent (Phase 3): To take per-still, corrected diffuse pixel data from Phase 2, map it to a common 3D reciprocal space grid, apply relative scaling between stills to account for
  experimental variations, and merge the data into a final, relatively-scaled 3D diffuse scattering map.
  Checklist Usage Instructions for Agent:
  Copy this entire checklist into your working memory or a dedicated scratchpad area.
  Context Priming: Before starting a new major section (e.g., Module), carefully read all "Context Priming" items for that section.
  Sequential Execution: Address checklist items in the order presented.
  Update State: As you work on an item, change its state field:
  [ ] Open -> [P] In Progress when you start.
  [P] In Progress -> [D] Done when completed successfully.
  [P] In Progress -> [B] Blocked if you encounter a blocker. Add a note explaining the blocker in the "Details/Notes/Path" column.
  Record Details (in the "Details/Notes/Path" column):
  If a step requires creating or modifying a file, add the full relative path to that file (e.g., src/diffusepipe/voxelization/global_voxel_grid.py).
  If a significant design decision or clarification is made, note it briefly.
  For "IDL Definition/Review" tasks, summarize key interface aspects (Inputs, Outputs, Behavior, Errors).
  Include specific API function calls or class names from libdocs/dials/ where relevant.
  Iterative Review: Periodically re-read completed sections and notes.
  Save State: Ensure this checklist with current states and notes is saved if pausing.
  Phase 3: Voxelization, Relative Scaling, and Merging of Diffuse Data
  Item ID    Task Description    State    Details/Notes/Path (API Refs from libdocs/dials/)
  3.A    Context Priming (Phase 3)    [ ]
  3.A.1    Re-read plan.md Module 3.S.1: Global Voxel Grid Definition.    [ ]
  3.A.2    Re-read plan.md Module 3.S.2: Binning Corrected Diffuse Pixels.    [ ]    Focus on HDF5 backend, Welford's, ASU mapping.
  3.A.3    Re-read plan.md Module 3.S.3: Relative Scaling of Binned Observations.    [ ]    Focus on custom DiffuseScalingModel, v1 parameter-guarded model, DIALS components, partiality strategy.
  3.A.4    Re-read plan.md Module 3.S.4: Merging Relatively Scaled Data.    [ ]    Focus on error propagation and weighted merge.
  3.A.5    Review plan.md "Implementation Note on Efficient Pixel Data Handling" (updated parts for Phase 3).    [ ]
  3.A.6    Review src/diffusepipe/types/types_IDL.md and .py (esp. RelativeScalingConfig, StillsPipelineConfig).    [ ]
  3.A.7    Review relevant API docs: libdocs/dials/dxtbx_models.md (B.3 Crystal Model), crystallographic_calculations.md (C.2, C.6, C.7), dials_scaling.md (D.0), flex_arrays.md (D.3, D.4).    [ ]
  3.A.8    Understand Goal: Implement voxel grid creation, pixel binning (HDF5), custom relative scaling model, and merging.    [ ]
  3.B    Module 3.S.1: Global Voxel Grid Definition    [ ]
  3.B.idl    Define/Review IDL for GlobalVoxelGrid    [ ]    Path: src/diffusepipe/voxelization/global_voxel_grid_IDL.md (New) <br>Purpose: Encapsulate the 3D grid definition. <br>Inputs (constructor):
  List of Experiment_dials_i (for crystal models), List of CorrectedDiffusePixelData_i (for q-ranges), grid config (d_min_target, d_max_target, ndiv_h,k,l). <br>Attributes: Crystal_avg_ref, A_avg_ref,
  HKL range, subdivisions. <br>Methods: hkl_to_voxel_idx, voxel_idx_to_hkl_center, get_q_vector_for_voxel_center.
  3.B.1    Implement GlobalVoxelGrid class structure.    [ ]    Path: src/diffusepipe/voxelization/global_voxel_grid.py (New)
  3.B.2    Implement robust averaging of Crystal_i.get_unit_cell():    [ ]    API Ref: crystallographic_calculations.md (C.6 average_unit_cells using cctbx.uctbx.unit_cell). dxtbx_models.md (B.3
  crystal.get_unit_cell()).
  3.B.3    Implement robust averaging of Crystal_i.get_U() matrices to get U_avg_ref:    [ ]    Note: libdocs/dials/ doesn't specify quaternion averaging. May need direct scitbx or custom robust rotation
   matrix averaging. C.6 in crystallographic_calculations.md is for unit cells, not U matrices directly.
  3.B.4    Calculate B_avg_ref from averaged unit cell:    [ ]    API Ref: crystallographic_calculations.md (C.6 avg_B_matrix = matrix.sqr(avg_unit_cell.fractionalization_matrix()).transpose()).
  3.B.5    Calculate A_avg_ref = U_avg_ref * B_avg_ref.    [ ]
  3.B.6    Implement diagnostic: RMSD of Δhkl for Bragg reflections using (A_avg_ref)⁻¹.    [ ]
  3.B.7    Implement diagnostic: RMS misorientation of U_i vs U_avg_ref.    [ ]
  3.B.8    Transform all q_vectors (from Phase 2 output) to fractional HKLs using (A_avg_ref)⁻¹:    [ ]    API Ref: crystallographic_calculations.md (C.2 q_to_miller_indices_static(crystal_avg_ref,
  q_vector)).
  3.B.9    Determine overall HKL range for grid boundaries considering d_min_target, d_max_target.    [ ]
  3.B.10    Store grid parameters (HKL range, ndiv_h,k,l) and implement conversion methods (hkl_to_voxel_idx, voxel_idx_to_hkl_center, etc.).    [ ]
  3.B.T    Tests for GlobalVoxelGrid    [ ]    Path: tests/voxelization/test_global_voxel_grid.py (New)
  3.B.T.1    Test: Unit cell and U-matrix averaging with known inputs.    [ ]
  3.B.T.2    Test: HKL range determination from q-vectors.    [ ]
  3.B.T.3    Test: hkl_to_voxel_idx and voxel_idx_to_hkl_center conversions.    [ ]
  3.C    Module 3.S.2: Binning Corrected Diffuse Pixels into Global Voxel Grid    [ ]
  3.C.idl    Define/Review IDL for VoxelAccumulator    [ ]    Path: src/diffusepipe/voxelization/voxel_accumulator_IDL.md (New) <br>Purpose: Bin pixels, manage HDF5 storage. <br>Inputs
  (add_observations): voxel_idx, intensities_array, sigmas_array, still_ids_array, q_vectors_lab_array. <br>Methods: add_observations, get_observations_for_voxel, get_all_binned_data_for_scaling,
  finalize. <br>Behavior: Uses HDF5 (h5py, zstd).
  3.C.1    Implement VoxelAccumulator class with HDF5 backend (h5py, zstd).    [ ]    Path: src/diffusepipe/voxelization/voxel_accumulator.py (New)
  3.C.2    Implement binning logic: For each observation (q_lab_i, I_corr, Sigma_corr, still_id):    [ ]    Input: Per-still arrays from Phase 2 (final_q_vectors_still_i, final_I_corrected_still_i, etc.)
  3.C.2.a    Transform q_lab_i to hkl_frac using GlobalVoxelGrid.A_avg_ref.inverse():    [ ]    API Ref: crystallographic_calculations.md (C.2).
  3.C.2.b    Map hkl_frac to ASU using GlobalVoxelGrid.Crystal_avg_ref.get_space_group().info().map_to_asu():    [ ]    API Ref: crystallographic_calculations.md (C.7). dxtbx_models.md (B.3 for
  crystal.get_space_group()).
  3.C.2.c    Determine voxel_idx using GlobalVoxelGrid.hkl_to_voxel_idx().    [ ]
  3.C.2.d    Store (I_corr, Sigma_corr, still_id, q_lab_i) to VoxelAccumulator for that voxel_idx.    [ ]    This is BinnedPixelData_Global as per plan.md.
  3.C.3    Implement logic to create ScalingModel_initial_list (list of initial scaling model objects, one per still/group).    [ ]    Scaling models will be DIALS-based, e.g., instances of a
  PerStillMultiplierComponent.
  3.C.T    Tests for VoxelAccumulator    [ ]    Path: tests/voxelization/test_voxel_accumulator.py (New)
  3.C.T.1    Test: Adding observations to HDF5 and retrieving them.    [ ]
  3.C.T.2    Test: Correct HKL transformation, ASU mapping, and voxel indexing for sample q-vectors.    [ ]
  3.D    Module 3.S.3: Relative Scaling of Binned Observations    [ ]
  3.D.idl    Define/Review IDLs for DiffuseScalingModel, PerStillMultiplierComponent, ResolutionSmootherComponent    [ ]    Path: src/diffusepipe/scaling/diffuse_scaling_model_IDL.md (New) <br>Purpose:
  Custom scaling model for diffuse data. <br>Inheritance: DiffuseScalingModel(ScalingModelBase), components from ScaleComponentBase. <br>Inputs: BinnedPixelData_Global, Reflections_dials_i,
  RelativeScalingConfig. <br>Methods: refine_parameters, get_scales_for_observation.
  3.D.1    Implement PerStillMultiplierComponent(ScaleComponentBase) for b_i parameter.    [ ]    Path: src/diffusepipe/scaling/components/per_still_multiplier.py (New). API Ref: dials_scaling.md (D.0
  ScaleComponentBase, example SimpleMultiplicativeComponent).
  3.D.2    Implement ResolutionSmootherComponent(ScaleComponentBase) for `a(    q    )usingGaussianSmoother1D`.
  3.D.3    Implement DiffuseScalingModel(ScalingModelBase) to aggregate components.    [ ]    Path: src/diffusepipe/scaling/diffuse_scaling_model.py (New). V1: b_i and optional `a(
  3.D.4    Integrate with DIALS active parameter manager:    [ ]    API Ref: dials_scaling.md (D.0 active_parameter_manager, multi_active_parameter_manager).
  3.D.5    Implement Bragg reference generation:    [ ]    Filter Reflections_dials_i by P_spot >= P_min_thresh (from RelativeScalingConfig). API Ref: flex_arrays.md (D.3 access "partiality"). Weighted
  average I_bragg_obs_spot scaled by current ScalingModel_i.
  3.D.6    Implement diffuse reference generation from VoxelAccumulator data (weighted average, scaled by current ScalingModel_i).    [ ]
  3.D.7    Implement DiffuseScalingTarget function for residuals: Res = (I_obs / M_i) - I_ref.    [ ]
  3.D.8    Integrate with DIALS/ScitBX minimizer (e.g., Levenberg-Marquardt).    [ ]    API Ref: dials_scaling.md (D.0 example normal_eqns_solving.levenberg_marquardt_iterations from scitbx.lstbx).
  3.D.9    Implement iterative refinement loop with convergence checks.    [ ]
  3.D.T    Tests for Relative Scaling    [ ]    Path: tests/scaling/test_diffuse_scaling_model.py (New)
  3.D.T.1    Test: Individual scaling components (PerStillMultiplier, ResolutionSmoother).    [ ]
  3.D.T.2    Test: Bragg and diffuse reference generation logic.    [ ]
  3.D.T.3    Test: Scaling refinement with synthetic data (known scales, check recovery).    [ ]
  3.D.T.4    Test: Correct use of P_spot as quality filter (not divisor).    [ ]
  3.E    Module 3.S.4: Merging Relatively Scaled Data into Voxels    [ ]
  3.E.idl    Define/Review IDL for DiffuseDataMerger (or as methods in scaling model/orchestrator)    [ ]    Path: src/diffusepipe/merging/merger_IDL.md (New) or integrate into existing. <br>Purpose:
  Apply scales, merge. <br>Inputs: BinnedPixelData_Global (from VoxelAccumulator), ScalingModel_refined_list, GlobalVoxelGrid. <br>Output: VoxelData_relative (structured array with I_merged_relative,
  Sigma_merged_relative, etc.).
  3.E.1    Implement logic to apply refined scales M_i (from DiffuseScalingModel.get_scales()) to each observation:    [ ]    API Ref: dials_scaling.md (D.0 model.get_scales()). I_final_relative = I_corr
   / M_i.
  3.E.2    Implement error propagation: Sigma_final_relative = Sigma_corr / abs(M_i). Verify C_i (additive offset) is effectively zero for v1.    [ ]
  3.E.3    Implement weighted merge for each voxel: weight = 1 / Sigma_final_relative^2.    [ ]    API Ref: flex_arrays.md (D.4 flex.sum, arithmetic).
  3.E.4    Calculate voxel center q-vector attributes (q_center_x,y,z, `    q    _center) usingGlobalVoxelGridmethods andA_avg_ref`.
  3.E.5    Structure and save VoxelData_relative output (e.g., NumPy structured array or reflection table).    [ ]
  3.E.T    Tests for Merging    [ ]    Path: tests/merging/test_merger.py (New) or tests/scaling/test_merging.py
  3.E.T.1    Test: Correct application of scale factors to observations.    [ ]
  3.E.T.2    Test: Weighted merging logic and error propagation for sample voxels.    [ ]
  3.E.T.3    Test: Correct calculation of voxel center q-attributes.    [ ]
  3.F    Orchestration, Configuration, and QC    [ ]
  3.F.1    Update StillsPipelineOrchestrator to manage Phase 3 flow, data passing, and calls to new components.    [ ]
  3.F.2    Define RelativeScalingConfig in types_IDL.py and .md. Add to StillsPipelineConfig.    [ ]
  3.F.3    Implement QC metrics/plots for Phase 3 as per plan.md (Overall R-factor, scale factor plots, redundancy map, mean intensity vs. resolution).    [ ]
  3.F.T    Integration Tests for Phase 3 Orchestration    [ ]    Path: tests/integration/test_phase3_workflow.py (New)
  3.F.T.1    Test: End-to-end Phase 3 workflow with mock Phase 2 output and synthetic scaling factors. Verify final VoxelData_relative.    [ ]
  3.Z    Phase 3 Review & Next Steps    [ ]
  3.Z.1    Self-Review: All Phase 3 items addressed? IDLs created/updated? Tests passing?    [ ]
  3.Z.2    Context Refresh: Re-read plan.md sections for Phase 4 (Absolute Scaling).    [ ]
  3.Z.3    Decision: Proceed to Phase 4 Checklist.
</file>

<file path="docs/LESSONS_LEARNED.md">
# Lessons Learned: Critical Insights from Development

## Overview

This document consolidates hard-won knowledge from real development experiences, debugging sessions, and implementation challenges. These lessons represent proven strategies for common problems and should be consulted before tackling similar issues.

## Test Suite Reliability and Remediation

### The Great Test Failure Crisis

**Context:** The project experienced a test suite reliability crisis with 22 failing tests blocking development progress.

**Root Causes Identified:**
1. **Excessive Mocking:** Tests passed but didn't validate real functionality
2. **DIALS API Changes:** Frequent breaking changes in DIALS library imports and methods
3. **Unrealistic Test Expectations:** Assertions with bounds that didn't match real detector geometries
4. **Mock Strategy Inadequacy:** Basic `Mock` objects couldn't handle magic methods like `__getitem__`, `__and__`

**Solution Strategy That Worked:**
1. **Mock Evolution:** Transition from `Mock` → `MagicMock` for proper magic method support
2. **Real Component Integration:** Replace complex mocking with actual DIALS `flex` arrays
3. **Realistic Bounds:** Update assertions to match actual detector physics (solid angle corrections < 3e6, not < 1e6)
4. **Systematic Fixes:** Applied targeted fixes by failure category rather than wholesale changes

**Results:** Achieved 64% reduction in test failures (22 → 8), with 186 tests passing reliably.

### DIALS API Compatibility Patterns

**Common Breaking Changes:**
- PHIL scope imports: `master_phil_scope` → `phil_scope`
- Method signature changes in processing adapters
- Import path modifications for DIALS components

**Proven Fix Strategy:**
1. Check DIALS documentation and version notes first
2. Test imports independently before integration
3. Use CLI-based adapters as fallback for unstable APIs
4. Implement version compatibility checks where feasible

### Test Authenticity vs Performance Trade-offs

**Learning:** Authentic integration tests with real DIALS components are more valuable than fast unit tests with complex mocks.

**Best Practice:**
- Use real DIALS `flex` arrays for mask and reflection operations
- Mock only external APIs with usage limits or complex infrastructure
- Prioritize test authenticity over test speed for critical integration points

## Safe Refactoring: The Corruption Incident

### The Failed QConsistencyChecker Extraction

**What Happened:** Attempted to extract a 350+ line method into a separate class resulted in file corruption with syntax errors and duplicate method definitions.

**Critical Failure Points:**
1. **Too Aggressive:** Attempted to replace 350+ lines in single operations
2. **No Incremental Testing:** Did not verify syntax after each change
3. **Complex String Matching:** Edit operations failed due to multi-line replacements
4. **Simultaneous Operations:** Created new file while modifying original simultaneously

**Why It "Appeared" to Work:**
- Python stopped parsing at first valid method
- Import system bypassed syntax errors in unused code paths
- Delegation call worked, masking underlying corruption

**Lessons for Safe Refactoring:**

#### The 5-10 Line Rule
- **Never attempt to refactor more than 5-10 lines at once**
- Test syntax after each increment: `python -m py_compile file.py`
- Commit working code after each successful change

#### Extract First, Then Modify
- Create and fully test new files before modifying originals
- Verify imports work before implementing method calls
- Never perform both extraction and modification simultaneously

#### Critical Warning Signs
- File "appears" to work but has syntax errors in unused sections
- Duplicate method definitions with different signatures
- Orphaned code blocks with incorrect indentation
- Import statements without corresponding usage

#### Recovery Strategy
1. **Immediate Revert:** Use version control to restore last known good state
2. **Restart Incrementally:** Re-implement refactoring with smaller steps
3. **Use Proper Tools:** IDE/editor with syntax highlighting and error detection

## Scientific Accuracy Implementation

### Air Attenuation Enhancement Success Story

**Challenge:** Replace "very rough approximation" with scientifically accurate air attenuation correction.

**Implementation Strategy:**
1. **NIST Data Integration:** Used tabulated X-ray mass attenuation coefficients for air components
2. **Proper Atmospheric Composition:** Standard dry air by mass (N: 78.084%, O: 20.946%, Ar: 0.934%, C: 0.036%)
3. **Thermodynamic Accuracy:** Ideal gas law with configurable temperature and pressure
4. **Energy Range Coverage:** 1-100 keV with log-log interpolation for accuracy

**Validation Approach:**
- Tests validate NIST coefficient values within 1% tolerance
- Reference data points from NIST XCOM database
- Cross-validation against known theoretical values

**Results:** Scientifically accurate corrections with proper unit conversions and dimensional consistency.

### Performance Optimization Insights

**Vectorization Success:**
- Achieved 2.4x speedup (4.0s → 1.7s) through comprehensive vectorization
- Maintained algorithmic equivalence between vectorized and iterative implementations
- Used rigorous testing to prove identical results within floating-point tolerance

**Key Strategies:**
1. **Batch Processing:** Process coordinates, q-vectors, and corrections as arrays
2. **Memory Efficiency:** Minimize Python loop overhead through vectorized NumPy operations
3. **Dual Implementation:** Keep both vectorized and iterative versions for comparison and fallback
4. **Performance Measurement:** Document speedups with structured testing and characterization

## DIALS Processing Mode Detection

### The Stills vs Sequences Challenge

**Critical Discovery:** CBF files with oscillation data (Angle_increment > 0) must use sequence processing, not stills processing.

**Error Patterns Observed:**
- Configuration errors: `'dict' object has no attribute 'spotfinder_threshold_algorithm'`
- Data type mismatches: `num stills: 0, sweep: 1` in DIALS logs
- Processing failures when using wrong adapter for data type

**Proven Solution Framework:**

#### Data Type Detection (Module 1.S.0)
1. **Check CBF Headers:** Parse `Angle_increment` value reliably
2. **Route Appropriately:** 
   - = 0°: Use `DIALSStillsProcessAdapter`
   - > 0°: Use `DIALSSequenceProcessAdapter`
3. **Configuration Validation:** Ensure configuration objects match expected types

#### Critical PHIL Parameters for Sequences
```python
phil_overrides = [
    "spotfinder.filter.min_spot_size=3",          # Not default 2
    "spotfinder.threshold.algorithm=dispersion",   # Not default
    "indexing.method=fft3d",                       # Not fft1d  
    "geometry.convert_sequences_to_stills=false"   # Preserve oscillation
]
```

#### Debugging Strategy
1. **Verify Data Type:** `grep "Angle_increment" file.cbf`
2. **Check Routing Logic:** Add explicit logging for adapter selection
3. **Validate Configuration:** Confirm object types and field names
4. **Compare with Working Examples:** Use existing successful processing logs

## Q-Vector Validation Strategy

### Primary vs Fallback Validation Methods

**Project Decision:** Q-vector consistency checking is the primary geometric validation method.

**Primary Method (Q-Vector Consistency):**
- Compare `q_model` (DIALS-refined) with `q_observed` (recalculated from pixels)
- Goal: `|Δq|` typically < 0.01 Å⁻¹
- Robust crystallographic validation following physics principles

**Fallback Method (Pixel Position):**
- Simple Euclidean distance between observed and calculated pixel positions
- Tolerance: ~1-2 pixels
- Useful for debugging coordinate transformation issues
- **Not** the primary validation criterion

**Implementation Insight:** The pixel-based method can help isolate whether geometric model is poor or Q-vector calculations are problematic.

## Visual Diagnostics Implementation

### End-to-End Pipeline Validation

**Architecture Decision:** Created both standalone diagnostic script and complete orchestration pipeline.

**Key Design Elements:**
1. **Pixel Coordinate Tracking:** Enhanced DataExtractor to conditionally save coordinates
2. **Comprehensive Plot Types:** 8 diagnostic visualizations covering all verification aspects
3. **Backward Compatibility:** Coordinate tracking only enabled when needed
4. **Error Handling:** Robust error handling with detailed logging and graceful failures

**Production Benefits:**
- Automated verification from CBF to final diagnostics
- Structured output directories with all intermediate files preserved
- JSON-based configuration for flexible parameter overrides

## Configuration Management Lessons

### Object vs Dictionary Pitfalls

**Common Error Pattern:**
```
AttributeError: 'dict' object has no attribute 'spotfinder_threshold_algorithm'
```

**Root Cause:** Code expects structured configuration objects but receives raw dictionaries.

**Solution Pattern:**
```python
# WRONG - passing raw dict
config = {"spotfinder_threshold_algorithm": "dispersion"}

# CORRECT - using proper configuration class
from diffusepipe.types.types_IDL import DIALSSequenceProcessConfig
config = DIALSSequenceProcessConfig(
    spotfinder_threshold_algorithm="dispersion"
)
```

**Debugging Approach:**
1. Verify configuration object types explicitly
2. Check field names match configuration class definitions
3. Ensure proper type conversion for all fields

## Development Workflow Enhancements

### Memory Log Maintenance

**Practice:** Maintain detailed working memory logs with:
- Current task focus and acceptance criteria
- Technical achievements with specific metrics
- Implementation decisions and their rationale
- Next steps and dependencies

**Benefits:** Provides continuity across development sessions and preserves decision context.

### Incremental Development Strategy

**Proven Approach:**
1. **Phase-Based Implementation:** Break large features into discrete phases
2. **Checklist Management:** Maintain phase-specific checklists with status tracking
3. **Integration Testing:** Test each phase independently before proceeding
4. **Documentation Updates:** Keep documentation synchronized with implementation

## Key Takeaways for Future Development

### Test Suite Management
- **Real components > complex mocks** for integration testing
- **MagicMock > Mock** for DIALS objects requiring magic methods
- **Realistic bounds** based on actual detector physics
- **Systematic fixes** by failure category, not wholesale changes

### Refactoring Safety
- **5-10 line increments** maximum for any single change
- **Syntax verification** after each increment
- **Extract first, modify second** - never simultaneously
- **Immediate revert** if corruption signs appear

### DIALS Integration
- **Data type detection first** - check CBF headers for oscillation
- **Configuration object validation** - ensure proper types, not dictionaries
- **CLI fallbacks** for unstable Python APIs
- **Working example comparison** for debugging parameters

### Scientific Implementation
- **NIST-based accuracy** over rough approximations
- **Vectorization with equivalence testing** for performance
- **Configurable parameters** for environmental conditions
- **Comprehensive validation** against reference data

### Documentation Maintenance
- **Preserve hard-won knowledge** in consolidated lessons learned
- **Update multiple docs** when patterns change
- **Cross-reference related sections** for discoverability
- **Include specific error patterns** and their solutions

These lessons represent significant development time investment and should be consulted whenever similar challenges arise. They provide proven strategies that work in practice, not just theory.
</file>

<file path="scripts/visual_diagnostics/check_phase3_outputs.py">
#!/usr/bin/env python3
"""
Visual diagnostics for Phase 3 diffuse scattering outputs verification.

This script takes outputs from Phase 3 (GlobalVoxelGrid definition, refined scaling
parameters, and merged voxel data) and generates a series of diagnostic plots and
summary reports to visually verify the correctness of the voxelization, relative
scaling, and merging processes.

Key diagnostic outputs include:
1. Global voxel grid summary (text + conceptual visualization)
2. Voxel occupancy/redundancy analysis (heatmap slices, histogram)
3. Relative scaling model parameter plots (per-still scales, resolution smoother)
4. Merged voxel data visualization (intensity slices, radial averages, I/sigma)
5. Comprehensive summary report

Author: DiffusePipe
"""

import argparse
import json
import logging
import sys
from pathlib import Path
from typing import Dict, Any, Optional, List, Tuple

import matplotlib
matplotlib.use("Agg")  # Set early for non-interactive backend
import matplotlib.pyplot as plt
import numpy as np

# Default maximum number of points to display in scatter plots for performance
DEFAULT_MAX_SCATTER_POINTS = 50000

# Import project utilities
try:
    from plot_utils import (
        plot_3d_grid_slice,
        plot_radial_average,
        plot_parameter_vs_index,
        plot_smoother_curve,
        ensure_output_dir,
        close_all_figures,
        setup_logging_for_plots,
    )
except ImportError as e:
    print(f"Error importing plot utilities: {e}")
    print("Please ensure you're running from the scripts/visual_diagnostics directory.")
    sys.exit(1)

logger = logging.getLogger(__name__)


def parse_arguments() -> argparse.Namespace:
    """Parse command-line arguments."""
    parser = argparse.ArgumentParser(
        description="Visual diagnostics for Phase 3 diffuse scattering outputs verification",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  # Basic usage
  python check_phase3_outputs.py \\
    --grid-definition-file phase3_outputs/global_voxel_grid_definition.json \\
    --scaling-model-params-file phase3_outputs/refined_scaling_model_params.json \\
    --voxel-data-file phase3_outputs/voxel_data_relative.npz \\
    --output-dir phase3_diagnostics

  # With optional additional data
  python check_phase3_outputs.py \\
    --grid-definition-file grid_def.json \\
    --scaling-model-params-file scaling_params.json \\
    --voxel-data-file voxel_data.npz \\
    --output-dir diagnostics \\
    --experiments-list-file experiments_list.txt \\
    --corrected-pixel-data-dir pixel_data_dirs.txt \\
    --max-plot-points 25000 \\
    --verbose
        """,
    )

    # Required arguments
    parser.add_argument(
        "--grid-definition-file",
        type=str,
        required=True,
        help="Path to GlobalVoxelGrid definition JSON file"
    )
    parser.add_argument(
        "--scaling-model-params-file",
        type=str,
        required=True,
        help="Path to refined scaling model parameters JSON file"
    )
    parser.add_argument(
        "--voxel-data-file",
        type=str,
        required=True,
        help="Path to VoxelData_relative NPZ/HDF5 file"
    )
    parser.add_argument(
        "--output-dir",
        type=str,
        required=True,
        help="Output directory for diagnostic plots and reports"
    )

    # Optional arguments
    parser.add_argument(
        "--experiments-list-file",
        type=str,
        help="Path to file containing list of experiment (.expt) file paths (optional)"
    )
    parser.add_argument(
        "--corrected-pixel-data-dir",
        type=str,
        help="Path to file containing list of corrected pixel data directories (optional)"
    )
    parser.add_argument(
        "--max-plot-points",
        type=int,
        default=DEFAULT_MAX_SCATTER_POINTS,
        help="Maximum number of points to display in scatter plots for performance"
    )
    parser.add_argument("--verbose", action="store_true", help="Enable verbose logging")

    return parser.parse_args()


def setup_logging(verbose: bool = False) -> None:
    """Set up logging configuration."""
    log_level = logging.DEBUG if verbose else logging.INFO
    logging.basicConfig(
        level=log_level,
        format="%(asctime)s - %(name)s - %(levelname)s - %(message)s"
    )


def validate_input_files(args: argparse.Namespace) -> None:
    """Validate that all required input files exist."""
    required_files = [
        args.grid_definition_file,
        args.scaling_model_params_file,
        args.voxel_data_file
    ]

    optional_files = [
        args.experiments_list_file,
        args.corrected_pixel_data_dir
    ]

    # Check required files
    for file_path in required_files:
        if not Path(file_path).exists():
            raise FileNotFoundError(f"Required file not found: {file_path}")

    # Check optional files
    for file_path in optional_files:
        if file_path and not Path(file_path).exists():
            logger.warning(f"Optional file not found: {file_path}")


def load_grid_definition(grid_file_path: str) -> Dict[str, Any]:
    """
    Load GlobalVoxelGrid definition from JSON file.

    Args:
        grid_file_path: Path to grid definition JSON file

    Returns:
        Dictionary containing grid definition parameters
    """
    try:
        with open(grid_file_path, 'r') as f:
            grid_def = json.load(f)
        
        logger.info(f"Loaded grid definition from {grid_file_path}")
        
        # Validate required keys
        required_keys = ["crystal_avg_ref", "hkl_bounds", "ndiv_h", "ndiv_k", "ndiv_l", "total_voxels"]
        for key in required_keys:
            if key not in grid_def:
                raise KeyError(f"Required key '{key}' not found in grid definition")
        
        return grid_def
        
    except Exception as e:
        logger.error(f"Failed to load grid definition from {grid_file_path}: {e}")
        raise


def load_scaling_parameters(scaling_file_path: str) -> Dict[str, Any]:
    """
    Load refined scaling model parameters from JSON file.

    Args:
        scaling_file_path: Path to scaling parameters JSON file

    Returns:
        Dictionary containing scaling parameters
    """
    try:
        with open(scaling_file_path, 'r') as f:
            scaling_params = json.load(f)
        
        logger.info(f"Loaded scaling parameters from {scaling_file_path}")
        
        # Validate required keys
        required_keys = ["refined_parameters", "refinement_statistics"]
        for key in required_keys:
            if key not in scaling_params:
                raise KeyError(f"Required key '{key}' not found in scaling parameters")
        
        return scaling_params
        
    except Exception as e:
        logger.error(f"Failed to load scaling parameters from {scaling_file_path}: {e}")
        raise


def load_voxel_data(voxel_file_path: str) -> Dict[str, np.ndarray]:
    """
    Load VoxelData_relative from NPZ or HDF5 file.

    Args:
        voxel_file_path: Path to voxel data file

    Returns:
        Dictionary containing voxel data arrays
    """
    try:
        if voxel_file_path.endswith('.npz'):
            data = np.load(voxel_file_path)
            result = {key: data[key] for key in data.files}
        elif voxel_file_path.endswith('.hdf5') or voxel_file_path.endswith('.h5'):
            import h5py
            result = {}
            with h5py.File(voxel_file_path, 'r') as f:
                for key in f.keys():
                    result[key] = f[key][:]
        else:
            raise ValueError("Voxel data file must be .npz or .hdf5/.h5 format")

        logger.info(f"Loaded voxel data from {voxel_file_path}")
        
        # Validate required keys
        required_keys = [
            "voxel_indices", "H_center", "K_center", "L_center",
            "q_center_x", "q_center_y", "q_center_z", "q_magnitude_center",
            "I_merged_relative", "Sigma_merged_relative", "num_observations"
        ]
        for key in required_keys:
            if key not in result:
                raise KeyError(f"Required key '{key}' not found in voxel data")
        
        logger.info(f"Voxel data contains {len(result['voxel_indices'])} voxels")
        return result
        
    except Exception as e:
        logger.error(f"Failed to load voxel data from {voxel_file_path}: {e}")
        raise


def generate_grid_summary(
    grid_def: Dict[str, Any],
    output_dir: Path
) -> None:
    """
    Generate grid summary text file and conceptual visualization.

    Args:
        grid_def: Grid definition dictionary
        output_dir: Output directory for files
    """
    logger.info("Generating grid summary")
    
    # Text summary
    summary_text = []
    summary_text.append("=== Global Voxel Grid Summary ===\n")
    
    # Crystal parameters
    crystal_info = grid_def["crystal_avg_ref"]
    if "unit_cell_params" in crystal_info:
        uc_params = crystal_info["unit_cell_params"]
        summary_text.append(f"Average Unit Cell: a={uc_params[0]:.3f}, b={uc_params[1]:.3f}, c={uc_params[2]:.3f}")
        summary_text.append(f"                   α={uc_params[3]:.2f}°, β={uc_params[4]:.2f}°, γ={uc_params[5]:.2f}°")
    
    if "space_group" in crystal_info:
        summary_text.append(f"Space Group: {crystal_info['space_group']}")
    
    summary_text.append("")
    
    # HKL bounds
    hkl_bounds = grid_def["hkl_bounds"]
    summary_text.append("HKL Bounds:")
    summary_text.append(f"  H: {hkl_bounds['h_min']} to {hkl_bounds['h_max']}")
    summary_text.append(f"  K: {hkl_bounds['k_min']} to {hkl_bounds['k_max']}")
    summary_text.append(f"  L: {hkl_bounds['l_min']} to {hkl_bounds['l_max']}")
    summary_text.append("")
    
    # Voxel dimensions
    ndiv_h, ndiv_k, ndiv_l = grid_def["ndiv_h"], grid_def["ndiv_k"], grid_def["ndiv_l"]
    summary_text.append("Voxel Divisions:")
    summary_text.append(f"  ndiv_h: {ndiv_h}")
    summary_text.append(f"  ndiv_k: {ndiv_k}")
    summary_text.append(f"  ndiv_l: {ndiv_l}")
    summary_text.append("")
    
    # Total voxels
    total_voxels = grid_def["total_voxels"]
    summary_text.append(f"Total Voxels: {total_voxels:,}")
    
    # Voxel size calculations
    h_range = hkl_bounds['h_max'] - hkl_bounds['h_min']
    k_range = hkl_bounds['k_max'] - hkl_bounds['k_min']
    l_range = hkl_bounds['l_max'] - hkl_bounds['l_min']
    
    voxel_size_h = h_range / ndiv_h
    voxel_size_k = k_range / ndiv_k
    voxel_size_l = l_range / ndiv_l
    
    summary_text.append("")
    summary_text.append("Voxel Sizes (in reciprocal lattice units):")
    summary_text.append(f"  ΔH: {voxel_size_h:.4f}")
    summary_text.append(f"  ΔK: {voxel_size_k:.4f}")
    summary_text.append(f"  ΔL: {voxel_size_l:.4f}")
    
    # Save text summary
    summary_file = output_dir / "grid_summary.txt"
    with open(summary_file, 'w') as f:
        f.write('\n'.join(summary_text))
    logger.info(f"Saved grid summary to {summary_file}")
    
    # Generate conceptual visualization
    fig, ax = plt.subplots(figsize=(10, 8), subplot_kw={'projection': '3d'})
    
    # Draw wireframe box representing grid bounds
    h_min, h_max = hkl_bounds['h_min'], hkl_bounds['h_max']
    k_min, k_max = hkl_bounds['k_min'], hkl_bounds['k_max']
    l_min, l_max = hkl_bounds['l_min'], hkl_bounds['l_max']
    
    # Define box corners
    corners = [
        [h_min, k_min, l_min], [h_max, k_min, l_min],
        [h_max, k_max, l_min], [h_min, k_max, l_min],
        [h_min, k_min, l_max], [h_max, k_min, l_max],
        [h_max, k_max, l_max], [h_min, k_max, l_max]
    ]
    
    # Draw edges
    edges = [
        [0, 1], [1, 2], [2, 3], [3, 0],  # bottom face
        [4, 5], [5, 6], [6, 7], [7, 4],  # top face
        [0, 4], [1, 5], [2, 6], [3, 7]   # vertical edges
    ]
    
    for edge in edges:
        points = np.array([corners[edge[0]], corners[edge[1]]])
        ax.plot3D(points[:, 0], points[:, 1], points[:, 2], 'b-', linewidth=2)
    
    # Add some sample grid points
    n_sample_points = min(1000, total_voxels // 100)
    if n_sample_points > 0:
        sample_h = np.random.uniform(h_min, h_max, n_sample_points)
        sample_k = np.random.uniform(k_min, k_max, n_sample_points)
        sample_l = np.random.uniform(l_min, l_max, n_sample_points)
        ax.scatter(sample_h, sample_k, sample_l, c='red', s=1, alpha=0.3)
    
    ax.set_xlabel('H')
    ax.set_ylabel('K')
    ax.set_zlabel('L')
    ax.set_title('Global Voxel Grid Conceptual Visualization\n(Blue wireframe shows HKL bounds)')
    
    # Save conceptual plot
    plot_file = output_dir / "grid_visualization_conceptual.png"
    fig.savefig(plot_file, dpi=150, bbox_inches="tight")
    plt.close(fig)
    logger.info(f"Saved grid visualization to {plot_file}")


def generate_voxel_occupancy_plots(
    voxel_data: Dict[str, np.ndarray],
    grid_def: Dict[str, Any],
    output_dir: Path
) -> Dict[str, Any]:
    """
    Generate voxel occupancy/redundancy analysis plots.

    Args:
        voxel_data: Voxel data dictionary
        grid_def: Grid definition dictionary
        output_dir: Output directory for plots

    Returns:
        Dictionary with occupancy statistics
    """
    logger.info("Generating voxel occupancy plots")
    
    num_observations = voxel_data["num_observations"]
    h_center = voxel_data["H_center"]
    k_center = voxel_data["K_center"]
    l_center = voxel_data["L_center"]
    
    # Calculate occupancy statistics
    stats = {
        "min_observations": int(np.min(num_observations)),
        "max_observations": int(np.max(num_observations)),
        "mean_observations": float(np.mean(num_observations)),
        "median_observations": float(np.median(num_observations)),
        "total_observations": int(np.sum(num_observations)),
        "voxels_with_data": int(np.sum(num_observations > 0)),
        "total_voxels": len(num_observations)
    }
    
    # Calculate percentage with low redundancy
    low_redundancy_thresh = 3
    stats[f"percent_voxels_lt_{low_redundancy_thresh}"] = float(
        100 * np.sum(num_observations < low_redundancy_thresh) / len(num_observations)
    )
    
    logger.info(f"Occupancy stats: {stats}")
    
    # Reshape occupancy into 3D grid for slicing
    ndiv_h, ndiv_k, ndiv_l = grid_def["ndiv_h"], grid_def["ndiv_k"], grid_def["ndiv_l"]
    hkl_bounds = grid_def["hkl_bounds"]
    
    # Create 3D occupancy grid
    occupancy_grid = np.zeros((ndiv_h, ndiv_k, ndiv_l))
    
    # Map HKL centers to grid indices
    h_indices = np.round((h_center - hkl_bounds['h_min']) / 
                        (hkl_bounds['h_max'] - hkl_bounds['h_min']) * (ndiv_h - 1)).astype(int)
    k_indices = np.round((k_center - hkl_bounds['k_min']) / 
                        (hkl_bounds['k_max'] - hkl_bounds['k_min']) * (ndiv_k - 1)).astype(int)
    l_indices = np.round((l_center - hkl_bounds['l_min']) / 
                        (hkl_bounds['l_max'] - hkl_bounds['l_min']) * (ndiv_l - 1)).astype(int)
    
    # Clip indices to valid range
    h_indices = np.clip(h_indices, 0, ndiv_h - 1)
    k_indices = np.clip(k_indices, 0, ndiv_k - 1)
    l_indices = np.clip(l_indices, 0, ndiv_l - 1)
    
    # Fill occupancy grid
    for i, (hi, ki, li) in enumerate(zip(h_indices, k_indices, l_indices)):
        occupancy_grid[hi, ki, li] = num_observations[i]
    
    # Generate 2D slice plots
    slice_configs = [
        (2, ndiv_l // 2, "H-K", "H", "K", "voxel_occupancy_slice_L0.png"),
        (1, ndiv_k // 2, "H-L", "H", "L", "voxel_occupancy_slice_K0.png"),
        (0, ndiv_h // 2, "K-L", "K", "L", "voxel_occupancy_slice_H0.png"),
    ]
    
    for slice_dim, slice_idx, title, xlabel, ylabel, filename in slice_configs:
        plot_3d_grid_slice(
            grid_data_3d=occupancy_grid,
            slice_dim_idx=slice_dim,
            slice_val_idx=slice_idx,
            title=f"Voxel Occupancy - {title} Slice",
            output_path=str(output_dir / filename),
            cmap="viridis",
            norm=None,
            xlabel=xlabel,
            ylabel=ylabel,
            aspect="auto"
        )
    
    # Generate occupancy histogram
    fig, ax = plt.subplots(figsize=(10, 6))
    
    # Use log bins for better visualization
    max_obs = max(stats["max_observations"], 1)
    bins = np.logspace(0, np.log10(max_obs + 1), 50)
    
    counts, _, _ = ax.hist(num_observations[num_observations > 0], bins=bins, 
                          alpha=0.7, edgecolor='black', linewidth=0.5)
    
    ax.set_xlabel("Number of Observations per Voxel")
    ax.set_ylabel("Number of Voxels")
    ax.set_title("Voxel Occupancy Distribution")
    ax.set_xscale("log")
    ax.grid(True, alpha=0.3)
    
    # Add statistics text
    stats_text = f"Mean: {stats['mean_observations']:.1f}\n"
    stats_text += f"Median: {stats['median_observations']:.1f}\n"
    stats_text += f"Min: {stats['min_observations']}\n"
    stats_text += f"Max: {stats['max_observations']}"
    
    ax.text(0.75, 0.95, stats_text, transform=ax.transAxes, 
            verticalalignment='top', bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))
    
    histogram_file = output_dir / "voxel_occupancy_histogram.png"
    fig.savefig(histogram_file, dpi=150, bbox_inches="tight")
    plt.close(fig)
    logger.info(f"Saved occupancy histogram to {histogram_file}")
    
    return stats


def generate_scaling_parameter_plots(
    scaling_params: Dict[str, Any],
    output_dir: Path
) -> None:
    """
    Generate plots for refined scaling model parameters.

    Args:
        scaling_params: Scaling parameters dictionary
        output_dir: Output directory for plots
    """
    logger.info("Generating scaling parameter plots")
    
    refined_params = scaling_params["refined_parameters"]
    
    # Extract per-still scales
    still_ids = list(refined_params.keys())
    scales = [refined_params[still_id]["multiplicative_scale"] for still_id in still_ids]
    still_indices = range(len(still_ids))
    
    # Plot per-still scales
    plot_parameter_vs_index(
        param_values=scales,
        index_values=list(still_indices),
        title="Per-Still Multiplicative Scale Factors",
        param_label="Multiplicative Scale (b_i)",
        index_label="Still Index",
        output_path=str(output_dir / "scaling_params_b_i.png")
    )
    
    # Plot resolution smoother if enabled
    if "resolution_smoother" in scaling_params and scaling_params["resolution_smoother"].get("enabled", False):
        control_points = scaling_params["resolution_smoother"].get("control_points", [])
        
        if control_points:
            # Create a simple evaluation function for the smoother
            def smoother_eval_func(q_values):
                # Simple linear interpolation between control points
                q_control = np.linspace(0.1, 2.0, len(control_points))
                return np.interp(q_values, q_control, control_points)
            
            # Plot smoother curve
            plot_smoother_curve(
                smoother_eval_func=smoother_eval_func,
                x_range=(0.1, 2.0),
                num_points=100,
                title="Resolution Smoother Function a(|q|)",
                output_path=str(output_dir / "scaling_resolution_smoother.png"),
                control_points_x=np.linspace(0.1, 2.0, len(control_points)),
                control_points_y=control_points
            )
    
    # Generate parameter summary text
    summary_text = []
    summary_text.append("=== Scaling Model Parameters Summary ===\n")
    
    summary_text.append(f"Number of Stills: {len(refined_params)}")
    summary_text.append(f"Scale Factor Statistics:")
    summary_text.append(f"  Mean: {np.mean(scales):.4f}")
    summary_text.append(f"  Std Dev: {np.std(scales):.4f}")
    summary_text.append(f"  Min: {np.min(scales):.4f}")
    summary_text.append(f"  Max: {np.max(scales):.4f}")
    summary_text.append("")
    
    # Refinement statistics
    if "refinement_statistics" in scaling_params:
        ref_stats = scaling_params["refinement_statistics"]
        summary_text.append("Refinement Statistics:")
        summary_text.append(f"  Iterations: {ref_stats.get('n_iterations', 'N/A')}")
        summary_text.append(f"  Final R-factor: {ref_stats.get('final_r_factor', 'N/A')}")
        summary_text.append(f"  Converged: {ref_stats.get('convergence_achieved', 'N/A')}")
        
        if "parameter_shifts" in ref_stats:
            summary_text.append("  Final Parameter Shifts:")
            for param, shift in ref_stats["parameter_shifts"].items():
                summary_text.append(f"    {param}: {shift}")
    
    # Resolution smoother info
    if "resolution_smoother" in scaling_params:
        res_smooth = scaling_params["resolution_smoother"]
        summary_text.append("")
        summary_text.append(f"Resolution Smoother: {'Enabled' if res_smooth.get('enabled', False) else 'Disabled'}")
        if res_smooth.get("enabled", False):
            control_points = res_smooth.get("control_points", [])
            summary_text.append(f"  Control Points: {len(control_points)}")
    
    # Save parameter summary
    params_summary_file = output_dir / "scaling_parameters_summary.txt"
    with open(params_summary_file, 'w') as f:
        f.write('\n'.join(summary_text))
    logger.info(f"Saved scaling parameters summary to {params_summary_file}")


def generate_merged_voxel_plots(
    voxel_data: Dict[str, np.ndarray],
    grid_def: Dict[str, Any],
    output_dir: Path,
    max_plot_points: int
) -> None:
    """
    Generate plots for merged voxel data visualization.

    Args:
        voxel_data: Voxel data dictionary
        grid_def: Grid definition dictionary
        output_dir: Output directory for plots
        max_plot_points: Maximum points for scatter plots
    """
    logger.info("Generating merged voxel data plots")
    
    intensities = voxel_data["I_merged_relative"]
    sigmas = voxel_data["Sigma_merged_relative"]
    q_magnitudes = voxel_data["q_magnitude_center"]
    h_center = voxel_data["H_center"]
    k_center = voxel_data["K_center"]
    l_center = voxel_data["L_center"]
    
    # Filter out zero/negative intensities for log plots
    positive_mask = intensities > 0
    
    # Calculate I/sigma
    i_sig_ratio = np.where(sigmas > 0, intensities / sigmas, 0)
    
    # Reshape intensity and sigma into 3D grids for slicing
    ndiv_h, ndiv_k, ndiv_l = grid_def["ndiv_h"], grid_def["ndiv_k"], grid_def["ndiv_l"]
    hkl_bounds = grid_def["hkl_bounds"]
    
    # Create 3D grids
    intensity_grid = np.zeros((ndiv_h, ndiv_k, ndiv_l))
    sigma_grid = np.zeros((ndiv_h, ndiv_k, ndiv_l))
    isigi_grid = np.zeros((ndiv_h, ndiv_k, ndiv_l))
    
    # Map HKL centers to grid indices
    h_indices = np.round((h_center - hkl_bounds['h_min']) / 
                        (hkl_bounds['h_max'] - hkl_bounds['h_min']) * (ndiv_h - 1)).astype(int)
    k_indices = np.round((k_center - hkl_bounds['k_min']) / 
                        (hkl_bounds['k_max'] - hkl_bounds['k_min']) * (ndiv_k - 1)).astype(int)
    l_indices = np.round((l_center - hkl_bounds['l_min']) / 
                        (hkl_bounds['l_max'] - hkl_bounds['l_min']) * (ndiv_l - 1)).astype(int)
    
    # Clip indices to valid range
    h_indices = np.clip(h_indices, 0, ndiv_h - 1)
    k_indices = np.clip(k_indices, 0, ndiv_k - 1)
    l_indices = np.clip(l_indices, 0, ndiv_l - 1)
    
    # Fill grids
    for i, (hi, ki, li) in enumerate(zip(h_indices, k_indices, l_indices)):
        intensity_grid[hi, ki, li] = intensities[i]
        sigma_grid[hi, ki, li] = sigmas[i]
        isigi_grid[hi, ki, li] = i_sig_ratio[i]
    
    # Generate intensity slice plots (log scale)
    intensity_slice_configs = [
        (2, ndiv_l // 2, "H-K", "H", "K", "merged_intensity_slice_L0.png"),
        (1, ndiv_k // 2, "H-L", "H", "L", "merged_intensity_slice_K0.png"),
        (0, ndiv_h // 2, "K-L", "K", "L", "merged_intensity_slice_H0.png"),
    ]
    
    for slice_dim, slice_idx, title, xlabel, ylabel, filename in intensity_slice_configs:
        plot_3d_grid_slice(
            grid_data_3d=intensity_grid,
            slice_dim_idx=slice_dim,
            slice_val_idx=slice_idx,
            title=f"Merged Intensity - {title} Slice (Log Scale)",
            output_path=str(output_dir / filename),
            cmap="viridis",
            norm="log",
            xlabel=xlabel,
            ylabel=ylabel,
            aspect="auto"
        )
    
    # Generate sigma slice plots
    sigma_slice_configs = [
        (2, ndiv_l // 2, "H-K", "H", "K", "merged_sigma_slice_L0.png"),
        (1, ndiv_k // 2, "H-L", "H", "L", "merged_sigma_slice_K0.png"),
        (0, ndiv_h // 2, "K-L", "K", "L", "merged_sigma_slice_H0.png"),
    ]
    
    for slice_dim, slice_idx, title, xlabel, ylabel, filename in sigma_slice_configs:
        plot_3d_grid_slice(
            grid_data_3d=sigma_grid,
            slice_dim_idx=slice_dim,
            slice_val_idx=slice_idx,
            title=f"Merged Sigma - {title} Slice",
            output_path=str(output_dir / filename),
            cmap="plasma",
            norm=None,
            xlabel=xlabel,
            ylabel=ylabel,
            aspect="auto"
        )
    
    # Generate I/sigma slice plots
    isigi_slice_configs = [
        (2, ndiv_l // 2, "H-K", "H", "K", "merged_isigi_slice_L0.png"),
        (1, ndiv_k // 2, "H-L", "H", "L", "merged_isigi_slice_K0.png"),
        (0, ndiv_h // 2, "K-L", "K", "L", "merged_isigi_slice_H0.png"),
    ]
    
    for slice_dim, slice_idx, title, xlabel, ylabel, filename in isigi_slice_configs:
        plot_3d_grid_slice(
            grid_data_3d=isigi_grid,
            slice_dim_idx=slice_dim,
            slice_val_idx=slice_idx,
            title=f"Merged I/σ - {title} Slice",
            output_path=str(output_dir / filename),
            cmap="coolwarm",
            norm=None,
            xlabel=xlabel,
            ylabel=ylabel,
            aspect="auto"
        )
    
    # Generate radial average plot
    mask_for_radial = positive_mask & (q_magnitudes > 0)
    if np.sum(mask_for_radial) > 0:
        plot_radial_average(
            q_magnitudes=q_magnitudes[mask_for_radial],
            intensities=intensities[mask_for_radial],
            num_bins=50,
            title="Radial Average of Merged Intensities",
            output_path=str(output_dir / "merged_radial_average.png"),
            sigmas=sigmas[mask_for_radial]
        )
    
    # Generate intensity histogram
    fig, ax = plt.subplots(figsize=(10, 6))
    
    positive_intensities = intensities[positive_mask]
    if len(positive_intensities) > 0:
        # Use log bins for better visualization
        bins = np.logspace(np.log10(positive_intensities.min()), 
                          np.log10(positive_intensities.max()), 50)
        
        ax.hist(positive_intensities, bins=bins, alpha=0.7, 
               edgecolor='black', linewidth=0.5)
        ax.set_xscale("log")
    else:
        ax.hist(intensities, bins=50, alpha=0.7, 
               edgecolor='black', linewidth=0.5)
    
    ax.set_xlabel("Merged Intensity")
    ax.set_ylabel("Number of Voxels")
    ax.set_title("Distribution of Merged Intensities")
    ax.grid(True, alpha=0.3)
    
    # Add statistics
    stats_text = f"Total Voxels: {len(intensities)}\n"
    stats_text += f"Positive Intensities: {np.sum(positive_mask)}\n"
    stats_text += f"Mean (positive): {np.mean(positive_intensities):.2f}\n" if len(positive_intensities) > 0 else "Mean: N/A\n"
    stats_text += f"Median (positive): {np.median(positive_intensities):.2f}" if len(positive_intensities) > 0 else "Median: N/A"
    
    ax.text(0.75, 0.95, stats_text, transform=ax.transAxes,
            verticalalignment='top', bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))
    
    histogram_file = output_dir / "merged_intensity_histogram.png"
    fig.savefig(histogram_file, dpi=150, bbox_inches="tight")
    plt.close(fig)
    logger.info(f"Saved intensity histogram to {histogram_file}")


def generate_comprehensive_summary(
    grid_def: Dict[str, Any],
    scaling_params: Dict[str, Any],
    voxel_data: Dict[str, np.ndarray],
    occupancy_stats: Dict[str, Any],
    input_files: Dict[str, str],
    output_dir: Path
) -> None:
    """
    Generate comprehensive summary report.

    Args:
        grid_def: Grid definition dictionary
        scaling_params: Scaling parameters dictionary
        voxel_data: Voxel data dictionary
        occupancy_stats: Occupancy statistics
        input_files: Input file paths
        output_dir: Output directory
    """
    logger.info("Generating comprehensive summary report")
    
    summary_text = []
    summary_text.append("=== Phase 3 Diagnostics Summary Report ===")
    summary_text.append(f"Generated on: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}")
    summary_text.append("")
    
    # Input files section
    summary_text.append("Input Files:")
    for file_type, file_path in input_files.items():
        summary_text.append(f"  {file_type}: {file_path}")
    summary_text.append("")
    
    # Grid definition summary
    summary_text.append("Global Voxel Grid:")
    crystal_info = grid_def["crystal_avg_ref"]
    if "unit_cell_params" in crystal_info:
        uc_params = crystal_info["unit_cell_params"]
        summary_text.append(f"  Unit Cell: {uc_params[0]:.2f} {uc_params[1]:.2f} {uc_params[2]:.2f} {uc_params[3]:.1f} {uc_params[4]:.1f} {uc_params[5]:.1f}")
    if "space_group" in crystal_info:
        summary_text.append(f"  Space Group: {crystal_info['space_group']}")
    
    hkl_bounds = grid_def["hkl_bounds"]
    summary_text.append(f"  HKL Range: H({hkl_bounds['h_min']}:{hkl_bounds['h_max']}) K({hkl_bounds['k_min']}:{hkl_bounds['k_max']}) L({hkl_bounds['l_min']}:{hkl_bounds['l_max']})")
    summary_text.append(f"  Divisions: {grid_def['ndiv_h']} × {grid_def['ndiv_k']} × {grid_def['ndiv_l']}")
    summary_text.append(f"  Total Voxels: {grid_def['total_voxels']:,}")
    summary_text.append("")
    
    # Occupancy statistics
    summary_text.append("Voxel Occupancy:")
    summary_text.append(f"  Voxels with Data: {occupancy_stats['voxels_with_data']:,} / {occupancy_stats['total_voxels']:,} ({100*occupancy_stats['voxels_with_data']/occupancy_stats['total_voxels']:.1f}%)")
    summary_text.append(f"  Total Observations: {occupancy_stats['total_observations']:,}")
    summary_text.append(f"  Mean Observations per Voxel: {occupancy_stats['mean_observations']:.1f}")
    summary_text.append(f"  Median Observations per Voxel: {occupancy_stats['median_observations']:.1f}")
    summary_text.append(f"  Range: {occupancy_stats['min_observations']} - {occupancy_stats['max_observations']}")
    summary_text.append(f"  Voxels with < 3 observations: {occupancy_stats['percent_voxels_lt_3']:.1f}%")
    summary_text.append("")
    
    # Scaling model summary
    refined_params = scaling_params["refined_parameters"]
    scales = [refined_params[still_id]["multiplicative_scale"] for still_id in refined_params.keys()]
    
    summary_text.append("Relative Scaling:")
    summary_text.append(f"  Number of Stills: {len(refined_params)}")
    summary_text.append(f"  Scale Factor Range: {np.min(scales):.4f} - {np.max(scales):.4f}")
    summary_text.append(f"  Scale Factor Mean ± Std: {np.mean(scales):.4f} ± {np.std(scales):.4f}")
    
    if "refinement_statistics" in scaling_params:
        ref_stats = scaling_params["refinement_statistics"]
        summary_text.append(f"  Refinement Iterations: {ref_stats.get('n_iterations', 'N/A')}")
        summary_text.append(f"  Final R-factor: {ref_stats.get('final_r_factor', 'N/A'):.4f}" if isinstance(ref_stats.get('final_r_factor'), (int, float)) else f"  Final R-factor: {ref_stats.get('final_r_factor', 'N/A')}")
        summary_text.append(f"  Convergence: {'Yes' if ref_stats.get('convergence_achieved', False) else 'No'}")
    
    if "resolution_smoother" in scaling_params:
        res_smooth = scaling_params["resolution_smoother"]
        summary_text.append(f"  Resolution Smoother: {'Enabled' if res_smooth.get('enabled', False) else 'Disabled'}")
    summary_text.append("")
    
    # Merged intensity statistics
    intensities = voxel_data["I_merged_relative"]
    sigmas = voxel_data["Sigma_merged_relative"]
    positive_mask = intensities > 0
    
    summary_text.append("Merged Intensities:")
    summary_text.append(f"  Total Voxels: {len(intensities):,}")
    summary_text.append(f"  Voxels with Positive Intensity: {np.sum(positive_mask):,} ({100*np.sum(positive_mask)/len(intensities):.1f}%)")
    
    if np.sum(positive_mask) > 0:
        positive_intensities = intensities[positive_mask]
        summary_text.append(f"  Intensity Range (positive): {np.min(positive_intensities):.2e} - {np.max(positive_intensities):.2e}")
        summary_text.append(f"  Intensity Mean ± Std (positive): {np.mean(positive_intensities):.2e} ± {np.std(positive_intensities):.2e}")
        
        # I/sigma statistics
        positive_sigmas = sigmas[positive_mask]
        valid_sigma_mask = positive_sigmas > 0
        if np.sum(valid_sigma_mask) > 0:
            i_sig_ratios = positive_intensities[valid_sigma_mask] / positive_sigmas[valid_sigma_mask]
            summary_text.append(f"  I/σ Mean ± Std: {np.mean(i_sig_ratios):.2f} ± {np.std(i_sig_ratios):.2f}")
            summary_text.append(f"  I/σ Range: {np.min(i_sig_ratios):.2f} - {np.max(i_sig_ratios):.2f}")
    
    summary_text.append("")
    
    # Resolution statistics
    q_magnitudes = voxel_data["q_magnitude_center"]
    summary_text.append("Resolution Coverage:")
    summary_text.append(f"  Q Range: {np.min(q_magnitudes):.3f} - {np.max(q_magnitudes):.3f} Å⁻¹")
    
    # Convert to d-spacing
    d_spacings = 2 * np.pi / q_magnitudes[q_magnitudes > 0]
    if len(d_spacings) > 0:
        summary_text.append(f"  d-spacing Range: {np.min(d_spacings):.2f} - {np.max(d_spacings):.2f} Å")
    
    summary_text.append("")
    
    # Generated plots summary
    summary_text.append("Generated Diagnostic Plots:")
    plot_files = [
        "grid_summary.txt",
        "grid_visualization_conceptual.png",
        "voxel_occupancy_slice_L0.png",
        "voxel_occupancy_histogram.png",
        "scaling_params_b_i.png",
        "merged_intensity_slice_L0.png",
        "merged_radial_average.png",
        "merged_intensity_histogram.png"
    ]
    
    for plot_file in plot_files:
        if (output_dir / plot_file).exists():
            summary_text.append(f"  ✓ {plot_file}")
        else:
            summary_text.append(f"  ✗ {plot_file} (missing)")
    
    # Save comprehensive summary
    summary_file = output_dir / "phase3_diagnostics_summary.txt"
    with open(summary_file, 'w') as f:
        f.write('\n'.join(summary_text))
    logger.info(f"Saved comprehensive summary to {summary_file}")


def main():
    """Main function."""
    args = parse_arguments()
    
    # Setup logging
    setup_logging(args.verbose)
    
    logger.info("Starting Phase 3 outputs visual diagnostics")
    logger.info(f"Grid definition file: {args.grid_definition_file}")
    logger.info(f"Scaling parameters file: {args.scaling_model_params_file}")
    logger.info(f"Voxel data file: {args.voxel_data_file}")
    logger.info(f"Output directory: {args.output_dir}")
    
    try:
        # Validate input files
        validate_input_files(args)
        
        # Ensure output directory exists
        output_dir = ensure_output_dir(args.output_dir)
        
        # Load input data
        logger.info("Loading input data...")
        grid_def = load_grid_definition(args.grid_definition_file)
        scaling_params = load_scaling_parameters(args.scaling_model_params_file)
        voxel_data = load_voxel_data(args.voxel_data_file)
        
        # Generate diagnostics
        logger.info("Generating diagnostics...")
        
        # Grid summary
        generate_grid_summary(grid_def, output_dir)
        
        # Voxel occupancy analysis
        occupancy_stats = generate_voxel_occupancy_plots(voxel_data, grid_def, output_dir)
        
        # Scaling parameter plots
        generate_scaling_parameter_plots(scaling_params, output_dir)
        
        # Merged voxel data visualization
        generate_merged_voxel_plots(voxel_data, grid_def, output_dir, args.max_plot_points)
        
        # Comprehensive summary
        input_files = {
            "Grid Definition": args.grid_definition_file,
            "Scaling Parameters": args.scaling_model_params_file,
            "Voxel Data": args.voxel_data_file
        }
        if args.experiments_list_file:
            input_files["Experiments List"] = args.experiments_list_file
        if args.corrected_pixel_data_dir:
            input_files["Pixel Data Directories"] = args.corrected_pixel_data_dir
        
        # Import pandas for timestamp (with fallback)
        try:
            import pandas as pd
        except ImportError:
            import datetime
            class pd:
                class Timestamp:
                    @staticmethod
                    def now():
                        return datetime.datetime.now()
        
        generate_comprehensive_summary(
            grid_def, scaling_params, voxel_data, 
            occupancy_stats, input_files, output_dir
        )
        
        # Clean up
        close_all_figures()
        
        logger.info("=== Phase 3 Visual Diagnostics Completed Successfully ===")
        logger.info(f"All diagnostic plots and reports saved to: {output_dir}")
        
        # List generated files
        logger.info("Generated files:")
        for file_path in sorted(output_dir.rglob("*")):
            if file_path.is_file():
                logger.info(f"  {file_path.name}")
        
    except Exception as e:
        logger.error(f"Phase 3 diagnostics failed: {e}")
        logger.error("Check the log for detailed error information")
        sys.exit(1)


if __name__ == "__main__":
    main()
</file>

<file path="tests/merging/__init__.py">
"""Tests for merging module."""
</file>

<file path="tests/orchestration/__init__.py">
# This file makes tests/orchestration a Python package.
</file>

<file path="tests/scaling/__init__.py">
"""Tests for scaling module."""
</file>

<file path="tests/scaling/test_diffuse_scaling_model.py">
"""
Tests for DiffuseScalingModel and components.
"""

import pytest
import numpy as np
from unittest.mock import Mock, patch

from dials.algorithms.scaling.active_parameter_managers import active_parameter_manager
from dials.array_family import flex

from diffusepipe.scaling.diffuse_scaling_model import DiffuseScalingModel
from diffusepipe.scaling.components.per_still_multiplier import PerStillMultiplierComponent
from diffusepipe.scaling.components.resolution_smoother import ResolutionSmootherComponent


class TestPerStillMultiplierComponent:
    """Test PerStillMultiplierComponent functionality."""
    
    @pytest.fixture
    def parameter_manager(self):
        """Create active parameter manager for testing."""
        # Create mock components and selection list for the parameter manager
        mock_components = {}  # Dictionary of components
        selection_list = []
        return active_parameter_manager(mock_components, selection_list)
    
    @pytest.fixture
    def component(self, parameter_manager):
        """Create component for testing."""
        still_ids = [1, 2, 3]
        return PerStillMultiplierComponent(parameter_manager, still_ids)
    
    def test_initialization(self, component):
        """Test component initialization."""
        assert component.n_params == 3
        assert component.n_stills == 3
        assert len(component.still_ids) == 3
        
        # Check initial parameters (should be 1.0)
        params = component.parameters
        assert len(params) == 3
        assert all(abs(p - 1.0) < 1e-6 for p in params)
    
    def test_initialization_with_custom_values(self, parameter_manager):
        """Test initialization with custom initial values."""
        still_ids = [1, 2]
        initial_values = {1: 1.5, 2: 0.8}
        
        component = PerStillMultiplierComponent(
            parameter_manager, still_ids, initial_values
        )
        
        assert component.get_scale_for_still(1) == 1.5
        assert component.get_scale_for_still(2) == 0.8
    
    def test_scale_calculation_flex_table(self, component):
        """Test scale calculation with DIALS-like reflection table."""
        # Mock reflection table
        reflection_table = Mock()
        still_ids = flex.int([1, 2, 3, 1, 2])
        reflection_table.get.return_value = still_ids
        
        scales, derivatives = component.calculate_scales_and_derivatives(reflection_table)
        
        assert len(scales) == 5
        assert len(derivatives) == 5
        assert all(abs(s - 1.0) < 1e-6 for s in scales)  # Initial scales
        assert all(abs(d - 1.0) < 1e-6 for d in derivatives)  # Unit derivatives
    
    def test_scale_calculation_dict_format(self, component):
        """Test scale calculation with dictionary format."""
        data_dict = {
            'still_ids': np.array([1, 2, 3, 1]),
            'intensities': np.array([100, 200, 150, 120])
        }
        
        scales, derivatives = component.calculate_scales_and_derivatives(data_dict)
        
        assert len(scales) == 4
        assert all(abs(s - 1.0) < 1e-6 for s in scales)
    
    def test_unknown_still_handling(self, component):
        """Test handling of unknown still IDs."""
        data_dict = {
            'still_ids': np.array([1, 999]),  # 999 is unknown
            'intensities': np.array([100, 200])
        }
        
        with patch('diffusepipe.scaling.components.per_still_multiplier.logger') as mock_logger:
            scales, derivatives = component.calculate_scales_and_derivatives(data_dict)
            
            assert len(scales) == 2
            assert abs(scales[0] - 1.0) < 1e-6  # Known still
            assert abs(scales[1] - 1.0) < 1e-6  # Unknown still (unity scale)
            
            # Should have logged warning
            mock_logger.warning.assert_called()
    
    def test_get_set_scale_for_still(self, component):
        """Test getting and setting scales for individual stills."""
        # Test getting initial scale
        assert component.get_scale_for_still(1) == 1.0
        
        # Test setting new scale
        component.set_scale_for_still(1, 1.5)
        assert component.get_scale_for_still(1) == 1.5
        
        # Test error for unknown still
        assert component.get_scale_for_still(999) == 1.0  # Default
        
        with pytest.raises(ValueError):
            component.set_scale_for_still(999, 1.5)
        
        # Test error for negative scale
        with pytest.raises(ValueError):
            component.set_scale_for_still(1, -0.5)
    
    def test_component_info(self, component):
        """Test component information retrieval."""
        info = component.get_component_info()
        
        required_keys = [
            'component_type', 'n_parameters', 'n_stills',
            'still_ids', 'current_scales', 'scale_statistics'
        ]
        
        for key in required_keys:
            assert key in info
        
        assert info['component_type'] == 'PerStillMultiplier'
        assert info['n_parameters'] == 3
        assert len(info['current_scales']) == 3


class TestResolutionSmootherComponent:
    """Test ResolutionSmootherComponent functionality."""
    
    @pytest.fixture
    def parameter_manager(self):
        """Create active parameter manager for testing."""
        # Create mock components and selection list for the parameter manager
        mock_components = {}  # Dictionary of components
        selection_list = []
        return active_parameter_manager(mock_components, selection_list)
    
    @pytest.fixture
    def component(self, parameter_manager):
        """Create component for testing."""
        return ResolutionSmootherComponent(
            parameter_manager, 
            n_control_points=3,
            resolution_range=(0.1, 1.0)
        )
    
    def test_initialization(self, component):
        """Test component initialization."""
        assert component.n_params == 3
        assert component.n_control_points == 3
        assert component.q_min == 0.1
        assert component.q_max == 1.0
        
        # Check initial parameters (should be 1.0)
        params = component.parameters
        assert len(params) == 3
        assert all(abs(p - 1.0) < 1e-6 for p in params)
    
    def test_v1_constraints(self, parameter_manager):
        """Test v1 parameter constraints."""
        # Test exceeding control point limit
        with pytest.raises(ValueError, match="v1 model allows ≤5 control points"):
            ResolutionSmootherComponent(
                parameter_manager, 
                n_control_points=6,
                resolution_range=(0.1, 1.0)
            )
        
        # Test invalid resolution range
        with pytest.raises(ValueError, match="Invalid resolution range"):
            ResolutionSmootherComponent(
                parameter_manager,
                n_control_points=3,
                resolution_range=(1.0, 0.1)  # Invalid: min >= max
            )
    
    def test_q_magnitude_extraction_dict(self, component):
        """Test q-magnitude extraction from dictionary format."""
        data_dict = {
            'q_vectors_lab': np.array([[0.1, 0.2, 0.3], [0.4, 0.5, 0.6]]),
            'still_ids': np.array([1, 2])
        }
        
        q_mags = component._extract_q_magnitudes(data_dict)
        
        assert len(q_mags) == 2
        expected_mag_1 = np.linalg.norm([0.1, 0.2, 0.3])
        expected_mag_2 = np.linalg.norm([0.4, 0.5, 0.6])
        assert abs(q_mags[0] - expected_mag_1) < 1e-6
        assert abs(q_mags[1] - expected_mag_2) < 1e-6
    
    def test_q_magnitude_extraction_direct(self, component):
        """Test q-magnitude extraction from direct q_magnitudes."""
        data_dict = {
            'q_magnitudes': [0.5, 0.8, 1.2],
            'still_ids': np.array([1, 2, 3])
        }
        
        q_mags = component._extract_q_magnitudes(data_dict)
        
        assert q_mags == [0.5, 0.8, 1.2]
    
    def test_scale_calculation(self, component):
        """Test resolution-dependent scale calculation."""
        data_dict = {
            'q_vectors_lab': np.array([[0.2, 0.0, 0.0], [0.5, 0.0, 0.0]]),
            'still_ids': np.array([1, 2])
        }
        
        scales, derivatives = component.calculate_scales_and_derivatives(data_dict)
        
        assert len(scales) == 2
        assert len(derivatives) == 2
        assert all(s > 0 for s in scales)  # Should be positive
    
    def test_get_scale_for_q(self, component):
        """Test getting scale for specific q-value."""
        scale_1 = component.get_scale_for_q(0.5)
        scale_2 = component.get_scale_for_q(0.8)
        
        assert scale_1 > 0
        assert scale_2 > 0
        
        # Test edge cases
        assert component.get_scale_for_q(0.0) == 1.0  # Invalid q
        assert component.get_scale_for_q(-0.1) == 1.0  # Negative q
    
    def test_component_info(self, component):
        """Test component information retrieval."""
        info = component.get_component_info()
        
        required_keys = [
            'component_type', 'n_parameters', 'n_control_points',
            'q_range', 'control_point_values', 'scale_statistics'
        ]
        
        for key in required_keys:
            assert key in info
        
        assert info['component_type'] == 'ResolutionSmoother'
        assert info['n_control_points'] == 3


class TestDiffuseScalingModel:
    """Test DiffuseScalingModel functionality."""
    
    @pytest.fixture
    def basic_config(self):
        """Create basic configuration for testing."""
        return {
            'still_ids': [1, 2, 3],
            'per_still_scale': {'enabled': True},
            'resolution_smoother': {'enabled': False},
            'experimental_components': {
                'panel_scale': {'enabled': False},
                'spatial_scale': {'enabled': False},
                'additive_offset': {'enabled': False}
            },
            'partiality_threshold': 0.1
        }
    
    @pytest.fixture
    def model(self, basic_config):
        """Create model for testing."""
        return DiffuseScalingModel(basic_config)
    
    def test_basic_initialization(self, model):
        """Test basic model initialization."""
        assert model.n_total_params == 3  # One per still
        assert 'per_still' in model.diffuse_components
        assert 'resolution' not in model.diffuse_components
        assert model.partiality_threshold == 0.1
    
    def test_v1_constraint_validation(self):
        """Test v1 constraint validation."""
        # Test forbidden component
        invalid_config = {
            'still_ids': [1, 2],
            'experimental_components': {
                'additive_offset': {'enabled': True}  # Forbidden in v1
            }
        }
        
        with pytest.raises(ValueError, match="additive_offset component is hard-disabled"):
            DiffuseScalingModel(invalid_config)
        
        # Test resolution smoother constraint
        invalid_config = {
            'still_ids': [1, 2],
            'resolution_smoother': {
                'enabled': True,
                'n_control_points': 6  # Exceeds limit
            }
        }
        
        with pytest.raises(ValueError, match="resolution smoother limited to ≤5 points"):
            DiffuseScalingModel(invalid_config)
    
    def test_parameter_limit_enforcement(self):
        """Test parameter limit enforcement."""
        # Create config that would exceed parameter limit
        many_stills = list(range(100))  # 100 stills
        config = {
            'still_ids': many_stills,
            'per_still_scale': {'enabled': True},
            'resolution_smoother': {
                'enabled': True,
                'n_control_points': 5  # Max allowed
            }
        }
        
        # Should exceed MAX_FREE_PARAMS_BASE + n_stills = 5 + 100 = 105
        # Total would be 100 (stills) + 5 (resolution) = 105, which should be OK
        model = DiffuseScalingModel(config)
        assert model.n_total_params == 105
    
    def test_get_scales_for_observation(self, model):
        """Test getting scales for individual observations."""
        mult_scale, add_offset = model.get_scales_for_observation(1, 0.5)
        
        assert mult_scale > 0
        assert add_offset == 0.0  # Always 0 in v1
    
    def test_model_with_resolution_smoother(self):
        """Test model with resolution smoother enabled."""
        config = {
            'still_ids': [1, 2],
            'per_still_scale': {'enabled': True},
            'resolution_smoother': {
                'enabled': True,
                'n_control_points': 3,
                'resolution_range': (0.1, 1.0)
            }
        }
        
        model = DiffuseScalingModel(config)
        
        assert model.n_total_params == 5  # 2 stills + 3 resolution points
        assert 'resolution' in model.diffuse_components
    
    def test_generate_references(self, model):
        """Test reference generation."""
        # Create mock binned data
        binned_data = {
            0: {
                'observations': [
                    {
                        'intensity': 100.0,
                        'sigma': 10.0,
                        'still_id': 1,
                        'q_vector_lab': np.array([0.1, 0.1, 0.1])
                    },
                    {
                        'intensity': 120.0,
                        'sigma': 12.0,
                        'still_id': 2,
                        'q_vector_lab': np.array([0.1, 0.1, 0.1])
                    }
                ]
            }
        }
        
        bragg_refs, diffuse_refs = model.generate_references(binned_data, {})
        
        assert isinstance(diffuse_refs, dict)
        assert 0 in diffuse_refs
        assert 'intensity' in diffuse_refs[0]
        assert 'sigma' in diffuse_refs[0]
        assert 'n_observations' in diffuse_refs[0]
    
    def test_model_info(self, model):
        """Test model information retrieval."""
        info = model.get_model_info()
        
        required_keys = [
            'model_type', 'n_total_params', 'components',
            'refinement_statistics', 'partiality_threshold'
        ]
        
        for key in required_keys:
            assert key in info
        
        assert info['model_type'] == 'DiffuseScalingModel_v1'
        assert info['n_total_params'] == 3
</file>

<file path="tests/visual_diagnostics/test_check_phase3_outputs.py">
"""
Tests for Phase 3 visual diagnostics script.

This module tests the check_phase3_outputs.py script functionality using
synthetic Phase 3 output data to verify plot generation and summary creation.
"""

import json
import tempfile
import pytest
import numpy as np
from pathlib import Path
from unittest.mock import patch, MagicMock
import sys

# Add the scripts directory to the path for importing the script
sys.path.insert(0, str(Path(__file__).resolve().parent.parent.parent / "scripts" / "visual_diagnostics"))

try:
    from check_phase3_outputs import (
        load_grid_definition,
        load_scaling_parameters,
        load_voxel_data,
        generate_grid_summary,
        generate_voxel_occupancy_plots,
        generate_scaling_parameter_plots,
        generate_merged_voxel_plots,
        generate_comprehensive_summary,
    )
except ImportError as e:
    pytest.skip(f"Could not import check_phase3_outputs module: {e}", allow_module_level=True)


@pytest.fixture
def synthetic_grid_definition():
    """Create synthetic grid definition data."""
    return {
        "crystal_avg_ref": {
            "unit_cell_params": [78.0, 78.0, 37.0, 90.0, 90.0, 90.0],
            "space_group": "P43212",
        },
        "hkl_bounds": {
            "h_min": -50, "h_max": 50,
            "k_min": -50, "k_max": 50,  
            "l_min": -25, "l_max": 25
        },
        "ndiv_h": 100,
        "ndiv_k": 100,
        "ndiv_l": 50,
        "total_voxels": 500000,
    }


@pytest.fixture
def synthetic_scaling_parameters():
    """Create synthetic scaling parameters data."""
    return {
        "refined_parameters": {
            "still_0": {
                "multiplicative_scale": 1.05,
                "additive_offset": 0.0
            },
            "still_1": {
                "multiplicative_scale": 0.95,
                "additive_offset": 0.0
            },
            "still_2": {
                "multiplicative_scale": 1.02,
                "additive_offset": 0.0
            }
        },
        "refinement_statistics": {
            "n_iterations": 5,
            "final_r_factor": 0.15,
            "convergence_achieved": True,
            "parameter_shifts": {"multiplicative_scale": 0.001}
        },
        "resolution_smoother": {
            "enabled": False,
            "control_points": []
        }
    }


@pytest.fixture
def synthetic_voxel_data():
    """Create synthetic voxel data."""
    n_voxels = 1000
    np.random.seed(42)  # For reproducible tests
    
    return {
        "voxel_indices": np.arange(n_voxels),
        "H_center": np.random.randint(-50, 51, n_voxels),
        "K_center": np.random.randint(-50, 51, n_voxels),
        "L_center": np.random.randint(-25, 26, n_voxels),
        "q_center_x": np.random.uniform(-2, 2, n_voxels),
        "q_center_y": np.random.uniform(-2, 2, n_voxels),
        "q_center_z": np.random.uniform(-1, 1, n_voxels),
        "q_magnitude_center": np.random.uniform(0.1, 2.5, n_voxels),
        "I_merged_relative": np.random.exponential(100, n_voxels),
        "Sigma_merged_relative": np.random.exponential(10, n_voxels),
        "num_observations": np.random.poisson(5, n_voxels) + 1,
    }


class TestDataLoading:
    """Test data loading functions."""
    
    def test_load_grid_definition(self, synthetic_grid_definition):
        """Test loading grid definition from JSON file."""
        with tempfile.NamedTemporaryFile(mode='w', suffix='.json', delete=False) as f:
            json.dump(synthetic_grid_definition, f)
            temp_file = f.name
        
        try:
            loaded_data = load_grid_definition(temp_file)
            assert loaded_data == synthetic_grid_definition
            assert "crystal_avg_ref" in loaded_data
            assert "hkl_bounds" in loaded_data
            assert "total_voxels" in loaded_data
        finally:
            Path(temp_file).unlink()
    
    def test_load_grid_definition_missing_key(self):
        """Test loading grid definition with missing required key."""
        incomplete_data = {"crystal_avg_ref": {}}
        
        with tempfile.NamedTemporaryFile(mode='w', suffix='.json', delete=False) as f:
            json.dump(incomplete_data, f)
            temp_file = f.name
        
        try:
            with pytest.raises(KeyError, match="Required key.*not found"):
                load_grid_definition(temp_file)
        finally:
            Path(temp_file).unlink()
    
    def test_load_scaling_parameters(self, synthetic_scaling_parameters):
        """Test loading scaling parameters from JSON file."""
        with tempfile.NamedTemporaryFile(mode='w', suffix='.json', delete=False) as f:
            json.dump(synthetic_scaling_parameters, f)
            temp_file = f.name
        
        try:
            loaded_data = load_scaling_parameters(temp_file)
            assert loaded_data == synthetic_scaling_parameters
            assert "refined_parameters" in loaded_data
            assert "refinement_statistics" in loaded_data
        finally:
            Path(temp_file).unlink()
    
    def test_load_voxel_data_npz(self, synthetic_voxel_data):
        """Test loading voxel data from NPZ file."""
        with tempfile.NamedTemporaryFile(suffix='.npz', delete=False) as f:
            np.savez_compressed(f.name, **synthetic_voxel_data)
            temp_file = f.name
        
        try:
            loaded_data = load_voxel_data(temp_file)
            
            # Check all required keys are present
            for key in synthetic_voxel_data.keys():
                assert key in loaded_data
                np.testing.assert_array_equal(loaded_data[key], synthetic_voxel_data[key])
        finally:
            Path(temp_file).unlink()
    
    def test_load_voxel_data_missing_key(self):
        """Test loading voxel data with missing required key."""
        incomplete_data = {"voxel_indices": np.array([1, 2, 3])}
        
        with tempfile.NamedTemporaryFile(suffix='.npz', delete=False) as f:
            np.savez_compressed(f.name, **incomplete_data)
            temp_file = f.name
        
        try:
            with pytest.raises(KeyError, match="Required key.*not found"):
                load_voxel_data(temp_file)
        finally:
            Path(temp_file).unlink()


class TestPlotGeneration:
    """Test plot generation functions."""
    
    @patch('matplotlib.pyplot.savefig')
    @patch('matplotlib.pyplot.close')
    def test_generate_grid_summary(self, mock_close, mock_savefig, synthetic_grid_definition):
        """Test grid summary generation."""
        with tempfile.TemporaryDirectory() as temp_dir:
            output_dir = Path(temp_dir)
            
            generate_grid_summary(synthetic_grid_definition, output_dir)
            
            # Check that text summary was created
            summary_file = output_dir / "grid_summary.txt"
            assert summary_file.exists()
            
            # Verify content
            content = summary_file.read_text()
            assert "Global Voxel Grid Summary" in content
            assert "78.0" in content  # Unit cell parameter
            assert "P43212" in content  # Space group
            assert "500000" in content  # Total voxels
            
            # Check that plot was attempted to be saved
            mock_savefig.assert_called()
            mock_close.assert_called()
    
    @patch('matplotlib.pyplot.savefig')
    @patch('matplotlib.pyplot.close')
    def test_generate_voxel_occupancy_plots(self, mock_close, mock_savefig, 
                                           synthetic_voxel_data, synthetic_grid_definition):
        """Test voxel occupancy plots generation."""
        with tempfile.TemporaryDirectory() as temp_dir:
            output_dir = Path(temp_dir)
            
            stats = generate_voxel_occupancy_plots(
                synthetic_voxel_data, synthetic_grid_definition, output_dir
            )
            
            # Check returned statistics
            assert isinstance(stats, dict)
            assert "min_observations" in stats
            assert "max_observations" in stats
            assert "mean_observations" in stats
            assert "total_voxels" in stats
            
            # Verify statistics are reasonable
            assert stats["min_observations"] >= 0
            assert stats["max_observations"] >= stats["min_observations"]
            assert stats["total_voxels"] == len(synthetic_voxel_data["num_observations"])
            
            # Check that plots were attempted to be saved
            assert mock_savefig.call_count >= 3  # At least slice plots + histogram
    
    @patch('matplotlib.pyplot.savefig')
    @patch('matplotlib.pyplot.close')
    def test_generate_scaling_parameter_plots(self, mock_close, mock_savefig, 
                                            synthetic_scaling_parameters):
        """Test scaling parameter plots generation."""
        with tempfile.TemporaryDirectory() as temp_dir:
            output_dir = Path(temp_dir)
            
            generate_scaling_parameter_plots(synthetic_scaling_parameters, output_dir)
            
            # Check that summary file was created
            summary_file = output_dir / "scaling_parameters_summary.txt"
            assert summary_file.exists()
            
            # Verify content
            content = summary_file.read_text()
            assert "Scaling Model Parameters Summary" in content
            assert "Number of Stills: 3" in content
            
            # Check that plots were attempted to be saved
            mock_savefig.assert_called()
    
    @patch('matplotlib.pyplot.savefig')
    @patch('matplotlib.pyplot.close')
    def test_generate_merged_voxel_plots(self, mock_close, mock_savefig,
                                       synthetic_voxel_data, synthetic_grid_definition):
        """Test merged voxel data plots generation."""
        with tempfile.TemporaryDirectory() as temp_dir:
            output_dir = Path(temp_dir)
            max_plot_points = 500
            
            generate_merged_voxel_plots(
                synthetic_voxel_data, synthetic_grid_definition, 
                output_dir, max_plot_points
            )
            
            # Check that multiple plots were attempted to be saved
            # Should include: intensity slices, sigma slices, I/sigma slices, 
            # radial average, intensity histogram
            assert mock_savefig.call_count >= 8


class TestSummaryGeneration:
    """Test summary generation functions."""
    
    @patch('check_phase3_outputs.pd')  # Mock pandas import
    def test_generate_comprehensive_summary(self, mock_pd, synthetic_grid_definition,
                                          synthetic_scaling_parameters, synthetic_voxel_data):
        """Test comprehensive summary generation."""
        # Setup mock timestamp
        mock_timestamp = MagicMock()
        mock_timestamp.now.return_value.strftime.return_value = "2023-12-01 10:00:00"
        mock_pd.Timestamp = mock_timestamp
        
        with tempfile.TemporaryDirectory() as temp_dir:
            output_dir = Path(temp_dir)
            
            # Create synthetic occupancy stats
            occupancy_stats = {
                "min_observations": 1,
                "max_observations": 15,
                "mean_observations": 5.2,
                "median_observations": 5.0,
                "total_observations": 5200,
                "voxels_with_data": 1000,
                "total_voxels": 1000,
                "percent_voxels_lt_3": 15.0
            }
            
            input_files = {
                "Grid Definition": "test_grid.json",
                "Scaling Parameters": "test_scaling.json",
                "Voxel Data": "test_voxel.npz"
            }
            
            generate_comprehensive_summary(
                synthetic_grid_definition, synthetic_scaling_parameters,
                synthetic_voxel_data, occupancy_stats, input_files, output_dir
            )
            
            # Check that summary file was created
            summary_file = output_dir / "phase3_diagnostics_summary.txt"
            assert summary_file.exists()
            
            # Verify content
            content = summary_file.read_text()
            assert "Phase 3 Diagnostics Summary Report" in content
            assert "Generated on: 2023-12-01 10:00:00" in content
            assert "Global Voxel Grid:" in content
            assert "Voxel Occupancy:" in content
            assert "Relative Scaling:" in content
            assert "Merged Intensities:" in content
            assert "Resolution Coverage:" in content
            assert "Generated Diagnostic Plots:" in content


class TestErrorHandling:
    """Test error handling in diagnostic functions."""
    
    def test_load_grid_definition_file_not_found(self):
        """Test loading grid definition with non-existent file."""
        with pytest.raises(FileNotFoundError):
            load_grid_definition("nonexistent_file.json")
    
    def test_load_voxel_data_unsupported_format(self):
        """Test loading voxel data with unsupported file format."""
        with tempfile.NamedTemporaryFile(suffix='.txt', delete=False) as f:
            f.write(b"dummy content")
            temp_file = f.name
        
        try:
            with pytest.raises(ValueError, match="must be .npz or .hdf5"):
                load_voxel_data(temp_file)
        finally:
            Path(temp_file).unlink()


@pytest.mark.integration
class TestEndToEndWorkflow:
    """Integration tests for end-to-end workflow."""
    
    @patch('matplotlib.pyplot.savefig')
    @patch('matplotlib.pyplot.close') 
    def test_full_diagnostic_workflow(self, mock_close, mock_savefig,
                                    synthetic_grid_definition, synthetic_scaling_parameters,
                                    synthetic_voxel_data):
        """Test the complete diagnostic workflow."""
        with tempfile.TemporaryDirectory() as temp_dir:
            temp_path = Path(temp_dir)
            
            # Create input files
            grid_file = temp_path / "grid_def.json"
            scaling_file = temp_path / "scaling_params.json"
            voxel_file = temp_path / "voxel_data.npz"
            output_dir = temp_path / "diagnostics"
            
            with open(grid_file, 'w') as f:
                json.dump(synthetic_grid_definition, f)
            
            with open(scaling_file, 'w') as f:
                json.dump(synthetic_scaling_parameters, f)
            
            np.savez_compressed(voxel_file, **synthetic_voxel_data)
            
            output_dir.mkdir()
            
            # Load data
            grid_def = load_grid_definition(str(grid_file))
            scaling_params = load_scaling_parameters(str(scaling_file))
            voxel_data = load_voxel_data(str(voxel_file))
            
            # Generate all diagnostics
            generate_grid_summary(grid_def, output_dir)
            occupancy_stats = generate_voxel_occupancy_plots(voxel_data, grid_def, output_dir)
            generate_scaling_parameter_plots(scaling_params, output_dir)
            generate_merged_voxel_plots(voxel_data, grid_def, output_dir, 500)
            
            # Verify expected files were created
            expected_files = [
                "grid_summary.txt",
                "scaling_parameters_summary.txt"
            ]
            
            for filename in expected_files:
                assert (output_dir / filename).exists(), f"{filename} was not created"
            
            # Verify plots were attempted to be saved
            assert mock_savefig.call_count > 10  # Multiple plots should be generated
</file>

<file path="tests/voxelization/__init__.py">
"""Tests for voxelization module."""
</file>

<file path="tests/voxelization/test_global_voxel_grid.py">
"""
Tests for GlobalVoxelGrid implementation.
"""

import pytest
import numpy as np
from unittest.mock import Mock, patch

from scitbx import matrix
from cctbx import uctbx
from dxtbx.model import Experiment, Crystal, Beam, Detector

from diffusepipe.voxelization.global_voxel_grid import (
    GlobalVoxelGrid, 
    GlobalVoxelGridConfig, 
    CorrectedDiffusePixelData
)


class TestGlobalVoxelGrid:
    """Test GlobalVoxelGrid functionality."""

    @pytest.fixture
    def mock_crystal_models(self):
        """Create mock crystal models for testing."""
        crystals = []
        for i in range(3):
            crystal = Mock(spec=Crystal)
            # Mock unit cell with slight variations
            unit_cell = uctbx.unit_cell((
                10.0 + i * 0.1,  # a
                15.0 + i * 0.1,  # b 
                20.0 + i * 0.1,  # c
                90.0,            # alpha
                90.0,            # beta
                90.0             # gamma
            ))
            crystal.get_unit_cell.return_value = unit_cell
            
            # Mock U matrix (identity with small rotations)
            u_matrix = matrix.rec((
                1.0, 0.01*i, 0.0,
                -0.01*i, 1.0, 0.0,
                0.0, 0.0, 1.0
            ), (3, 3))
            crystal.get_U.return_value = u_matrix
            
            # Mock space group
            from cctbx.sgtbx import space_group_info
            crystal.get_space_group.return_value = space_group_info("P1").group()
            
            crystals.append(crystal)
        
        return crystals

    @pytest.fixture
    def mock_experiments(self, mock_crystal_models):
        """Create mock experiments with crystal models."""
        experiments = []
        for crystal in mock_crystal_models:
            exp = Mock(spec=Experiment)
            exp.crystal = crystal
            experiments.append(exp)
        return experiments

    @pytest.fixture  
    def sample_diffuse_data(self):
        """Create sample corrected diffuse pixel data."""
        # Generate sample q-vectors in a reasonable range
        n_points = 100
        q_vectors = np.random.normal(0, 0.5, (n_points, 3))  # Around origin
        intensities = np.random.exponential(100, n_points)
        sigmas = np.sqrt(intensities) + 1.0
        still_ids = np.random.randint(0, 3, n_points)
        
        return [CorrectedDiffusePixelData(
            q_vectors=q_vectors,
            intensities=intensities,
            sigmas=sigmas,
            still_ids=still_ids
        )]

    @pytest.fixture
    def grid_config(self):
        """Create test grid configuration."""
        return GlobalVoxelGridConfig(
            d_min_target=1.0,
            d_max_target=10.0,
            ndiv_h=2,
            ndiv_k=2, 
            ndiv_l=2,
            max_rms_delta_hkl=0.1
        )

    def test_grid_initialization(self, mock_experiments, sample_diffuse_data, grid_config):
        """Test basic grid initialization."""
        grid = GlobalVoxelGrid(mock_experiments, sample_diffuse_data, grid_config)
        
        assert grid.crystal_avg_ref is not None
        assert grid.A_avg_ref is not None
        assert hasattr(grid, 'hkl_min')
        assert hasattr(grid, 'hkl_max')
        assert grid.total_voxels > 0

    def test_config_validation(self, mock_experiments, sample_diffuse_data):
        """Test grid configuration validation."""
        # Test invalid d_min
        with pytest.raises(ValueError, match="d_min_target must be positive"):
            invalid_config = GlobalVoxelGridConfig(
                d_min_target=-1.0, d_max_target=10.0,
                ndiv_h=2, ndiv_k=2, ndiv_l=2
            )
            GlobalVoxelGrid(mock_experiments, sample_diffuse_data, invalid_config)

        # Test invalid d_max
        with pytest.raises(ValueError, match="d_max_target must be greater than d_min_target"):
            invalid_config = GlobalVoxelGridConfig(
                d_min_target=10.0, d_max_target=5.0,
                ndiv_h=2, ndiv_k=2, ndiv_l=2
            )
            GlobalVoxelGrid(mock_experiments, sample_diffuse_data, invalid_config)

        # Test invalid subdivisions
        with pytest.raises(ValueError, match="Grid subdivisions must be positive"):
            invalid_config = GlobalVoxelGridConfig(
                d_min_target=1.0, d_max_target=10.0,
                ndiv_h=0, ndiv_k=2, ndiv_l=2
            )
            GlobalVoxelGrid(mock_experiments, sample_diffuse_data, invalid_config)

    def test_input_validation(self, sample_diffuse_data, grid_config):
        """Test input validation."""
        # Test empty experiment list
        with pytest.raises(ValueError, match="experiment_list cannot be empty"):
            GlobalVoxelGrid([], sample_diffuse_data, grid_config)

        # Test empty diffuse data
        with pytest.raises(ValueError, match="corrected_diffuse_pixel_data cannot be empty"):
            mock_exp = Mock(spec=Experiment)
            mock_exp.crystal = Mock(spec=Crystal)
            GlobalVoxelGrid([mock_exp], [], grid_config)

    def test_hkl_voxel_conversion(self, mock_experiments, sample_diffuse_data, grid_config):
        """Test HKL to voxel index conversion and back."""
        grid = GlobalVoxelGrid(mock_experiments, sample_diffuse_data, grid_config)
        
        # Test conversion consistency
        test_hkl = (0.5, -0.3, 1.2)
        voxel_idx = grid.hkl_to_voxel_idx(*test_hkl)
        recovered_hkl = grid.voxel_idx_to_hkl_center(voxel_idx)
        
        # Should be close due to rounding
        assert abs(recovered_hkl[0] - test_hkl[0]) < 1.0
        assert abs(recovered_hkl[1] - test_hkl[1]) < 1.0
        assert abs(recovered_hkl[2] - test_hkl[2]) < 1.0

    def test_q_vector_calculation(self, mock_experiments, sample_diffuse_data, grid_config):
        """Test q-vector calculation for voxel centers.""" 
        grid = GlobalVoxelGrid(mock_experiments, sample_diffuse_data, grid_config)
        
        # Test first voxel
        voxel_idx = 0
        q_vector = grid.get_q_vector_for_voxel_center(voxel_idx)
        
        assert hasattr(q_vector, 'length')
        assert q_vector.length() >= 0

    def test_crystal_averaging_diagnostics(self, mock_experiments, sample_diffuse_data, grid_config):
        """Test crystal averaging diagnostic calculations."""
        grid = GlobalVoxelGrid(mock_experiments, sample_diffuse_data, grid_config)
        
        diagnostics = grid.get_crystal_averaging_diagnostics()
        
        required_keys = [
            "rms_misorientation_deg", "n_crystals_averaged", 
            "hkl_range_min", "hkl_range_max", "total_voxels"
        ]
        
        for key in required_keys:
            assert key in diagnostics
        
        assert diagnostics["n_crystals_averaged"] == 3
        assert diagnostics["total_voxels"] > 0

    def test_unit_cell_averaging(self, mock_crystal_models):
        """Test unit cell averaging functionality.""" 
        # Create a minimal grid to test unit cell averaging
        experiments = []
        for crystal in mock_crystal_models:
            exp = Mock(spec=Experiment)
            exp.crystal = crystal
            experiments.append(exp)
        
        # Simple diffuse data
        diffuse_data = [CorrectedDiffusePixelData(
            q_vectors=np.array([[0.1, 0.1, 0.1]]),
            intensities=np.array([100.0]),
            sigmas=np.array([10.0]),
            still_ids=np.array([0])
        )]
        
        config = GlobalVoxelGridConfig(
            d_min_target=1.0, d_max_target=10.0,
            ndiv_h=1, ndiv_k=1, ndiv_l=1
        )
        
        grid = GlobalVoxelGrid(experiments, diffuse_data, config)
        
        # Check that averaging happened
        assert grid.crystal_avg_ref is not None
        assert grid.diagnostics["n_crystals_averaged"] == 3

    def test_orientation_spread_warning(self, mock_experiments, sample_diffuse_data, grid_config):
        """Test orientation spread warning for large misorientations."""
        # Modify one crystal to have large misorientation
        large_rotation = matrix.rec((
            0.8, 0.6, 0.0,
            -0.6, 0.8, 0.0,
            0.0, 0.0, 1.0
        ), (3, 3))
        mock_experiments[2].crystal.get_U.return_value = large_rotation
        
        with patch('diffusepipe.voxelization.global_voxel_grid.logger') as mock_logger:
            grid = GlobalVoxelGrid(mock_experiments, sample_diffuse_data, grid_config)
            
            # Check if warning was logged (misorientation should be large)
            warning_calls = [call for call in mock_logger.warning.call_args_list 
                           if 'misorientation' in str(call)]
            assert len(warning_calls) > 0 or grid.rms_misorientation_deg > 5.0
</file>

<file path="tests/voxelization/test_voxel_accumulator.py">
"""
Tests for VoxelAccumulator implementation.
"""

import pytest
import numpy as np
import tempfile
import os
from unittest.mock import Mock, patch

from cctbx import sgtbx
from scitbx import matrix

from diffusepipe.voxelization.voxel_accumulator import VoxelAccumulator, ObservationData
from diffusepipe.voxelization.global_voxel_grid import GlobalVoxelGrid, GlobalVoxelGridConfig


class TestVoxelAccumulator:
    """Test VoxelAccumulator functionality."""

    @pytest.fixture
    def mock_grid(self):
        """Create mock GlobalVoxelGrid for testing."""
        grid = Mock(spec=GlobalVoxelGrid)
        
        # Mock A matrix inverse for HKL transformation
        A_matrix = matrix.rec((
            1.0, 0.0, 0.0,
            0.0, 1.0, 0.0, 
            0.0, 0.0, 1.0
        ), (3, 3))
        grid.A_avg_ref = A_matrix
        
        # Mock voxel mapping methods
        def mock_hkl_to_voxel(h, k, l):
            # Simple mapping for testing
            if abs(h) > 5 or abs(k) > 5 or abs(l) > 5:
                raise ValueError("Outside grid")
            return int(h + 5) * 121 + int(k + 5) * 11 + int(l + 5)
        
        def mock_voxel_to_hkl(voxel_idx):
            l = voxel_idx % 11 - 5
            k = (voxel_idx // 11) % 11 - 5
            h = (voxel_idx // 121) - 5
            return float(h), float(k), float(l)
        
        def mock_get_q_for_voxel(voxel_idx):
            h, k, l = mock_voxel_to_hkl(voxel_idx)
            return matrix.col((h * 0.1, k * 0.1, l * 0.1))
        
        grid.hkl_to_voxel_idx.side_effect = mock_hkl_to_voxel
        grid.voxel_idx_to_hkl_center.side_effect = mock_voxel_to_hkl
        grid.get_q_vector_for_voxel_center.side_effect = mock_get_q_for_voxel
        
        return grid

    @pytest.fixture
    def space_group_info(self):
        """Create space group info for testing."""
        return sgtbx.space_group_info("P1")

    @pytest.fixture
    def sample_observations(self):
        """Create sample observation data."""
        n_obs = 50
        # Create q-vectors near origin to stay within mock grid bounds
        q_vectors = np.random.normal(0, 0.3, (n_obs, 3))
        intensities = np.random.exponential(100, n_obs)
        sigmas = np.sqrt(intensities) + 1.0
        still_id = 1
        
        return still_id, q_vectors, intensities, sigmas

    def test_memory_backend_initialization(self, mock_grid, space_group_info):
        """Test initialization with memory backend."""
        accumulator = VoxelAccumulator(
            mock_grid, space_group_info, backend="memory"
        )
        
        assert accumulator.backend == "memory"
        assert accumulator.n_total_observations == 0
        assert hasattr(accumulator, '_voxel_data')

    @pytest.mark.skipif(
        not hasattr(VoxelAccumulator, 'HDF5_AVAILABLE') or 
        not getattr(VoxelAccumulator, 'HDF5_AVAILABLE', False),
        reason="h5py not available"
    )
    def test_hdf5_backend_initialization(self, mock_grid, space_group_info):
        """Test initialization with HDF5 backend."""
        with tempfile.TemporaryDirectory() as temp_dir:
            storage_path = os.path.join(temp_dir, "test.h5")
            
            accumulator = VoxelAccumulator(
                mock_grid, space_group_info, 
                backend="hdf5", storage_path=storage_path
            )
            
            assert accumulator.backend == "hdf5"
            assert accumulator.storage_path == storage_path
            assert hasattr(accumulator, 'h5_file')
            
            accumulator.finalize()

    def test_invalid_backend(self, mock_grid, space_group_info):
        """Test error with invalid backend."""
        with pytest.raises(ValueError, match="Unknown backend"):
            VoxelAccumulator(mock_grid, space_group_info, backend="invalid")

    def test_add_observations_memory(self, mock_grid, space_group_info, sample_observations):
        """Test adding observations with memory backend."""
        accumulator = VoxelAccumulator(
            mock_grid, space_group_info, backend="memory"
        )
        
        still_id, q_vectors, intensities, sigmas = sample_observations
        
        n_binned = accumulator.add_observations(
            still_id, q_vectors, intensities, sigmas
        )
        
        assert n_binned > 0
        assert accumulator.n_total_observations == n_binned
        assert len(accumulator._voxel_data) > 0

    def test_add_observations_array_mismatch(self, mock_grid, space_group_info):
        """Test error with mismatched array lengths."""
        accumulator = VoxelAccumulator(
            mock_grid, space_group_info, backend="memory"
        )
        
        q_vectors = np.random.normal(0, 0.1, (10, 3))
        intensities = np.random.exponential(100, 5)  # Wrong length
        sigmas = np.sqrt(intensities)
        
        with pytest.raises(ValueError, match="Array length mismatch"):
            accumulator.add_observations(1, q_vectors, intensities, sigmas)

    def test_get_observations_for_voxel_memory(self, mock_grid, space_group_info):
        """Test retrieving observations for specific voxel."""
        accumulator = VoxelAccumulator(
            mock_grid, space_group_info, backend="memory"
        )
        
        # Add some test observations
        q_vectors = np.array([[0.1, 0.1, 0.1], [0.2, 0.2, 0.2]])
        intensities = np.array([100.0, 200.0])
        sigmas = np.array([10.0, 15.0])
        
        accumulator.add_observations(1, q_vectors, intensities, sigmas)
        
        # Get observations for a voxel that should exist
        voxel_idx = list(accumulator._voxel_data.keys())[0]
        voxel_obs = accumulator.get_observations_for_voxel(voxel_idx)
        
        assert voxel_obs["n_observations"] > 0
        assert len(voxel_obs["intensities"]) == voxel_obs["n_observations"]
        assert voxel_obs["q_vectors_lab"].shape == (voxel_obs["n_observations"], 3)

    def test_get_observations_empty_voxel(self, mock_grid, space_group_info):
        """Test retrieving observations from empty voxel."""
        accumulator = VoxelAccumulator(
            mock_grid, space_group_info, backend="memory"
        )
        
        # Request non-existent voxel
        voxel_obs = accumulator.get_observations_for_voxel(999999)
        
        assert voxel_obs["n_observations"] == 0
        assert len(voxel_obs["intensities"]) == 0
        assert voxel_obs["q_vectors_lab"].shape == (0, 3)

    def test_get_all_binned_data_for_scaling(self, mock_grid, space_group_info, sample_observations):
        """Test getting complete binned dataset."""
        accumulator = VoxelAccumulator(
            mock_grid, space_group_info, backend="memory"
        )
        
        still_id, q_vectors, intensities, sigmas = sample_observations
        accumulator.add_observations(still_id, q_vectors, intensities, sigmas)
        
        binned_data = accumulator.get_all_binned_data_for_scaling()
        
        assert isinstance(binned_data, dict)
        assert len(binned_data) > 0
        
        # Check data structure
        for voxel_idx, voxel_data in binned_data.items():
            assert "observations" in voxel_data
            assert "n_observations" in voxel_data
            assert "voxel_center_hkl" in voxel_data
            assert "voxel_center_q" in voxel_data
            
            # Check observations format
            for obs in voxel_data["observations"]:
                assert "intensity" in obs
                assert "sigma" in obs
                assert "still_id" in obs
                assert "q_vector_lab" in obs

    def test_accumulation_statistics(self, mock_grid, space_group_info, sample_observations):
        """Test statistics calculation."""
        accumulator = VoxelAccumulator(
            mock_grid, space_group_info, backend="memory"
        )
        
        still_id, q_vectors, intensities, sigmas = sample_observations
        n_binned = accumulator.add_observations(still_id, q_vectors, intensities, sigmas)
        
        stats = accumulator.get_accumulation_statistics()
        
        required_keys = [
            "total_observations", "unique_voxels", 
            "observations_per_voxel_stats", "still_distribution", "backend"
        ]
        
        for key in required_keys:
            assert key in stats
        
        assert stats["total_observations"] == n_binned
        assert stats["unique_voxels"] > 0
        assert stats["backend"] == "memory"
        assert still_id in stats["still_distribution"]

    def test_multiple_stills(self, mock_grid, space_group_info):
        """Test accumulating observations from multiple stills."""
        accumulator = VoxelAccumulator(
            mock_grid, space_group_info, backend="memory"
        )
        
        # Add observations from multiple stills
        for still_id in range(3):
            q_vectors = np.random.normal(0, 0.2, (20, 3))
            intensities = np.random.exponential(100, 20)
            sigmas = np.sqrt(intensities) + 1.0
            
            accumulator.add_observations(still_id, q_vectors, intensities, sigmas)
        
        stats = accumulator.get_accumulation_statistics()
        
        assert len(stats["still_distribution"]) == 3
        assert stats["total_observations"] > 0

    def test_asu_mapping(self, mock_grid, space_group_info):
        """Test ASU mapping functionality."""
        accumulator = VoxelAccumulator(
            mock_grid, space_group_info, backend="memory"
        )
        
        # Test with simple HKL array
        hkl_array = np.array([[1, 2, 3], [-1, -2, -3]])
        hkl_asu = accumulator._map_to_asu(hkl_array)
        
        assert hkl_asu.shape == hkl_array.shape
        assert hkl_asu.dtype == float

    def test_empty_observations(self, mock_grid, space_group_info):
        """Test handling of empty observation arrays."""
        accumulator = VoxelAccumulator(
            mock_grid, space_group_info, backend="memory"
        )
        
        # Add empty arrays
        empty_q = np.array([]).reshape(0, 3)
        empty_i = np.array([])
        empty_s = np.array([])
        
        n_binned = accumulator.add_observations(1, empty_q, empty_i, empty_s)
        assert n_binned == 0

    def test_out_of_bounds_observations(self, mock_grid, space_group_info):
        """Test handling observations outside grid bounds."""
        accumulator = VoxelAccumulator(
            mock_grid, space_group_info, backend="memory"
        )
        
        # Create q-vectors that will be outside mock grid bounds
        q_vectors = np.array([[10.0, 10.0, 10.0]])  # Large values
        intensities = np.array([100.0])
        sigmas = np.array([10.0])
        
        n_binned = accumulator.add_observations(1, q_vectors, intensities, sigmas)
        
        # Should handle gracefully with fewer binned observations
        assert n_binned >= 0
        assert n_binned <= len(q_vectors)
</file>

<file path="critical.md">
the following project files are 'critical' and should always be included as context, regardless of the specific task. They provide the fundamental "rules of the road," project goals, and architectural principles.

These files can be categorized into three groups: **Core Plans**, **Project Rules & Conventions**, and **Essential Guides**.

### 1. Core Plans & High-Level Strategy

These files define the "what" and "why" of the project. An agent cannot understand the purpose of its task without them.

*   `plan.md`: **The Master Plan.** This is the highest-authority technical specification for the entire processing pipeline. It is the single most critical document for understanding any task's place in the larger system.
*   `plan_adaptation.md`: **Critical Modifications to the Master Plan.** This document details the crucial pivot to support both stills and sequence data, a fundamental architectural decision that impacts how the pipeline is orchestrated. It's an essential addendum to `plan.md`.
*   `CLAUDE.md`: **Project-Specific AI Guidance & High-Level Log.** This file appears to be a working log of high-level decisions, goals, and technical achievements. For an AI agent, this provides invaluable context on recent progress, proven strategies, and the project's "state of mind."
*   `README.md`: **The Project Entry Point.** Provides the highest-level overview of the project's purpose and features.

### 2. Project Rules & Conventions

These files define the "how" of the project. They ensure that any work done by the agent is consistent with the established development process, coding standards, and architectural patterns.

*   `docs/00_START_HERE.md`: **Developer Orientation.** The primary onboarding document that explains the project's core philosophy, including the critical role of IDL files as specifications.
*   `docs/01_IDL_GUIDELINES.md`: **The "Language" of the Project.** This defines the syntax and semantics of the Interface Definition Language (IDL) used to specify every component. Without this, an agent cannot correctly interpret or create component contracts.
*   `docs/02_IMPLEMENTATION_RULES.md`: **Coding and Testing Standards.** This dictates how IDLs are translated into code, how testing should be approached (emphasizing integration over unit tests), and other critical implementation patterns.
*   `docs/03_PROJECT_RULES.md`: **Project Organization.** Defines the directory structure, version control workflow, and conventions for file organization. Essential for any task that involves creating or moving files.
*   `src/diffusepipe/types/types_IDL.md` (and `docs/ARCHITECTURE/types.md`): **Shared Data Contracts.** Defines the core data structures (`OperationOutcome`, configuration objects, etc.) that are used across the entire pipeline. Understanding these is fundamental to any component interaction.

### 3. Essential Guides & Distilled Knowledge

These documents contain "hard-won" knowledge and troubleshooting information that can prevent common errors and significantly accelerate development.

*   `docs/LESSONS_LEARNED.md`: **The "Wisdom" of the Project.** This file consolidates critical insights from past development challenges, such as the "Test Failure Crisis" and "Safe Refactoring" incident. It's a high-leverage document that helps avoid repeating past mistakes.
*   `docs/06_DIALS_DEBUGGING_GUIDE.md`: **The DIALS Bible.** DIALS is the most critical and complex external dependency. This guide is essential for debugging any task involving DIALS processing, which is a core part of the pipeline.
*   `docs/VISUAL_DIAGNOSTICS_GUIDE.md`: **Validation and Verification Guide.** This documents the tools used to visually verify the correctness of the pipeline's outputs. It's crucial for any task involving pipeline development or debugging.
</file>

<file path="dp.md">
# Debug Payload: Geometric Validation Failure in DiffusePipe

  ## 1. Description of the Issue

  The script `scripts/demo_phase1_processing.py` reports a `FAILURE_GEOMETRY_VALIDATION` when processing an image (e.g., `747/lys_nitr_10_6_0491.cbf`). This failure is attributed to a "Q-consistency
  failed" check, with reported discrepancies:
  - Mean |Δq|: (e.g., 0.8449 Å⁻¹)
  - Max |Δq|: (e.g., 1.3120 Å⁻¹)

  These discrepancies are significantly larger than typical thresholds. The user has indicated that this issue is NOT due to:
  - Processing oscillation data (e.g., non-zero `Angle_increment` in CBF header) with a stills processing workflow.
  - Inherent problems with the input CBF data file itself.

  This suggests the root cause likely lies in one of the following areas:
  - **Incorrect Crystallographic Model:** The DIALS processing step might be producing an incorrect geometric model (unit cell, orientation) for this specific image, even if DIALS itself completes
  without apparent error.
  - **Error in Q-Vector Calculation/Comparison:** There might be an issue in the Python scripts responsible for calculating `q_bragg` (from the DIALS model and Miller indices) and/or
  `q_pixel_recalculated` (from pixel positions and experimental geometry), or in the logic that compares these two vectors.

  ## 2. Context Information & Relevant Codebase Documentation

  The following files provide essential context for understanding the DiffusePipe project, its intended processing logic, and debugging guidelines:

  -   Project-specific instructions for Claude Code, DIALS integration details, CBF format, common tasks, and debugging advice (especially regarding Q-vector validation and DIALS processing modes):
      `{CLAUDE.md}`
  -   Detailed plan for stills diffuse scattering processing, including definitions of Q-vectors, validation steps, and DIALS integration strategy (highly relevant for understanding the geometric
  validation):
      `{plan.md}`
  -   Developer orientation, explaining IDL usage, coding standards, and general project workflow:
      `{docs/00_START_HERE.md}`
  -   Interface Definition Language (IDL) for the consistency checker component, defining its contract for Q-vector comparison:
      `{src/diffusepipe/diagnostics/consistency_checker_IDL.md}`
  -   IDL for various data types used in the project, including `ExtractionConfig` which contains tolerance parameters like `q_consistency_tolerance_angstrom_inv` and `pixel_position_tolerance_px`:
      `{src/diffusepipe/types/types_IDL.md}`

  ## 3. Input Data (Headers/Text Representations) & Generated Output/Logs

  To reproduce and diagnose the issue, the following are critical:

  -   **CBF Header Information:** The text header of the CBF image file being processed (e.g., `747/lys_nitr_10_6_0491.cbf`).
      **CRITICAL:** *Please extract and include the full text header section of the problematic CBF file here. This typically includes lines detailing Detector_distance, Wavelength, Beam_xy,
  Angle_increment, Oscillation_axis, Pixel_size, etc.*

  -   **Input PDB File:** The reference PDB file used for consistency checks (text-based):
      `{6o2h.pdb}`

  -   **Script Execution Log:** The full console output from running the `demo_phase1_processing.py` script with the `--verbose` flag for the failing image. This is text-based.
      *(Please include the complete log output here or reference its path if very long)*
      `{logs/demo_phase1_processing_IMAGE_NAME.log}` (Example path for the full log)

  -   **Validation Failure Report:** The text file generated by the script detailing the validation failure:
      `{phase1_demo_output/validation_failure_report.txt}`

  -   **DIALS Processing Logs:** All log files (text-based) generated by the DIALS software during the processing of the problematic image. These are typically found in a subdirectory within
  `phase1_demo_output` (e.g., `phase1_demo_output/IMAGE_NAME_processing/`) and would include logs like `dials.import.log`, `dials.find_spots.log`, `dials.index.log`, `dials.integrate.log` (or a combined
  `dials.stills_process.log`):
      `{phase1_demo_output/IMAGE_NAME_processing/DIALS_LOGS_DIRECTORY/}` (Specify actual log file names)

  -   **DIALS Output Files (Text/JSON based):** The key output files from DIALS processing for the problematic image, which are inputs to the validation step. These are typically text-based (JSON-like
  for .expt, text for .refl).
      -   Refined experiments: `{phase1_demo_output/IMAGE_NAME_processing/indexed_refined_detector.expt}` (or `integrated.expt` or similar, check script output)
      -   Refined reflections: `{phase1_demo_output/IMAGE_NAME_processing/indexed_refined_detector.refl}` (or `integrated.refl` or similar)

  ## 4. Potentially Relevant Source Code Files

  The bug likely resides in or is related to the logic within these text-based source files:

  -   **Main Demo Script:** Orchestrates the Phase 1 processing and calls the validation steps.
      `{scripts/demo_phase1_processing.py}`

  -   **Still Processing & Validation Module:** Contains the core logic for processing stills and performing geometric validation. This is a primary suspect for the validation failure logic.
      `{src/diffusepipe/crystallography/still_processing_and_validation.py}`

  -   **Consistency Checker Module:** Implements the Q-consistency check (calculates and compares q_bragg and q_pixel_recalculated). This is a primary suspect for the Q-vector discrepancy.
      `{src/diffusepipe/diagnostics/consistency_checker.py}`

  -   **Q-Value Calculator Module:** May be used by the consistency checker or validation module for calculating one of the Q-vector types.
      `{src/diffusepipe/diagnostics/q_calculator.py}`

  -   **DIALS Adapter Module:** Handles the interface with DIALS processing. An incorrect model from DIALS could originate from issues here or in its configuration.
      `{src/diffusepipe/adapters/dials_stills_process_adapter.py}` (or sequence equivalent if that pathway is somehow being triggered)

  -   **Types Implementation:** The Python implementation of data structures (like `ExtractionConfig`) used to pass parameters (e.g., tolerances).
      `{src/diffusepipe/types/types_IDL.py}`

  -   **DIALS Configuration Files (PHIL):** If custom PHIL files (text-based) are being used by `demo_phase1_processing.py` or the underlying DIALS adapter for this run, they are critical. Standard
  locations:
      `{src/diffusepipe/config/find_spots.phil}`
      `{src/diffusepipe/config/refine_detector.phil}`
      *(Specify any other PHIL files actively used by the failing script run)*

  ## 5. Debugging Steps to Consider

  1.  **Verify DIALS Model Quality (using text logs and .expt/.refl):**
      *   Manually inspect the DIALS text logs for the problematic image for any warnings, unusual metrics (e.g., very low number of spots indexed, high RMSD in refinement).
      *   Examine the content of the `.expt` file for the refined unit cell and compare it with the input PDB (`6o2h.pdb`).
      *   Check the `.refl` file for the number of indexed reflections and their reported positions.

  2.  **Examine Q-Vector Calculation Logic (in Python code):**
      *   Step through the code in `src/diffusepipe/diagnostics/consistency_checker.py` (and potentially `src/diffusepipe/crystallography/still_processing_and_validation.py`) for a single problematic
  reflection from the problematic image's `.refl` file.
      *   Print out intermediate values for `q_bragg` and `q_pixel_recalculated` and verify their coordinate systems and calculation steps against the definitions in `plan.md` and standard
  crystallographic formulas.
      *   Ensure consistent use of the *refined* crystal model and experimental geometry from DIALS (loaded from the `.expt` file).

  3.  **Check Pixel Position Consistency (using .refl file data):**
      *   As suggested in `CLAUDE.md`, implement or enable a simpler check of `|observed_px - calculated_px|` using data from the `.refl` file. If this difference is small (e.g., ~1-2 pixels) while |Δq|
  is large, it strongly points to an error in the q-vector specific calculations rather than the overall DIALS model. The `validation_failure_report.txt` might already contain this information.

  4.  **Isolate the Failing Component:**
      *   If possible, run the consistency check logic with known good inputs (e.g., a DIALS model from an `.expt` file and reflection table from a `.refl` file from a dataset that passes validation) to
  see if the error is specific to the data from the problematic image or inherent in the checking code.
</file>

<file path=".claude/commands/context.md">
## **Context Preparation Protocol**

You are an expert context preparation agent. Your primary function is to analyze a codebase and a planned task, then produce a comprehensive context package in a specific JSON format. Your analysis must be entirely focused on serving the **`<planned task>`**.

### **Core Philosophy: Erring on the Side of Inclusion**

Your primary goal is to prevent missing **any potentially useful file**. It is better to include a file that is only peripherally related than to omit a file that provides crucial context. Think of your role as maximizing *recall* over *precision*. You must actively look for indirect relationships. For any given file, ask:

*   Does it define a data structure used by a relevant file?
*   Is it a configuration file that a relevant file might read?
*   Is it a parent component that uses a relevant component?
*   Is it a shared utility, helper, or service called by a relevant file?
*   Is it a test for a relevant file?
*   Does it establish a style, pattern, or architectural standard that should be followed?

If the answer to any of these is "yes," the file is likely relevant.

### **CRITICAL INCLUSION RULES**

These rules are mandatory and **override** your normal relevance assessment. You MUST follow them.

1.  **Critical Files Mandate:** You **MUST** read the list of files provided in the **`<critical>`** section of this prompt. For **every file** listed in that section, you **MUST**:
    *   Classify the file as "Relevant".
    *   Assign it a **very high score (e.g., 9.0 or higher)**.
    *   Use the justification: "Included as a mandatory core project document as specified in the <critical> context section."

2.  **Test File Mandate:** If the `<planned task>` involves modifying an existing file (e.g., `src/utils/parser.py`), you **MUST** find its corresponding test file (e.g., `tests/test_parser.py`). You **MUST** classify this test file as "Relevant" and assign it a **high score (e.g., 8.0 or higher)**. Its justification should state: "Mandatory inclusion of the test file for the modified source code."

3.  **Checklist Example Mandate:** If the `<planned task>` involves drafting, creating, or modifying a checklist, you **MUST** find the best example of an existing checklist (see checklists/) in the codebase. You **MUST** classify this file as "Relevant" and assign it a **high score (e.g., 8.0 or higher)**. Its justification should state: "Included as a mandatory style and format reference for the checklist creation task."

4.  **Plan Example Mandate:** If the `<planned task>` involves drafting a plan, you **MUST** find the best example of an existing plan document. You **MUST** classify this file as "Relevant" and assign it a **high score (e.g., 8.0 or higher)**. Its justification should state: "Included as a mandatory style and format reference for the planning task."

### **Instructions**

**Step 1: High-Level Analysis**
First, deeply analyze the **`<planned task>`** description and the overall codebase. From this analysis, generate the initial high-level fields for the JSON output:
*   `planned_task`: Write a concise, one-sentence summary of the task.
*   `task_requirements`: Create a list of specific, actionable requirements derived from the task description.
*   `codebase_summary`: Write a brief summary of the codebase's architecture, specifically focusing on the parts relevant to the planned task.

**Step 2: File-by-File Analysis & Classification**
Iterate through every file provided in the code context, applying the **Core Philosophy** at all times. This analysis will populate the `file_classification` object. For each file, perform the following:
1.  **Classify its Relevance:** Categorize the file as "Relevant", "Maybe Relevant", or "Irrelevant". **When in doubt, classify as "Maybe Relevant" instead of "Irrelevant".**
2.  **Score its Importance:** For all "Relevant" and "Maybe Relevant" files, assign an importance score from 1.0 to 10.0, keeping the **CRITICAL INCLUSION RULES** in mind. Use the following revised guide:
    *   **10.0: Critically Important.** The file is being directly created, edited, or is the primary subject of the task.
    *   **7.0 - 9.9: Highly Relevant.** Provides core logic, data models, direct dependencies (imports/exports), is a mandatory test file, or is a core project document from the `<critical>` section.
    *   **4.0 - 6.9: Contextually Relevant.** Provides useful surrounding context, shared utilities, configuration, or is a parent/child component that is not directly modified but is affected.
    *   **1.0 - 3.9: Potentially Relevant.** Included for broader context or as a style/pattern example. Useful for understanding the "bigger picture" or related conventions.
3.  **Justify the Classification:** For every file, write a `description` of its purpose and a `justification` for its classification and score. The justification must explain *why* the file is or isn't relevant, referencing your analysis (e.g., "This file defines the data model consumed by `relevant_file.py`," or "This config file sets variables used in the target module.").

**Step 3: Content Inclusion**
Prepare the file content for the final package. This step populates the `included_files` and `documentation_excerpts` arrays.
*   **For "Relevant" and "Maybe Relevant" code files:** Add an entry to the `included_files` array. Use the Jinja-style template syntax `{file_path}` in the `content` field.
*   **For large documentation files (e.g., under `libdocs/`):** Do not include the full file. Instead, identify the most relevant sections, create a targeted excerpt, and add an entry to the `documentation_excerpts` array.

**Step 4: Final Validation and Assembly**
Before generating the final JSON, perform a final review of your work. **Verify the following:**
- **Have you fully embraced the Core Philosophy of including all potentially useful files?**
- **Have you obeyed all CRITICAL INCLUSION RULES? Specifically:**
    - **Have you included all files from the `<critical>` section with a high score?**
    - Have you included the required test files, checklist examples, or plan examples if the task dictated it?
- Is the JSON structure perfectly valid?
- Are all file lists sorted by `score` in descending order?

If any revisions are necessary, write a non-json revision preamble section where you describe the adjustments.

Assemble all information into a single, valid JSON object according to the format below.

### **Final JSON Output Format**

```json
{
  "context_package": {
    "planned_task": "A one-sentence summary of the planned task.",
    "task_requirements": [
      "A specific, actionable requirement derived from the task.",
      "Another specific requirement."
    ],
    "codebase_summary": "A brief summary of the codebase architecture relevant to the task.",
    "file_classification": {
      "relevant_files": [
        {
          "path": "path/to/relevant/file.py",
          "description": "Brief description of the file's purpose.",
          "justification": "Why this file is essential for the planned task.",
          "score": 9.5
        }
      ],
      "maybe_relevant_files": [
        {
          "path": "path/to/maybe/relevant/file.py",
          "description": "Brief description of the file's purpose.",
          "justification": "Why this file provides useful context for the task.",
          "score": 6.0
        }
      ],
      "irrelevant_files": [
        {
          "path": "path/to/irrelevant/file.js",
          "description": "Brief description of the file's purpose.",
          "justification": "Why this file has no bearing on the planned task."
        }
      ]
    },
    "included_files": [
      {
        "score": 9.5,
        "path": "path/to/relevant/file.py",
        "description": "Brief description of the file's purpose.",
        "content": "{path/to/relevant/file.py}"
      }
    ],
    "documentation_excerpts": [
      {
        "score": 7.5,
        "source": "libdocs/api_reference.md",
        "title": "API Reference - Relevant Section Title",
        "content": "A short, relevant excerpt from the documentation file that is specific to the planned task..."
      }
    ]
  }
}
```
</file>

<file path="agent/writing_checklists.md">
**Guideline: Drafting Implementation Checklists**

**1. Purpose of this Guideline**

This document provides a standard template and conventions for creating detailed implementation checklists. These checklists are intended to guide an AI agent (or a human developer) through the systematic implementation of project phases and modules, ensuring all planned tasks, context priming, and progress tracking are consistently handled.

**2. Checklist Structure**

Each checklist should adhere to the following overall structure:

```markdown
**Agent Task: [Brief, High-Level Task Title, e.g., Implement Phase X of Y Plan]**

**Overall Goal for Agent ([Phase/Task Name]):** [One or two sentences describing the primary objective of this specific checklist/phase.]

<checklist instructions>
**Checklist Usage Instructions for Agent:**

1.  **Copy this entire checklist into your working memory or a dedicated scratchpad area.**
2.  **Context Priming:** Before starting a new major section (e.g., Phase, Module), carefully read all "Context Priming" items for that section.
3.  **Sequential Execution:** Address checklist items in the order presented, unless an item explicitly states it can be done in parallel or depends on a later item being drafted first (e.g., an IDL definition).
4.  **Update State:** As you work on an item, change its state field:
    *   `[ ] Open` -> `[P] In Progress` when you start.
    *   `[P] In Progress` -> `[D] Done` when completed successfully.
    *   `[P] In Progress` -> `[B] Blocked` if you encounter a blocker. Add a note explaining the blocker in the "Details/Notes/Path" column.
5.  **Record Details (in the "Details/Notes/Path" column):**
    *   If a step requires creating or modifying a file, add the **full relative path** to that file (e.g., `src/package/module.py`).
    *   If a significant design decision or clarification is made during the task, note it briefly.
    *   If a task is broken down into further sub-tasks not originally listed, add them as indented items with their own Item ID and State.
    *   For "IDL Definition/Review" tasks, summarize the key interface aspects (Inputs, Outputs, Behavior, Errors) or reference the document where these are detailed.
6.  **Iterative Review:** Periodically re-read completed sections of the checklist and your notes to ensure continued alignment with the overall plan and previous decisions.
7.  **Save State:** If work needs to be paused, ensure this checklist with its current progress and notes is saved so work can be resumed effectively.
</checklist instructions>

---

**[Phase/Major Section Title, e.g., Phase X: Section Title]**

| Item ID | Task Description                                     | State | Details/Notes/Path                                     |
| :------ | :--------------------------------------------------- | :---- | :----------------------------------------------------- |
| **X.A** | **Context Priming ([Phase/Section Name])**           | `[ ]` |                                                        |
| X.A.1   | [Specific document/section to review for context]    | `[ ]` | [Optional: Note on what to focus on during review]     |
| X.A.2   | [Another document/section or concept to understand]  | `[ ]` |                                                        |
| ...     |                                                      |       |                                                        |
| X.A.N   | Understand Goal: [Restate goal for this specific section] | `[ ]` |                                                        |
|         |                                                      |       |                                                        |
| **X.B** | **[First Major Group of Tasks/Module within Phase]** | `[ ]` |                                                        |
| **X.B.idl**| **Define/Review Conceptual IDL for [Component/Module]** | `[ ]` | **Purpose:** [Brief purpose of this IDL step]. <br>Input: [Key inputs]. <br>Output: [Key outputs]. <br>Behavior: [Core behavior summary]. <br>Errors: [Key error conditions]. <br>[Optional: Path to where IDL thoughts are captured, e.g., comments in the target Python file, or a link to a formal IDL doc if it exists.] |
| X.B.1   | [Specific implementation task derived from plan]     | `[ ]` | [Note expected file path, key decisions, or dependencies] |
| X.B.1.a |   [Sub-task if needed]                               | `[ ]` |                                                        |
| X.B.1.b |   [Another sub-task]                                 | `[ ]` |                                                        |
| X.B.2   | [Another specific implementation task]               | `[ ]` |                                                        |
| ...     |                                                      |       |                                                        |
| X.B.T   | **Unit/Integration Tests for [Component/Module X.B]**| `[ ]` | Path: `tests/path/to/test_module_xb.py`                |
| X.B.T.1 |   Test Case: `test_scenario_one`                     | `[ ]` |                                                        |
|         |     - Setup: [Brief setup description]               | `[ ]` | [Note any specific test data files used]               |
|         |     - Execution: [How the component is called]       | `[ ]` |                                                        |
|         |     - Verification: [Key assertions]                 | `[ ]` |                                                        |
| ...     |                                                      |       |                                                        |
|         |                                                      |       |                                                        |
| **X.C** | **[Second Major Group of Tasks/Module within Phase]**| `[ ]` |                                                        |
| ...     | *(Repeat structure as for X.B)*                      |       |                                                        |
|         |                                                      |       |                                                        |
| **X.Z** | **[Phase/Section] Review & Next Steps**              | `[ ]` |                                                        |
| X.Z.1   | Self-Review: All [Phase/Section] items addressed?    | `[ ]` | [Confirm IDLs defined/reviewed, code implemented, tests written] |
| X.Z.2   | Context Refresh: Re-read [relevant plan sections for next phase]. | `[ ]` |                                                        |
| X.Z.3   | Decision: Proceed to [Next Phase/Section] Checklist. | `[ ]` |                                                        |

---
```

**3. Key Elements of a Checklist Item:**

*   **Item ID:**
    *   Hierarchical (e.g., `X.A.1`, `X.B.1.a`).
    *   `X` represents the Phase number or a major section letter.
    *   Second letter (`A`, `B`, `C`) represents a major group of tasks or a module within that phase.
    *   Numbers (`1`, `2`, `3`) represent specific tasks.
    *   Lowercase letters (`a`, `b`, `c`) represent sub-tasks.
    *   Use `**.idl**` suffix for items specifically about defining/reviewing an Interface Definition (conceptual or formal).
    *   Use `**.T**` suffix for items grouping test cases for a module.

*   **Task Description:**
    *   Clear, concise, and actionable.
    *   Start with a verb (e.g., "Implement...", "Define...", "Review...", "Test...").
    *   Derived directly from the project plan (e.g., `plan.md`).
    *   If it's a high-level task, break it down into sub-tasks.
    *   For "IDL Definition/Review" tasks, use the "Details/Notes/Path" column to sketch out:
        *   A brief **Purpose** statement for the interface.
        *   Key **Input(s)**.
        *   Key **Output(s)**.
        *   A summary of its core **Behavior**.
        *   Anticipated **Error(s)** or exceptional conditions.
        *   Optionally, the intended file path for the Python implementation of this interface.

*   **State:**
    *   A single field indicating current progress.
    *   Use predefined state markers:
        *   `[ ]` (Open / To Do)
        *   `[P]` (In Progress)
        *   `[D]` (Done / Completed)
        *   `[B]` (Blocked)

*   **Details/Notes/Path:**
    *   **Crucial for context and tracking.**
    *   **File Paths:** For tasks involving file creation/modification, list the full relative path to the primary file(s) involved.
    *   **Decisions:** Briefly note any significant design choices, clarifications, or assumptions made while performing the task if they deviate from or elaborate on the plan.
    *   **Blockers:** If State is `[B]`, explain the blocker here.
    *   **References:** Links to specific sections of other documents if helpful.
    *   For IDL tasks, this column should contain the sketch of the interface as described above.

**4. Content Guidelines:**

*   **Granularity:** Aim for tasks that are manageable units of work. A single task shouldn't be overly broad (e.g., "Implement entire module"). Break down larger tasks from the plan into smaller checklist items.
*   **IDL-First:** For any new component, class, or significant function, include an "IDL Definition/Review" item *before* its corresponding Python implementation item(s). This reinforces thinking about the interface first.
*   **Testing:** Include specific groups of test cases for each implemented module or significant piece of functionality. Test descriptions should cover setup, execution, and verification.
*   **Context Priming:** Each major section (Phase or significant Module group) should start with "Context Priming" tasks to ensure the agent re-orients itself with relevant plans and documents.
*   **Review and Next Steps:** Each major section should end with a review item and a clear pointer to the next steps or checklist.
*   **Consistency with `plan.md`:** Task descriptions should closely mirror the language and intent of the corresponding sections in `plan.md` or other planning documents.

**5. Formatting:**

*   Use Markdown tables for the main checklist structure.
*   Use bolding for section titles (e.g., `**X.A Context Priming...**`) and for emphasizing the "IDL Definition/Review" task type.
*   Use fixed-width font for `[ ]`, `[P]`, `[D]`, `[B]` states for visual clarity.
*   Use consistent indentation for sub-tasks if not using the table format for them (though the table format with hierarchical IDs is preferred).
</file>

<file path="docs/ARCHITECTURE/adr/ADR_TEMPLATE.md">
# ADR-XXX: [Descriptive Title of Decision]

*   **Status:** Proposed | Accepted | Deprecated | Superseded by ADR-YYY
*   **Date:** YYYY-MM-DD
*   **Deciders:** `[List of names or roles of people involved in the decision]`
*   **Consulted:** `[List of names or roles of people consulted (optional)]`
*   **Informed:** `[List of names or roles of people informed (optional)]`

---

## Context and Problem Statement

`[Describe the context and problem that this decision addresses. What is the issue, need, or opportunity? What are the constraints, forces, or requirements at play? This section should be detailed enough for someone unfamiliar with the immediate problem to understand the motivation for the decision.]`

---

## Decision Drivers

*   `[Driver 1: e.g., Improve performance for X use case]`
*   `[Driver 2: e.g., Reduce operational complexity for Y component]`
*   `[Driver 3: e.g., Need for better scalability of Z feature]`
*   `[Driver 4: e.g., Adherence to [Project Principle/Standard]]`

---

## Considered Options

### Option 1: [Name of Option 1]
`[Brief description of the option. How would it solve the problem? Include diagrams if helpful.]`
*   **Pros:**
    *   `[Pro 1]`
    *   `[Pro 2]`
*   **Cons:**
    *   `[Con 1]`
    *   `[Con 2]`

### Option 2: [Name of Option 2]
`[Brief description of the option. How would it solve the problem? Include diagrams if helpful.]`
*   **Pros:**
    *   `[Pro 1]`
    *   `[Pro 2]`
*   **Cons:**
    *   `[Con 1]`
    *   `[Con 2]`

### Option 3: [Name of Option 3 (etc.)]
`[Brief description of the option... ]`
*   **Pros:** ...
*   **Cons:** ...

*(Add more options as necessary)*

---

## Decision Outcome

**Chosen Option:** `[Name of Chosen Option]`

**Rationale:**
`[Explain why this option was chosen. Justify the trade-offs made. How does it address the decision drivers better than the alternatives? Be specific and link back to the context and problem statement.]`

### Positive Consequences
*   `[Expected benefit 1]`
*   `[Expected benefit 2]`

### Negative Consequences (and Mitigation)
*   `[Potential drawback 1, and how it might be mitigated or accepted]`
*   `[Potential drawback 2, and how it might be mitigated or accepted]`

### Impact
*   **On Codebase:** `[e.g., New modules X, Y; refactoring of Z; changes to API of W]`
*   **On Team/Process:** `[e.g., Requires learning new technology; changes deployment process]`
*   **On Documentation:** `[e.g., Update documents A, B; new guide for C needed]`
*   **On Non-Functional Requirements:** `[e.g., Expected impact on performance, security, scalability, maintainability]`

### Risks
*   `[Risk 1: e.g., Integration with legacy system Z might be more complex than anticipated.]`
*   `[Risk 2: e.g., Chosen library X has a small community, potential support issues.]`

---

## Implementation Plan (High-Level, Optional)

*   `[Key steps or phases for implementing this decision.]`
*   `[Any prerequisites or dependencies for implementation.]`
*   `[How will this decision be validated or tested? (e.g., PoC, specific metrics)]`

---

## Links and References (Optional)

*   `[Link to relevant issue tracker ticket]`
*   `[Link to external documentation or articles supporting the decision]`
*   `[Link to related internal documents or previous ADRs]`

---

## Review Date (Optional)

*   `[Set a future date or condition to review this decision, e.g., "After 6 months of use", "When user load exceeds X"]`
</file>

<file path="docs/ARCHITECTURE/adr/README.md">
# Architecture Decision Records (ADRs)

This directory stores Architecture Decision Records (ADRs) for the `[YourProjectName]` project.

## What is an ADR?

An ADR is a short document that captures a significant architectural decision made along with its context and consequences. ADRs are immutable once "Accepted"; if a decision is changed, a new ADR is created that supersedes the old one.

The goal of using ADRs is to:
*   Document important architectural choices and their rationale.
*   Provide context for future developers or team members.
*   Avoid re-litigating past decisions without new information.
*   Communicate architectural changes clearly.

## When to Create an ADR

Consider creating an ADR when:
*   Making a choice that has a significant impact on the system's architecture (e.g., structure, non-functional characteristics, dependencies, interfaces between components).
*   Introducing a new technology, library, or framework that is core to the system.
*   Choosing a specific pattern or approach over others for a critical part of the system.
*   Changing a previous architectural decision.
*   Deprecating a major feature, component, or architectural pattern.

If a decision is small, local to a single component, and easily reversible, an ADR might not be necessary (though documenting the rationale in code comments or commit messages is still good practice).

## ADR Format

All ADRs should follow the template defined in [ADR_TEMPLATE.md](./ADR_TEMPLATE.md).

## ADR Lifecycle

1.  **Proposed:** An ADR is drafted and submitted for discussion/review (e.g., via a Pull Request).
2.  **Accepted:** After discussion and agreement by the relevant stakeholders (e.g., tech lead, team), the ADR is merged and its status is marked as "Accepted."
3.  **Rejected/Withdrawn (Optional):** If an ADR is not accepted, it can be marked as such or withdrawn.
4.  **Deprecated/Superseded:** If a new decision makes an existing ADR obsolete, the old ADR's status is updated to "Deprecated" or "Superseded by ADR-XXX" (linking to the new ADR). The new ADR should also reference the one it supersedes.

## Naming Convention

ADRs should be named sequentially, e.g., `001-decision-summary.md`, `002-another-decision.md`. The title within the ADR document should be more descriptive.

## Current ADRs

*   *(This section should list links to actual ADRs as they are created)*
*   Example: `[000-use-of-adrs.md](./000-use-of-adrs.md)` (An ADR about using ADRs itself!)
</file>

<file path="docs/LIBRARY_INTEGRATION/README.md">
# Documenting Key Library Integrations

## Purpose

This directory is intended to house concise guides, notes, or links related to the integration of significant third-party libraries that are crucial to the project's architecture, core functionality, or common development patterns.

While comprehensive documentation for external libraries should be sought from their official sources, this space can be used to:

*   Summarize **project-specific usage patterns** of a library.
*   Highlight **key APIs or features** of a library that are frequently used within this project.
*   Document any **custom wrappers, abstractions, or configurations** applied to a library for project use.
*   Provide quick references or "gotchas" specific to how the library is integrated here.
*   Link to the official documentation for more in-depth information.

## When to Add a Document Here

Consider adding a document or notes for a library if:

*   It plays a central role in the system's architecture (e.g., a primary data validation library, an ORM, an API client framework for a critical external service).
*   The project employs specific, non-obvious patterns when using the library that developers need to be aware of.
*   There are common pitfalls or configuration details specific to this project's use of the library.
*   Onboarding new developers would be significantly aided by a quick project-centric overview of how a particular library is used.

## Examples of Content

For each key library, you might create a Markdown file (e.g., `[library_name]_integration.md`) containing:

*   **Library Name and Version:** e.g., "Pydantic v2.5"
*   **Purpose in this Project:** e.g., "Used for all data validation, serialization, and settings management."
*   **Key Project-Specific Patterns:**
    *   How models are typically defined.
    *   Common validation techniques employed.
    *   How library errors are handled within the project.
    *   Examples of custom validators or serializers used.
*   **Important APIs/Features Used:** A shortlist of the most relevant classes/functions from the library that developers will frequently encounter in this codebase.
*   **Configuration Notes:** Any project-specific setup or configuration for the library.
*   **Common "Gotchas" or Tips:**
    *   e.g., "Remember that `model_dump()` by default excludes `None` values unless `exclude_none=False` is set."
    *   e.g., "When creating custom root types, ensure..."
*   **Link to Official Documentation:** Always provide a link to the library's official, comprehensive documentation.

## Example File Structure

```
LIBRARY_INTEGRATION/
├── README.md                   (This file)
├── pydantic_integration.md     (Example for a data validation library)
├── requests_usage.md           (Example for an HTTP client library)
└── [your_orm]_patterns.md      (Example for an Object-Relational Mapper)
```

By maintaining focused notes on key library integrations, we can accelerate developer onboarding and ensure consistent usage of these important tools across the project.
</file>

<file path="docs/TEMPLATES/IDL_IMPLEMENTATION_CHECKLIST.md">
**IDL Implementation Readiness Checklist**

**Project:** `[Your Project Name]`
**IDL File(s) Reviewed:** `_________________________` (e.g., `src/component_x/module_y_IDL.md`)

**Instructions:** For each IDL file defining interfaces, modules, or types intended for implementation, review against the following criteria. Mark each item as Yes (✅), No (❌), or N/A. Provide comments for any "No" answers, detailing the required changes or clarifications. An IDL is generally considered "implementation ready" when all applicable items are marked "Yes".

**I. Feature / User Story Coverage**

*   *(List the key features/user stories this IDL is intended to support. Examples below - **replace/add specific stories relevant to the IDL being reviewed**)*

| Feature/Story                                       | Relevant Methods/Types in IDL | Sufficiently Specified? | Comments / Missing Details                                                                 |
| :-------------------------------------------------- | :--------------------------- | :---------------------- | :----------------------------------------------------------------------------------------- |
| **Ex: User Registration via API Endpoint**          | `UserManager.register_user`  | ☐ ✅ ☐ ❌ ☐ N/A         | _e.g., Need clarity on password hashing requirements if not handled by a dependency._        |
| **Ex: Process Batch Data from Message Queue**       | `BatchProcessor.process_next_batch`, `DataItem` struct | ☐ ✅ ☐ ❌ ☐ N/A         | _e.g., Error handling for individual item failures within a batch needs more detail._      |
| **Ex: Retrieve Configuration for Component [X]**    | `ConfigService.get_config`   | ☐ ✅ ☐ ❌ ☐ N/A         | _e.g., How are dynamic/overridden configurations handled by this interface?_               |
| **Ex: Execute [CoreAlgorithm] with given inputs**   | `CoreModule.execute_algorithm` | ☐ ✅ ☐ ❌ ☐ N/A         | _e.g., Preconditions for input data ranges are not fully specified._                       |
| *(Add other relevant features/stories for this IDL)* |                              |                         |                                                                                            |

**II. Interface Definition & Clarity (e.g., `interface ...` or `class ...` in IDL)**

| #   | Criteria                                                                                                                                                                                             | Status          | Comments / Required Changes |
| :-- | :--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | :-------------- | :-------------------------- |
| 2.1 | **Clear Name:** Is the interface name clear, concise, and accurately reflect its purpose (e.g., `UserManager`, `ConfigService`)?                                                                       | ☐ ✅ ☐ ❌ ☐ N/A |                             |
| 2.2 | **Purpose Defined:** Is the overall purpose/responsibility of the interface clearly stated in the module or interface docstring/comment block?                                                         | ☐ ✅ ☐ ❌ ☐ N/A |                             |
| 2.3 | **Dependencies Declared:** Are all significant dependencies (other modules/interfaces, external resources like Filesystem, Shell, Database, ExternalAPI) explicitly listed (e.g., using `@depends_on`)? | ☐ ✅ ☐ ❌ ☐ N/A |                             |
| 2.4 | **Inheritance/Extension Clear:** If the interface conceptually extends another, is this clearly stated in comments or via IDL syntax?                                                                  | ☐ ✅ ☐ ❌ ☐ N/A |                             |
| 2.5 | **Interaction Documentation:** Are complex or key interactions involving this interface/module clearly documented, preferably with sequence diagrams (e.g., using Mermaid in the IDL file or linked doc)? | ☐ ✅ ☐ ❌ ☐ N/A |                             |

**III. Method / Function Signatures (e.g., `returnType methodName(...)` in IDL)**

| #   | Criteria                                                                                                                                                                                                                            | Status          | Comments / Required Changes |
| :-- | :---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | :-------------- | :-------------------------- |
| 3.1 | **Clear Method Names:** Are method names clear, using standard conventions (e.g., snake_case or camelCase per project style), and accurately describing the action performed?                                                        | ☐ ✅ ☐ ❌ ☐ N/A |                             |
| 3.2 | **Accurate Return Types:** Is the return type specified using appropriate primitives (`string`, `int`, `boolean`, `list<T>`, `map<K,V>`) or defined types/structs? Is `optional` used correctly? Is `union` used correctly?           | ☐ ✅ ☐ ❌ ☐ N/A |                             |
| 3.3 | **Accurate Parameter Types:** Are all parameters clearly named and typed using primitives, defined types/structs, `optional`, or `union`?                                                                                             | ☐ ✅ ☐ ❌ ☐ N/A |                             |
| 3.4 | **Complex Parameter Formats:** For complex dictionary/object parameters, is the expected structure clearly documented (e.g., via `Expected Data Format: { ... }` comment or by referencing a `struct`)?                               | ☐ ✅ ☐ ❌ ☐ N/A |                             |
| 3.5 | **Preconditions Documented:** Are the necessary conditions required *before* calling the method clearly listed under "Preconditions"?                                                                                                 | ☐ ✅ ☐ ❌ ☐ N/A |                             |
| 3.6 | **Postconditions Documented:** Are the expected outcomes, state changes, or guarantees *after* successful method execution clearly listed under "Postconditions"?                                                                    | ☐ ✅ ☐ ❌ ☐ N/A |                             |
| 3.7 | **Behavior Described:** Is the core logic and sequence of actions performed by the method clearly described under "Behavior"? Does it mention interactions with dependencies?                                                        | ☐ ✅ ☐ ❌ ☐ N/A |                             |
| 3.8 | **Error Conditions Declared:** Are potential error conditions and how they are signaled (e.g., raised exception type, specific return value/status code) documented (e.g., using `@raises_error` or within Behavior/Postconditions)? | ☐ ✅ ☐ ❌ ☐ N/A |                             |

**IV. Type Definitions / Custom Structures (If applicable, e.g., `struct ...` in IDL or `ARCHITECTURE/types.md`)**

| #   | Criteria                                                                                                                                                           | Status          | Comments / Required Changes |
| :-- | :----------------------------------------------------------------------------------------------------------------------------------------------------------------- | :-------------- | :-------------------------- |
| 4.1 | **Clear Type Names:** Are custom type names (e.g., `UserProfile`, `BatchResult`, `ApiRequestOptions`) clear and descriptive?                                         | ☐ ✅ ☐ ❌ ☐ N/A |                             |
| 4.2 | **Structure Defined:** Is the structure of custom types (e.g., fields within a struct/object, elements of a list/tuple) clearly defined with types for each element? | ☐ ✅ ☐ ❌ ☐ N/A |                             |
| 4.3 | **Consistency:** Are types used consistently across different IDL files where they are referenced?                                                                    | ☐ ✅ ☐ ❌ ☐ N/A |                             |

**V. Architectural Alignment**

| #   | Criteria                                                                                                                                                                                                                                                          | Status          | Comments / Required Changes |
| :-- | :---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | :-------------- | :-------------------------- |
| 5.1 | **Component Responsibilities:** Does the interface adhere to established component responsibilities (e.g., `[DataServiceComponent]` handles data persistence, `[BusinessLogicComponent]` handles core logic, `[ExternalAPIClientComponent]` isolates external calls)? | ☐ ✅ ☐ ❌ ☐ N/A |                             |
| 5.2 | **[Project-Specific Concern 1, e.g., Legacy System Decoupling]:** Does the IDL avoid defining or referencing [specific legacy patterns/components]?                                                                                                                 | ☐ ✅ ☐ ❌ ☐ N/A |                             |
| 5.3 | **[Project-Specific Concern 2, e.g., DSL/Orchestration Role]:** Does the IDL correctly reflect that [specific type of workflow composition] is handled exclusively by [the designated DSL/Orchestrator Component]?                                                    | ☐ ✅ ☐ ❌ ☐ N/A |                             |
| 5.4 | **[Project-Specific Concern 3, e.g., Core Executor Isolation]:** Does the `[CoreExecutorComponent]` IDL behavior forbid implicit environment access and mandate use of only passed parameters?                                                                        | ☐ ✅ ☐ ❌ ☐ N/A |                             |
| 5.5 | **Data Handling:** Does the design encourage parsing into specific types (aligning with "Parse, Don't Validate") rather than passing raw dicts/lists widely? (Assessed via parameter types and data format comments).                                                | ☐ ✅ ☐ ❌ ☐ N/A |                             |
| 5.6 | **IDL Versioning:** Is the interface version clearly marked (e.g., `[Interface:ComponentName:1.0]`) and consistent with related documentation?                                                                                                                      | ☐ ✅ ☐ ❌ ☐ N/A |                             |
| 5.7 | **External API Interaction (If applicable):** Does the interface correctly use any designated Manager/Bridge classes for external API interactions where applicable?                                                                                                | ☐ ✅ ☐ ❌ ☐ N/A |                             |
| 5.8 | **[Project-Specific Concern 4, e.g., Configuration Handling]:** Does the component correctly receive, pass, or act upon `[ConfigObjectType]` if it's involved in a configurable workflow?                                                                           | ☐ ✅ ☐ ❌ ☐ N/A |                             |

**VI. Overall Readiness**

| #   | Criteria                                                                                                                                         | Status          | Comments / Required Changes |
| :-- | :----------------------------------------------------------------------------------------------------------------------------------------------- | :-------------- | :-------------------------- |
| 6.1 | **Completeness:** Does the IDL provide enough detail to implement the described functionality *for the targeted features/stories* without ambiguity? | ☐ ✅ ☐ ❌ ☐ N/A |                             |
| 6.2 | **Consistency:** Is the terminology, naming, and style consistent within the file and with other project IDLs?                                    | ☐ ✅ ☐ ❌ ☐ N/A |                             |
| 6.3 | **Actionability:** Can a developer reasonably translate this IDL specification into code following the project's implementation rules?              | ☐ ✅ ☐ ❌ ☐ N/A |                             |

**Summary:**

*   **Feature Coverage Assessment:** `___________________________________________________________`
    *(Summarize if the IDLs sufficiently cover the necessary interfaces/types/behaviors for the target features)*
*   **Overall Readiness:** ☐ Ready for Implementation / ☐ Needs Revision
*   **Key Issues / Blockers (if any):**
    *   `___________________________________________________________`
    *   `___________________________________________________________`
*   **Next Steps:** `___________________________________________________________`
</file>

<file path="docs/TEMPLATES/TASK_INSTRUCTION_TEMPLATE.md">
# Task Implementation Instructions

*   **Task Name/ID:** `[Link to Issue Tracker Ticket or Task Name]`
*   **Assigned To:** `[Developer Name]`
*   **Assigned By:** `[Tech Lead/Assigner Name]`
*   **Date Assigned:** `[Date]`
*   **Relevant ADRs/Docs:**
    *   `[Link to relevant ADR(s), e.g., ../ARCHITECTURE/adr/ADR_001.md]`
    *   `[Link to relevant pattern document(s), e.g., ../ARCHITECTURE/patterns/pattern_x.md]`
    *   `[Link to relevant library integration guide(s), e.g., ../LIBRARY_INTEGRATION/library_y_guide.md]`
    *   `[Link to project's 01_IDL_GUIDELINES.md, 02_IMPLEMENTATION_RULES.md]`

---

**1. Task Goal:**

*   `[Clearly describe the overall objective of this task. What feature should be implemented or what bug should be fixed? What is the desired outcome? Keep it concise.]`

**2. Context & Requirements:**

*   **Primary IDL File(s) & Contract:**
    *   The main specification for the component(s) you will be working on is defined in:
        *   `[Link to primary IDL file, e.g., ../../src/component_x/module_y_IDL.md]`
        *   `[Link to another relevant IDL file, if any]`
    *   **Key Focus Areas in IDL(s):** Pay close attention to the following methods/behaviors/error conditions defined in the IDL specification(s):
        *   `[Method/Behavior 1 from IDL - e.g., ComponentX.process_data]`
        *   `[Method/Behavior 2 from IDL - e.g., Handling of 'Expected Data Format' for parameter Z]`
        *   `[Error condition from IDL - e.g., @raises_error(condition="DataNotFound", ...)]`
        *   `[...]`
*   **Dependencies & Interactions:**
    **Action Required:** Before implementing the interactions below, please review the linked `IDL / Docs` for each dependency. Pay close attention to the documentation for any external libraries, APIs, or tools to fully understand their expected usage, parameters, and return values.
    *   This task involves interacting with the following components/libraries:
        *   **`[Dependency 1 Name, e.g., DataStoreService]`:**
            *   IDL/Docs: `[Link to Dependency 1 IDL or relevant docs, e.g., ../../src/services/data_store_IDL.md]`
            *   Key Interaction: `[Explain how your code should use this dependency. E.g., "You will need to call data_store_service.fetch_record(record_id). Expect a RecordObject or null back. Handle potential connection errors."] `
        *   **`[Dependency 2 Name, e.g., External [SomeAPI] Service]`:**
            *   IDL/Docs: `[Link to relevant docs, e.g., ../LIBRARY_INTEGRATION/[SomeAPI]_client_guide.md]`
            *   Key Interaction: `[E.g., "Use the [SomeAPI]Client instance (passed as api_client) to call api_client.submit_job(job_params). Ensure you construct the job_params dictionary correctly based on the API's requirements. Handle potential API rate limits or authentication errors."] `
        *   `[...]`
    *   **Data Structures:** You will primarily work with these data structures defined in `[e.g., ../ARCHITECTURE/types.md or component-specific IDL files]`:
        *   `[List relevant data models/structs, e.g., UserProfile, OrderDetails, OperationOutcome]`

**3. Provided Stubs (If Applicable):**

*   The following skeleton files and test stubs have been created for you:
    *   `[path/to/implementation_file.ext]` (Contains class/function stubs)
    *   `[path/to/test_file.ext]` (Contains empty test function stubs)
    *   `[...]`
*   Please implement the logic directly within these files. Ensure you fetch the latest changes if working on a shared branch.

**4. Implementation Plan:**

*   Implement the logic for `[ClassName.method_name or function_name]` in `[path/to/implementation_file.ext]`.
*   Follow these specific steps:
    1.  `[Detailed step 1: e.g., "Retrieve the 'config_options' dictionary from the 'settings' parameter."]`
    2.  `[Detailed step 2: e.g., "Validate that 'config_options' contains the required 'timeout' key using the 'get_required_config' helper."]`
    3.  `[Detailed step 3: e.g., "If validation passes, call the 'dependency_x.process_item(item_data, timeout_value)' method. Wrap this call in a try...except block to catch 'DependencyXTimeoutError'."]`
        *   `[Sub-step/detail: e.g., "If 'DependencyXTimeoutError' occurs, log a warning and return a FAILED OperationOutcome with reason 'DEPENDENCY_TIMEOUT'."]`
    4.  `[Detailed step 4: e.g., "If the call to 'dependency_x.process_item' is successful, transform its result into the 'ProcessedData' model."]`
    5.  `[...]`
*   **Data Models:** `[Specify any data models the developer needs to use or be aware of, e.g., "Ensure the final return value is an OperationOutcome object/dictionary."]`
*   **Key Considerations:** `[Highlight any tricky parts, edge cases to consider, or specific project patterns to follow, e.g., "Remember to use the shared 'error_reporting_util' for consistent error logging.", "Ensure all external calls are idempotent if possible."]`
    *   **Host Language/DSL Interaction (If Applicable):** If this task involves invoking a Domain-Specific Language (DSL) evaluator with complex inputs, ensure data preparation logic resides in the host language code. Pass prepared data via the DSL environment bindings. Refer to `02_IMPLEMENTATION_RULES.md` for guidelines.

**5. Testing Plan:**

*   **Testing Strategy Overview:** `[Tech Lead provides brief context, e.g., "Focus on integration testing the main success path and key error conditions. Mock the ExternalAPIService."]`
*   **Test Files:** Implement tests in `[path/to/test_file.ext]`.
*   **Detailed Test Cases:** Implement the following test functions (stubs may be provided):
    *   **`test_function_name_scenario_1`**:
        *   **Purpose:** `[e.g., "Verify successful processing when all inputs are valid and dependencies behave as expected."]`
        *   **Setup/Fixtures:** `[e.g., "Use 'mock_data_store' fixture. Configure 'mock_external_api.submit.return_value = successful_api_response'."]`
        *   **Assertions:** `[e.g., "mock_data_store.save.assert_called_once_with(expected_record)`. `assert result.status == 'SUCCESS'". "assert result.data['processed_id'] is not None".]`
    *   **`test_function_name_scenario_2_error_case`**:
        *   **Purpose:** `[e.g., "Verify correct error handling when 'DependencyX' raises 'ItemNotFoundError'."]`
        *   **Setup/Fixtures:** `[e.g., "Configure 'mock_dependency_x.process_item.side_effect = ItemNotFoundError('item not found')'."]`
        *   **Assertions:** `[e.g., "assert result.status == 'FAILURE'". "assert result.error_code == 'ITEM_NOT_FOUND_IN_DEPENDENCY'". "mock_dependency_x.process_item.assert_called_once()".]`
    *   `[...]` *(Add entries for all required test cases)*

**6. Running Tests & Debugging:**

*   **Running Tests:**
    *   To run all tests for this component: `[e.g., pytest tests/component_x/test_module_y.py]`
    *   To run a specific test: `[e.g., pytest tests/component_x/test_module_y.py -k test_function_name_scenario_1]`
*   **Debugging Tips:**
    *   `[Tip 1: e.g., "Use logging.debug() extensively within your implemented logic to trace variable values."]`
    *   `[Tip 2: e.g., "Set breakpoints using breakpoint() before critical calls to inspect inputs."]`
    *   `[Tip 3: e.g., "If tests fail on assertions about mock calls, print the mock_dependency.method_calls attribute to see exactly how the mock was called."]`
    *   `[Tip 4: e.g., "Common Issue: Ensure data models are being instantiated correctly if parsing external data."]`
    *   **Asking for Help:** If you are stuck for more than `[e.g., 30-60 minutes]` after trying to debug, please reach out. Explain what you are trying to achieve, what you have tried, and what error you are seeing.

**7. Definition of Done:**

*   [ ] All implementation steps in Section 4 are completed.
*   [ ] All detailed test cases in Section 5 are implemented and pass.
*   [ ] Code passes linting (`[e.g., make lint]`).
*   [ ] Code passes formatting (`[e.g., make format]`).
*   [ ] The full project test suite passes (`[e.g., make test]`).
*   [ ] You have performed a self-review of your code for clarity and correctness against the IDL specification and project rules.
*   [ ] Your working memory log (`TEMPLATES/WORKING_MEMORY_LOG_TEMPLATE.md` or equivalent) has been updated with your work.
*   [ ] Code is committed with a clear commit message.
*   [ ] Pull Request (if applicable) is created and linked to the issue ticket.
*   [ ] Relevant IDL specification(s) have been reviewed and updated if the implementation required contract changes (discuss with Tech Lead).

**8. Notes/Questions (For Developer to fill in):**

*   `[Space for the developer to jot down questions, observations, or issues encountered during implementation.]`
</file>

<file path="docs/WORKFLOWS/TASK_PREPARATION_WORKFLOW.md">
# Tech Lead Workflow: Task Preparation & Instruction Generation

**Purpose:** This guide outlines a standard process for a Tech Lead (TL) or senior developer to prepare a development task based on requirements and IDL specifications. The key output of this process is a detailed instruction document for the implementing developer.

**Goal:** To ensure tasks are well-defined, architecturally sound, and have a clear implementation and testing plan before coding begins, facilitating a smooth handoff and successful implementation.

---

**Phase 0: Preparation & Context Gathering**

*   **Goal:** Thoroughly understand the task requirements, the primary component's contract (IDL), and the context of its dependencies and interactions.
*   **Actions:**
    1.  **Define Task:** Clearly understand the overall goal (e.g., from an issue tracker, user story, feature request). Identify the scope and desired outcome.
    2.  **Locate Primary IDL File(s):** Find the main IDL file(s) (e.g., `*_IDL.md`) for the component(s) being implemented or modified. If no IDL exists for modifications, consider creating/updating one first as per `01_IDL_GUIDELINES.md`.
    3.  **Analyze IDL Contract:**
        *   Read the primary IDL(s) carefully.
        *   Identify: Interface/Module Purpose, Method/Function Signatures, Preconditions, Postconditions, Documented Behavior, Error Conditions, Expected Data Structures. Note any ambiguities.
    4.  **Identify & Review Dependencies:**
        *   List all declared dependencies (e.g., `@depends_on`, `@depends_on_resource`) from the primary IDL(s).
        *   Locate and **review the IDLs** for any internal components listed as dependencies. Understand *their* contracts.
    5.  **Study Relevant External/Internal APIs & Libraries:**
        *   Based on the task goal and dependencies, identify relevant external libraries, third-party tools, internal APIs, or system primitives that will be used.
        *   **Action:** **Thoroughly consult** the relevant documentation:
            *   `../LIBRARY_INTEGRATION/` for guides on key project libraries.
            *   `../ARCHITECTURE/types.md` or component-specific IDLs for internal APIs/protocols/types.
            *   Official external documentation for third-party libraries/tools.
        *   **Goal:** Fully understand the specific functions/methods/endpoints to call, required parameters, expected data formats, and error handling of these dependencies.
    6.  **Review Related Project Docs:** Check relevant ADRs (`../ARCHITECTURE/adr/`), architectural patterns, and project rules (`02_IMPLEMENTATION_RULES.md`, `03_PROJECT_RULES.md`).
    7.  **Formulate Testing Strategy:** Based on dependencies and project guidelines (see `02_IMPLEMENTATION_RULES.md`), decide the primary testing approach (e.g., prioritize integration tests, identify necessary unit tests, determine key boundaries for mocking).
    8.  **Update Working Memory:** Record the task, key findings, reviewed documents, identified risks/ambiguities, and initial testing strategy in your working memory log (see `../TEMPLATES/WORKING_MEMORY_LOG_TEMPLATE.md`).

**Phase 1: Stubbing & Plan Generation**

*   **Goal:** Create the necessary code/test skeletons and generate a detailed, actionable instruction document for the implementing developer.
*   **Actions:**
    1.  **Stub Skeleton Code:** Create basic code file(s) and class/function structures matching the IDL. Implement exact signatures with type hints. Copy/adapt IDL documentation into docstrings/comments. Add placeholder bodies (`pass` or `raise NotImplementedError`).
    2.  **Stub Tests & Outline Strategy:** Create empty test methods corresponding to key aspects identified in Phase 0 (success paths, error conditions, edge cases). Briefly outline the overall testing strategy in comments.
    3.  **Compile Detailed Implementation Plan for Developer:**
        *   Based on the IDL behavior, dependency knowledge (Phase 0, Step 5), and architectural constraints, outline specific implementation steps.
        *   Specify algorithms, data structures (e.g., from `../ARCHITECTURE/types.md` or new ones to define).
        *   Detail *exactly* how to interact with dependencies (methods, parameters, handling results/errors).
        *   Specify precise error handling logic.
        *   **Define Host Language/DSL Boundary (If Applicable):** Clearly specify which parts of the logic involve complex data preparation (to be done in the host language) and which parts involve DSL/script execution. Detail how data prepared in the host language should be bound to the DSL environment.
    4.  **Compile Detailed Testing Plan for Developer:**
        *   For each stubbed test function:
            *   Specify required test framework fixtures.
            *   Detail necessary mock configurations (e.g., `mock_dependency.method.return_value = ...`, `mock_dependency.method.side_effect = ...`).
            *   List the *exact* assertions needed.
    5.  **Add Execution & Debugging Guidance for Developer:**
        *   Provide specific commands to run relevant tests.
        *   Include debugging tips relevant to the task.
        *   Clarify when and how the developer should seek help.
    6.  **Define Definition of Done:** Create a checklist outlining criteria for task completion (e.g., code implemented per plan, all specified tests passing, linting/formatting clean, self-review done, IDL updated if contract changed).
    7.  **Assemble Instruction Document:** Collate all the above details into the **Task Instruction Template** (see `../TEMPLATES/TASK_INSTRUCTION_TEMPLATE.md`). Ensure clarity, precision, and completeness.

**Phase 2: Handoff & Follow-up**

*   **Goal:** Assign the task with clear instructions and ensure necessary code stubs are available.
*   **Actions:**
    1.  **Finalize & Review Instructions:** Read through the completed instruction template.
    2.  **Commit Stubs:** Commit the stubbed code and test files created in Phase 1.
    3.  **Assign Task:** Assign the task to the developer via the issue tracker, linking to the detailed instruction document and relevant commit/branch.
    4.  **Plan Code Review:** Mentally note or schedule time for reviewing the developer's implementation upon completion.
</file>

<file path="docs/01_IDL_GUIDELINES.md">
# Interface Definition Language (IDL) Guidelines

**i. Overview & Purpose**

These guidelines define a system for creating and understanding Interface Definition Language (IDL) specifications and their corresponding code implementations. The process is designed to be **bidirectional**:

1.  **IDL-to-Code:** Use a defined IDL as a comprehensive specification to generate compliant, high-quality code.
2.  **Code-to-IDL:** Reduce existing code to its essential IDL specification, abstracting away implementation details and presentation logic to reveal the core behavioral specification and functional contract.

The goal is a clear, language- and UI-agnostic **specification (SPEC)** that defines the *what* (the functional contract and essential behavior) and separates it from the *how* (the specific implementation details within the code).

**ii. IDL Creation & Structure Guidelines**

*(These apply both when writing IDLs from scratch and when reducing code to IDL)*

1.  **Object Oriented (or Component-Based):** Structure the IDL using modules and interfaces (or equivalent concepts like components/services) to represent logical groupings and entities.
2.  **Dependency Declaration:**
    *   **Purpose:** To explicitly declare dependencies required to fulfill the interface's contract. This clarifies coupling at the design level.
    *   **Syntax (Example using comments):** Use comment lines placed immediately before the `interface` definition (or equivalent).
        *   For dependencies on other IDL-defined interfaces/modules:
            `# @depends_on([idl_module_or_interface_name], ...)`
        *   For dependencies on abstract types of external resources or systems:
            `# @depends_on_resource(type="Database", purpose="Storing user profiles")`
            `# @depends_on_resource(type="MessageQueue", purpose="Processing background jobs")`
            `# @depends_on_resource(type="FileSystem", purpose="Reading/writing files")`
    *   **Target:**
        *   `@depends_on`: Names must refer to other `module` or `interface` definitions within the IDL system.
        *   `@depends_on_resource`: `type` is an abstract category (e.g., "Database", "FileSystem", "ExternalAPI"), `purpose` is a brief description.
    *   **Implication:** These declarations signal requirements for the implementation. `@depends_on` implies needing access to implementations of other IDL contracts. `@depends_on_resource` implies needing access to a specific *kind* of external system or resource, configured appropriately.

### Implementation Conformance

The IDL file serves as a **strict contract**. Implementations (e.g., in Python) MUST adhere precisely to all aspects defined in the IDL, including:

*   **Interfaces and Signatures:** Class/component names, method names, parameter names, parameter order, type hints (mapping IDL types to language equivalents), and return types must match exactly.
*   **Behavior:** The implemented logic must fulfill the functional description provided in the `Behavior:` block.
*   **Preconditions/Postconditions:** The implementation must respect documented `Preconditions:` and guarantee `Postconditions:` upon successful execution.
*   **Data Structures:** Implementations must correctly handle data structures as defined (e.g., via `Expected Data Format`, `struct` definitions, or linked type definitions), including required keys or fields. Internal validation logic within components must align with these IDL specifications.
*   **Error Conditions:** Documented error conditions (e.g., via `@raises_error`) must be handled, either by raising the specified (or a corresponding mapped) exception or by returning a failure status as defined by the project's error handling philosophy (see `02_IMPLEMENTATION_RULES.md`).

3.  **Design Patterns:** Utilize (when creating IDLs) or identify (when reducing code) interfaces supporting established design patterns like Factory, Builder, Strategy where they clarify the design or improve flexibility. The IDL defines the *contract* and potentially the *behavioral role* of these patterns.
4.  **Complex Parameters (e.g., JSON, Structured Objects):**
    *   **Preference for Structure:** If the IDL syntax supports defining custom structures/records/data classes (see `struct` below), prefer them for complex data transfer to maximize type safety at the interface level.
    *   **Serialized String Fallback (e.g., JSON):** Where structured types are not feasible in the IDL language or cross-language string simplicity is paramount, a single serialized string parameter (e.g., JSON string) can be used.
    *   **Mandatory Documentation:** *Always* document the exact expected data format within the IDL comment block (e.g., `// Expected Data Format (JSON): { "key1": "type1", "key2": "type2" }`).
5.  **Defining Custom Data Structures (`struct` or equivalent):**

*   **Purpose:** To explicitly define the structure of complex data types passed between components, improving clarity and mapping directly to implementation structures (like Pydantic models in Python, or classes/records in other languages). Replaces reliance on generic `dict`/`map` types with comments for complex data.
*   **Syntax (Example):** Use a `struct` block (or similar keyword based on IDL language), typically within a `module` or global types definition.

    ```
    // Example IDL-like syntax for a struct
    struct StructName {
        field_name1: type; // e.g., string, int, boolean, list<AnotherStruct>, map<string, int>
        field_name2: optional type; // Use 'optional' for fields that may be absent/null
        field_name3: union<type1, type2>; // Use 'union' if a field can be one of multiple types
        // ... other fields
        // Use consistent naming (e.g., snake_case or camelCase) for field names.
    }
    ```

*   **Placement Strategy:**
    *   **`ARCHITECTURE/types.md` (or equivalent):** Define globally shared structures used across many modules/components (e.g., `OperationOutcome`, `StandardRequest`) in this central file.
    *   **Module-Specific IDL File:** Define structures primarily used by interfaces within a single module/component directly within that module's IDL file.

*   **Referencing:** Use the defined `StructName` as a type name in method parameters, return types, or within other `struct` definitions.

*   **Example:**

    ```
    // In ARCHITECTURE/types.md (Conceptual Example)
    struct OperationOutcome {
        status: string; // e.g., "SUCCESS", "FAILURE", "PENDING"
        message: optional string;
        data: optional Any; // Or specific union type if known
    }

    // In src/data_processor/data_processor_IDL.md
    module src.data_processor {

        // Locally defined struct
        struct ProcessedItem {
             item_id: string;
             processed_value: float;
             notes: optional string;
        }

        // Struct referencing another struct
        struct BatchProcessingResult {
             batch_id: string;
             items_processed: list<ProcessedItem>;
             overall_status: string; // Could reference OperationOutcome.status conceptually
             error_details: optional string;
        }

        // Interface using the defined structs
        interface DataProcessor {
             BatchProcessingResult process_data_batch(list<string> raw_data_items);

             // Method using a globally defined struct
             OperationOutcome check_system_health();
        }
    }
    ```
*   **Dependency Note (Optional Convention):** If an interface relies heavily on a type defined in the global types file, you *may* add a comment like `# @depends_on_type(ARCHITECTURE.types.OperationOutcome)` near the interface definition for extra clarity.

6.  **Clarity and Behavioral Specification:** The IDL must clearly express the *purpose* and *complete expected behavior* of each interface and method. This is achieved through:
    *   Well-chosen names for interfaces, methods, and parameters.
    *   **Comprehensive documentation comments:** These are critical for the SPEC. They must detail:
        *   **Preconditions:** Necessary conditions before calling.
        *   **Postconditions:** Expected outcomes, return value guarantees, and significant state changes after successful execution.
        *   **Essential Algorithms/Logic:** A conceptual description of any core algorithms or business logic the method implements, sufficient to understand its behavior without seeing the code.
        *   **Conceptual State:** Description of key internal state variables managed by the interface (if any) and how methods affect them.
        *   **Error Conditions:** Use an error annotation (see below) for specific, named error conditions that are part of the contract.
        *   Data format documentation if applicable.
7.  **Completeness (Behavioral Specification):** The IDL must represent the *complete behavioral specification* of the component's public interface and its essential, observable behavior.
8.  **Error Condition Annotation:** Use a dedicated annotation to formally define specific, named error conditions that are part of the interface's contract.
    *   **Syntax (Example using comments):** Use comment lines within the method's documentation block.
        `// @raises_error(condition="UniqueErrorCode", description="Explanation of when this error occurs.")`
    *   **Purpose:** To standardize the reporting of specific failure modes beyond simple success/failure, making the contract more precise. `condition` should be a stable identifier.
    *   **Note:** While this documents potential exceptions/errors, the project's error handling philosophy (see `02_IMPLEMENTATION_RULES.md`) should guide whether errors are raised as exceptions or returned as structured error responses.

**iii. Code-to-IDL Reduction Guidelines**

*(These specific rules apply only when generating an IDL from existing code)*

1.  **Goal: Extract the Contract:** The primary objective is to distill the code down to its public interface and functional guarantees, omitting all non-essential implementation details.
2.  **Mapping:**
    *   Public classes/modules generally map to IDL `module` or `interface`.
    *   Public methods/functions map to IDL method definitions.
    *   Method signatures (name, parameter types, return type) must be accurately reflected.
3.  **Dependency Identification:** Identify dependencies on other components *that are also being defined via IDL*. Represent these using the **dependency declaration** syntax (e.g., `# @depends_on(...)`) described in section ii.2. Exclude dependencies on third-party libraries or internal implementation details not represented by an interface in the IDL.
4.  **Documentation Extraction:**
    *   Infer **preconditions** from input validation, assertions, and documentation comments in the code.
    *   Infer **postconditions** from return value guarantees, state changes described in documentation, or observable outcomes. Document these clearly.
    *   Identify and document **invariants** – properties of the object's conceptual state that hold true between public method calls.
    *   Extract descriptions of **essential algorithms or core logic** from code/comments and summarize them conceptually in the method documentation.
    *   Identify **specific error conditions** raised or returned that represent contractual failure modes. Document these using the error annotation.
    *   If complex objects/dictionaries are passed, represent them using the **complex parameters** guideline (ii.4) and document the format.
5.  **Exclusion Criteria:** The following elements **must be excluded** from the generated IDL *structure* (interfaces, methods) as they are implementation details, presentation logic, or non-functional aspects. However, *essential behavioral aspects* derived from these (like core logic or error conditions) should be *described* in the IDL documentation comments or captured via annotations.
    *   **Presentation Logic:** Any code related to GUIs, TUIs, web page rendering, console output formatting.
    *   **Internal Implementation Code:** Private methods, helper functions, specific algorithms, internal data structures. (Note: The *behavior* or *purpose* of essential algorithms or the *conceptual* nature of key internal state *should* be documented).
    *   **Type Enforcement/Validation Code:** The *internal logic* for validation. The *requirement* for valid input is a precondition. Specific validation failure *error conditions* might be documented if contractual.
    *   **Non-Functional Code:** Logging statements, metrics collection, performance monitoring, debugging utilities, internal comments explaining *how* the code works (vs. *what* it guarantees).
    *   **Language/Platform Specifics:** Boilerplate code, language-specific idioms, environment configuration loading, build system artifacts, specific library dependencies (unless they form part of public signatures or are abstracted via `@depends_on_resource`).
    *   **Internal Error Handling Mechanisms:** Specific exception types thrown/caught internally. Contractual, observable error conditions should be documented. Generic internal failures are not part of the IDL spec.
    *   Dependencies on concrete libraries or modules *not* represented by an IDL interface or abstracted via `@depends_on_resource`.
    *   **Internal Helper Components:** Classes, functions, or modules created solely as internal implementation details to support a public interface, and not intended for direct use by other independent components. The delegation of work to such helpers *should*, however, be documented in the `Behavior` section of the public interface's method(s) that use them.

**iv. IDL Template (Conceptual)**

```
// == BEGIN IDL TEMPLATE (Conceptual) ==
module [your_system_name].[component_name] {

    // Optional: Define shared data structures if IDL language supports
    // struct SharedData { ... }

    // Example interface demonstrating dependency declarations and annotations
    # @depends_on([another_interface_name], [shared_module_name]) // Dependency on other IDL contracts
    # @depends_on_resource(type="KeyValueStore", purpose="Caching intermediate results") // Abstract resource dependency
    interface [EntityName] {

        // Action/method definition
        // Preconditions:
        // - Define necessary conditions before calling.
        // - [parameter_name] must be positive.
        // - (If using complex param) Expected Data Format: { "key1": "type1", ... }
        // Postconditions:
        // - Define expected outcomes and state changes after successful execution.
        // - Returns the calculated result based on input.
        // - Internal cache (conceptual state) may be updated.
        // Behavior:
        // - Describe essential algorithm/logic here, e.g., "Calculates result using the [algorithm_name] algorithm."
        // - "If the result is found in the KeyValueStore cache, it's returned directly."
        // @raises_error(condition="InvalidInput", description="Raised if [parameter_name] is non-positive.")
        // @raises_error(condition="ResourceUnavailable", description="Raised if the KeyValueStore cannot be accessed.")
        [return_type] [method_name]([parameter_type] [parameter_name]);

        // Additional methods...

        // Invariants: (Optional: define properties that always hold true for this entity's conceptual state)
        // - Describe state invariants here, e.g., "Internal cache size never exceeds max limit."
    }

    // Another entity or component that [EntityName] might depend on
    interface [another_interface_name] {
        // ... methods ...
    }

    // Could also be a module containing utility interfaces/functions
    module [shared_module_name] {
        // ... interfaces or potentially functions if IDL language supports them ...
    }
}
// == END IDL TEMPLATE (Conceptual) ==
```

**v. Code Creation Rules**

*(These apply when implementing code based on an IDL)*

1.  **Strict Typing:** Always use strict typing in your implementation language. Avoid ambiguous or "any" types where possible.
2.  **Primitive Types Focus (Balanced):** Prefer built-in primitive and standard collection types where they suffice. Use well-defined data classes/structs (like Pydantic models) for related data elements passed together. Avoid "primitive obsession." Match IDL types precisely.
3.  **Portability Mandate:** Write code intended for potential porting to other languages (e.g., Java, Go, JavaScript). Use language-agnostic logic and avoid platform-specific dependencies or language features without clear equivalents where feasible.
4.  **Minimize Side Effects:** Strive for pure functions for data processing. Clearly document all necessary side effects (state mutation, I/O, external calls) associated with methods defined in the IDL, typically in the implementation's documentation, aligning with the IDL's postconditions.
5.  **Testability & Dependency Injection:** Design for testability. Use dependency injection; avoid tight coupling. Ensure methods corresponding to IDL definitions are unit-testable. Pay attention to the dependency declarations in the IDL to identify required dependencies that should likely be injected.
6.  **Documentation:** Thoroughly document implementation details, especially nuances not obvious from the IDL or code signature. Link back to the IDL contract being fulfilled.
7.  **Contractual Obligation:** The IDL is a strict contract. Implement *all* specified interfaces, methods, and constraints *precisely* as defined. Do not add public methods or change signatures defined in the IDL without updating the IDL.

**vi. Example (Generic Social Media Post)**

```
module [social_platform].[post_service] {

    # @depends_on([user_service.UserValidator]) // Depends on a user validation contract
    # @depends_on_resource(type="DataStore", purpose="Persisting post data")
    # @depends_on_resource(type="NotificationService", purpose="Sending notifications")
    interface PostManager {
        // Preconditions:
        // - User referenced by `user_id` in `post_data_json` exists (verified via UserValidator).
        // - Post content is non-null and within allowable size limits.
        // Expected Data Format (JSON for post_data_json): { "user_id": "string", "content": "string" }
        // Postconditions:
        // - A new post is created and persisted in the DataStore.
        // - If content contains mentions (e.g., @username), a notification task may be queued via NotificationService.
        // Behavior:
        // - Validates input JSON structure.
        // - Checks user existence via UserValidator.
        // - Checks content length.
        // - Persists post data to DataStore.
        // - Parses content for mentions and potentially triggers notifications.
        // @raises_error(condition="InvalidPostFormat", description="JSON format is incorrect.")
        // @raises_error(condition="UserNotFound", description="User specified in user_id does not exist.")
        // @raises_error(condition="ContentTooLong", description="Post content exceeds the size limit.")
        // @raises_error(condition="StorageFailure", description="Failed to persist post to DataStore.")
        void create_post(string post_data_json);

        // Preconditions:
        // - Post referenced by `post_id` exists (verified via DataStore lookup).
        // Postconditions:
        // - Returns the details of the post (content, author, stats) as a JSON string.
        // Behavior:
        // - Retrieves post data from DataStore.
        // - Formats data into the specified JSON structure.
        // @raises_error(condition="PostNotFound", description="Post specified by post_id does not exist.")
        // @raises_error(condition="StorageFailure", description="Failed to retrieve post details from DataStore.")
        string get_post_details(string post_id);
    }
}
```

**vii. Documenting Component Interactions (Optional but Recommended)**

To enhance understanding of how components collaborate, especially for complex workflows or critical interaction patterns, IDL files can optionally include a dedicated section for interaction documentation. This section typically uses sequence diagrams (e.g., using Mermaid syntax) and textual explanations.

**7.1. Standard Location and Markers**

Interaction documentation should be placed within the main IDL definition, but *outside* any specific `module` or `interface` blocks, usually towards the end of the file or in a clearly marked section.

Example Markers:
```
// == BEGIN COMPONENT INTERACTIONS ==
// ... diagrams and explanations ...
// == END COMPONENT INTERACTIONS ==
```

**7.2. Content**

This section can contain:

*   **Scenario Titles:** Use headings (e.g., `// === Workflow: Descriptive Title ===`) to delineate different interaction scenarios.
*   **Sequence Diagrams:** Use a diagramming syntax like Mermaid, enclosed in appropriate comment or code fences:
  ```
  // ```mermaid
  // sequenceDiagram
  //    participant A
  //    participant B
  //    A->>B: Request
  //    B-->>A: Response
  // ```
  ```
*   **Textual Explanations:** Provide a brief explanation of the diagram, the context of the interaction, and key steps or data flows.

**7.3. Example**

```
// == BEGIN COMPONENT INTERACTIONS ==
// This section illustrates how the [OrchestratorComponent] interacts with its dependencies.

// === Workflow: Processing a Complex Request ===
// This diagram shows the call flow when `process_complex_request` is invoked.

// ```mermaid
// sequenceDiagram
//    Client->>+[OrchestratorComponent]: process_complex_request("input_data")
//    [OrchestratorComponent]->>+[ValidatorComponent]: validate("input_data")
//    [ValidatorComponent]-->>-[OrchestratorComponent]: validation_result
//    alt validation_result is OK
//        [OrchestratorComponent]->>+[ProcessorComponent]: execute_processing("input_data")
//        [ProcessorComponent]-->>-[OrchestratorComponent]: processing_output
//        [OrchestratorComponent]->>+[LoggerComponent]: log_success("input_data", processing_output)
//        [LoggerComponent]-->>-[OrchestratorComponent]: ack
//    else validation_result is ERROR
//        [OrchestratorComponent]->>+[LoggerComponent]: log_failure("input_data", validation_result)
//        [LoggerComponent]-->>-[OrchestratorComponent]: ack
//    end
//    [OrchestratorComponent]-->>-Client: final_status
// ```
//
// **Explanation:**
// 1. The `Client` calls `process_complex_request` on the `[OrchestratorComponent]`.
// 2. The `[OrchestratorComponent]` first validates the input using the `[ValidatorComponent]`.
// 3. If validation succeeds, it proceeds to the `[ProcessorComponent]` and logs success.
// 4. If validation fails, it logs the failure.
// 5. A final status is returned to the `Client`.

// == END COMPONENT INTERACTIONS ==
```

**7.4. Alternative: Linked Interaction Documents**

For components with exceptionally numerous or detailed interaction scenarios, a separate Markdown file may be used. In such cases, the IDL file should include a clear reference:

```
// For detailed interaction diagrams and explanations, see:
// ./[component_name]_interactions.md
// (Example: ./orchestrator_interactions.md)
```
Or, using a custom tag:
```
// @see_interactions_doc(./[component_name]_interactions.md)
```
The linked file would then contain the diagrams and explanations. However, embedding directly within the IDL is preferred for co-location of contract and behavioral examples when feasible.

**7.5. Purpose and Maintenance**

This section is intended to:
* Clarify the behavioral contract of the component in relation to its dependencies.
* Aid developers in understanding how to use the component and how it fits into larger workflows.
* Serve as living documentation that should be updated when significant interaction patterns change.

While optional, adding this section is highly recommended for components that play a central role in orchestrating other services or have complex internal call flows involving multiple dependencies.
</file>

<file path="docs/04_REFACTORING_GUIDE.md">
# Refactoring Guide

**1. Purpose**

This guide provides conventions and a recommended workflow for refactoring code within this project. Refactoring is the process of restructuring existing computer code – changing the factoring – without changing its external behavior. It aims to improve non-functional attributes like readability, maintainability, simplicity, and adherence to design principles (like Single Responsibility Principle - SRP).

**Related Documents:**
*   `02_IMPLEMENTATION_RULES.md` (Coding standards)
*   `03_PROJECT_RULES.md` (Module length guideline, Version control workflow)
*   `TEMPLATES/WORKING_MEMORY_LOG_TEMPLATE.md` (Developer working memory log)

**2. When to Refactor**

Consider refactoring when you encounter:

*   **Code Smells:** Duplicated code, long methods/classes, complex conditional logic, excessive parameters, tight coupling, etc.
*   **Module Length:** A module significantly exceeds the guideline set in `03_PROJECT_RULES.md` (e.g., > `[e.g., 300-500]` LoC).
*   **Complexity:** A component becomes difficult to understand, test, or modify.
*   **Poor Cohesion:** A module or class handles too many unrelated responsibilities.
*   **New Requirements:** Adapting existing code for new features reveals structural weaknesses.
*   **Technical Debt:** Addressing known shortcuts or suboptimal designs.

**3. General Principles**

*   **Small, Incremental Steps:** Break down large refactorings into smaller, manageable changes.
*   **Test Frequently:** Run tests after each small step to ensure behavior is preserved.
*   **Preserve External Behavior:** The primary goal is to improve internal structure *without* altering how the component interacts with others (its public contract/API, as defined in its IDL specification). If the contract *must* change, it's technically more than just refactoring and requires careful consideration of dependents and updates to the IDL specification.
*   **Improve Structure:** Focus on enhancing clarity, reducing complexity, and improving modularity (e.g., better adherence to SRP).
*   **Use Version Control:** Commit frequently with clear messages describing each refactoring step.

**4. Formalized Refactoring Decision Workflow**

This section outlines a structured approach for identifying refactoring needs, analyzing candidates, choosing a strategy, and verifying the results.

**Phase 1: Identification & Initial Assessment**

*   **Input:**
    *   Code base or component under review.
    *   Project quality metrics (if available).
    *   Developer observations/pain points.
*   **Actions:**
    *   Scan for code smells.
    *   Review module length against guidelines in `03_PROJECT_RULES.md`.
    *   Identify components with high change frequency or bug density.
    *   Note areas where developers frequently express confusion or frustration.
    *   Check for violations of project architecture or implementation rules.
    *   Run automated checks (e.g., line count, cyclomatic complexity) to identify overly long or complex components.
*   **Output:**
    *   List of refactoring candidates with brief descriptions of issues.
    *   Initial severity assessment (High/Medium/Low) based on impact on development velocity, risk of bugs, maintenance burden, and violation of core architectural principles.

**Phase 2: Detailed Analysis**

*   **Input:**
    *   Refactoring candidates from Phase 1.
    *   Relevant documentation (IDL files, architecture diagrams).
    *   Test coverage reports.
*   **Actions:**
    *   For each high-priority candidate:
        *   Analyze dependencies (what components depend on this code?).
        *   Assess test coverage (is behavior well-tested?).
        *   Identify specific design problems (SRP violations, tight coupling, etc.).
        *   Analyze complexity metrics (e.g., cyclomatic complexity).
        *   Determine root causes.
        *   Estimate effort required to refactor.
        *   Evaluate risks (complexity, potential for regression).
*   **Output:**
    *   Detailed analysis document for each candidate, including: specific problems, dependencies, test coverage, effort estimate, risk assessment.

**Phase 3: Strategy Selection**

*   **Input:**
    *   Detailed analysis from Phase 2.
    *   Project priorities and constraints.
    *   Available developer resources.
*   **Actions:**
    *   For each refactoring candidate, select an appropriate strategy from common refactoring techniques (e.g., Extract Method/Class, Move Method/Field, Replace Conditional with Polymorphism, Introduce Parameter Object, Rename, etc.).
*   **Output:**
    *   Refactoring plan for each candidate, including: selected strategy, specific steps, acceptance criteria.

**Phase 4: Planning & Preparation**

*   **Input:**
    *   Refactoring plans from Phase 3.
    *   Project schedule and priorities.
    *   Test infrastructure status.
*   **Actions:**
    *   Break down large refactorings into smaller, incremental steps.
    *   Identify or create tests that verify the current behavior. **Ensure test coverage is adequate before starting.**
    *   Schedule refactoring work.
    *   Update your working memory log (e.g., `TEMPLATES/WORKING_MEMORY_LOG_TEMPLATE.md`) with the refactoring task.
*   **Output:**
    *   Detailed, step-by-step refactoring plan with: specific files/sections to modify, order of operations, test strategy for each step, rollback plan, documentation updates needed.

**Phase 5: Execution (Iterative)**
*(This phase aligns with the "Recommended Refactoring Execution Workflow" in Section 6)*

*   **Actions:**
    *   Implement the refactoring in small, incremental steps.
    *   Run tests frequently after each step.
    *   Commit changes regularly with clear messages.
    *   Adapt tests as the internal structure changes, ensuring they still verify the external contract.

**Phase 6: Verification & Cleanup**

*   **Input:**
    *   Completed refactoring changes.
    *   Test results.
    *   Code review feedback.
*   **Actions:**
    *   Verify all tests pass (existing and new).
    *   Conduct code review.
    *   Check for any unintended side effects or regressions.
    *   Update documentation (IDL specifications, architecture diagrams if impacted) to reflect the new structure.
    *   Remove any dead code or unused imports.
    *   Run linters and formatters.
    *   Update your working memory log with the completed refactoring.
*   **Output:**
    *   Verified, clean, refactored code.
    *   Updated documentation.
    *   Lessons learned.

**6. Recommended Refactoring Execution Workflow**

This workflow details the execution steps once a refactoring is planned:

1.  **Identify & Plan (Covered in Phases 1-4 above):**
    *   Clearly define scope and target structure.
    *   Log task in your working memory.

2.  **Ensure Test Coverage (Before Refactoring):**
    *   Review existing tests for the code being refactored.
    *   **Crucial:** If coverage is insufficient, **write tests first** to capture the current behavior.

3.  **Extract Mechanically (Example: Extracting a Class/Function):**
    *   Create the new function, class, or module.
    *   Carefully move the identified code block(s) to the new location.
    *   Ensure necessary imports are added to the new location.

4.  **Delegate / Integrate:**
    *   Modify the original code location to *call* or *use* the newly extracted code/component.
    *   Pass necessary data or dependencies to the new code.
    *   Ensure imports are updated in the original location.

5.  **Adapt Tests (After Refactoring):**
    *   Run existing tests; expect some to fail.
    *   **Modify Original Tests:**
        *   Update tests that relied on the *internal implementation* that was moved.
        *   Mock the *newly extracted dependency* (the function/class you created).
        *   Change assertions to verify that the original code now correctly *delegates* the call.
        *   Remove assertions that tested internal logic now moved.
    *   **Add New Tests:**
        *   Write new, focused tests for the *extracted component*.
    *   Run all relevant tests again until they pass.

6.  **Clean Up:**
    *   Remove any dead code from the original location.
    *   Run linters and formatters.
    *   Review changes for clarity and simplicity.

7.  **Document:**
    *   Update your working memory log.
    *   If public interfaces or significant structures changed, update relevant IDL specifications or architecture diagrams.

**7. Common Pitfalls & How to Avoid Them**

*   **Breaking Behavior:**
    *   **Avoidance:** Good test coverage *before* starting. Test frequently.
*   **Brittle Tests:** Tests failing due to internal restructuring.
    *   **Avoidance:** Test the component's *contract* and *behavior*, not its specific internal implementation. Mock primarily at boundaries.
*   **Incorrect Mocking:** `patch` failing or mocks not behaving as expected.
    *   **Avoidance:** Understand how to use your mocking framework (e.g., `patch` where it's *looked up*). Verify mock calls.
*   **Scope Issues in Tests:** Mock instances or fixtures not available.
    *   **Avoidance:** Understand test fixture scopes.
*   **Trying to Do Too Much:**
    *   **Avoidance:** Small, incremental steps. Commit after each logical step passes tests.
*   **Forgetting Cleanup:** Leaving unused imports, variables, or methods.
    *   **Avoidance:** Explicitly review for dead code. Use linters.

**8. Example (Conceptual: Extracting a Helper Function)**

1.  **Identify:** A complex calculation is repeated within a method `process_data`.
2.  **Test (Before):** Ensure `test_process_data` covers cases involving this calculation.
3.  **Extract:** Create a new private method `_perform_calculation(input)`. Move logic there.
4.  **Delegate:** Replace logic in `process_data` with a call to `self._perform_calculation(input)`.
5.  **Test (After):** `test_process_data` should ideally still pass. Add `test__perform_calculation`.
6.  **Clean Up:** Format/lint.
7.  **Document:** Update working memory log.

By following this guide, we can iteratively improve the codebase's quality while minimizing disruption and maintaining correctness.
</file>

<file path="docs/05_DOCUMENTATION_GUIDE.md">
# Guide: Keeping Documentation Consistent

**1. Purpose**

This guide outlines a standard process for reviewing and updating project documentation to ensure it accurately reflects the current state of the codebase, architecture, and development practices. Consistent and up-to-date documentation is crucial for onboarding new developers, maintaining architectural integrity, and reducing confusion.

**2. When to Use This Process**

This process should be triggered:

*   **After Significant Technical Changes:** Following the merge of code that implements a major feature, refactoring, architectural decision (ADR), or change in core dependencies.
*   **Before Starting Major New Work:** As part of the preparation phase for a new feature that builds upon existing components, verifying the documentation for those components is accurate.
*   **Periodically:** As part of scheduled maintenance to catch drift between documentation and implementation over time.

**3. Goal**

To identify and rectify inconsistencies, inaccuracies, or outdated information within the project's documentation (typically the `docs/` directory and potentially IDL files).

**4. The Process**

**Phase 1: Define Scope & Identify Affected Documents**

1.  **Identify the Trigger/Scope:** Clearly define the technical change or area of focus that necessitates the documentation review.
    *   *Example Trigger:* "ADR-XYZ (New Authentication System) has been implemented."
    *   *Example Scope:* "Review documentation related to user authentication, security, and relevant component interfaces."

2.  **List Potentially Affected Documents:** Brainstorm and list all documentation files that *might* be impacted by the change or are relevant to the area of focus.
    *   **Core Guides:** Always check `00_START_HERE.md`, `03_PROJECT_RULES.md`, `02_IMPLEMENTATION_RULES.md`, `01_IDL_GUIDELINES.md`.
    *   **Project Plan/Roadmap (if applicable):** Check any high-level planning documents if the technical change impacts scope, timelines, or dependencies.
    *   **Related ADRs:** Review the ADR itself and any ADRs it supersedes or relates to (see `docs/ARCHITECTURE/adr/`).
    *   **Component Docs & IDL Files:** Identify components directly affected. Check their IDL files (e.g., `src/component_x/component_x_IDL.md`), READMEs, or other specific documentation.
    *   **Shared Types/Contracts:** Check `docs/ARCHITECTURE/types.md` if the change impacts shared data structures or protocols.
    *   **Architectural Patterns:** Check `docs/ARCHITECTURE/patterns/` (if such a directory exists) if the change implements or modifies a core pattern.
    *   **Search:** Use project-wide search tools for keywords related to the change across the `docs/` directory (and `src/` for IDL files) to find less obvious references.
    *   *Example Output for New Auth System:* `00_START_HERE.md`, `02_IMPLEMENTATION_RULES.md` (security section), `src/auth_service/auth_service_IDL.md`, `src/user_component/user_component_IDL.md`, `docs/ARCHITECTURE/adr/ADR-XYZ.md`.

**Phase 2: Review and Analyze**

1.  **Read Through Identified Documents:** Carefully read each document identified in Phase 1.
2.  **Compare Against Reality:** Compare the documentation against:
    *   The **specific technical changes** that triggered the review.
    *   The **current codebase structure**.
    *   The **current behavior** of the code (if known or testable).
    *   Other **related documentation** (check for contradictions).
3.  **Identify Gaps and Inconsistencies:** Look for:
    *   **Outdated Information:** Descriptions, code examples, diagrams, or procedures that no longer match the implementation.
    *   **Contradictions:** Information that conflicts with the implemented changes or other documentation.
    *   **Inconsistent Terminology:** Using different names for the same concept across documents.
    *   **Broken Links/References:** Cross-references pointing to non-existent files or sections.
    *   **Missing Information:** Failure to document new components, features, patterns, or conventions.
    *   **Ambiguity:** Sections that are unclear or open to multiple interpretations.
    *   *Example Gaps Found:* `02_IMPLEMENTATION_RULES.md` described old auth method; `00_START_HERE.md` pointed to wrong component for user data.

**Phase 3: Draft and Apply Updates**

1.  **Draft Targeted Edits:** For each identified gap, draft the specific changes needed.
    *   Focus on correcting inaccuracies or adding missing information.
    *   Ensure changes align with the project's documentation style.
    *   Update code examples, diagrams, and cross-references.
    *   Reference the trigger (e.g., "Updated to reflect New Authentication System per ADR-XYZ").
2.  **Apply Changes:** Edit the documentation files.
3.  **Self-Review:** Read through your changes. Are they accurate, clear, and easy to understand? Do they introduce new inconsistencies?

**Phase 4: Review and Commit**

1.  **Peer Review (Recommended):** If possible, have another team member review substantial documentation changes for clarity and accuracy.
2.  **Commit Changes:** Use clear, specific commit messages explaining the purpose of the documentation update. Reference the triggering change if applicable.
    *   *Example Commit Message:* `docs: Align auth guides with new system per ADR-XYZ`
    *   *Example Commit Message:* `docs: Clarify data model usage in component_x_IDL.md`

**5. Key Considerations**

*   **Scope:** Keep the update focused on the triggering change or area under review.
*   **Single Source of Truth:** Ensure information is updated in the *authoritative* source document. Avoid duplicating detailed explanations; use cross-references.
*   **Clarity:** Write for someone unfamiliar with the specific change or component. Define terms or link to definitions.
*   **Consistency:** Ensure terminology, formatting, and style are consistent with surrounding documentation.

By following this process, we can maintain accurate, consistent, and helpful documentation that reflects the ongoing evolution of the project.
</file>

<file path="docs/VISUAL_DIAGNOSTICS_GUIDE.md">
# Visual Diagnostics Guide for DiffusePipe

This guide documents the visual diagnostic tools for verifying the correctness of diffuse scattering data extraction and processing in the DiffusePipe pipeline.

## Table of Contents

1. [Overview](#overview)
2. [Phase 2 Visual Diagnostics](#phase-2-visual-diagnostics)
   - [check_diffuse_extraction.py](#check_diffuse_extractionpy)
   - [run_phase2_e2e_visual_check.py](#run_phase2_e2e_visual_checkpy)
3. [Phase 3 Visual Diagnostics](#phase-3-visual-diagnostics)
   - [check_phase3_outputs.py](#check_phase3_outputspy)
   - [run_phase3_e2e_visual_check.py](#run_phase3_e2e_visual_checkpy)
4. [Integration Workflow](#integration-workflow)
5. [Troubleshooting](#troubleshooting)

## Overview

The visual diagnostics system provides comprehensive tools for validating DiffusePipe processing at different pipeline stages:

### Phase 2 Diagnostics
1. **`check_diffuse_extraction.py`** - Generates visual diagnostics from Phase 2 processing outputs
2. **`run_phase2_e2e_visual_check.py`** - Orchestrates Phase 1-2 pipeline with automatic diagnostics

### Phase 3 Diagnostics  
3. **`check_phase3_outputs.py`** - Generates visual diagnostics from Phase 3 processing outputs
4. **`run_phase3_e2e_visual_check.py`** - Orchestrates Phase 1-3 pipeline with automatic diagnostics

These tools are essential for:
- Validating implementation correctness at each processing stage
- Debugging voxelization, scaling, and merging pipeline issues
- Generating reference outputs for testing
- Quality control of diffuse scattering analysis
- Visual verification of 3D reciprocal space maps

---

## Phase 2 Visual Diagnostics

## check_diffuse_extraction.py

### Purpose

Generates visual diagnostics to verify diffuse scattering extraction and correction processes. Takes outputs from Phase 1 (DIALS processing) and Phase 2 (DataExtractor) to create comprehensive diagnostic plots.

### Location

`scripts/visual_diagnostics/check_diffuse_extraction.py`

### Key Features

- **8 Diagnostic Plot Types**: Comprehensive visual verification suite
- **Pixel Coordinate Support**: Enhanced visualizations when coordinates are available
- **Flexible Input**: Works with various mask types and background maps
- **Summary Reports**: Automated generation of diagnostic summaries

### Usage

#### Basic Command

```bash
python check_diffuse_extraction.py \
  --raw-image path/to/image.cbf \
  --expt path/to/experiment.expt \
  --total-mask path/to/total_mask.pickle \
  --npz-file path/to/extracted_data.npz
```

#### Full Command with Options

```bash
python check_diffuse_extraction.py \
  --raw-image path/to/image.cbf \
  --expt path/to/experiment.expt \
  --total-mask path/to/total_mask.pickle \
  --npz-file path/to/extracted_data.npz \
  --bragg-mask path/to/bragg_mask.pickle \
  --pixel-mask path/to/pixel_mask.pickle \
  --bg-map path/to/background.npy \
  --output-dir my_diagnostics \
  --verbose
```

### Command-Line Arguments

| Argument | Required | Description |
|----------|----------|-------------|
| `--raw-image` | Yes | Path to raw CBF image file |
| `--expt` | Yes | Path to DIALS experiment .expt file |
| `--total-mask` | Yes | Path to total diffuse mask .pickle file |
| `--npz-file` | Yes | Path to DataExtractor output .npz file |
| `--bragg-mask` | No | Path to Bragg mask .pickle file (for overlay plots) |
| `--pixel-mask` | No | Path to global pixel mask .pickle file |
| `--bg-map` | No | Path to background map .npy/.pickle file |
| `--output-dir` | No | Output directory (default: extraction_visual_check) |
| `--verbose` | No | Enable verbose logging |

### Generated Diagnostics

#### 1. Diffuse Pixel Overlay (`diffuse_pixel_overlay.png`)
- **Purpose**: Verify mask application and pixel selection
- **Content**: Raw detector image with extracted diffuse pixels highlighted
- **Requirements**: Pixel coordinates in NPZ file
- **Key Checks**: 
  - Diffuse pixels avoid Bragg regions
  - Proper mask boundaries
  - No obvious Bragg peaks in diffuse selection

#### 2. Intensity Correction Summary (`intensity_correction_summary.txt`)
- **Purpose**: Document correction effects
- **Content**: Sample of corrected intensity values with Q-vectors
- **Format**: Tabular text with I, σ, I/σ values
- **Note**: Full transformation tracking requires enhanced DataExtractor logging

#### 3. Q-Space Projections (`q_projection_*.png`)
- **Purpose**: Verify Q-space coverage and distribution
- **Files**: 
  - `q_projection_qx_qy.png`
  - `q_projection_qx_qz.png`
  - `q_projection_qy_qz.png`
- **Content**: Scatter plots colored by intensity
- **Key Checks**: Even Q-space sampling, reasonable coverage

#### 4. Radial Q Distribution (`radial_q_distribution.png`)
- **Purpose**: Analyze intensity vs resolution
- **Content**: Intensity vs |Q| scatter plot
- **Key Checks**: Expected intensity falloff with resolution

#### 5. Intensity Histogram (`intensity_histogram.png`)
- **Purpose**: Validate intensity distribution
- **Content**: Dual histograms (linear and log scale)
- **Key Checks**: 
  - Positive intensities
  - Reasonable distribution shape
  - No unexpected bimodality

#### 6. Intensity Heatmap (`intensity_heatmap_panel_0.png`)
- **Purpose**: Spatial intensity distribution on detector
- **Content**: 2D heatmap of intensities at pixel positions
- **Requirements**: Pixel coordinates in NPZ file
- **Key Checks**: Smooth spatial variations, no artifacts

#### 7. Sigma vs Intensity (`sigma_vs_intensity.png`)
- **Purpose**: Validate error propagation
- **Content**: Scatter plot with Poisson noise reference
- **Key Checks**: Errors follow expected σ ≈ √I relationship

#### 8. I/σ Histogram (`isigi_histogram.png`)
- **Purpose**: Data quality assessment
- **Content**: Distribution with mean/median markers
- **Key Checks**: Reasonable I/σ values (typically > 1)

### Output Structure

```
output-dir/
├── diffuse_pixel_overlay.png
├── intensity_correction_summary.txt
├── q_projection_qx_qy.png
├── q_projection_qx_qz.png
├── q_projection_qy_qz.png
├── radial_q_distribution.png
├── intensity_histogram.png
├── intensity_heatmap_panel_0.png
├── sigma_vs_intensity.png
├── isigi_histogram.png
└── extraction_diagnostics_summary.txt
```

### NPZ File Requirements

The input NPZ file must contain:
- **Required**: `q_vectors`, `intensities`, `sigmas`
- **Optional**: `original_panel_ids`, `original_fast_coords`, `original_slow_coords`

Note: Pixel coordinate arrays enable additional diagnostic plots (overlay, heatmap).

---

## run_phase2_e2e_visual_check.py

### Purpose

Orchestrates the complete Phase 1 and Phase 2 pipeline for a single CBF image, then automatically runs visual diagnostics. Provides an end-to-end verification pathway from raw data to diagnostic plots.

### Location

`scripts/dev_workflows/run_phase2_e2e_visual_check.py`

### Key Features

- **Complete Pipeline Automation**: DIALS → Masking → Extraction → Diagnostics
- **Configurable Processing**: JSON-based parameter overrides
- **Pixel Coordinate Tracking**: Automatically enables for visual diagnostics
- **Organized Output**: Structured directory with all intermediate files
- **Comprehensive Logging**: Detailed progress and error tracking

### Usage

#### Basic Command

```bash
python run_phase2_e2e_visual_check.py \
  --cbf-image path/to/image.cbf \
  --output-base-dir ./e2e_outputs
```

#### With PDB Validation

```bash
python run_phase2_e2e_visual_check.py \
  --cbf-image path/to/image.cbf \
  --output-base-dir ./e2e_outputs \
  --pdb-path path/to/reference.pdb
```

#### Full Configuration Example

```bash
python run_phase2_e2e_visual_check.py \
  --cbf-image path/to/image.cbf \
  --output-base-dir ./e2e_outputs \
  --dials-phil-path custom_params.phil \
  --pdb-path reference.pdb \
  --static-mask-config '{"beamstop": {"type": "circle", "center_x": 1250, "center_y": 1250, "radius": 50}}' \
  --bragg-mask-config '{"border": 3}' \
  --extraction-config-json '{"pixel_step": 2, "min_intensity": 10.0}' \
  --use-bragg-mask-option-b \
  --verbose
```

### Command-Line Arguments

| Argument | Required | Description |
|----------|----------|-------------|
| `--cbf-image` | Yes | Path to input CBF image file |
| `--output-base-dir` | Yes | Base output directory |
| `--dials-phil-path` | No | Custom DIALS PHIL configuration |
| `--pdb-path` | No | External PDB for validation |
| `--static-mask-config` | No | JSON for static mask parameters |
| `--bragg-mask-config` | No | JSON for Bragg mask parameters |
| `--use-bragg-mask-option-b` | No | Use shoebox-based Bragg masking |
| `--extraction-config-json` | No | JSON for extraction overrides |
| `--pixel-step` | No | Pixel sampling step size |
| `--save-pixel-coords` | No | Save pixel coordinates (default: True) |
| `--verbose` | No | Enable verbose logging |

### Pipeline Stages

#### Phase 1: DIALS Processing
1. **Import**: Load CBF file into DIALS format
2. **Find Spots**: Detect diffraction spots
3. **Index**: Determine crystal orientation
4. **Refine**: Optimize detector geometry

#### Phase 1: Mask Generation
1. **Pixel Masks**: Create static (beamstop, untrusted) and dynamic (hot pixels) masks
2. **Bragg Masks**: Create masks around Bragg peaks (spot or shoebox-based)
3. **Total Mask**: Combine all masks for diffuse region selection

#### Phase 2: Data Extraction
1. **Configure**: Apply extraction parameters
2. **Process**: Extract diffuse scattering with corrections
3. **Save**: Generate NPZ file with pixel coordinates

#### Phase 3: Visual Diagnostics
1. **Invoke**: Run check_diffuse_extraction.py
2. **Generate**: Create all diagnostic plots
3. **Report**: Save summaries and logs

### Configuration Examples

#### Static Mask Configuration

```json
{
  "beamstop": {
    "type": "circle",
    "center_x": 1250,
    "center_y": 1250,
    "radius": 50
  },
  "untrusted_rects": [
    {"min_x": 0, "max_x": 100, "min_y": 0, "max_y": 100}
  ],
  "untrusted_panels": [1, 3]
}
```

#### Bragg Mask Configuration

```json
{
  "border": 3,
  "algorithm": "simple",
  "resolution_range": [50.0, 2.0]
}
```

#### Extraction Configuration

```json
{
  "pixel_step": 2,
  "min_intensity": 10.0,
  "max_intensity": 100000.0,
  "min_res": 50.0,
  "max_res": 2.0,
  "gain": 1.0,
  "lp_correction_enabled": true,
  "air_temperature_k": 293.15,
  "air_pressure_atm": 1.0
}
```

### Output Structure

For input `image_001.cbf`:

```
output-base-dir/
└── image_001/
    ├── e2e_visual_check.log                # Complete pipeline log
    ├── imported.expt                       # DIALS import
    ├── strong.refl                         # Found spots
    ├── indexed_initial.expt                # Initial indexing
    ├── indexed_initial.refl                
    ├── indexed_refined_detector.expt       # Final refined
    ├── indexed_refined_detector.refl       
    ├── global_pixel_mask.pickle            # Pixel masks
    ├── bragg_mask.pickle                   # Bragg mask
    ├── total_diffuse_mask.pickle           # Combined mask
    ├── diffuse_data.npz                    # Extracted data
    └── extraction_diagnostics/             # Visual plots
        ├── diffuse_pixel_overlay.png
        ├── q_projection_*.png
        ├── intensity_histogram.png
        ├── [... all diagnostic plots ...]
        └── extraction_diagnostics_summary.txt
```

### Error Handling

The script provides detailed error reporting for each phase:

- **DIALS Failures**: Missing files, indexing failures, insufficient spots
- **Mask Failures**: Invalid parameters, memory issues
- **Extraction Failures**: Configuration errors, no valid pixels
- **Diagnostic Failures**: Missing dependencies (non-fatal)

---

## Integration Workflow

### Recommended Workflow

1. **Initial Testing**: Run on a single representative CBF file
2. **Parameter Optimization**: Adjust configurations based on diagnostics
3. **Batch Processing**: Apply optimized parameters to dataset
4. **Quality Control**: Review diagnostic plots for anomalies

### Example Workflow

```bash
# Step 1: Test with defaults
python run_phase2_e2e_visual_check.py \
  --cbf-image test_image.cbf \
  --output-base-dir ./test_run

# Step 2: Review diagnostics
ls test_run/test_image/extraction_diagnostics/

# Step 3: Adjust parameters based on results
python run_phase2_e2e_visual_check.py \
  --cbf-image test_image.cbf \
  --output-base-dir ./optimized_run \
  --extraction-config-json '{"pixel_step": 1, "min_intensity": 5.0}' \
  --bragg-mask-config '{"border": 5}'

# Step 4: Apply to multiple images
for cbf in data/*.cbf; do
  python run_phase2_e2e_visual_check.py \
    --cbf-image "$cbf" \
    --output-base-dir ./batch_run \
    --extraction-config-json '{"pixel_step": 1, "min_intensity": 5.0}'
done
```

### Integration with CI/CD

Both scripts can be integrated into automated testing:

```yaml
# Example GitHub Actions workflow
- name: Run E2E Visual Check
  run: |
    python scripts/dev_workflows/run_phase2_e2e_visual_check.py \
      --cbf-image test_data/reference.cbf \
      --output-base-dir ./ci_output \
      --pdb-path test_data/reference.pdb
    
- name: Verify Outputs
  run: |
    test -f ci_output/reference/diffuse_data.npz
    test -f ci_output/reference/extraction_diagnostics/extraction_diagnostics_summary.txt
```

---

## Troubleshooting

### Common Issues

#### 1. "No pixel coordinates in NPZ file"
**Cause**: DataExtractor not configured to save coordinates
**Solution**: Ensure `--save-pixel-coords` is set (default: True)

#### 2. "DIALS processing failed"
**Cause**: Poor diffraction, wrong parameters, or format issues
**Solution**: 
- Check CBF file is valid
- Review DIALS logs in output directory
- Try different indexing methods

#### 3. "No pixels passed filtering criteria"
**Cause**: Overly restrictive filters or masking
**Solution**:
- Reduce `min_intensity` threshold
- Check mask coverage isn't too aggressive
- Increase `pixel_step` for faster testing

#### 4. Memory errors with large detectors
**Cause**: Processing all pixels at once
**Solution**: Increase `pixel_step` to reduce memory usage

#### 5. Visual diagnostics subprocess fails
**Cause**: Missing dependencies or path issues
**Solution**: 
- Ensure matplotlib is installed
- Check script paths are correct
- Review subprocess error logs

### Performance Optimization

#### Processing Time Factors
- **Detector Size**: 6M pixel detector takes longer than 1M
- **Pixel Step**: step=1 (all pixels) vs step=2 (1/4 pixels)
- **DIALS Parameters**: Spot finding thresholds affect speed
- **Diagnostic Plots**: Disable with `plot_diagnostics: false`

#### Recommended Settings by Use Case

**Quick Testing**:
```json
{"pixel_step": 4, "plot_diagnostics": false}
```

**Full Analysis**:
```json
{"pixel_step": 1, "plot_diagnostics": true}
```

**Memory Limited**:
```json
{"pixel_step": 2, "min_intensity": 10.0}
```

### Debug Mode

For detailed debugging, use verbose mode and check logs:

```bash
# Enable verbose logging
python run_phase2_e2e_visual_check.py \
  --cbf-image image.cbf \
  --output-base-dir ./debug_run \
  --verbose

# Check detailed logs
tail -f debug_run/image/e2e_visual_check.log

# Check DIALS processing logs
cat debug_run/image/dials.*.log
```

### File Format Requirements

#### CBF Files
- Must have valid crystallographic headers
- Oscillation or still image data
- Supported detector formats

#### Pickle Files
- DIALS-compatible mask tuples
- Proper panel indexing
- Boolean mask arrays

#### NPZ Files
- Required arrays: q_vectors, intensities, sigmas
- Optional: original_panel_ids, original_fast_coords, original_slow_coords
- Compressed format supported

---

## Phase 3 Visual Diagnostics

### Purpose

Phase 3 visual diagnostics provide comprehensive verification of the voxelization, relative scaling, and merging processes that transform corrected diffuse pixel observations into a 3D reciprocal space map. These tools are essential for validating the final stages of the DiffusePipe pipeline.

### Key Components

1. **`check_phase3_outputs.py`** - Standalone diagnostic generation from Phase 3 outputs
2. **`run_phase3_e2e_visual_check.py`** - Complete Phase 1-3 pipeline orchestration with automatic diagnostics

---

## check_phase3_outputs.py

### Purpose

Generates comprehensive visual diagnostics to verify Phase 3 voxelization, relative scaling, and merging processes. Takes outputs from the complete Phase 3 pipeline (global voxel grid, refined scaling parameters, and merged voxel data) to create diagnostic plots and summary reports.

### Location

`scripts/visual_diagnostics/check_phase3_outputs.py`

### Key Features

- **Multi-dimensional Visualization**: 3D reciprocal space slicing and projections
- **Scaling Analysis**: Per-still scale factor and resolution smoother visualization
- **Quality Metrics**: Voxel occupancy, redundancy, and intensity/sigma analysis
- **Comprehensive Reporting**: Automated generation of summary statistics
- **Performance Optimized**: Configurable point limits for large datasets

### Usage

#### Basic Command

```bash
python check_phase3_outputs.py \
  --grid-definition-file phase3_outputs/global_voxel_grid_definition.json \
  --scaling-model-params-file phase3_outputs/refined_scaling_model_params.json \
  --voxel-data-file phase3_outputs/voxel_data_relative.npz \
  --output-dir phase3_diagnostics
```

#### Full Command with Options

```bash
python check_phase3_outputs.py \
  --grid-definition-file grid_def.json \
  --scaling-model-params-file scaling_params.json \
  --voxel-data-file voxel_data.npz \
  --output-dir diagnostics \
  --experiments-list-file experiments_list.txt \
  --corrected-pixel-data-dir pixel_data_dirs.txt \
  --max-plot-points 25000 \
  --verbose
```

### Command-Line Arguments

| Argument | Required | Description |
|----------|----------|-------------|
| `--grid-definition-file` | Yes | Path to GlobalVoxelGrid definition JSON file |
| `--scaling-model-params-file` | Yes | Path to refined scaling model parameters JSON file |
| `--voxel-data-file` | Yes | Path to VoxelData_relative NPZ/HDF5 file |
| `--output-dir` | Yes | Output directory for diagnostic plots and reports |
| `--experiments-list-file` | No | Path to file containing list of experiment (.expt) file paths |
| `--corrected-pixel-data-dir` | No | Path to file containing list of corrected pixel data directories |
| `--max-plot-points` | No | Maximum number of points in scatter plots (default: 50000) |
| `--verbose` | No | Enable verbose logging |

### Generated Diagnostics

#### 1. Grid Summary (`grid_summary.txt`, `grid_visualization_conceptual.png`)
- **Purpose**: Document global voxel grid parameters and visualize coverage
- **Text Content**: 
  - Average crystal parameters (unit cell, space group)
  - HKL bounds and voxel dimensions
  - Total voxels and voxel sizes
- **Plot Content**: 3D wireframe showing HKL bounds with sample grid points
- **Key Checks**: Reasonable grid coverage, appropriate voxel resolution

#### 2. Voxel Occupancy Analysis
- **Files**: 
  - `voxel_occupancy_slice_L0.png`, `voxel_occupancy_slice_K0.png`, `voxel_occupancy_slice_H0.png`
  - `voxel_occupancy_histogram.png`
- **Purpose**: Analyze data redundancy and completeness
- **Content**: 
  - 2D heatmap slices of observation counts per voxel
  - Histogram of voxel occupancy distribution
- **Key Checks**: Even data distribution, adequate redundancy, identification of gaps

#### 3. Scaling Model Parameters
- **Files**:
  - `scaling_params_b_i.png` - Per-still multiplicative scale factors
  - `scaling_resolution_smoother.png` - Resolution smoother curve (if enabled)
  - `scaling_parameters_summary.txt` - Parameter statistics
- **Purpose**: Validate relative scaling convergence and parameters
- **Content**: Scale factor trends, smoother function, refinement statistics
- **Key Checks**: Reasonable scale factor range, smooth convergence, stable parameters

#### 4. Merged Voxel Data Visualization
- **Intensity Slices**: `merged_intensity_slice_L0.png`, etc. (log scale)
- **Sigma Slices**: `merged_sigma_slice_L0.png`, etc.
- **I/Sigma Slices**: `merged_isigi_slice_L0.png`, etc.
- **Radial Analysis**: `merged_radial_average.png` - Intensity vs |q|
- **Distribution**: `merged_intensity_histogram.png` - Intensity distribution
- **Purpose**: Verify final merged diffuse scattering map quality
- **Key Checks**: Smooth intensity variations, reasonable I/σ ratios, proper resolution trends

#### 5. Comprehensive Summary (`phase3_diagnostics_summary.txt`)
- **Purpose**: Consolidated report of all Phase 3 processing statistics
- **Content**:
  - Input file paths and processing parameters
  - Grid definition summary
  - Voxel occupancy statistics
  - Scaling model convergence metrics
  - Merged intensity quality metrics
  - Resolution coverage analysis
  - Generated plot inventory

### Output Structure

```
output-dir/
├── grid_summary.txt
├── grid_visualization_conceptual.png
├── voxel_occupancy_slice_L0.png
├── voxel_occupancy_slice_K0.png
├── voxel_occupancy_slice_H0.png
├── voxel_occupancy_histogram.png
├── scaling_params_b_i.png
├── scaling_resolution_smoother.png (if enabled)
├── scaling_parameters_summary.txt
├── merged_intensity_slice_L0.png
├── merged_intensity_slice_K0.png
├── merged_intensity_slice_H0.png
├── merged_sigma_slice_L0.png
├── merged_sigma_slice_K0.png
├── merged_sigma_slice_H0.png
├── merged_isigi_slice_L0.png
├── merged_isigi_slice_K0.png
├── merged_isigi_slice_H0.png
├── merged_radial_average.png
├── merged_intensity_histogram.png
└── phase3_diagnostics_summary.txt
```

### Input File Requirements

#### Grid Definition JSON
- `crystal_avg_ref`: Average crystal parameters (unit cell, space group)
- `hkl_bounds`: H/K/L min/max values
- `ndiv_h/k/l`: Voxel divisions per unit cell
- `total_voxels`: Total number of voxels

#### Scaling Parameters JSON
- `refined_parameters`: Per-still scale factors and offsets
- `refinement_statistics`: Convergence metrics and R-factors
- `resolution_smoother`: Smoother configuration and control points

#### Voxel Data NPZ/HDF5
- **Required Arrays**: `voxel_indices`, `H_center`, `K_center`, `L_center`, `q_center_x/y/z`, `q_magnitude_center`, `I_merged_relative`, `Sigma_merged_relative`, `num_observations`
- **Format**: Compressed NPZ or HDF5 with consistent array lengths

---

## run_phase3_e2e_visual_check.py

### Purpose

Orchestrates the complete Phase 1 (DIALS processing), Phase 2 (diffuse extraction), and Phase 3 (voxelization, scaling, merging) pipeline for multiple CBF images, then automatically runs Phase 3 visual diagnostics. Provides end-to-end verification from raw data to final 3D diffuse maps.

### Location

`scripts/dev_workflows/run_phase3_e2e_visual_check.py`

### Key Features

- **Multi-Image Processing**: Supports processing multiple CBF files simultaneously
- **Complete Pipeline**: Phases 1-3 orchestration with automatic diagnostics
- **Configurable Parameters**: JSON-based configuration for all pipeline stages
- **Intermediate Output Management**: Optional saving of all intermediate files
- **Robust Error Handling**: Per-image error tracking with graceful continuation
- **Comprehensive Logging**: Detailed progress tracking and debugging information

### Usage

#### Basic Command

```bash
python run_phase3_e2e_visual_check.py \
  --cbf-image-paths image1.cbf image2.cbf image3.cbf \
  --output-base-dir ./e2e_outputs_phase3
```

#### With PDB Validation and Configuration

```bash
python run_phase3_e2e_visual_check.py \
  --cbf-image-paths path/to/image1.cbf path/to/image2.cbf \
  --output-base-dir ./outputs \
  --pdb-path reference.pdb \
  --dials-phil-path custom_dials.phil \
  --extraction-config-json '{"pixel_step": 2}' \
  --relative-scaling-config-json '{"enable_res_smoother": true}' \
  --grid-config-json '{"ndiv_h": 100, "ndiv_k": 100, "ndiv_l": 50}' \
  --save-intermediate-phase-outputs \
  --verbose
```

### Command-Line Arguments

| Argument | Required | Description |
|----------|----------|-------------|
| `--cbf-image-paths` | Yes | Space-separated paths to input CBF image files |
| `--output-base-dir` | Yes | Base output directory (subdirectories created automatically) |
| `--pdb-path` | No | Path to external PDB file for validation |
| `--dials-phil-path` | No | Path to custom DIALS PHIL configuration file |
| `--static-mask-config` | No | JSON string for static mask configuration |
| `--bragg-mask-config` | No | JSON string for Bragg mask configuration |
| `--use-bragg-mask-option-b` | No | Use shoebox-based Bragg masking |
| `--extraction-config-json` | No | JSON string for Phase 2 extraction configuration |
| `--relative-scaling-config-json` | No | JSON string for Phase 3 relative scaling configuration |
| `--grid-config-json` | No | JSON string for voxel grid configuration |
| `--save-intermediate-phase-outputs` | No | Save manifest of all intermediate files |
| `--verbose` | No | Enable verbose logging |

### Pipeline Stages

#### Phase 1: Multi-Image DIALS Processing
- **Per-Image Processing**: Independent DIALS processing for each CBF file
- **Mask Generation**: Pixel masks (static + dynamic) and Bragg masks
- **Validation**: Geometric consistency checks using PDB reference (if provided)
- **Output**: Experiment files, reflection tables, mask files per image

#### Phase 2: Diffuse Data Extraction
- **Per-Image Extraction**: DataExtractor processing with pixel corrections
- **Coordinate Tracking**: Automatic saving of original pixel coordinates
- **Configuration**: Flexible extraction parameter overrides
- **Output**: NPZ files with corrected diffuse observations per image

#### Phase 3: Voxelization and Merging
- **Grid Definition**: Global voxel grid creation from all crystal models
- **Observation Binning**: Assignment of diffuse pixels to voxels with symmetry
- **Relative Scaling**: Iterative refinement of per-image scale factors
- **Data Merging**: Weighted combination of scaled observations per voxel
- **Output**: Grid definition, scaling parameters, merged voxel data

#### Phase 3 Diagnostics
- **Automatic Invocation**: `check_phase3_outputs.py` called with generated outputs
- **Comprehensive Analysis**: All Phase 3 diagnostic plots and summaries
- **Quality Assessment**: Visual verification of voxelization and scaling

### Configuration Examples

#### Basic Grid Configuration
```json
{
  "ndiv_h": 100,
  "ndiv_k": 100, 
  "ndiv_l": 50
}
```

#### Advanced Scaling Configuration
```json
{
  "enable_res_smoother": true,
  "max_iterations": 10,
  "convergence_tolerance": 0.001
}
```

#### Extraction Configuration
```json
{
  "pixel_step": 2,
  "min_intensity": 0.0,
  "save_original_pixel_coordinates": true
}
```

### Output Structure

```
output-base-dir/
├── phase1_image1/
│   ├── indexed_refined_detector.expt
│   ├── indexed_refined_detector.refl
│   ├── global_pixel_mask.pickle
│   ├── bragg_mask.pickle
│   └── total_diffuse_mask.pickle
├── phase1_image2/
│   └── ... (similar structure)
├── phase3_outputs/
│   ├── global_voxel_grid_definition.json
│   ├── refined_scaling_model_params.json
│   ├── voxel_data_relative.npz
│   └── diagnostics/
│       ├── grid_summary.txt
│       ├── voxel_occupancy_slice_L0.png
│       ├── scaling_params_b_i.png
│       ├── merged_intensity_slice_L0.png
│       └── phase3_diagnostics_summary.txt
├── intermediate_outputs_manifest.json (if --save-intermediate-phase-outputs)
└── e2e_phase3_visual_check.log
```

### Performance Considerations

- **Memory Management**: Large datasets may require disk-based voxel accumulation
- **Processing Time**: Scales with number of images and voxel grid resolution
- **Disk Space**: Intermediate files can be substantial for large datasets
- **Parallelization**: Phase 1 processing parallelized across available CPU cores

---

## Summary

These visual diagnostic tools provide comprehensive verification of the complete DiffusePipe pipeline:

### Phase 2 Diagnostics
- **`check_diffuse_extraction.py`**: Standalone diagnostic generation from Phase 2 outputs
- **`run_phase2_e2e_visual_check.py`**: Phase 1-2 pipeline orchestration with automatic diagnostics

### Phase 3 Diagnostics
- **`check_phase3_outputs.py`**: Standalone diagnostic generation from Phase 3 outputs  
- **`run_phase3_e2e_visual_check.py`**: Complete Phase 1-3 pipeline orchestration with automatic diagnostics

Together, these tools enable thorough validation of:
- **Phase 2**: Diffuse scattering extraction, pixel corrections, and data quality
- **Phase 3**: Voxelization, relative scaling, merging, and 3D reciprocal space map generation

This comprehensive diagnostic framework is essential for ensuring the correctness of the entire DiffusePipe processing pipeline, from raw crystallographic images to final diffuse scattering maps ready for scientific analysis.
</file>

<file path="libdocs/dials/crystallographic_calculations.md">
# Crystallographic Calculations

This documentation covers crystallographic calculations including Q-vector computations, Miller index operations, geometric corrections, and scattering factor calculations for diffuse scattering analysis.

**Version Information:** Compatible with DIALS 3.x series. Some methods may differ in DIALS 2.x.

**Key Dependencies:**
- `cctbx`: Unit cells, space groups, scattering factors
- `scitbx`: Matrix operations and mathematical utilities
- `dials.array_family.flex`: Reflection tables and array operations
- `dials.algorithms.integration`: Corrections framework

## C.1. Calculating Q-vector for a Pixel

**1. Purpose:**
Calculate momentum transfer vector q = k_scattered - k_incident for diffuse scattering analysis. This is fundamental for transforming pixel positions to reciprocal space coordinates.

**2. Primary Python Call(s):**
```python
from scitbx import matrix

def calculate_q_vector(detector, beam, panel_id, pixel_coord):
    """Calculate q-vector for a pixel position"""
    panel = detector[panel_id]
    
    # Convert pixel to lab coordinates
    lab_coord = panel.get_pixel_lab_coord(pixel_coord)
    
    # Calculate scattered beam vector s1 (magnitude = 1/λ)
    s1_direction = matrix.col(lab_coord).normalize()
    s1 = s1_direction * (1.0 / beam.get_wavelength())
    
    # Get incident beam vector s0
    s0 = matrix.col(beam.get_s0())
    
    # Calculate q-vector (momentum transfer)
    q = s1 - s0
    
    return q, s1
```

**3. Key Arguments:**
- `detector`: dxtbx.model.Detector object
- `beam`: dxtbx.model.Beam object
- `panel_id` (int): Panel index
- `pixel_coord` (tuple): (fast, slow) pixel coordinates

**4. Return Type:**
- `q`: scitbx.matrix.col object (momentum transfer vector)
- `s1`: scitbx.matrix.col object (scattered beam vector)

**5. Example Usage Snippet:**
```python
detector = experiment.detector
beam = experiment.beam
panel_id = 0
pixel_coord = (1000, 1000)

q_vec, s1_vec = calculate_q_vector(detector, beam, panel_id, pixel_coord)
q_magnitude = q_vec.length()
d_spacing = 1.0 / q_magnitude if q_magnitude > 0 else float('inf')

print(f"Q-vector: {q_vec}")
print(f"|q| = {q_magnitude:.6f} Å⁻¹")
print(f"d-spacing: {d_spacing:.3f} Å")
```

**6. Notes/Caveats:**
- Assumes elastic scattering (|s1| = |s0| = 1/λ)
- Lab coordinates must be normalized to unit vector before scaling by 1/λ
- Q-vector is in units of Å⁻¹
- **Numerical stability:** Handle case where pixel is very close to beam center (lab_coord ≈ 0)
- Vector normalization may fail for pixels directly on the beam path

---

## C.2. Transforming Q-vector to Fractional Miller Indices

**1. Purpose:**
Convert momentum transfer vectors to fractional Miller indices for lattice analysis and identifying diffuse scattering relative to Bragg peak positions. Handles both static crystal orientations and scan-varying crystal models.

**2. Primary Python Call(s):**
```python
from scitbx import matrix

def q_to_miller_indices_static(crystal, q_vector):
    """Transform q-vector to Miller indices for static crystal orientation"""
    # For static crystals or reference orientations
    # q_vector should be in same lab frame as crystal.get_A() is defined
    A_matrix = matrix.sqr(crystal.get_A())
    A_inverse = A_matrix.inverse()
    hkl_fractional = A_inverse * q_vector
    return hkl_fractional

def q_to_miller_indices_scan_varying(crystal, q_vector, scan_point_index):
    """Transform q-vector to Miller indices for scan-varying crystals"""
    # For scan-varying crystal orientations
    A_matrix = matrix.sqr(crystal.get_A_at_scan_point(scan_point_index))
    A_inverse = A_matrix.inverse()
    hkl_fractional = A_inverse * q_vector
    return hkl_fractional

def q_to_miller_indices_with_goniometer(crystal, q_vector_lab, goniometer, scan_point_index):
    """Transform lab-frame q-vector to Miller indices accounting for goniometer rotation"""
    # This method is for cases where q_vector is in a fixed lab frame
    # but crystal orientation changes due to rotation during scan
    
    # Get goniometer rotation for this scan point
    if hasattr(goniometer, 'get_rotation_matrix_at_scan_point'):
        rotation = goniometer.get_rotation_matrix_at_scan_point(scan_point_index)
    else:
        # Fallback: use scan to get angle, then calculate rotation
        rotation = matrix.identity(3)  # Simplified - implement based on specific goniometer
    
    # Transform q from lab frame to crystal setting frame
    # Rotation matrix transforms crystal coords to lab coords, so use transpose
    q_crystal_setting = rotation.transpose() * q_vector_lab
    
    # Now use crystal A matrix (defined in crystal setting frame)
    A_matrix = matrix.sqr(crystal.get_A())
    A_inverse = A_matrix.inverse()
    hkl_fractional = A_inverse * q_crystal_setting
    
    return hkl_fractional
```

**3. Key Arguments:**
- `crystal`: dxtbx.model.Crystal object
- `q_vector`: scitbx.matrix.col (q-vector - ensure frame consistency with method)
- `q_vector_lab`: scitbx.matrix.col (q-vector explicitly in lab frame)
- `goniometer`: dxtbx.model.Goniometer object (for rotation experiments)
- `scan_point_index`: int (scan point index for scan-varying data)

**4. Return Type:**
- `hkl_fractional`: scitbx.matrix.col with fractional Miller indices

**5. Example Usage Snippet:**
```python
crystal = experiment.crystal
goniometer = experiment.goniometer
q_vec = matrix.col((0.1, 0.2, 0.3))  # Example q-vector

# Method 1: Static crystal (stills or reference orientation)
hkl_frac_static = q_to_miller_indices_static(crystal, q_vec)
print(f"Static orientation: {q_vec} → {hkl_frac_static}")

# Method 2: Scan-varying crystal (if crystal model varies with scan)
if hasattr(crystal, 'get_A_at_scan_point'):
    scan_point = 0  # First scan point
    hkl_frac_scan = q_to_miller_indices_scan_varying(crystal, q_vec, scan_point)
    print(f"Scan point {scan_point}: {q_vec} → {hkl_frac_scan}")

# Method 3: With explicit goniometer handling (advanced case)
if goniometer is not None:
    scan_point = 0
    hkl_frac_gonio = q_to_miller_indices_with_goniometer(crystal, q_vec, goniometer, scan_point)
    print(f"With goniometer rotation: {q_vec} → {hkl_frac_gonio}")

# Convert to integer indices
hkl_int = tuple(round(x) for x in hkl_frac_static)
print(f"Nearest integer indices: {hkl_int}")

# Check if close to integer (potential Bragg reflection)
tolerance = 0.1
is_bragg_like = all(abs(x - round(x)) < tolerance for x in hkl_frac_static)
print(f"Close to Bragg reflection: {is_bragg_like}")
```

**6. Notes/Caveats:**
- Returns fractional Miller indices; round to nearest integers for identifying Bragg reflections
- **Coordinate frame consistency:** Ensure q-vector and crystal A matrix are in compatible frames:
  - For `crystal.get_A()`: q-vector should be in the lab frame where A is defined
  - For scan-varying: use `crystal.get_A_at_scan_point(i)` with q-vectors from that scan point
  - For goniometer rotations: account for rotation between lab frame and crystal setting
- **Scan-varying crystals:** Use appropriate method based on whether crystal orientation changes
- **Fractional indices interpretation:** Values close to integers indicate proximity to Bragg reflections
- **Diffuse scattering analysis:** Non-integer values represent diffuse scattering in fractional hkl space

**7. Coordinate Frame Decision Tree:**
```python
# Decision logic for choosing the right method:
if experiment.scan is None:
    # Still experiment - use static method
    hkl = q_to_miller_indices_static(crystal, q_vector)
elif hasattr(crystal, 'num_scan_points') and crystal.num_scan_points > 1:
    # Scan-varying crystal - use scan-specific A matrix
    hkl = q_to_miller_indices_scan_varying(crystal, q_vector, scan_point)
elif goniometer is not None and "need_gonio_correction":
    # Fixed crystal with goniometer rotation (advanced case)
    hkl = q_to_miller_indices_with_goniometer(crystal, q_vector, goniometer, scan_point)
else:
    # Standard rotation with fixed crystal orientation
    hkl = q_to_miller_indices_static(crystal, q_vector)
```

**8. See Also:**
- Section C.1: Calculating q-vectors using detector geometry
- Section C.3: Calculating d-spacings from Miller indices
- Section B.3: Crystal model orientation matrices
- Section B.4: Goniometer model for rotation handling

---

## C.3. Calculating D-spacing from Q-vector or Miller Indices

**1. Purpose:**
Calculate resolution (d-spacing) from momentum transfer or crystallographic indices. Essential for resolution-dependent analysis and filtering in diffuse scattering studies.

**2. Primary Python Call(s):**
```python
import math
from scitbx import matrix

def d_spacing_from_q(q_vector):
    """Calculate d-spacing from q-vector magnitude"""
    q_magnitude = q_vector.length()
    if q_magnitude > 0:
        return 2 * math.pi / q_magnitude
    else:
        return float('inf')

def d_spacing_from_miller_indices(unit_cell, hkl):
    """Calculate d-spacing from Miller indices using unit cell"""
    # Use cctbx unit cell d-spacing calculation
    from cctbx import miller
    d_spacing = unit_cell.d(hkl)
    return d_spacing
```

**3. Key Arguments:**
- `q_vector`: scitbx.matrix.col (momentum transfer vector)
- `unit_cell`: cctbx.uctbx.unit_cell object
- `hkl`: tuple of (h, k, l) Miller indices

**4. Return Type:**
- `d_spacing`: float in Angstroms

**5. Example Usage Snippet:**
```python
# From q-vector
q_vec = matrix.col((0.1, 0.2, 0.3))
d_from_q = d_spacing_from_q(q_vec)

# From Miller indices
unit_cell = experiment.crystal.get_unit_cell()
hkl = (1, 2, 3)
d_from_hkl = d_spacing_from_miller_indices(unit_cell, hkl)

print(f"d-spacing from |q|: {d_from_q:.3f} Å")
print(f"d-spacing from Miller indices {hkl}: {d_from_hkl:.3f} Å")
```

**6. Notes/Caveats:**
- d = 2π/|q| for X-ray crystallography convention
- cctbx unit_cell.d() handles complex unit cell geometries correctly
- Both methods should give identical results for Bragg reflections

---

## C.4. Geometric Corrections

**1. Purpose:**
Apply Lorentz-polarization, solid angle, detector efficiency, and air attenuation corrections essential for accurate diffuse scattering intensity analysis. **CRITICAL: For the diffuse scattering pipeline, LP and QE corrections should be obtained from the robust DIALS Corrections API to avoid reimplementation errors. Only Solid Angle and Air Attenuation require custom implementations for diffuse pixels.**

**2. Primary Python Call(s):**
```python
from dials.algorithms.integration import CorrectionsMulti, Corrections
from dials.algorithms.integration.kapton_correction import KaptonAbsorption
import math
from scitbx import matrix

# Method 1: Using DIALS correction calculator (RECOMMENDED for LP and QE)
corrector = CorrectionsMulti()
for exp in experiments:
    corrector.append(Corrections(exp.beam, exp.goniometer, exp.detector))

# Calculate Lorentz-polarization correction (returns divisors)
lp_correction = corrector.lp(reflections["id"], reflections["s1"])

# Calculate quantum efficiency correction (returns multipliers)
qe_correction = corrector.qe(reflections["id"], reflections["s1"], reflections["panel"])

# For diffuse scattering: Use single Corrections object per still
corrections_obj = Corrections(experiment.beam, experiment.goniometer, experiment.detector)
# Apply to arrays of s1 vectors for diffuse pixels

# Method 2: Manual geometric corrections - ONLY for Solid Angle & Air Attenuation
# (LP and QE should use DIALS API above to avoid errors)
def calculate_solid_angle_correction(detector, panel_id, pixel_coord, beam):
    """Calculate solid angle subtended by a pixel"""
    panel = detector[panel_id]
    
    # Get pixel position in lab coordinates
    lab_coord = matrix.col(panel.get_pixel_lab_coord(pixel_coord))
    
    # Distance from sample to pixel
    distance = lab_coord.length()
    
    # Pixel area in mm^2
    pixel_size = panel.get_pixel_size()
    pixel_area = pixel_size[0] * pixel_size[1]
    
    # Angle between pixel normal and beam direction
    panel_normal = matrix.col(panel.get_fast_axis()).cross(matrix.col(panel.get_slow_axis()))
    beam_to_pixel = lab_coord.normalize()
    cos_angle = abs(panel_normal.dot(beam_to_pixel))
    
    # Solid angle = projected_area / distance^2
    solid_angle = (pixel_area * cos_angle) / (distance * distance)
    
    return solid_angle

def calculate_air_attenuation_correction(lab_coord, wavelength_angstrom, 
                                       air_path_mm=None, pressure_atm=1.0, temp_kelvin=293.15):
    """Calculate X-ray attenuation through air"""
    if air_path_mm is None:
        # Use distance to detector as air path
        air_path_mm = matrix.col(lab_coord).length()
    
    # X-ray attenuation coefficient for air at standard conditions
    # Approximate values for common wavelengths
    if wavelength_angstrom < 1.0:  # Hard X-rays
        mu_air_per_cm = 0.001  # Very approximate
    else:  # Softer X-rays
        mu_air_per_cm = 0.01 * (wavelength_angstrom ** 3)  # Rough scaling
    
    # Adjust for pressure and temperature
    density_correction = (pressure_atm * 273.15) / (1.0 * temp_kelvin)
    mu_air_corrected = mu_air_per_cm * density_correction
    
    # Attenuation factor
    path_cm = air_path_mm / 10.0
    attenuation_factor = math.exp(-mu_air_corrected * path_cm)
    
    return attenuation_factor
```

**3. Key Arguments:**
- `experiments`: ExperimentList object
- `reflections`: flex.reflection_table with s1 vectors and panel IDs
- For manual calculations: detector, beam, goniometer models and geometric parameters
- `attenuation_coeff`: Detector material attenuation coefficient (mm⁻¹)
- `thickness_mm`: Detector active layer thickness

**4. Return Type:**
- `correction_factor`: float or flex.double array of correction factors
- **Application convention:** See "Physical Interpretation and Application" below for how to apply each correction type

**5. Comprehensive Example Usage:**
```python
from dials.algorithms.integration import CorrectionsMulti, Corrections

# Method A: Hybrid approach for diffuse scattering (RECOMMENDED)
# Use DIALS API for LP & QE, custom calculations for Solid Angle & Air Attenuation

corrections_obj = Corrections(experiment.beam, experiment.goniometer, experiment.detector)

# For arrays of diffuse pixel s1 vectors and panel IDs
lp_corrections = corrections_obj.lp(s1_array)  # Returns divisors
qe_corrections = corrections_obj.qe(s1_array, panel_ids)  # Returns multipliers

# Custom calculations for remaining factors
solid_angle_factors = []  # Custom calculation needed
air_attenuation_factors = []  # Custom calculation needed

# Convert all to multiplicative form for consistency
lp_mult = 1.0 / lp_corrections
total_multiplicative_corrections = lp_mult * qe_corrections * solid_angle_mult * air_mult
corrected_intensities = raw_intensities * total_multiplicative_corrections

print(f"LP corrections (divisors): {flex.min(lp_corrections):.3f} to {flex.max(lp_corrections):.3f}")
print(f"QE corrections (multipliers): {flex.min(qe_corrections):.3f} to {flex.max(qe_corrections):.3f}")
```

**6. Physical Interpretation and Application:**

**Correction Factor Definitions:**
- **Lorentz-Polarization (LP):** Corrects for geometric and polarization effects → **Apply as divisor**
- **Solid Angle (Ω):** Normalizes intensity per unit solid angle → **Apply as divisor** 
- **Polarization (P):** Additional polarization correction → **Apply as multiplier**
- **Detector Efficiency (QE):** Corrects for incomplete absorption → **Apply as divisor**
- **Air Attenuation (A):** Corrects for X-ray absorption in air → **Apply as divisor**

**Combined Correction Formula:**
```python
# Standard diffuse scattering intensity correction:
I_corrected = I_raw / (LP_factor * solid_angle * QE_factor * air_attenuation_factor) * polarization_factor

# Or equivalently, using multiplicative forms:
LP_mult = 1.0 / LP_factor
solid_angle_mult = 1.0 / solid_angle  
QE_mult = 1.0 / QE_factor
air_mult = 1.0 / air_attenuation_factor
pol_mult = polarization_factor

I_corrected = I_raw * LP_mult * solid_angle_mult * QE_mult * air_mult * pol_mult
```

**7. Notes/Caveats:**
- **Coordinate systems:** Ensure s1 vectors are in lab frame and properly normalized
- **Detector efficiency:** Values depend on detector type (silicon, CdTe, etc.) and X-ray energy
- **Air attenuation:** Significant for low-energy X-rays and long air paths
- **Solid angle:** Critical for accurate diffuse scattering intensity comparisons
- **DIALS integration:** `CorrectionsMulti.lp()` returns factors to divide by; `CorrectionsMulti.qe()` returns factors to multiply by
- **Sign conventions:** Always verify factor definitions in your specific analysis context

---

## C.5. Theoretical Scattering Factor Calculation

**1. Purpose:**
Calculate atomic form factors and incoherent scattering intensities (from cctbx.eltbx) for comparison with experimental diffuse scattering and theoretical model validation.

**2. Primary Python Call(s):**
```python
from cctbx.eltbx import sasaki, henke, henke_cdb
from cctbx.eltbx.xray_scattering import wk1995
import math

def get_atomic_form_factor(element, q_magnitude, table="wk1995"):
    """Get atomic form factor for an element at given q"""
    
    # Convert q to sin(theta)/lambda (s parameter)
    s = q_magnitude / (4 * math.pi)
    
    if table == "wk1995":
        # Waasmaier-Kirfel 1995 form factors
        scattering_factor_info = wk1995(element, True)
        f0 = scattering_factor_info.at_stol(s)
    elif table == "sasaki":
        # Sasaki 1989 form factors
        fp_fdp = sasaki.table(element).at_angstrom(wavelength)
        f0 = fp_fdp.fp()  # Real part
    
    return f0

def get_incoherent_scattering(element, q_magnitude):
    """Get incoherent scattering intensity for an element"""
    s = q_magnitude / (4 * math.pi)
    
    # Use Hubbell et al. tables for incoherent scattering
    scattering_factor_info = wk1995(element, True)
    # Incoherent scattering approximation
    atomic_number = scattering_factor_info.atomic_number()
    s_incoh = atomic_number * (1 - math.exp(-2 * s**2))
    
    return s_incoh
```

**3. Key Arguments:**
- `element`: str (element symbol, e.g., "C", "N", "O")
- `q_magnitude`: float (magnitude of q-vector in Å⁻¹)
- `wavelength`: float (X-ray wavelength in Å)
- `table`: str (form factor table to use)

**4. Return Type:**
- `f0`: float (atomic form factor in electrons)
- `s_incoh`: float (incoherent scattering intensity)

**5. Example Usage Snippet:**
```python
# Calculate form factors for carbon at different q values
import math
import numpy as np

element = "C"
q_values = np.linspace(0.1, 2.0, 20)

for q in q_values:
    f0 = get_atomic_form_factor(element, q)
    s_incoh = get_incoherent_scattering(element, q)
    d_spacing = 2 * math.pi / q
    
    print(f"q={q:.3f} Å⁻¹, d={d_spacing:.3f} Å: f0={f0:.3f}, I_incoh={s_incoh:.3f}")
```

**6. Notes/Caveats:**
- Form factors depend on scattering parameter s = |q|/(4π) = sin(θ)/λ
- Different tables available: wk1995 (Waasmaier-Kirfel), sasaki, henke
- Anomalous scattering corrections available through sasaki and henke tables

---

## C.6. CCTBX Crystal Model Averaging

**1. Purpose:**
Average multiple crystal unit cells and orientation matrices from stills diffraction for obtaining consensus crystal parameters and assessing crystal parameter distributions in diffuse scattering analysis.

**2. Primary Python Call(s):**
```python
from cctbx import uctbx, sgtbx
from scitbx import matrix
import math

def average_unit_cells(unit_cells, weights=None):
    """
    Average multiple unit cell parameters with optional weighting
    
    Args:
        unit_cells: List of cctbx.uctbx.unit_cell objects
        weights: Optional list of weights for each unit cell
    
    Returns:
        cctbx.uctbx.unit_cell: Averaged unit cell
    """
    if not unit_cells:
        return None
    
    if weights is None:
        weights = [1.0] * len(unit_cells)
    
    # Extract parameters from all unit cells
    all_params = [uc.parameters() for uc in unit_cells]
    
    # Calculate weighted averages
    total_weight = sum(weights)
    avg_params = []
    
    for param_idx in range(6):  # a, b, c, alpha, beta, gamma
        weighted_sum = sum(params[param_idx] * weight 
                          for params, weight in zip(all_params, weights))
        avg_params.append(weighted_sum / total_weight)
    
    # Create averaged unit cell
    averaged_unit_cell = uctbx.unit_cell(avg_params)
    return averaged_unit_cell

def create_average_crystal_model(crystal_models, weights=None):
    """
    Create averaged crystal model from multiple crystal models
    
    Args:
        crystal_models: List of dxtbx.model.Crystal objects
        weights: Optional list of weights for each crystal
    
    Returns:
        Averaged crystal parameters as dictionary
    """
    if not crystal_models:
        return None
    
    # Extract unit cells and orientation matrices
    unit_cells = [crystal.get_unit_cell() for crystal in crystal_models]
    U_matrices = [crystal.get_U() for crystal in crystal_models]
    space_groups = [crystal.get_space_group() for crystal in crystal_models]
    
    # Check space group consistency
    reference_sg = space_groups[0]
    if not all(sg.info().symbol_and_number() == reference_sg.info().symbol_and_number() 
               for sg in space_groups):
        print("Warning: Space groups are not consistent across crystal models")
    
    # Average unit cells and orientations
    avg_unit_cell = average_unit_cells(unit_cells, weights)
    
    # Calculate average B matrix from averaged unit cell
    avg_B_matrix = matrix.sqr(avg_unit_cell.fractionalization_matrix()).transpose()
    
    return {
        'unit_cell': avg_unit_cell,
        'space_group': reference_sg,
        'B_matrix': avg_B_matrix,
        'unit_cell_parameters': avg_unit_cell.parameters(),
        'unit_cell_volume': avg_unit_cell.volume()
    }
```

**3. Example Usage Snippet:**
```python
# Load multiple crystal models from stills processing
crystal_models = []
for expt_file in ["still_001_integrated.expt", "still_002_integrated.expt", "still_003_integrated.expt"]:
    experiments = ExperimentListFactory.from_json_file(expt_file)
    if len(experiments) > 0:
        crystal_models.append(experiments[0].crystal)

print(f"Loaded {len(crystal_models)} crystal models for averaging")

# Create averaged crystal model
avg_crystal_params = create_average_crystal_model(crystal_models)

if avg_crystal_params:
    print("Averaged Crystal Parameters:")
    print(f"Unit cell: {avg_crystal_params['unit_cell_parameters']}")
    print(f"Volume: {avg_crystal_params['unit_cell_volume']:.1f} Å³")
    print(f"Space group: {avg_crystal_params['space_group'].info().symbol_and_number()}")
```

---

## C.7. CCTBX Asymmetric Unit (ASU) Mapping

**1. Purpose:**
Map Miller indices and fractional coordinates to crystallographic asymmetric unit for proper handling of symmetry-equivalent reflections in diffuse scattering analysis and comparison with theoretical models.

**2. Primary Python Call(s):**
```python
from cctbx import miller, sgtbx, crystal
from scitbx import matrix

def map_miller_indices_to_asu(miller_indices, space_group):
    """
    Map Miller indices to asymmetric unit
    
    Args:
        miller_indices: List of (h,k,l) tuples or flex.miller_index
        space_group: cctbx.sgtbx.space_group object
    
    Returns:
        List of ASU-mapped Miller indices
    """
    # Create miller set for batch ASU mapping
    miller_set = miller.set(
        crystal_symmetry=crystal.symmetry(
            space_group=space_group
        ),
        indices=miller_indices
    )
    
    # Map to ASU
    asu_miller_set = miller_set.map_to_asu()
    return asu_miller_set.indices()

def map_fractional_coords_to_asu(fractional_coords, space_group, epsilon=1e-6):
    """
    Map fractional coordinates to asymmetric unit
    
    Args:
        fractional_coords: List of (x,y,z) fractional coordinate tuples
        space_group: cctbx.sgtbx.space_group object
        epsilon: Tolerance for boundary conditions
    
    Returns:
        List of ASU-mapped fractional coordinates
    """
    asu_coords = []
    space_group_info = space_group.info()
    
    for coord in fractional_coords:
        # Map individual coordinate to ASU
        asu_coord = space_group_info.map_to_asu(coord, epsilon=epsilon)
        asu_coords.append(asu_coord)
    
    return asu_coords
```

**3. Example Usage Snippet:**
```python
from cctbx import crystal
from dxtbx.model.experiment_list import ExperimentListFactory

# Load crystal model with space group
experiments = ExperimentListFactory.from_json_file("experiments.expt")
crystal_model = experiments[0].crystal
space_group = crystal_model.get_space_group()
unit_cell = crystal_model.get_unit_cell()

# Create crystal symmetry object
crystal_symmetry = crystal.symmetry(
    unit_cell=unit_cell,
    space_group=space_group
)

print(f"Space group: {space_group.info().symbol_and_number()}")
print(f"ASU conditions: {space_group.info().asu()}")

# Example Miller indices from diffuse scattering analysis
test_indices = [(1, 2, 3), (2, 4, 6), (-1, -2, -3), (3, 2, 1)]

# Map to ASU
asu_indices = map_miller_indices_to_asu(test_indices, space_group)

print("Miller index ASU mapping:")
for orig, asu in zip(test_indices, asu_indices):
    print(f"  {orig} → {asu}")
```

---

## C.8. CCTBX Tabulated Compton Scattering Factors

**1. Purpose:**
Access tabulated incoherent (Compton) scattering factors for elements as a function of momentum transfer, essential for calculating theoretical diffuse scattering backgrounds and separating elastic from inelastic contributions.

**2. Primary Python Call(s):**
```python
from cctbx.eltbx import sasaki  # Primary CCTBX scattering factor access
from cctbx.eltbx.xray_scattering import wk1995
import math

def get_compton_scattering_factor(element, q_magnitude_or_stol, energy_kev=12.4, table="it1992"):
    """
    Get incoherent (Compton) scattering factor for an element using CCTBX tabulated data
    
    Args:
        element: Element symbol (e.g., "C", "N", "O")
        q_magnitude_or_stol: |q| in Å⁻¹ or sin(θ)/λ value
        energy_kev: X-ray energy in keV (for future compatibility)
        table: Scattering factor table ("it1992" ONLY - others have limitations)
    
    Returns:
        Incoherent scattering factor (electrons)
        
    Note:
        Based on CCTBX codebase investigation:
        - IT1992 is the ONLY table with direct incoherent() method access
        - Chantler tables are NOT available in current CCTBX distribution
        - Henke tables only provide fp()/fdp() anomalous factors, not incoherent
        - Use table="it1992" exclusively for reliable incoherent factor access
    """
    # Convert q to sin(theta)/lambda if needed
    if q_magnitude_or_stol > 2.0:  # Assume it's |q| in Å⁻¹
        stol = q_magnitude_or_stol / (4 * math.pi)
    else:  # Assume it's already sin(theta)/lambda
        stol = q_magnitude_or_stol
    
    if table == "it1992":
        # International Tables for Crystallography 1992 - RECOMMENDED
        # This is the ONLY table with reliable incoherent() method access in CCTBX
        try:
            scattering_factor_table = sasaki.table(element)
            s_incoh = scattering_factor_table.at_stol(stol).incoherent()
            return s_incoh
            
        except (ImportError, AttributeError, RuntimeError) as e:
            print(f"Warning: IT1992 incoherent factors not accessible for {element}: {e}")
            # Fallback to approximation using atomic number
            atomic_number = sasaki.table(element).atomic_number()
            return atomic_number * (1 - math.exp(-2 * stol**2))
    
    else:
        raise ValueError(f"Unknown table type: {table}. Use 'it1992' for reliable incoherent access")

def calculate_theoretical_compton_background(elements, concentrations, q_values, 
                                           wavelength_angstrom=1.0):
    """
    Calculate theoretical Compton scattering background for a compound
    
    Args:
        elements: List of element symbols
        concentrations: List of atomic fractions for each element
        q_values: Array of q-values (Å⁻¹)
        wavelength_angstrom: X-ray wavelength in Angstroms
    
    Returns:
        Array of Compton scattering intensities
    """
    compton_intensities = []
    
    for q_mag in q_values:
        total_compton = 0.0
        
        for element, concentration in zip(elements, concentrations):
            # Get incoherent scattering factor
            s_incoh = get_compton_scattering_factor(element, q_mag, table="it1992")
            
            # Weight by concentration
            total_compton += concentration * s_incoh
        
        compton_intensities.append(total_compton)
    
    return compton_intensities
```

**3. Example Usage Snippet:**
```python
import numpy as np

# Define compound composition (e.g., protein: C, N, O, S)
elements = ["C", "N", "O", "S"]
concentrations = [0.50, 0.16, 0.23, 0.03]  # Approximate protein composition
wavelength = 1.0  # Å

# Define q-range for analysis
q_values = np.linspace(0.1, 2.0, 100)

# Calculate Compton background
compton_background = calculate_theoretical_compton_background(
    elements, concentrations, q_values, wavelength
)

print("Compton scattering factors for different elements at |q| = 0.5 Å⁻¹:")
for element in ["C", "N", "O", "P", "S"]:
    s_incoh = get_compton_scattering_factor(element, 0.5)
    print(f"{element}: Compton = {s_incoh:.3f}")
```

**4. Notes/Caveats:**
- **Table Accuracy:** IT1992 is the only table with reliable incoherent factor access in CCTBX
- **Energy Dependence:** Incoherent scattering is weakly energy-dependent; coherent scattering shows strong energy dependence near absorption edges
- **Q-dependence:** Compton scattering factors increase with q; coherent factors decrease
- **Units:** Ensure consistent units (Å⁻¹ for q, keV for energy, electrons for scattering factors)

**5. Integration with Diffuse Scattering Pipeline:**
Compton scattering calculations provide theoretical baselines for separating elastic diffuse scattering from inelastic backgrounds, essential for accurate diffuse scattering analysis.

---

## See Also

- **File I/O Operations**: [dials_file_io.md](dials_file_io.md)
- **Detector Models**: [dxtbx_models.md](dxtbx_models.md)
- **DIALS Scaling**: [dials_scaling.md](dials_scaling.md)
- **Array Operations**: [flex_arrays.md](flex_arrays.md)
</file>

<file path="libdocs/dials/dials_conventions.md">
# Conventions

## Coordinate frames

### The diffractometer equation

We use the vector $\vec{h}$ to describe a position in *fractional reciprocal space* in terms of the reciprocal lattice basis vectors $\vec{a^*}$, $\vec{b^*}$ and $\vec{c^*}$.

$$\vec{h} = \begin{pmatrix}
h \\
k \\
l \\
\end{pmatrix} = h \vec{a^*} + k \vec{b^*} + l \vec{c^*}$$

The special positions at which h, k and l are integer define the *reciprocal lattice points* for which (hkl) are the *Miller indices*.

The basic diffractometer equation relates a position $\vec{h}$ to a position $\vec{r_\phi}$ in *Cartesian reciprocal space*. This space is defined so that its axes coincide with the axes of the *laboratory frame*. The distinction is necessary because distances in reciprocal space are measured in units of $\text{\AA}^{-1}$. However, for convenience it is often acceptable to refer to either Cartesian reciprocal space or the real space laboratory frame as the "lab frame", when the correct choice is clear by context. The diffractometer equation is:

$$\vec{r_\phi} = \mathbf{R} \mathbf{A} \vec{h}$$

where $\mathbf{R}$ is the *goniostat rotation matrix* and $\mathbf{A}$ is the *crystal setting matrix*, while its inverse $\mathbf{A}^{-1}$ is referred to as the *indexing matrix*. The product $\mathbf{A} \vec{h}$ may be written as $\vec{r_0}$, which is a position in the $\phi$-axis frame, a Cartesian frame that coincides with the laboratory frame at a rotation angle of $\phi=0$. This makes clear that the setting matrix does not change during the course of a rotation experiment (notwithstanding small "misset" rotations — see *Orientation matrix*).

For an experiment performed using the rotation method we use here $\phi$ to refer to the angle about the actual axis of rotation, even when this is effected by a differently labelled axis on the sample positioning equipment (such as an $\omega$ axis of a multi-axis goniometer). Only in code specifically dealing with sample positioning equipment might we need to redefine the labels of axes. Outside of such modules, the rotation angle is $\phi$ and the axis of rotation is $\vec{e}$, which together with the definition of the laboratory frame determine the rotation matrix $\mathbf{R}$.

### Orthogonalisation convention

Following [2] we may decompose the setting matrix $\mathbf{A}$ into the product of two matrices, conventionally labelled $\mathbf{U}$ and $\mathbf{B}$. We name $\mathbf{U}$ the *orientation matrix* and $\mathbf{B}$ the *reciprocal space orthogonalisation matrix*. These names are in common, but not universal use. In particular, some texts (for example [6]) refer to the product (i.e. our setting matrix) as the "orientation matrix".

Of these two matrices, $\mathbf{U}$ is a pure rotation matrix and is dependent on the definition of the lab frame, whilst $\mathbf{B}$ is not dependent on this definition. $\mathbf{B}$ does depend however on a choice of orthogonalisation convention, which relates $\vec{h}$ to a position in the *crystal-fixed Cartesian system*. The basis vectors of this orthogonal Cartesian frame are fixed to the reciprocal lattice *via* this convention.

There are infinitely many ways that $\mathbf{A}$ may be decomposed into a pair $\mathbf{U} \mathbf{B}$. The symbolic expression of $\mathbf{B}$ is simplified when the crystal-fixed Cartesian system is chosen to be aligned with crystal real or reciprocal space axes. For example, [2] use a frame in which the basis vector $\vec{i}$ is parallel to reciprocal lattice vector $\vec{a^*}$, while $\vec{j}$ is chosen to lie in the plane of $\vec{a^*}$ and $\vec{b^*}$. Unfortunately, this convention is then disconnected from the standard *real space* orthogonalisation convention, usually called the *PDB convention* [7]. This standard is essentially universal in crystallographic software for the transformation of fractional crystallographic coordinates to positions in orthogonal space, with units of $\text{\AA}$. In particular, it is the convention used in the cctbx [4]. The convention states that the orthogonal coordinate $x$ is determined from a fractional coordinate $u$ by:

$$\vec{x} = \mathbf{O} \vec{u}$$

where the matrix $\mathbf{O}$ is the *real space orthogonalisation matrix*. This matrix transforms to a crystal-fixed Cartesian frame that is defined such that its basis vector $\vec{i}$ is parallel to the real space lattice vector $\vec{a}$, while $\vec{j}$ lies in the $(\vec{a}, \vec{b})$ plane. The elements of this matrix made explicit in a compact form are:

$$\mathbf{O} =
\begin{pmatrix}
a & b\cos{\gamma} &  c\cos{\beta} \\
0 & b\sin{\gamma} & -c\sin{\beta}\cos{\alpha^*} \\
0 & 0             &  c\sin{\beta}\sin{\alpha^*} \\
\end{pmatrix}$$

It is desirable to specify our *reciprocal space* orthogonalisation convention in terms of this real space orthogonalisation convention. [3] derives relationships between real and reciprocal space. Of particular interest from that text we have:

$$
\begin{align}
\vec{x} &= \mathbf{M}^\mathsf{T} \vec{x}^\prime \\
\vec{x^*} &= \mathbf{M}^{-1} \vec{x^*}^\prime
\end{align}
$$

By analogy, equate $\vec{x^*}^\prime$ with $\vec{h}$ and $\mathbf{B}$ with $\mathbf{M}^{-1}$. Also equate $\mathbf{M}^\mathsf{T}$ with $\mathbf{O}$ and $\vec{x}^\prime$ with $\vec{u}$. We then see that:

$$\mathbf{B} = \left( \mathbf{O}^{-1} \right)^\mathsf{T} = \mathbf{F}^\mathsf{T}$$

where $\mathbf{F}$ is designated the *real space fractionalisation matrix*. This is easily obtained in cctbx by a method of a `cctbx.uctbx.unit_cell` object.

A symbolic expression for $\mathbf{F}$ in terms of the real space unit cell parameters is given by [8] from which we derive $\mathbf{B}$ simply:

$$\mathbf{B} =
\begin{pmatrix}
\frac{1}{a} &
0 &
0 \\
-\frac{\cos{\gamma}}{a\sin{\gamma}} &
\frac{1}{b\sin{\gamma}} &
0 \\
\frac{bc}{V}\left( \frac{\cos{\gamma} \left( \cos{\alpha} - \cos{\beta}\cos{\gamma} \right)}{\sin{\gamma}} - \cos{\beta}\sin{\gamma} \right) &
-\frac{ac \left( \cos{\alpha} - \cos{\beta}\cos{\gamma} \right)}{V\sin{\gamma}} &
\frac{ab\sin{\gamma}}{V} \\
\end{pmatrix}$$

with $V = abc \sqrt{ 1 - \cos^2{\alpha} - \cos^2{\beta} - \cos^2{\gamma} + 2 \cos{\alpha}\cos{\beta}\cos{\gamma}}$

## Orientation matrix

The matrix $\mathbf{U}$ "corrects" for the orthogonalisation convention implicit in the choice of $\mathbf{B}$. As the crystal-fixed Cartesian system and the $\phi$-axis frame are both orthonormal, Cartesian frames with the same scale, it is clear that $\mathbf{U}$ must be a pure rotation matrix. Its elements are clearly dependent on the mutual orientation of these frames.

It is usual to think of the orientation as a fixed property of the "sequence". In practice the orientation is parameterised such that it becomes a function of time, to account for crystal slippage (the true degree of this is unknown but expected to be small; Mosflm uses crystal orientation parameters to account for inadequacies in other aspects of the experimental description). To reconcile these points, the current orientation may be expanded into a fixed, datum part and a variable time-dependent part that is parameterised. That gives:

$$\vec{r_\phi} = \mathbf{\Psi}\mathbf{R}\mathbf{U_0}\mathbf{B}\vec{h}$$

where $\mathbf{\Psi}$ is the combined rotation matrix for the misset expressed as three angles, $\psi_x, \psi_y$ and $\psi_z$ in the laboratory frame.

In Mosflm these angles are converted to their equivalents in the $\phi-$ axis frame, where:

$$\vec{r_\phi} = \mathbf{R}\mathbf{\Phi}\mathbf{U_0}\mathbf{B}\vec{h}$$

At this stage it is unclear which set of angles are the best choice for parameterisation of the crystal orientation.

### The laboratory frame

An important design goal of the DIALS project is that all algorithms should be fully vectorial. By this we mean that it should be possible to change the reference frame arbitrarily and all calculations should work appropriately in the new frame.

Nevertheless, it is useful to adopt a particular standard frame of reference for meaningful comparison of results, communication between components of the software and for an agreed definition of what the laboratory consists of (incompatible definitions can be reasonably argued for, such as that it should be either fixed to the detector, or to the rotation axis and beam).

In the interests of both standardisation and practicality, we choose to adopt the Image CIF (imgCIF) reference frame [1], [5], for cases with a single axis horizontal goniometer. For beamlines with a vertical goniometer, we align the rotation axis with the $Y$ rather than the $X$ axis. This ensures the axis appears vertical in the image viewer, reducing confusion for the users. Such decisions can be made on a case-by-case basis within the specific format class used to read the images, giving the freedom to choose the most convenient coordinate system for the geometry of each experiment.

### Summary of coordinate frames

* $\vec{h}$ gives a position in *fractional reciprocal space*, fixed to the crystal.
* $\mathbf{B}\vec{h}$ gives that position in the *crystal-fixed Cartesian system* (basis aligned to crystal axes by the orthogonalization convention)
* $\mathbf{UB}\vec{h}$ gives the $\phi$-axis frame (rotates with the crystal, axes aligned to lab frame at $\phi=0$)
* $\mathbf{RUB}\vec{h}$ gives *Cartesian reciprocal space* (fixed wrt the laboratory)
* The diffraction geometry relates this to the direction of the scattering vector $\vec{s}$ in the *laboratory frame*
* Projection along $\vec{s}$ impacts an *abstract sensor frame* giving a 2D position of the reflection position on a sensor.
* This position is converted to the *pixel position* for the 2D position on an image in number of pixels (starting 0,0 at the origin).

### The DXTBX goniometer model

The following information is likely only to be of interest to developers of DIALS, since it is concerned with internal conventions for the representation of the goniostat rotation operator.

When one performs a rotation diffraction experiment, the goniostat rotation operator $\mathbf{R}$, which was introduced in the diffractometer equation, represents the rotation of the sample in the real-space laboratory frame. Equivalently, it relates the laboratory frame to the rotated real-space coordinate system of the sample. By numbering the physical motors of a goniometer such that motor 1 is mounted to the laboratory floor, motor 2 is mounted on motor 1, motor 3 is mounted on motor 2, etc., $\mathbf{R}$ can be expressed as a composition of rotation operators, one for each motor:

$$\mathbf{R} = \mathbf{R}_1 \circ \mathbf{R}_2 \circ \mathbf{R}_3 \circ \cdots$$

It is useful to represent $\mathbf{R}$ this way because, in practice, the position of a gonimometer is usually recorded as an angular displacement for each motor $i$, which gives the magnitude of the rotation $\mathbf{R}_i$, combined with prior knowledge of the orientation of each motor's axis in the laboratory frame when the goniometer is at its zero datum, which gives the axis vector of $\mathbf{R}_i$.

DIALS uses the DXTBX package to handle the geometry of diffraction experiments. In DXTBX, it is assumed that only one goniometer motor will turn during a measurement scan. Numbering that motor $n$, the various operators $\mathbf{R}_i$ are grouped into three:

$$\mathbf{R} = \mathbf{S} \circ \mathbf{R}' \circ \mathbf{F}$$

where $\mathbf{R}' = \mathbf{R}_n$ is the scanning axis rotation and $\mathbf{S}$ and $\mathbf{F}$ are combined rotation operators:

$$
\begin{align}
   \mathbf{S} &= \cdots \circ \mathbf{R}_{n - 1} \\
   \mathbf{F} &= \mathbf{R}_{n + 1} \circ \cdots
\end{align}
$$

$\mathbf{S}$ is referred to as the 'setting rotation' (not to be confused with the crystal setting operator $\mathbf{A}$) and represents the combined rotation of all motors between the laboratory floor and the scanning motor $n$. The setting rotation is so-named because it sets the orientation of the axis of $n$. $\mathbf{F}$ is referred to as the 'fixed rotation' and represents the combined rotation of all axes between $n$ and the sample. Both $\mathbf{S}$ and $\mathbf{F}$ are constant throughout a scan, since they represent motors whose positions are set before starting the scan and remain fixed throughout the scan.

For example, a common diffractometer apparatus is the three-circle $\kappa$ geometry. In this arrangement, the $\omega$ motor is fixed to the laboratory floor, the $\kappa$ motor is mounted on $\omega$, and the $\phi$ motor is mounted on $\kappa$ and holds the sample mount. During a typical rotation scan, either $\phi$ or $\omega$ will rotate, while the other two axes are held in a fixed orientation, chosen so as to explore a particular region of reciprocal space. In the case of a $\phi$ scan, $\mathbf{S} = \mathbf{R}_\omega \circ \mathbf{R}_\kappa$, $\mathbf{R}' = \mathbf{R}_\phi$ and $\mathbf{F} = \mathbf{1}$. In the case of an $\omega$ scan, $\mathbf{S} = \mathbf{1}$, $\mathbf{R}' = \mathbf{R}_\omega$ and $\mathbf{F} = \mathbf{R}_\kappa \circ \mathbf{R}_\phi$.

## References

[1] [Bernstein, H. J. in Int. Tables Crystallogr. 199–205 (IUCr, 2006).](http://it.iucr.org/Ga/ch3o7v0001/)

[2] Busing, W. R. & Levy, H. A. Angle calculations for 3- and 4-circle X-ray and neutron diffractometers. Acta Crystallogr. 22, 457–464 (1967).

[3] Giacovazzo, C. Fundamentals of Crystallography. (Oxford University Press, USA, 2002).

[4] Grosse-Kunstleve, R. W., Sauter, N. K., Moriarty, N. W. & Adams, P. D. The Computational Crystallography Toolbox: crystallographic algorithms in a reusable software framework. J. Appl. Crystallogr. 35, 126–136 (2002).

[5] [Hammersley, A. P., Bernstein, H. J. & Westbrook, D. in Int. Tables Crystallogr. 444–458 (IUCr, 2006).](http://it.iucr.org/Ga/ch4o6v0001/)

[6] Paciorek, W. A., Meyer, M. & Chapuis, G. On the geometry of a modern imaging diffractometer. Acta Crystallogr. Sect. A Found. Crystallogr. 55, 543–557 (1999).

[7] [PDB. Atomic Coordinate and Bibliographic Entry Format Description. Brookhaven Natl. Lab. (1992).](http://www.wwpdb.org/docs/documentation/file-format/PDB_format_1992.pdf)

[8] [Rupp, B. Coordinate system transformation.](http://www.ruppweb.org/Xray/tutorial/Coordinate%20system%20transformation.htm)
</file>

<file path="libdocs/dials/dials_file_io.md">
# DIALS File I/O and Model Loading

This documentation covers file I/O operations and model loading for DIALS/`dxtbx`/`cctbx` Python libraries needed for implementing diffuse scattering data processing modules.

**Version Information:** Compatible with DIALS 3.x series. Some methods may differ in DIALS 2.x.

**Key Dependencies:**
- `dxtbx`: Detector models, beam models, image handling
- `dials.array_family.flex`: Reflection tables and array operations  
- `cctbx`: Unit cells, space groups, scattering factors
- `scitbx`: Matrix operations and mathematical utilities

---

## A.0. dials.stills_process Python API

**1. Purpose:**
Provides a complete pipeline for processing still diffraction images through spot finding, indexing, refinement, and integration. Essential for generating per-still crystal models, reflection data with partiality, and shoebox data for Bragg mask generation in stills diffuse scattering pipelines.

**2. Primary Python Call(s):**
```python
from dials.command_line.stills_process import Processor, phil_scope, do_import
from libtbx.phil import parse

# Initialize processor with parameters
params = phil_scope.fetch(parse("")).extract()  # Default parameters
processor = Processor(params, composite_tag="pipeline_run", rank=0)

# Import image to experiment
experiments = do_import("image.cbf", load_models=True)

# Process through full pipeline
processor.process_experiments(tag="image_001", experiments=experiments)

# Access results
integrated_experiments = processor.all_integrated_experiments
integrated_reflections = processor.all_integrated_reflections
```

**3. Key Classes and Methods:**

**Processor Class:**
- `__init__(self, params, composite_tag=None, rank=0)`: Initialize processor
  - `params`: PHIL parameter object controlling processing behavior
  - `composite_tag`: Tag for aggregated output files
  - `rank`: Process rank for parallel processing
- `process_experiments(self, tag, experiments)`: Main processing pipeline
  - `tag`: String identifier for this processing run
  - `experiments`: ExperimentList object from do_import()
- `find_spots(self, experiments)` → flex.reflection_table: Spot finding step
- `index(self, experiments, reflections)` → (ExperimentList, flex.reflection_table): Indexing step
- `refine(self, experiments, reflections)` → (ExperimentList, flex.reflection_table): Refinement step  
- `integrate(self, experiments, reflections)` → flex.reflection_table: Integration step
- `finalize(self)`: Write final outputs to files

**do_import Function:**
- `do_import(filename, load_models=True)` → ExperimentList: Convert image file to experiments
  - `filename`: Path to image file (CBF, HDF5, etc.)
  - `load_models`: Whether to load detector/beam models from headers

**4. Key Output Attributes:**
- `processor.all_integrated_experiments`: ExperimentList with refined crystal models
- `processor.all_integrated_reflections`: Reflection table with partiality data
- `processor.all_strong_reflections`: Reflection table from spot finding
- `processor.all_indexed_reflections`: Reflection table with Miller indices

**5. Example Usage Snippet:**
```python
from dials.command_line.stills_process import Processor, phil_scope, do_import
from libtbx.phil import parse

# Configure parameters for stills processing
custom_phil = """
dispatch {
  find_spots = True
  index = True
  refine = True
  integrate = True
  squash_errors = False
}
output {
  composite_output = True
  shoeboxes = True
}
integration {
  debug {
    output = True
    separate_files = False
    delete_shoeboxes = True
  }
}
"""

# Initialize processor
params = phil_scope.fetch(parse(custom_phil)).extract()
processor = Processor(params, composite_tag="diffuse_pipeline")

# Process a still image
image_path = "still_001.cbf"
experiments = do_import(image_path)
processor.process_experiments(tag="still_001", experiments=experiments)

# Extract results for pipeline
for i, expt in enumerate(processor.all_integrated_experiments):
    # Per-still crystal model (Experiment_dials_i)
    crystal_model = expt.crystal
    unit_cell = crystal_model.get_unit_cell().parameters()
    space_group = crystal_model.get_space_group().info().symbol_and_number()
    A_matrix = crystal_model.get_A()  # Orientation + metric matrix
    U_matrix = crystal_model.get_U()  # Orientation matrix only
    
    print(f"Still {i} crystal model:")
    print(f"  Unit cell: a={unit_cell[0]:.3f}, b={unit_cell[1]:.3f}, c={unit_cell[2]:.3f} Å")
    print(f"  Space group: {space_group}")
    print(f"  A matrix determinant: {A_matrix.determinant():.6f}")
    
    # Per-still reflections with partiality (Reflections_dials_i)
    expt_reflections = processor.all_integrated_reflections.select(
        processor.all_integrated_reflections["id"] == i
    )
    
    # Access partiality data (P_spot)
    if "partiality" in expt_reflections:
        partialities = expt_reflections["partiality"]
        print(f"  {len(expt_reflections)} reflections, "
              f"partiality range: {flex.min(partialities):.3f}-{flex.max(partialities):.3f}")
    
    # Access shoeboxes for mask generation (BraggMask_2D_raw_i)
    if "shoebox" in expt_reflections:
        shoeboxes = expt_reflections["shoebox"]
        print(f"  {len(shoeboxes)} shoeboxes with 3D mask data")

processor.finalize()
```

**6. Parameter Configuration:**
The PHIL parameter system controls all processing aspects:
```python
# Essential parameters for diffuse scattering pipeline
key_params = """
# Algorithm control
dispatch.find_spots = True      # Enable spot finding
dispatch.index = True           # Enable indexing  
dispatch.refine = True          # Enable refinement (recommended)
dispatch.integrate = True       # Enable integration

# Hit finder (quality control)
dispatch.hit_finder.minimum_number_of_reflections = 10
dispatch.hit_finder.maximum_number_of_reflections = None

# Output control
output.composite_output = True  # Aggregate results efficiently
output.shoeboxes = True         # Save shoebox data for masking

# Integration settings for diffuse scattering
integration.integrator = stills  # Use stills-specific algorithms
integration.debug.output = True  # Enable shoebox saving
integration.debug.delete_shoeboxes = True  # Manage memory usage

# Refinement constraints for stills
refinement.parameterisation.beam.fix = all      # Fix beam parameters
refinement.parameterisation.detector.fix = all  # Fix detector parameters
"""
```

**7. Notes/Caveats:**
- **Environment Dependency:** Requires full DIALS/cctbx environment to be properly initialized
- **Memory Management:** Enable `integration.debug.delete_shoeboxes=True` for large datasets
- **Error Handling:** Set `dispatch.squash_errors=False` for debugging; `True` for production batch processing
- **Partiality Quality:** Partiality calculations are production-quality, based on 3-sigma Gaussian profile modeling
- **Per-Still Processing:** Each input image becomes a separate experiment with its own crystal model
- **Composite Output Behavior:** When `composite_output=True`, all processed stills are aggregated into single output files (processor.all_integrated_experiments, processor.all_integrated_reflections). When `False`, each still generates separate files with tag-based naming (e.g., `tag_integrated.refl`)

**8. Integration with Diffuse Scattering Pipeline:**
```python
# Module 1.S.1 Implementation Pattern
def process_still_for_diffuse_pipeline(image_path, base_params):
    """Process single still for diffuse scattering pipeline"""
    # Initialize processor
    processor = Processor(base_params, composite_tag=f"still_{image_id}")
    
    # Import and process
    experiments = do_import(image_path)
    processor.process_experiments(tag=f"image_{image_id}", experiments=experiments)
    
    # Extract pipeline outputs
    if len(processor.all_integrated_experiments) > 0:
        # Experiment_dials_i: Per-still crystal model
        crystal_model_i = processor.all_integrated_experiments[0]
        
        # Reflections_dials_i: Per-still reflections with P_spot  
        reflections_i = processor.all_integrated_reflections.select(
            processor.all_integrated_reflections["id"] == 0
        )
        
        # Extract P_spot partiality data
        partiality_i = reflections_i["partiality"] if "partiality" in reflections_i else None
        
        # BraggMask_2D_raw_i: From shoebox mask projection
        shoeboxes_i = reflections_i["shoebox"] if "shoebox" in reflections_i else None
        
        return crystal_model_i, reflections_i, partiality_i, shoeboxes_i
    else:
        return None, None, None, None
```

**9. See Also:**
- Section B.3: Crystal model access for per-still crystal parameters
- Section D.3: Reflection table operations for partiality data
- Section A.1: Loading experiment lists from stills_process outputs
- Section A.2: Loading reflection tables from stills_process outputs

---

## A.1. Loading Experiment Lists (`.expt` files)

**1. Purpose:**
Load DIALS experiment JSON files containing detector geometry, beam parameters, crystal orientation, and scan information for diffuse scattering pipeline modules that require geometric models and experimental metadata.

**2. Primary Python Call(s):**
```python
from dxtbx.model.experiment_list import ExperimentListFactory
from dxtbx.serialize import load

# Method 1: Using ExperimentListFactory (recommended)
experiments = ExperimentListFactory.from_json_file(
    filename="indexed_refined_detector.expt",
    check_format=True
)

# Method 2: Using generic load function
experiments = load.experiment_list("indexed_refined_detector.expt")

# Method 3: Direct from ExperimentList class
from dxtbx.model import ExperimentList
experiments = ExperimentList.from_file("indexed_refined_detector.expt")
```

**3. Key Arguments:**
- `filename` (str): Path to the `.expt` file
- `check_format` (bool, optional, default=True): Whether to perform format validation

**4. Return Type:**
- `dxtbx.model.experiment_list.ExperimentList` - A list-like container for `Experiment` objects with detector, beam, crystal, goniometer, scan, and imageset models.

**5. Example Usage Snippet:**
```python
experiments = ExperimentListFactory.from_json_file("models.expt")
if experiments:
    first_experiment = experiments[0]
    detector = first_experiment.detector
    beam = first_experiment.beam
    crystal = first_experiment.crystal
    print(f"Loaded {len(experiments)} experiments. First detector has {len(detector)} panels.")
```

**6. Notes/Caveats:**
- Requires DIALS environment to be properly sourced
- The file must contain valid JSON experiment data in DIALS format
- Multi-experiment files (e.g., from multi-lattice indexing) return multiple experiments; select appropriate one with `experiments[index]`
- Use error handling for robust file loading

**7. Error Handling Example:**
```python
from dials.util import Sorry

try:
    experiments = ExperimentListFactory.from_json_file(filename)
    if not experiments:
        raise Sorry(f"No experiments found in {filename}")
except (FileNotFoundError, IOError) as e:
    raise Sorry(f"Could not load experiments from {filename}: {e}")
except Exception as e:
    raise Sorry(f"Invalid experiment file format: {e}")
```

**8. See Also:**
- Section B: Accessing dxtbx.model Objects for working with loaded experiments
- Section A.2: Loading reflection tables to pair with experiments

---

## A.2. Loading Reflection Tables (`.refl` files)

**1. Purpose:**
Load DIALS reflection table files containing spot positions, intensities, Miller indices, and other crystallographic data for diffuse scattering analysis and intensity processing.

**2. Primary Python Call(s):**
```python
from dials.array_family import flex
from dials.serialize import load

# Method 1: Direct from reflection_table class
reflections = flex.reflection_table.from_file("indexed_refined_detector.refl")

# Method 2: Using generic load function
reflections = load.reflections("indexed_refined_detector.refl")
```

**3. Key Arguments:**
- `filename` (str): Path to the `.refl` file

**4. Return Type:**
- `dials.array_family.flex.reflection_table` - A dictionary-like container with flex arrays for each column (miller_index, intensity, xyzcal.px, etc.)

**5. Example Usage Snippet:**
```python
reflections = flex.reflection_table.from_file("reflections.refl")
print(f"Loaded {len(reflections)} reflections")
print(f"Available columns: {list(reflections.keys())}")
miller_indices = reflections['miller_index']
intensities = reflections['intensity.sum.value']
```

**6. Notes/Caveats:**
- Returns empty table if file doesn't exist rather than throwing an error
- Column names depend on processing stage (e.g., indexed, integrated, scaled)
- Can also load `.json` reflection files, though `.refl` (pickle) format is preferred for performance
- Common flag selections: `reflections.get_flags(reflections.flags.indexed)` for indexed reflections

**7. Error Handling and Flag Selection Example:**
```python
from dials.util import Sorry

try:
    reflections = flex.reflection_table.from_file(filename)
    if len(reflections) == 0:
        raise Sorry(f"No reflections found in {filename}")
    
    # Select only indexed reflections
    indexed_sel = reflections.get_flags(reflections.flags.indexed)
    indexed_reflections = reflections.select(indexed_sel)
    print(f"Loaded {len(indexed_reflections)} indexed reflections")
    
except (FileNotFoundError, IOError) as e:
    raise Sorry(f"Could not load reflections from {filename}: {e}")
```

**8. See Also:**
- Section D.3: Accessing reflection table properties and operations
- Section A.1: Loading experiments to pair with reflections

---

## A.3. Loading Image Data via ImageSet

**1. Purpose:**
Create ImageSet objects from image file paths to access raw pixel data for diffuse scattering analysis, background subtraction, and pixel-level intensity extraction.

**2. Primary Python Call(s):**
```python
from dxtbx.imageset import ImageSetFactory
from dxtbx import load

# Method 1: Create from file list
imageset = ImageSetFactory.make_imageset(
    filenames=["image_001.cbf", "image_002.cbf"],
    format_kwargs=None
)

# Method 2: Access from experiment
imageset = experiment.imageset

# Method 3: Load single image format
format_instance = load("image_001.cbf")
```

**3. Key Arguments:**
- `filenames` (list of str): List of image file paths
- `format_kwargs` (dict, optional): Format-specific parameters

**4. Return Type:**
- `dxtbx.imageset.ImageSet` - Container providing access to raw and corrected image data

**5. Example Usage Snippet:**
```python
imageset = experiment.imageset
for i in range(len(imageset)):
    raw_data = imageset.get_raw_data(i)  # Returns tuple for multi-panel
    if isinstance(raw_data, tuple):
        for panel_id, panel_data in enumerate(raw_data):
            print(f"Frame {i}, Panel {panel_id}: {panel_data.all()}")
```

**6. Notes/Caveats:**
- Multi-panel detectors return data as tuple of flex arrays
- Image data is returned as `flex.int` or `flex.double` arrays
- Use `get_corrected_data()` for pedestal/gain-corrected data
- Modern DIALS uses `ImageSetFactory.new()` and `make_imageset()` methods
- Access exposure times via: `imageset.get_scan().get_exposure_times()[frame_index]` if scan available
- Get format object: `format_class = imageset.get_format_class()`

**7. Extended Usage Examples:**
```python
# Access exposure times and format information
imageset = experiment.imageset
if hasattr(imageset, 'get_scan') and imageset.get_scan():
    scan = imageset.get_scan()
    exposure_times = scan.get_exposure_times()
    print(f"Frame 0 exposure time: {exposure_times[0]:.3f} s")

# Get format-specific metadata
format_class = imageset.get_format_class()
format_instance = format_class.get_instance(imageset.get_path(0))
print(f"Format: {format_class.__name__}")

# Handle multi-panel data correctly
for frame_idx in range(len(imageset)):
    raw_data = imageset.get_raw_data(frame_idx)
    if isinstance(raw_data, tuple):
        # Multi-panel detector
        for panel_id, panel_data in enumerate(raw_data):
            print(f"Frame {frame_idx}, Panel {panel_id}: {panel_data.all()}")
    else:
        # Single panel detector
        print(f"Frame {frame_idx}: {raw_data.all()}")
```

**8. See Also:**
- Section B.1: Detector model for coordinate transformations
- Section A.4: Loading masks to apply to image data

---

## A.4. Loading Mask Files (`.pickle` files)

**1. Purpose:**
Load detector masks that define valid/invalid pixel regions for diffuse scattering analysis.

**2. Primary Python Call(s):**
```python
import pickle
from dials.array_family import flex

# Load mask file
with open("mask.pickle", "rb") as f:
    mask_data = pickle.load(f)
```

**3. Key Arguments:**
- Standard Python pickle.load() arguments

**4. Return Type:**
- Tuple of `dials.array_family.flex.bool` arrays, one per detector panel
- `True` indicates valid pixels, `False` indicates masked pixels

**5. Example Usage Snippet:**
```python
with open("mask.pickle", "rb") as f:
    mask_tuple = pickle.load(f)
    
if isinstance(mask_tuple, tuple):
    for panel_id, panel_mask in enumerate(mask_tuple):
        valid_pixels = flex.sum(panel_mask.as_1d())
        print(f"Panel {panel_id}: {valid_pixels} valid pixels")
```

**6. Notes/Caveats:**
- Mask structure depends on how it was created (single panel vs multi-panel)
- Some masks may be stored as single flex.bool arrays for single-panel detectors
- Handle both cases: `isinstance(mask_data, tuple)` for multi-panel, single array for single-panel

**7. Comprehensive Mask Handling Example:**
```python
import pickle
from dials.array_family import flex

try:
    with open("mask.pickle", "rb") as f:
        mask_data = pickle.load(f)
    
    if isinstance(mask_data, tuple):
        # Multi-panel detector mask
        print(f"Multi-panel mask with {len(mask_data)} panels")
        for panel_id, panel_mask in enumerate(mask_data):
            valid_pixels = flex.sum(panel_mask.as_1d())
            total_pixels = len(panel_mask)
            print(f"Panel {panel_id}: {valid_pixels}/{total_pixels} valid pixels")
    else:
        # Single panel detector mask
        if isinstance(mask_data, flex.bool):
            valid_pixels = flex.sum(mask_data.as_1d())
            print(f"Single panel: {valid_pixels} valid pixels")
        else:
            raise ValueError(f"Unexpected mask data type: {type(mask_data)}")
            
except (FileNotFoundError, IOError) as e:
    print(f"Could not load mask from file: {e}")
```

**8. See Also:**
- Section A.3: Loading image data to apply masks to
- Section B.1: Detector model for understanding panel geometry

---

## A.5. Shoebox Data and MaskCode (for Bragg Mask Generation)

**1. Purpose:**
Access and manipulate reflection shoebox data structures containing 3D pixel data and mask codes for generating 2D Bragg masks (BraggMask_2D_raw_i) from integrated reflections in diffuse scattering analysis.

**2. Primary Python Call(s):**
```python
from dials.array_family import flex
from dials.model.data import Shoebox
from dials.algorithms.shoebox import MaskCode

# Access shoeboxes from reflection table
reflections = flex.reflection_table.from_file("integrated.refl")
if "shoebox" in reflections:
    shoeboxes = reflections["shoebox"]
    
    for i, shoebox in enumerate(shoeboxes):
        # Shoebox properties
        data = shoebox.data           # flex.double - 3D pixel intensities
        mask = shoebox.mask           # flex.int - 3D mask codes
        bbox = shoebox.bbox           # (x1, x2, y1, y2, z1, z2) bounding box
        
        # Check mask consistency
        is_consistent = shoebox.is_consistent()
        
        # Count pixels by mask code
        n_foreground = shoebox.count_mask_values(MaskCode.Foreground)
        n_background = shoebox.count_mask_values(MaskCode.Background)
        n_valid = shoebox.count_mask_values(MaskCode.Valid)
```

**3. Key Shoebox Attributes and Methods:**

**Shoebox Object:**
- `data`: flex.double - 3D array of pixel intensities (z, y, x)
- `mask`: flex.int - 3D array of mask codes (same dimensions as data)
- `bbox`: tuple - Bounding box (x1, x2, y1, y2, z1, z2) in panel coordinates
- `panel`: int - Panel ID for multi-panel detectors
- `is_consistent()` → bool: Check data/mask size consistency
- `count_mask_values(mask_code)` → int: Count pixels with specific mask code
- `flatten()`: Project 3D shoebox to 2D (if available)

**MaskCode Enumeration:**
- `MaskCode.Valid`: Pixel is within trusted detector region
- `MaskCode.Foreground`: Pixel classified as signal/reflection
- `MaskCode.Background`: Pixel classified as background
- `MaskCode.Strong`: Pixel above strong threshold
- `MaskCode.BackgroundUsed`: Pixel used in background calculation
- `MaskCode.Overlapped`: Pixel overlaps with another reflection

**4. Return Types:**
- `data`: flex.double array with shape (nz, ny, nx)
- `mask`: flex.int array with same shape, containing bitwise mask codes
- `bbox`: tuple of 6 integers defining bounding box
- Count methods: integer pixel counts

**5. Example Usage Snippet:**
```python
from dials.array_family import flex
from dials.algorithms.shoebox import MaskCode

# Load reflections with shoebox data
reflections = flex.reflection_table.from_file("integrated.refl")

if "shoebox" in reflections:
    shoeboxes = reflections["shoebox"]
    print(f"Found {len(shoeboxes)} shoeboxes")
    
    # Analyze first few shoeboxes
    for i in range(min(5, len(shoeboxes))):
        shoebox = shoeboxes[i]
        
        # Basic shoebox information
        bbox = shoebox.bbox
        panel_id = shoebox.panel
        data_shape = shoebox.data.accessor().all()
        
        # Count pixels by classification
        n_valid = shoebox.count_mask_values(MaskCode.Valid)
        n_foreground = shoebox.count_mask_values(MaskCode.Foreground)
        n_background = shoebox.count_mask_values(MaskCode.Background)
        n_strong = shoebox.count_mask_values(MaskCode.Strong)
        
        print(f"Shoebox {i}:")
        print(f"  Panel: {panel_id}, BBox: {bbox}")
        print(f"  Shape: {data_shape} (z, y, x)")
        print(f"  Valid: {n_valid}, Foreground: {n_foreground}, Background: {n_background}")
        print(f"  Strong pixels: {n_strong}")
        print(f"  Consistent: {shoebox.is_consistent()}")
```

**6. Generating 2D Bragg Masks from 3D Shoeboxes:**
```python
def create_2d_bragg_mask_from_shoeboxes(reflections, detector, mask_type="foreground"):
    """
    Generate 2D Bragg mask for each detector panel from 3D shoebox data.
    
    Args:
        reflections: flex.reflection_table with shoebox data
        detector: dxtbx.model.Detector object
        mask_type: "foreground", "strong", or "all_signal"
    
    Returns:
        List of flex.bool arrays (one per panel) for 2D Bragg masks
    """
    from dials.algorithms.shoebox import MaskCode
    
    # Initialize 2D masks for each panel
    panel_masks = []
    for panel_id, panel in enumerate(detector):
        panel_shape = panel.get_image_size()  # (fast, slow)
        panel_mask = flex.bool(flex.grid(panel_shape[1], panel_shape[0]), False)
        panel_masks.append(panel_mask)
    
    # Process each shoebox
    if "shoebox" in reflections:
        shoeboxes = reflections["shoebox"]
        
        for shoebox in shoeboxes:
            panel_id = shoebox.panel
            bbox = shoebox.bbox  # (x1, x2, y1, y2, z1, z2)
            mask_3d = shoebox.mask
            
            # Select mask criteria based on type
            if mask_type == "foreground":
                target_mask = MaskCode.Foreground | MaskCode.Valid
            elif mask_type == "strong": 
                target_mask = MaskCode.Strong | MaskCode.Valid
            elif mask_type == "all_signal":
                target_mask = (MaskCode.Foreground | MaskCode.Strong) & MaskCode.Valid
            
            # Project 3D mask to 2D by OR-ing across z-slices
            mask_shape = mask_3d.accessor().all()  # (nz, ny, nx)
            
            for y in range(mask_shape[1]):  # slow direction
                for x in range(mask_shape[0]):  # fast direction
                    # Check if any z-slice has the target mask
                    has_signal = False
                    for z in range(mask_shape[2]):
                        mask_value = mask_3d[z, y, x]
                        if (mask_value & target_mask) == target_mask:
                            has_signal = True
                            break
                    
                    # Set 2D mask pixel
                    if has_signal:
                        panel_y = bbox[2] + y  # Global panel coordinates
                        panel_x = bbox[0] + x
                        if (0 <= panel_x < panel_masks[panel_id].accessor().all()[1] and
                            0 <= panel_y < panel_masks[panel_id].accessor().all()[0]):
                            panel_masks[panel_id][panel_y, panel_x] = True
    
    return panel_masks

# Usage example
bragg_masks = create_2d_bragg_mask_from_shoeboxes(
    reflections, detector, mask_type="foreground"
)

for panel_id, mask in enumerate(bragg_masks):
    n_bragg_pixels = flex.sum(mask.as_1d())
    total_pixels = len(mask)
    print(f"Panel {panel_id}: {n_bragg_pixels}/{total_pixels} Bragg pixels")
```

**7. Advanced Shoebox Analysis:**
```python
def analyze_shoebox_properties(reflections):
    """Detailed analysis of shoebox data for quality assessment"""
    if "shoebox" not in reflections:
        return
    
    shoeboxes = reflections["shoebox"]
    
    # Collect shoebox statistics
    volumes = []
    foreground_fractions = []
    signal_to_noise = []
    
    for i, shoebox in enumerate(shoeboxes):
        if not shoebox.is_consistent():
            continue
            
        # Volume analysis
        data_shape = shoebox.data.accessor().all()
        volume = data_shape[0] * data_shape[1] * data_shape[2]
        volumes.append(volume)
        
        # Mask analysis
        n_valid = shoebox.count_mask_values(MaskCode.Valid)
        n_foreground = shoebox.count_mask_values(MaskCode.Foreground)
        n_background = shoebox.count_mask_values(MaskCode.Background)
        
        if n_valid > 0:
            fg_fraction = n_foreground / n_valid
            foreground_fractions.append(fg_fraction)
        
        # Signal analysis (if intensity data available)
        if hasattr(reflections, 'select') and i < len(reflections):
            refl_subset = reflections.select(flex.size_t([i]))
            if "intensity.sum.value" in refl_subset and "intensity.sum.variance" in refl_subset:
                intensity = refl_subset["intensity.sum.value"][0]
                variance = refl_subset["intensity.sum.variance"][0]
                if variance > 0:
                    snr = intensity / (variance ** 0.5)
                    signal_to_noise.append(snr)
    
    # Print statistics
    if volumes:
        print(f"Shoebox volumes: {min(volumes)} - {max(volumes)} pixels")
        print(f"Mean volume: {sum(volumes)/len(volumes):.1f} pixels")
    
    if foreground_fractions:
        print(f"Foreground fraction: {min(foreground_fractions):.3f} - {max(foreground_fractions):.3f}")
        print(f"Mean foreground fraction: {sum(foreground_fractions)/len(foreground_fractions):.3f}")
    
    if signal_to_noise:
        print(f"Signal-to-noise: {min(signal_to_noise):.1f} - {max(signal_to_noise):.1f}")
        print(f"Mean S/N: {sum(signal_to_noise)/len(signal_to_noise):.1f}")

# Usage
analyze_shoebox_properties(reflections)
```

**8. Notes/Caveats:**
- **Memory Usage:** Shoeboxes can consume significant memory; use `integration.debug.delete_shoeboxes=True` for production
- **3D Structure:** Shoebox data is organized as (z, y, x) with z=frame, y=slow, x=fast detector coordinates
- **Mask Codes:** Use bitwise operations for combining mask criteria: `(mask & MaskCode.Foreground) != 0`
- **Bounding Box Coordinates:** bbox coordinates are in panel pixel units, not global detector coordinates
- **Panel Consistency:** Always check `shoebox.panel` when working with multi-panel detectors
- **Consistency Check:** Use `shoebox.is_consistent()` to verify data integrity before processing

**9. Integration with Diffuse Scattering Pipeline:**
This shoebox data provides the foundation for generating `BraggMask_2D_raw_i` in Module 2.1.D by projecting 3D reflection regions to 2D detector masks, enabling accurate separation of Bragg and diffuse scattering components.

**10. See Also:**
- Section A.0: dials.stills_process for generating shoebox data
- Section A.4: Loading mask files for comparison with generated Bragg masks
- Section D.3: Reflection table operations for accessing shoebox columns

---

## A.6. Python API for dials.generate_mask

**1. Purpose:**
Generate detector pixel masks programmatically using DIALS masking utilities, including resolution limits, geometric exclusions, and powder ring masking for diffuse scattering data preprocessing.

**2. Primary Python Call(s):**
```python
from dials.util.masking import generate_mask
from dials.command_line.generate_mask import Script as GenerateMaskScript
from libtbx.phil import parse
from dxtbx.model import ExperimentList

# Method 1: Using generate_mask utility function directly
def create_detector_mask(experiments, phil_params):
    """Generate mask using DIALS masking utilities"""
    mask = generate_mask(experiments, phil_params)
    return mask

# Method 2: Using generate_mask command-line script programmatically  
def create_mask_from_script(experiments, mask_params):
    """Use dials.generate_mask script logic programmatically"""
    script = GenerateMaskScript()
    
    # Combine experiments with mask parameters
    mask_data = script.generate_mask_from_experiments(
        experiments, mask_params
    )
    return mask_data
```

**3. Key Functions and Parameters:**

**generate_mask Function:**
- `generate_mask(experiments, params)` → tuple of flex.bool arrays
  - `experiments`: ExperimentList object with detector/beam models
  - `params`: PHIL parameter object with masking criteria
  - Returns: tuple of flex.bool masks (one per detector panel)

**Key PHIL Parameters for Masking:**
```python
mask_phil_str = """
border = 0
  .type = int
  .help = "Number of pixels to mask around detector edge"

d_min = None
  .type = float  
  .help = "Minimum resolution limit (Angstroms)"

d_max = None
  .type = float
  .help = "Maximum resolution limit (Angstroms)"

resolution_range = None
  .type = floats(size=2)
  .help = "Resolution range to mask [d_min, d_max]"
  .multiple = True

untrusted {
  circle = None
    .type = floats(size=3)
    .help = "Circular exclusion: x_center y_center radius"
    .multiple = True
  
  rectangle = None
    .type = floats(size=4) 
    .help = "Rectangular exclusion: x1 y1 x2 y2"
    .multiple = True
    
  polygon = None
    .type = floats
    .help = "Polygon vertices: x1 y1 x2 y2 ..."
    .multiple = True
    
  pixel = None
    .type = ints(size=2)
    .help = "Individual pixel: x y"
    .multiple = True
}

ice_rings {
  filter = False
    .type = bool
    .help = "Apply ice ring masking"
    
  d_min = 10.0
    .type = float
    .help = "Minimum d-spacing for ice ring detection"
}
"""
```

**4. Return Type:**
- Tuple of `flex.bool` arrays, one per detector panel
- `True` indicates valid pixels, `False` indicates masked pixels
- Compatible with DIALS masking conventions

**5. Example Usage Snippet:**
```python
from dials.util.masking import generate_mask
from dxtbx.model.experiment_list import ExperimentListFactory
from libtbx.phil import parse

# Load experiments
experiments = ExperimentListFactory.from_json_file("experiments.expt")

# Define masking parameters
mask_phil = """
border = 5
d_min = 1.5
d_max = 50.0
resolution_range = 2.0 3.0
resolution_range = 4.0 5.0
untrusted {
  circle = 1000 1000 50
  circle = 500 500 30
  rectangle = 0 0 100 100
}
ice_rings {
  filter = True
  d_min = 8.0  
}
"""

# Parse parameters
mask_params = parse(mask_phil).extract()

# Generate mask
detector_mask = generate_mask(experiments, mask_params)

# Apply mask to image data
if isinstance(detector_mask, tuple):
    for panel_id, panel_mask in enumerate(detector_mask):
        n_masked = flex.sum((~panel_mask).as_1d())
        n_total = len(panel_mask)
        print(f"Panel {panel_id}: {n_masked}/{n_total} pixels masked")
```

**6. Diffuse Scattering Mask Strategy:**
```python
def create_diffuse_scattering_mask(experiments, bragg_masks=None):
    """
    Create comprehensive mask for diffuse scattering analysis
    combining resolution limits, geometric exclusions, and Bragg masking
    
    Args:
        experiments: ExperimentList with detector/beam models
        bragg_masks: Optional tuple of flex.bool masks for Bragg reflection exclusion
    
    Returns:
        Tuple of flex.bool masks optimized for diffuse scattering analysis
    """
    from dials.util.masking import generate_mask
    from libtbx.phil import parse
    
    # Define comprehensive masking strategy for diffuse scattering
    phil_string = """
    # Edge masking - exclude detector borders
    border = 10
    
    # Resolution limits for diffuse scattering range
    d_min = 1.0    # High resolution cutoff (avoid noise)
    d_max = 100.0  # Low resolution cutoff (avoid beamstop shadow)
    
    # Exclude problematic resolution ranges
    resolution_range = 3.9 3.7   # Ice ring ~3.8 Å
    resolution_range = 2.7 2.6   # Ice ring ~2.65 Å
    resolution_range = 2.25 2.20 # Ice ring ~2.22 Å
    
    # Geometric exclusions for common artifacts
    untrusted {
      circle = 1024 1024 80     # Beamstop shadow (adjust coordinates)
      rectangle = 1020 0 1030 2048  # Module gap (adjust for detector)
      rectangle = 0 1020 2048 1030  # Module gap (adjust for detector)
    }
    
    # Automatic ice ring detection and masking
    ice_rings {
      filter = True
      d_min = 10.0
    }
    """
    
    params = parse(phil_string).extract()
    detector_mask = generate_mask(experiments, params)
    
    # Combine with Bragg masks if provided
    if bragg_masks is not None:
        combined_masks = []
        for panel_id, (detector_panel_mask, bragg_panel_mask) in enumerate(zip(detector_mask, bragg_masks)):
            # Exclude pixels that are geometrically excluded OR contain Bragg reflections
            combined_mask = detector_panel_mask & (~bragg_panel_mask)
            combined_masks.append(combined_mask)
        return tuple(combined_masks)
    
    return detector_mask

# Example usage in diffuse scattering pipeline
detector_mask = create_diffuse_scattering_mask(experiments)
print(f"Created base diffuse scattering mask for {len(detector_mask)} panels")

# Optional: combine with Bragg masks from Section A.5
if have_bragg_reflections:
    bragg_masks = create_2d_bragg_mask_from_shoeboxes(reflections, experiments[0].detector)
    combined_mask = create_diffuse_scattering_mask(experiments, bragg_masks)
    print("Combined detector mask with Bragg exclusions")
```

**7. Advanced Custom Masking Functions:**
```python
def create_custom_resolution_mask(experiments, d_min=None, d_max=None, exclude_ranges=None):
    """
    Create resolution-based mask with custom d-spacing exclusions
    
    Args:
        experiments: ExperimentList
        d_min: Minimum d-spacing (high resolution limit)
        d_max: Maximum d-spacing (low resolution limit)  
        exclude_ranges: List of (d_min, d_max) tuples to exclude
    """
    detector = experiments[0].detector
    beam = experiments[0].beam
    wavelength = beam.get_wavelength()
    
    panel_masks = []
    
    for panel_id, panel in enumerate(detector):
        panel_shape = panel.get_image_size()  # (fast, slow)
        panel_mask = flex.bool(flex.grid(panel_shape[1], panel_shape[0]), True)
        
        # Check each pixel's resolution
        for slow in range(panel_shape[1]):
            for fast in range(panel_shape[0]):
                # Calculate resolution for this pixel
                lab_coord = panel.get_pixel_lab_coord((fast, slow))
                distance = (lab_coord[0]**2 + lab_coord[1]**2 + lab_coord[2]**2)**0.5
                
                # Scattering angle
                two_theta = math.atan2(
                    (lab_coord[0]**2 + lab_coord[1]**2)**0.5, 
                    lab_coord[2]
                )
                
                # d-spacing calculation: d = λ / (2 * sin(θ))
                if two_theta > 0:
                    d_spacing = wavelength / (2 * math.sin(two_theta / 2))
                else:
                    d_spacing = float('inf')
                
                # Apply resolution limits
                mask_pixel = True
                
                if d_min is not None and d_spacing < d_min:
                    mask_pixel = False
                if d_max is not None and d_spacing > d_max:
                    mask_pixel = False
                    
                # Apply exclusion ranges
                if exclude_ranges:
                    for range_min, range_max in exclude_ranges:
                        if range_min <= d_spacing <= range_max:
                            mask_pixel = False
                            break
                
                panel_mask[slow, fast] = mask_pixel
        
        panel_masks.append(panel_mask)
    
    return tuple(panel_masks)

# Usage example
resolution_mask = create_custom_resolution_mask(
    experiments, 
    d_min=1.2, 
    d_max=20.0,
    exclude_ranges=[(3.9, 3.7), (2.7, 2.6), (2.25, 2.20)]  # Ice rings
)
```

**8. Mask Persistence and Reuse:**
```python
import pickle

def save_detector_mask(mask_tuple, filename):
    """Save detector mask to pickle file"""
    with open(filename, 'wb') as f:
        pickle.dump(mask_tuple, f, protocol=pickle.HIGHEST_PROTOCOL)
    print(f"Saved mask to {filename}")

def load_detector_mask(filename):
    """Load detector mask from pickle file"""
    with open(filename, 'rb') as f:
        mask_tuple = pickle.load(f)
    return mask_tuple

# Usage
save_detector_mask(detector_mask, "diffuse_scattering_mask.pickle")
loaded_mask = load_detector_mask("diffuse_scattering_mask.pickle")
```

**9. Notes/Caveats:**
- **Coordinate Systems:** Masking coordinates are in panel pixel coordinates (fast, slow)
- **Resolution Calculations:** Require accurate detector geometry and beam models
- **Ice Ring Detection:** Built-in ice ring positions may need adjustment for specific wavelengths
- **Memory Efficiency:** Large detector masks can consume significant memory; save/load as needed
- **Panel Indexing:** Always verify panel indexing consistency across mask operations
- **Boolean Convention:** DIALS uses `True` for valid pixels, `False` for masked pixels
- **Strategy Recommendation:** For diffuse scattering, use the comprehensive masking approach in example 6 rather than individual custom functions

**10. Integration with Pipeline:**
Generated masks are essential for Module 2.1.D geometric corrections and Module 2.2.D diffuse scattering extraction, providing precise control over which detector regions are included in analysis.

**11. See Also:**
- Section A.5: Shoebox data for Bragg mask generation to combine with detector masks
- Section B.1: Detector model for coordinate transformations in masking
- Section C.1: Q-vector calculations for resolution-based masking
- Section A.4: Loading existing mask files
</file>

<file path="libdocs/dials/dials_programs.md">
# DIALS Programs Documentation

This document contains compiled documentation for all major DIALS (Diffraction Integration for Advanced Light Sources) programs.

## Table of Contents

1. [dials.import](#dialsimport)
2. [dials.find_spots](#dialsfind_spots)
3. [dials.index](#dialsindex)
4. [dials.refine_bravais_settings](#dialsrefine_bravais_settings)
5. [dials.reindex](#dialsreindex)
6. [dials.refine](#dialsrefine)
7. [dials.integrate](#dialsintegrate)
8. [dials.two_theta_refine](#dialstwo_theta_refine)
9. [dials.cosym](#dialscosym)
10. [dials.symmetry](#dialssymmetry)
11. [dials.scale](#dialsscale)
12. [dials.export](#dialsexport)
13. [xia2.multiplex](#xia2multiplex)
14. [dials.show](#dialsshow)
15. [dials.image_viewer](#dialsimage_viewer)
16. [dials.generate_mask](#dialsgenerate_mask)
17. [dials.check_indexing_symmetry](#dialscheck_indexing_symmetry)
18. [dials.search_beam_position](#dialssearch_beam_position)
19. [dials.report](#dialsreport)
20. [dials.plot_scan_varying_model](#dialsplot_scan_varying_model)
21. [dials.find_spots_server](#dialsfind_spots_server)
22. [dials.apply_mask](#dialsapply_mask)
23. [dials.create_profile_model](#dialscreate_profile_model)
24. [dials.estimate_gain](#dialsestimate_gain)
25. [dials.estimate_resolution](#dialsestimate_resolution)
26. [dials.predict](#dialspredict)
27. [dials.merge_cbf](#dialsmerge_cbf)
28. [dials.export_bitmaps](#dialsexport_bitmaps)
29. [dials.slice_sequence](#dialsslice_sequence)
30. [dials.compare_orientation_matrices](#dialscompare_orientation_matrices)
31. [dials.spot_counts_per_image](#dialsspot_counts_per_image)
32. [dials.stereographic_projection](#dialsstereographic_projection)
33. [dials.combine_experiments](#dialscombine_experiments)
34. [dials.align_crystal](#dialsalign_crystal)
35. [dials.anvil_correction](#dialsanvil_correction)
36. [dials.missing_reflections](#dialsmissing_reflections)
37. [dials.filter_reflections](#dialsfilter_reflections)
38. [dials.import_xds](#dialsimport_xds)

---

## dials.import

### Introduction

The `dials.import` program is used to import image data files into a format compatible with DIALS. Key features include:

- Analyzing metadata and filenames to determine image set relationships
- Creating an experiments object specifying file relationships
- Supporting multiple input methods:
  - Command line arguments
  - Stdin input
  - File template specification

### Basic Usage Examples

```
dials.import /data/directory-containing-images/
dials.import image_*.cbf
dials.import template=image_1_####.cbf
find . -name "image_*.cbf" | dials.import
```

### Key Parameters

#### Output Parameters
- `experiments`: Output experiment file (default: `imported.expt`)
- `log`: Log filename (default: `dials.import.log`)
- `compact`: Use compact JSON representation (default: `False`)

#### Input Parameters
- `ignore_unhandled`: Ignore unhandled input files (default: `True`)
- `template`: Image sequence template
- `directory`: Directory containing images
- `reference_geometry`: Override geometry from reference experiment file

#### Geometry Overrides
Allows manual setting of:
- Beam properties
- Detector characteristics
- Goniometer settings
- Scan parameters

### Advanced Features
- Support for multi-panel detectors
- Geometry reference override options
- Beam centre adjustment
- Scan sequence manipulation

---

## dials.find_spots

### Introduction

The `dials.find_spots` program is designed to identify strong spots on a sequence of images. Key features include:

- Can process images via "models.expt" file or direct image files
- Performs spot finding on logically grouped image sets
- Identifies strong pixels and forms spots using connected component labelling
- Calculates spot centroids and intensities
- Filters spots based on user preferences
- Outputs a "strong.refl" file for use in subsequent indexing

### Basic Usage Examples

```
dials.find_spots image1.cbf
dials.find_spots imager_00*.cbf
dials.find_spots models.expt
dials.find_spots models.expt output.reflections=strong.refl
```

### Key Parameters

#### Output Options
- `reflections`: Output filename (default: 'strong.refl')
- `shoeboxes`: Save raw pixel values (default: True)
- `log`: Log filename (default: 'dials.find_spots.log')

#### Spotfinder Configuration
- Supports various threshold algorithms (dispersion, radial profile)
- Configurable filtering options:
  - Spot size limits
  - Resolution range
  - Border settings
  - Untrusted region masking

#### Threshold Algorithms
1. Dispersion
2. Dispersion Extended
3. Radial Profile

### Detailed Configuration

Users can extensively customize spot finding through parameters like:
- Scan ranges
- Region of interest
- Background computation
- Spot filtering criteria
- Multiprocessing settings

For full parameter details, refer to the comprehensive configuration section in the documentation.

---

## dials.index

### Overview

The `dials.index` program is a critical component of the DIALS crystallography software suite designed to perform autoindexing on strong spots detected in diffraction data.

### Key Features

- Attempts to index strong spots output by `dials.find_spots`
- Uses input files: "imported.expt" and "strong.refl"
- Provides multiple indexing methods:
  - One-dimensional FFT
  - Three-dimensional FFT
  - Real space grid search
  - Other specialized methods

### Basic Usage

```bash
dials.index imported.expt strong.refl
```

Optional parameters include:
- Specifying unit cell
- Specifying space group
- Choosing indexing method (e.g., `indexing.method=fft1d`)

### Indexing Process

1. Searches for a primitive lattice
2. Refines crystal orientation and experimental geometry
3. Minimizes differences between observed and predicted spot centroids
4. Outputs:
   - "indexed.expt" file with crystal model
   - "indexed.refl" file with Miller indices and predicted centroids

### Advanced Configuration

The program offers extensive configuration options for:
- Indexing methods
- Refinement protocols
- Reflection management
- Outlier rejection
- Experimental geometry parameterization

### Recommended Use

Ideal for processing crystallographic diffraction data, particularly when dealing with:
- Rotation data
- Still image collections
- Complex lattice scenarios

---

## dials.refine_bravais_settings

### Overview
The `dials.refine_bravais_settings` program is a tool for refining Bravais settings consistent with a primitive unit cell. It takes indexed experimental data and performs full refinement of crystal and experimental geometry parameters across potential Bravais settings.

### Key Features
- Refinement of crystal and experimental geometry parameters
- Generation of `.expt` files for each Bravais setting
- Detailed output including:
  - Metric fit
  - Root-mean-square deviations (RMSD)
  - Refined unit cell parameters
  - Change of basis operators

### Basic Usage
```
dials.refine_bravais_settings indexed.expt indexed.refl
dials.refine_bravais_settings indexed.expt indexed.refl nproc=4
```

### Main Parameters
- `lepage_max_delta`: Default 5
- `nproc`: Number of processors (default Auto)
- `best_monoclinic_beta`: Prefer less oblique monoclinic cells (default True)

### Detailed Refinement Options
The program offers extensive configuration for:
- Beam parameters
- Crystal parameters
- Detector settings
- Goniometer configuration
- Reflection management
- Outlier rejection strategies

### Output
- Generates Bravais setting `.expt` files
- Produces a log file with refinement details
- Provides a table of potential Bravais settings

---

## dials.reindex

### Introduction

The `dials.reindex` program allows users to re-index experimental and reflection files from one setting to another. Key features include:

- Change of basis operator in multiple conventions (h,k,l, a,b,c, x,y,z)
- Optional space group modification
- Ability to reindex datasets using a reference dataset

### Basic Usage Examples

```
dials.reindex indexed.expt change_of_basis_op=b+c,a+c,a+b
dials.reindex indexed.refl change_of_basis_op=-b,a+b+2*c,-a
dials.reindex indexed.expt indexed.refl change_of_basis_op=l,h,k
```

### Key Parameters

- `change_of_basis_op`: Specifies the change of basis operator
- `hkl_offset`: Optional integer offset
- `space_group`: Space group to apply after change of basis
- `reference`: Optional reference experiments/reflections for reindexing

### Output Options

- `experiments`: Reindexed experimental models (default: `reindexed.expt`)
- `reflections`: Reindexed reflections (default: `reindexed.refl`)
- `log`: Logging file (default: `dials.reindex.log`)

---

## dials.refine

### Overview

`dials.refine` is a tool for refining the diffraction geometry of experimental models against indexed reflections. Key features include:

#### Basic Functionality
- Refines experimental models for rotation scan data
- Supports static or scan-varying model parameterization
- Allows fixing specific model parameters

#### Key Parameters
- Input: Indexed experiments and reflections
- Output options for experiments, reflections, and logs
- Configurable refinement strategies

#### Refinement Modes
1. Static refinement (same parameters for all reflections)
2. Scan-varying refinement (parameters dependent on image number)

#### Example Usage
```
dials.refine indexed.expt indexed.refl
dials.refine indexed.expt indexed.refl scan_varying=(False/True/Auto)
```

#### Configurable Components
- Beam parameters
- Crystal parameters
- Detector parameters
- Goniometer settings

#### Advanced Features
- Outlier rejection algorithms
- Weighting strategies
- Multiprocessing support
- Detailed logging and tracking options

The documentation provides extensive configuration possibilities for precise geometric refinement in crystallographic data processing.

---

## dials.integrate

### Overview
`dials.integrate` is a program used to integrate reflections on diffraction images. It processes experiment lists and strong spots to generate integrated reflections and experiment data.

### Basic Usage Examples
```
dials.integrate models.expt refined.refl
dials.integrate models.expt refined.refl output.reflections=integrated.refl
dials.integrate models.expt refined.refl profile.fitting=False
```

### Key Features
- Integrates reflections from indexed and refined experiment data
- Creates profile models
- Supports multiple integration and background algorithms
- Configurable output options

### Main Parameters
- `output`: Controls output file names and formats
- `integration`: Configures integration process
  - Background algorithms
  - Profile fitting
  - Multiprocessing options
- `profile`: Defines profile modeling approach
- `prediction`: Sets resolution and prediction parameters

### Profile Modeling Algorithms
1. Ellipsoid
2. Gaussian Reciprocal Space (default)

### Background Subtraction Options
- Auto (default)
- GLM
- Global model
- Null
- Simple

### Multiprocessing Methods
- Multiprocessing (default)
- DRMAA
- SGE
- LSF
- PBS

The documentation provides extensive configuration options for customizing the integration process across different experimental setups.

---

## dials.two_theta_refine

### Introduction

A DIALS tool to "Refine the unit cell(s) of input experiments against the input indexed reflections using a 2θ angle target."

### Basic Usage

```
dials.two_theta_refine integrated.expt integrated.refl
```

Optional additional parameters:
```
dials.two_theta_refine integrated.expt integrated.refl \
    correlation_plot.filename=corrplot.png cif=refined_cell.cif
```

### Key Parameters

#### Output Options
- `experiments`: Filename for refined experimental models
- `log`: Log file name
- `cif`: Optional Crystallographic Information File output
- `correlation_plot`: Optional parameter correlation visualization

#### Refinement Options
- `filter_integrated_centroids`: Filter centroids (default: True)
- `partiality_threshold`: Minimum reflection partiality (default: 0.4)
- `combine_crystal_models`: Combine multiple experiment models (default: True)
- `triclinic`: Remove symmetry constraints (default: False)

### Typical Workflow

1. Provide indexed experimental and reflection files
2. Optionally specify output and refinement parameters
3. Generate refined unit cell parameters

---

## dials.cosym

### Overview
`dials.cosym` is a program for determining Patterson group symmetry from multi-crystal datasets, implementing methods from "Gildea, R. J. & Winter, G. (2018)".

### Key Features
- Analyzes symmetry elements in multiple crystal datasets
- Handles indexing ambiguities
- Performs unit cell clustering
- Normalizes intensities
- Determines resolution limits
- Identifies symmetry operations and Patterson groups

### Basic Usage
```
dials.cosym models.expt observations.refl
```

### Main Analysis Steps
1. Unit cell metric symmetry analysis
2. Hierarchical unit cell clustering
3. Intensity normalization
4. Resolution limit determination
5. Symmetry element scoring
6. Laue group identification

### Key Parameters
- `partiality_threshold`: Reflection inclusion threshold (default 0.4)
- `min_reflections`: Minimum merged reflections per experiment (default 10)
- `d_min`: High-resolution cutoff
- `min_cc_half`: Minimum correlation coefficient threshold (default 0.6)

### Output
- Reindexed experiments and reflections
- HTML and JSON reports
- Log file with detailed analysis

### Typical Workflow
The program automatically:
- Clusters unit cells
- Normalizes intensities
- Determines optimal analysis dimensions
- Scores symmetry elements
- Identifies best Patterson group

---

## dials.symmetry

### Introduction

The `dials.symmetry` program implements symmetry determination methods from POINTLESS, referencing two key publications:
- Evans, P. (2006). Acta Cryst. D62, 72-82
- Evans, P. R. (2011). Acta Cryst. D67, 282-292

### Basic Usage

```
dials.symmetry models.expt observations.refl
```

### Key Parameters

#### Basic Configuration
- `d_min`: Resolution limit (default: Auto)
- `min_i_mean_over_sigma_mean`: 4
- `min_cc_half`: 0.6
- `normalisation`: Options include kernel quasi, ml_iso, ml_aniso
- `lattice_group`: None by default
- `laue_group`: Auto detection

#### Systematic Absences
- Configurable checking method
- Direct or Fourier analysis
- Significance level: 0.95

### Output Files
- Log file: `dials.symmetry.log`
- Experiments: `symmetrized.expt`
- Reflections: `symmetrized.refl`
- JSON report: `dials.symmetry.json`
- HTML report: `dials.symmetry.html`

### Advanced Options
- Image exclusion
- Partiality threshold
- Lattice symmetry tolerances
- Monoclinic cell optimization

---

## dials.scale

### Overview

`dials.scale` is a program for scaling integrated X-ray diffraction datasets, designed to improve the internal consistency of reflection intensities by correcting for various experimental effects.

### Key Features

- Performs scaling on integrated datasets
- Corrects for experimental effects like scale, decay, and absorption
- Supports multiple datasets scaled against a common target
- Outputs scaled reflection and experiment files
- Generates an HTML report with interactive plots

### Basic Usage Examples

```bash
# Regular single-sequence scaling without absorption correction
dials.scale integrated.refl integrated.expt physical.absorption_correction=False

# Scaling multiple datasets with resolution limit
dials.scale 1_integrated.refl 1_integrated.expt 2_integrated.refl 2_integrated.expt d_min=1.4

# Incremental scaling with different options
dials.scale integrated.refl integrated.expt physical.scale_interval=10.0
dials.scale integrated_2.refl integrated_2.expt scaled.refl scaled.expt physical.scale_interval=15.0
```

### Scaling Models

The program supports several scaling models:
- KB (default)
- Array
- Dose decay
- Physical

### Key Parameters

- `model`: Choose scaling model (KB, array, dose_decay, physical)
- `output`: Configure output files
- `reflection_selection`: Control reflection subset selection
- `weighting`: Define error model and weighting scheme
- `scaling_options`: Set refinement and outlier rejection parameters

### Additional Resources

More detailed documentation is available in the [DIALS scale user guide](https://dials.github.io/dials_scale_user_guide.html).

---

## dials.export

### Overview
`dials.export` is a program used to export crystallographic processing results in various file formats.

### Supported Output Formats
- MTZ: Unmerged MTZ file for downstream processing
- NXS: NXmx file
- MMCIF: mmcif file
- XDS_ASCII: Intensity data in XDS CORRECT step format
- SADABS: Intensity data in ersatz-SADABS format
- MOSFLM: Matrix and instruction files
- XDS: XDS.INP and XPARM.XDS files
- SHELX: Intensity data in HKLF 4 format
- PETS: CIF format for dynamic diffraction refinement
- JSON: Reciprocal lattice point data

### Basic Usage Examples
```
# Export to MTZ
dials.export integrated.expt integrated.refl
dials.export scaled.expt scaled.refl intensity=scale

# Export to Nexus
dials.export integrated.expt integrated.refl format=nxs

# Export to XDS
dials.export indexed.expt indexed.refl format=xds
```

### Key Parameters
- `format`: Choose output file format (default: MTZ)
- `intensity`: Select intensity type (default: auto)
- `debug`: Output additional debugging information

### Detailed Configuration
Each format has specific configuration options, such as:
- MTZ: Partiality thresholds, resolution cutoffs
- MMCIF: Compression, PDB version
- SHELX: Composition, scaling
- PETS: Virtual frame settings

### Notes
Supports exporting both integrated and scaled crystallographic data with flexible configuration options.

---

## xia2.multiplex

### Overview

xia2.multiplex is a DIALS program for processing multi-crystal X-ray diffraction datasets. It performs several key functions:

#### Key Features
- Symmetry analysis
- Scaling and merging of multi-crystal datasets
- Analysis of dataset pathologies like:
  - Non-isomorphism
  - Radiation damage
  - Preferred orientation

#### Internal Programs Used
- dials.cosym
- dials.two_theta_refine
- dials.scale
- dials.symmetry
- dials.estimate_resolution

### Example Usage

Basic command syntax:
```
xia2.multiplex integrated.expt integrated.refl
```

Advanced examples:
```
# Multiple input files
xia2.multiplex integrated_1.expt integrated_1.refl \
  integrated_2.expt integrated_2.refl

# Override space group and resolution
xia2.multiplex space_group=C2 resolution.d_min=2.5 \
  integrated_1.expt integrated_1.refl

# Filter datasets using ΔCC½ method
xia2.multiplex filtering.method=deltacchalf \
  integrated.expt integrated.refl
```

### Citation

For academic use, cite: "Gildea, R. J. et al. (2022) Acta Cryst. D78, 752-769"

### Key Parameters

The documentation provides extensive configuration options for:
- Unit cell clustering
- Scaling
- Symmetry determination
- Resolution limits
- Filtering
- Multi-crystal analysis

### Detailed Documentation

The full documentation includes comprehensive parameter definitions with help text, types, and expert-level settings for advanced users.

---

## dials.show

### Overview
`dials.show` is a command-line tool for displaying information about experimental data, reflections, and images in crystallography.

### Basic Usage Examples
- `dials.show models.expt`
- `dials.show image_*.cbf`
- `dials.show observations.refl`

### Parameters

#### Display Options
- `show_scan_varying` (default: False)
  - "Whether or not to show the crystal at each scan point."

- `show_shared_models` (default: False)
  - "Show which models are linked to which experiments"

- `show_all_reflection_data` (default: False)
  - "Whether or not to print individual reflections"

- Additional toggles:
  - `show_intensities`
  - `show_centroids`
  - `show_profile_fit`
  - `show_flags`
  - `show_identifiers`

#### Image Statistics
```
image_statistics {
  show_corrected = False  # "Show statistics on the distribution of values in each corrected image"
  show_raw = False        # "Show statistics on the distribution of values in each raw image"
}
```

#### Additional Parameter
- `max_reflections` (default: None)
  - "Limit the number of reflections in the output."

### Supported by
- Diamond Light Source
- CCP4
- STFC
- Lawrence Berkeley National Laboratory
- SSRL/SMB

---

## dials.image_viewer

### Overview
The `dials.image_viewer` is a program for viewing diffraction images with optional overlays of analysis results.

### Usage Examples
```
dials.image_viewer image.cbf
dials.image_viewer models.expt
dials.image_viewer models.expt strong.refl
dials.image_viewer models.expt integrated.refl
```

### Key Features
- View diffraction images
- Optional overlays:
  - Spot finding results
  - Indexing results
  - Integration results

### Basic Parameters
#### Display Options
- `brightness`: Image brightness level
- `color_scheme`: Options include grayscale, rainbow, heatmap, invert
- `projection`: Lab or image view
- Toggleable display elements:
  - Beam center
  - Resolution rings
  - Ice rings
  - Center of mass
  - Maximum pixels
  - Predictions
  - Miller indices
  - Indexed/integrated reflections

#### Image Processing
- Background calculation
- Thresholding
- Masking
- Powder arc analysis
- Calibration options

### Advanced Features
- Profile modeling
- Reflection prediction
- Detailed masking controls
- Extensive configuration options for image analysis

### Recommended Use
Primarily used by crystallographers for detailed examination and analysis of diffraction image data.

---

## dials.generate_mask

### Introduction

The `dials.generate_mask` program is used to "mask images to remove unwanted pixels" during crystallography data processing. It allows users to:

- Create masks using detector trusted range
- Define masks with simple shapes
- Set resolution range filters
- Combine multiple mask files

### Basic Usage Examples

```
dials.generate_mask models.expt border=5

dials.generate_mask models.expt \
  untrusted.rectangle=50,100,50,100 \
  untrusted.circle=200,200,100

dials.generate_mask models.expt d_max=2.00

dials.generate_mask backstop.mask shadow.mask
```

### Key Parameters

- `output.mask`: Name of output mask file
- `border`: Border size around image edge
- `d_min`: High resolution limit (Angstrom)
- `d_max`: Low resolution limit (Angstrom)
- `untrusted`: Options to mask specific regions
  - Panels
  - Circles
  - Rectangles
  - Polygons
  - Individual pixels

### Additional Features

- Can generate masks based on resolution ranges
- Optional ice ring filtering
- Supports parallax correction toggle

---

## dials.check_indexing_symmetry

### Introduction

This DIALS program analyzes correlation coefficients between reflections related by symmetry operators in a space group. It can help detect potential misindexing of a diffraction pattern, possibly due to an incorrect beam centre.

### Example Usage

```
dials.check_indexing_symmetry indexed.expt indexed.refl \
  grid=1 symop_threshold=0.7

dials.check_indexing_symmetry indexed.expt indexed.refl \
  grid_l=3 symop_threshold=0.7
```

### Basic Parameters

- `d_min`: High resolution limit (default: 0)
- `d_max`: Low resolution limit (default: 0)
- `symop_threshold`: Threshold for symmetry operator confidence (default: 0)
- `grid`: Search scope for testing misindexing on h, k, l
- `asu`: Perform search comparing within ASU
- `normalise`: Normalize intensities before calculating correlation coefficients
- `reference`: Correctly indexed reference set for comparison

### Output

- Default log file: `dials.check_indexing_symmetry.log`

The program is part of the DIALS software suite, developed collaboratively by Diamond Light Source, Lawrence Berkeley National Laboratory, and STFC.

---

## dials.search_beam_position

### Overview

A DIALS function to find beam center from diffraction images. The default method is based on the work of Sauter et al. (J. Appl. Cryst. 37, 399-409 (2004)) and uses spot finding results.

### Basic Usage

#### Default Method
```
dials.search_beam_position imported.expt strong.refl
```

#### Projection Method
```
dials.search_beam_position method=midpoint imported.exp
```

### Key Methods

Available beam position search methods:
- `default`
- `midpoint`
- `maximum`
- `inversion`

### Main Parameters

#### Default Parameters
- `nproc`: Number of processors (default: Auto)
- `max_reflections`: Maximum reflections to use (default: 10,000)
- `mm_search_scope`: Global radius of origin offset search (default: 4.0)
- `n_macro_cycles`: Iterative beam centre search cycles (default: 1)

#### Projection Parameters
- `method_x`: Projection method for x-axis
- `method_y`: Projection method for y-axis
- Options for plotting and image processing

### Output

- Optimized experiment file: `optimised.expt`
- Log file: `dials.search_beam_position.log`
- JSON beam positions: `beam_positions.json`

More details available at: https://autoed.readthedocs.io/en/latest/pages/beam_position_methods.html

---

## dials.report

### Overview
`dials.report` is a DIALS program that "Generates a html report given the output of various DIALS programs (observations.refl and/or models.expt)".

### Example Usage
```
dials.report strong.refl
dials.report indexed.refl
dials.report refined.refl
dials.report integrated.refl
dials.report refined.expt
dials.report integrated.refl integrated.expt
```

### Basic Parameters
- `output.html`: Default filename is `dials.report.html`
- `output.json`: Optional JSON file for plot data
- `output.external_dependencies`: Options are remote, local, or embed
- `grid_size`: Defaults to Auto
- `pixels_per_bin`: Default is 40

### Advanced Parameters
- `centroid_diff_max`: Optional parameter for heatmap color mapping
- `orientation_decomposition`: Configures orientation matrix decomposition
  - Allows setting rotation axes
  - Option to decompose relative to static orientation

### Key Features
- Generates HTML reports from DIALS processing output
- Flexible input file types (.refl and .expt)
- Configurable visualization and reporting options

---

## dials.plot_scan_varying_model

### Overview
A DIALS tool to "Generate plots of scan-varying models, including crystal orientation, unit cell and beam centre"

### Basic Usage
```
dials.plot_scan_varying_model refined.expt
```

### Parameters

#### Output Options
- `directory`: Directory to store results (default: current directory)
- `format`: Output file format (png, pdf)
- `debug`: Print tables of plotted values (expert level)

#### Orientation Decomposition
- `e1`, `e2`, `e3`: Rotation axes (default: standard coordinate axes)
- `relative_to_static_orientation`: Whether rotations are relative to reference crystal model

### Key Features
- Visualizes changes in:
  - Crystal orientation
  - Unit cell
  - Beam centre

### Supported Outputs
- PNG images
- PDF files

### Notes
- Requires a refined experiment file (`refined.expt`) as input
- Provides detailed visualization of scan-varying crystallographic parameters

---

## dials.find_spots_server

### Overview
A client/server version of `dials.find_spots` designed for quick feedback on image quality during grid scans and data collections.

### Server Setup
```
dials.find_spots_server [nproc=8] [port=1234]
```

### Client Usage
```
dials.find_spots_client [host=hostname] [port=1234] [nproc=8] /path/to/image.cbf
```

### XML Response Example
```xml
<response>
<image>/path/to/image_0001.cbf</image>
<spot_count>352</spot_count>
<spot_count_no_ice>263</spot_count_no_ice>
<d_min>1.46</d_min>
<d_min_method_1>1.92</d_min_method_1>
<d_min_method_2>1.68</d_min_method_2>
<total_intensity>56215</total_intensity>
</response>
```

#### Response Fields
- `spot_count`: Total spots found
- `spot_count_no_ice`: Spots excluding ice ring regions
- `d_min_method_1/2`: Resolution estimates
- `total_intensity`: Total intensity of strong spots

### Additional Client Options
- Can pass standard `dials.find_spots` parameters
- Example: `dials.find_spots_client /path/to/image.cbf min_spot_size=2 d_min=2`

### Stopping the Server
```
dials.find_spots_client stop [host=hostname] [port=1234]
```

### Default Parameters
- `nproc`: Auto
- `port`: 1701

---

## dials.apply_mask

### Introduction

This program allows users to augment an experiments JSON file with masks. "Its only function is to input the mask file paths to the experiments JSON file"

### Key Requirements

- Mask files must be provided in the same order as their corresponding imagesets in the experiments JSON file

### Usage Examples

```
dials.apply_mask models.expt input.mask=pixels.mask
dials.apply_mask models.expt input.mask=pixels1.mask input.mask=pixels2.mask
```

### Parameters

#### Input
- `mask`: Mask filename(s)
  - Type: String
  - Multiple masks allowed
  - Default: None

#### Output
- `experiments`: Output experiments file
  - Default: `masked.expt`

### Full Parameter Definitions

```
input {
  mask = None
    .help = "The mask filenames, one mask per imageset"
    .type = str
    .multiple = True
}
output {
  experiments = masked.expt
    .help = "Name of output experiments file"
    .type = str
}
```

---

## dials.create_profile_model

### Overview

This DIALS program computes a profile model from input reflections and saves a modified experiments file with profile model information. It can be run independently of integration, though it's typically performed during that process.

### Basic Usage

```
dials.create_profile_model models.expt observations.refl
```

### Key Parameters

#### Profile Modeling Algorithms
- Two primary algorithms:
  1. Ellipsoid
  2. Gaussian RS (recommended)

#### Profile Configuration Options
- Scan-varying model
- Minimum spot requirements
- Mosaicity computation
- Centroid definition
- Filtering parameters

#### Example Configuration
```
profile {
  algorithm = gaussian_rs
  gaussian_rs {
    scan_varying = False
    min_spots {
      overall = 50
      per_degree = 20
    }
  }
}
```

### Important Settings
- Can subtract background before profile computation
- Configurable resolution limits
- Refinement parameters for profile model
- Grid methods for profile fitting

### Recommended Use
Typically used during crystal diffraction data processing to characterize reflection profiles and improve integration accuracy.

---

## dials.estimate_gain

### Introduction

This program estimates the detector's gain. For pixel array detectors, the gain is typically 1.00, representing Poisson statistics. For older CCD detectors, the gain may vary, which can impact spot finding algorithms.

### Example Usage

```
dials.estimate_gain models.expt
```

### Parameters

#### Basic Parameters
- `kernel_size`: Default is 10,10
- `max_images`: Default is 1
- `output.gain_map`: Default is None

#### Full Parameter Definitions

##### kernel_size
- Type: 2 integers
- Minimum value: 1

##### max_images
- Type: Integer (can be None)
- Help: "For multi-file images (NeXus for example), report a gain for each image, up to max_images, and then report an average gain"

##### output.gain_map
- Type: String
- Help: "Name of output gain map file"

### Key Points
- Useful for understanding detector pixel behavior
- Particularly important for older CCD detectors
- Can help improve spot finding accuracy by understanding noise characteristics

---

## dials.estimate_resolution

### Overview
A DIALS command-line tool for estimating resolution limits in crystallographic data processing based on various statistical metrics.

### Supported Resolution Metrics
- cc_half (default)
- isigma (unmerged <I/sigI>)
- misigma (merged <I/sigI>)
- i_mean_over_sigma_mean
- cc_ref
- completeness
- rmerge

### Basic Usage Examples
```
dials.estimate_resolution scaled.expt scaled.refl
dials.estimate_resolution scaled_unmerged.mtz
dials.estimate_resolution scaled.expt scaled.refl cc_half=0.1
```

### Key Features
- Estimates resolution by fitting curves to merging statistics in resolution bins
- Chooses resolution limit based on specified criteria
- Supports multiple resolution estimation methods
- Generates log, HTML, and optional JSON output

### Resolution Estimation Methods
Different metrics use specific fitting approaches:
- cc_half: Fits tanh function
- Other metrics: Polynomial fits to log-transformed data

### Parameters
Important configurable parameters include:
- `cc_half`: Minimum CC½ threshold (default 0.3)
- `reflections_per_bin`: Minimum reflections per resolution bin
- `nbins`: Maximum number of resolution bins
- `reference`: Optional reference dataset

### Output
Generates resolution estimates with optional visualization of curve fits.

---

## dials.predict

### Overview
`dials.predict` is a program that "takes a set of experiments and predicts the reflections" which are then saved to a file.

### Basic Usage Examples
```
dials.predict models.expt
dials.predict models.expt force_static=True
dials.predict models.expt d_min=2.0
```

### Key Parameters

#### Basic Parameters
- `output`: Filename for predicted reflections (default: `predicted.refl`)
- `force_static`: Force static prediction for scan-varying model (default: `False`)
- `ignore_shadows`: Ignore dynamic shadowing (default: `True`)
- `buffer_size`: Prediction buffer zone around scan images (default: `0`)
- `d_min`: Minimum d-spacing for predicted reflections (default: `None`)

#### Profile Modeling
Two primary algorithms:
1. Ellipsoid
2. Gaussian Reciprocal Space (default)

##### Gaussian RS Options
- `scan_varying`: Calculate scan-varying model (default: `False`)
- Minimum spot requirements
- Mosaicity computation algorithm
- Centroid definition
- Filtering and fitting parameters

### Detailed Configuration
The program offers extensive configuration options for reflection prediction, including:
- Mosaicity models
- Wavelength spread
- Unit cell and orientation refinement
- Indexing and prediction tolerances

### Recommended Use
Ideal for crystallography experiments requiring precise reflection prediction and modeling.

---

## dials.merge_cbf

### Introduction

The `dials.merge_cbf` program merges consecutive CBF image files into fewer images. For example:

- Merging 100 images with `merge_n_images=2` will produce 50 summed images
- Currently supports only CBF format images

### Usage Examples

```
dials.merge_cbf image_*.cbf
dials.merge_cbf image_*.cbf merge_n_images=10
```

### Basic Parameters

- `merge_n_images = 2`: Number of input images to average into a single output image
- `output.image_prefix = sum_`: Prefix for output image files

### Full Parameter Definitions

#### merge_n_images
- Type: Integer
- Minimum value: 1
- Allows None: Yes
- Help: "Number of input images to average into a single output image"

#### get_raw_data_from_imageset
- Type: Boolean
- Default: True
- Expert level: 2
- Help: "By default the raw data is read via the imageset. This limits use to single panel detectors"

### Output

Images will be prefixed with `sum_` by default.

---

## dials.export_bitmaps

### Overview
A DIALS utility to export raw diffraction image files as bitmap images, with options for:
- Exporting images from intermediate spot-finding steps
- Adjusting image appearance
- Controlling image output parameters

### Basic Usage Examples
```
dials.export_bitmaps image.cbf
dials.export_bitmaps models.expt
dials.export_bitmaps image.cbf display=variance colour_scheme=inverse_greyscale
```

### Key Parameters
- `binning`: Pixel binning (default: 1)
- `brightness`: Image brightness (default: 100)
- `colour_scheme`: Options include greyscale, rainbow, heatmap
- `display`: Image display mode (default: image)
- `output format`: PNG (default), JPEG, TIFF

### Advanced Features
- Resolution ring display
- Ice ring visualization
- Threshold and sigma-based image processing
- Customizable image compression/quality settings

### Supported Input Types
- Raw image files (.cbf)
- Experiment files (.expt)

### Typical Use Cases
- Visualizing diffraction images
- Analyzing spot-finding intermediate steps
- Generating diagnostic image outputs

---

## dials.slice_sequence

### Overview
A DIALS command-line tool for slicing experimental sequences and reflections within specified image ranges.

### Purpose
"Slice a sequence to produce a smaller sequence within the bounds of the original"

### Usage Examples
```
dials.slice_sequence models.expt observations.refl "image_range=1 20"
dials.slice_sequence models.expt "image_range=1 20"
dials.slice_sequence models.expt observations.refl "image_range=1 20" "image_range=5 30"
```

### Key Parameters
- `output.reflections_filename`: Output filename for sliced reflections
- `output.experiments_filename`: Output filename for sliced experiments
- `image_range`: Specify image ranges to slice
- `block_size`: Optional parameter to split sequences into equal blocks
- `exclude_images_multiple`: Advanced option for splitting scans at specific intervals

### Behavior
- Modifies scan objects in experiments
- Removes reflections outside specified image ranges
- Supports multiple experiments with different image ranges

### Advanced Features
Includes an expert-level option for handling interrupted scans, particularly useful for cRED (continuous rotation electron diffraction) data.

---

## dials.compare_orientation_matrices

### Overview
A DIALS tool that "Computes the change of basis operator that minimises the difference between two orientation matrices" and calculates related transformation details.

### Usage Examples
```
dials.compare_orientation_matrices models.expt
dials.compare_orientation_matrices models_1.expt models_2.expt
dials.compare_orientation_matrices models_1.expt models_2.expt hkl=1,0,0
```

### Parameters
- `hkl`: Miller indices for comparison (default: None)
  - Type: Integer array of size 3
  - Can specify multiple indices
- `comparison`: Comparison method
  - Default: Pairwise sequential
- `space_group`: Optional space group specification

### Key Features
- Calculates rotation matrix between orientation matrices
- Computes Euler angles
- Optional Miller index angle calculation

### Supported Inputs
- Experimental model files (.expt)
- Multiple model file comparisons

Note: This documentation is derived from the DIALS project documentation, with attribution to Diamond Light Source, Lawrence Berkeley National Laboratory, and STFC.

---

## dials.spot_counts_per_image

### Introduction

A DIALS tool that "Reports the number of strong spots and computes an estimate of the resolution limit for each image" after running dials.find_spots.

### Example Usage

```
dials.spot_counts_per_image imported.expt strong.refl
dials.spot_counts_per_image imported.expt strong.refl plot=per_image.png
```

### Parameters

#### Basic Parameters
- `resolution_analysis`: Boolean (default: True)
- `plot`: Path (default: None)
- `json`: Path (default: None)
- `split_json`: Boolean (default: False)
- `joint_json`: Boolean (default: True)
- `id`: Integer (default: None)

### Full Parameter Definitions

All parameters match the basic parameter descriptions, with additional type specifications:
- `resolution_analysis`: Boolean
- `plot`: Path
- `json`: Path
- `split_json`: Boolean
- `joint_json`: Boolean
- `id`: Integer (minimum 0, can be None)

---

## dials.stereographic_projection

### Overview
A DIALS tool for calculating stereographic projection images of crystal models and Miller indices.

### Key Features
- Generates stereographic projections for crystal models
- Supports projection in crystal or laboratory frame
- Can expand to symmetry equivalents
- Eliminates systematically absent reflections optionally

### Basic Usage Examples
```
dials.stereographic_projection indexed.expt hkl=1,0,0 hkl=0,1,0
dials.stereographic_projection indexed.expt hkl_limit=2
dials.stereographic_projection indexed_1.expt indexed_2.expt hkl=1,0,0 expand_to_p1=True
```

### Key Parameters
- `hkl`: Specific Miller indices to project
- `hkl_limit`: Maximum Miller index to include
- `expand_to_p1`: Expand to symmetry equivalents (default: True)
- `eliminate_sys_absent`: Remove systematically absent reflections (default: False)
- `frame`: Projection frame (laboratory or crystal)

### Plot Configuration
Supports customizing:
- Filename
- Marker size
- Font size
- Color mapping
- Grid size
- Labels

### Output Options
- PNG image generation
- Optional JSON coordinate export

---

## dials.combine_experiments

### Overview

A DIALS utility script for combining multiple reflections and experiments files into a single multi-experiment reflections and experiments file.

### Key Features

- Matches experiments to reflections in the order they are provided
- Allows selection of reference models from input experiment files
- Can replace models like beam, crystal, detector, etc.
- Supports complex experiment combinations through multiple runs

### Basic Usage Example

```
dials.combine_experiments experiments_0.expt experiments_1.expt \
  reflections_0.refl reflections_1.refl \
  reference_from_experiment.beam=0 \
  reference_from_experiment.detector=0
```

### Main Parameters

#### Output Options
- Log file
- Experiments filename
- Reflections filename
- Subset selection methods

#### Reference Model Selection
- Choose reference models from specific experiments
- Options to average or compare models
- Configurable tolerances for model comparisons

#### Clustering Options
- Optional experiment clustering
- Dendrogram generation
- Cluster threshold and size controls

### Advanced Features

- Significance filtering
- Resolution cutoff
- Reflection count filtering
- Batch size management

### Typical Use Cases

- Combining datasets from multiple experiments
- Standardizing experimental models
- Preparing data for further analysis

---

## dials.align_crystal

### Introduction

The `dials.align_crystal` program calculates possible goniometer settings to re-align crystal axes. By default, it attempts to align primary crystal axes with the principle goniometer axis.

### Key Features

- Can align vectors in two modes:
  1. `mode=main` (default): First vector aligned along principle goniometer axis
  2. `mode=cusp`: First vector aligned perpendicular to beam and goniometer axis

### Example Commands

```
dials.align_crystal models.expt
dials.align_crystal models.expt vector=0,0,1 vector=0,1,0
dials.align_crystal models.expt frame=direct
```

### Parameters

#### Basic Parameters
- `space_group`: Default is None
- `align.mode`: Choose between `main` or `cusp`
- `align.crystal.vector`: Specify crystal vectors
- `align.crystal.frame`: Choose `reciprocal` or `direct`
- `output.json`: Default output file is `align_crystal.json`

### Modes of Operation

- **Main Mode**: Aligns first vector along principle goniometer axis
- **Cusp Mode**: Aligns first vector perpendicular to beam and goniometer axis

---

## dials.anvil_correction

### Overview
A DIALS utility to correct integrated reflection intensities for attenuation caused by diamond anvil cells in high-pressure X-ray diffraction experiments.

### Purpose
"Correct integrated intensities to account for attenuation by a diamond anvil cell."

### Key Features
- Calculates path lengths of incident and diffracted beams through diamond anvils
- Corrects reflection intensities before scaling
- Uses absorption and density calculations to estimate intensity attenuation

### Usage Examples
```
dials.anvil_correction integrated.expt integrated.refl
dials.anvil_correction integrated.expt integrated.refl thickness=1.2 normal=1,0,0
```

### Parameters
#### Anvil Properties
- `density`: Diamond density (default 3510 kg/m³)
- `thickness`: Anvil thickness in mm (default 1.5925 mm)
- `normal`: 3-vector orthogonal to anvil surfaces

#### Output Options
- `experiments`: Output experiment list file
- `reflections`: Output reflection table file (default: corrected.refl)
- `log`: Log file (default: dials.anvil_correction.log)

### Theoretical Basis
The correction calculates beam path lengths through diamond anvils and applies an exponential attenuation factor based on:
- Linear absorption coefficient
- Beam vector orientation
- Anvil material properties

### Reference
Hubbell & Seltzer (2004), NIST X-Ray Mass Attenuation Coefficients

---

## dials.missing_reflections

### Overview
A DIALS program designed to "Identify connected regions of missing reflections in the asymmetric unit."

### Method
The program works by:
1. Generating a complete set of possible Miller indices
2. Performing connected components analysis on missing reflections

### Usage Examples
```
dials.missing_reflections integrated.expt integrated.refl
dials.missing_reflections scaled.expt scaled.refl min_component_size=10
```

### Parameters
- `min_component_size` (int, default=0): "Only show connected regions larger than or equal to this."
- `d_min` (float, optional): Minimum resolution limit
- `d_max` (float, optional): Maximum resolution limit

### Key Characteristics
- Helps identify gaps in reflection data
- Provides analysis of missing reflection regions
- Configurable through resolution and component size parameters

### Licensing
Part of the DIALS software suite, with copyright held by Diamond Light Source, Lawrence Berkeley National Laboratory, and STFC.

---

## dials.filter_reflections

### Overview
A DIALS program that filters reflection files based on user-specified criteria, allowing selective output of reflection data.

### Key Features
- Filter reflections using boolean flag expressions
- Filter by resolution (d_min, d_max)
- Select reflections by:
  - Experiment IDs
  - Panels
  - Partiality
  - Intensity quality
  - Dead time
  - Ice rings

### Basic Usage Examples
```
dials.filter_reflections refined.refl flag_expression=used_in_refinement
dials.filter_reflections integrated.refl flag_expression="indexed & (failed_during_summation | failed_during_profile_fitting)"
dials.filter_reflections indexed.refl indexed.expt d_max=20 d_min=2.5
```

### Key Parameters
- `output.reflections`: Output filename for filtered reflections
- `flag_expression`: Boolean expression to select reflections
- `d_min`/`d_max`: Resolution limits
- `partiality`: Min/max reflection partiality
- `select_good_intensities`: Filter for trustworthy intensities
- `ice_rings.filter`: Option to filter ice ring reflections

### Filtering Logic
1. Evaluate optional boolean flag expression
2. Apply additional filters on reflection table values
3. If no parameters set, print available flag values

---

## dials.import_xds

### Introduction

The `dials.import_xds` program imports XDS processed data for use in DIALS. It requires up to three components:

1. An XDS.INP file to specify geometry
2. One of:
   - "INTEGRATE.HKL"
   - "XPARM.XDS"
   - Alternatively, "XDS_ASCII.HKL" or "GXPARM.XDS"
3. INTEGRATE.HKL or SPOT.XDS file to create a reflection table

### Example Usage

```bash
# Extract files from a directory
dials.import_xds /path/to/folder/containing/xds/inp/

# Specify INTEGRATE.HKL path
dials.import_xds /path/to/folder/containing/xds/inp/INTEGRATE.HKL

# Be explicit about reflection file
dials.import_xds /path/to/folder/containing/xds/inp/ SPOT.XDS

# Specify experiment metadata file
dials.import_xds /path/to/folder/containing/xds/inp/ xds_file=XPARM.XDS
```

### Basic Parameters

- `input.xds_file`: Specify XDS file (default: None)
- `output.reflections`: Output reflections filename
- `output.xds_experiments`: Output experiment list filename (default: "xds_models.expt")
- `remove_invalid`: Remove non-index reflections
- `add_standard_columns`: Add empty standard columns
- `read_varying_crystal`: Create scan-varying crystal model

### Additional Details

The program is flexible in handling XDS input files and can extract necessary metadata and reflection information from various XDS-generated files.
</file>

<file path="libdocs/dials/dxtbx_models.md">
# DXTBX Model Objects

This documentation covers accessing and using dxtbx.model objects for detector geometry, beam properties, crystal orientation, and experimental parameters needed for diffuse scattering analysis.

**Version Information:** Compatible with DIALS 3.x series. Some methods may differ in DIALS 2.x.

**Key Dependencies:**
- `dxtbx`: Detector models, beam models, image handling
- `scitbx`: Matrix operations and mathematical utilities

---

## B.0. DXTBX Masking Utilities

**1. Purpose:**
Access detector-level masking utilities from DXTBX for creating trusted range masks and basic geometric masks needed for diffuse scattering preprocessing.

**2. Primary Python Call(s):**
```python
from dxtbx.model import Panel
from dials.array_family import flex

# Panel trusted range masking
def create_trusted_range_mask(panel, image_data):
    """Generate mask based on panel's trusted pixel value range"""
    trusted_range = panel.get_trusted_range()
    min_trusted, max_trusted = trusted_range
    
    # Create mask where True = valid pixels
    mask = (image_data >= min_trusted) & (image_data <= max_trusted)
    return mask

# Multi-panel trusted range masking
def create_detector_trusted_masks(detector, raw_data):
    """Create trusted range masks for all detector panels"""
    panel_masks = []
    
    if isinstance(raw_data, tuple):
        # Multi-panel detector
        for panel_id, (panel, panel_data) in enumerate(zip(detector, raw_data)):
            panel_mask = create_trusted_range_mask(panel, panel_data)
            panel_masks.append(panel_mask)
    else:
        # Single panel detector
        panel = detector[0]
        panel_mask = create_trusted_range_mask(panel, raw_data)
        panel_masks.append(panel_mask)
    
    return panel_masks
```

**3. Key Panel Methods for Masking:**
- `panel.get_trusted_range()` → (float, float): Valid pixel value range (min, max)
- `panel.get_image_size()` → (int, int): Panel dimensions (fast, slow) for mask creation
- `panel.get_pixel_size()` → (float, float): Physical pixel size for geometric masking

**4. Return Types:**
- `trusted_range`: tuple of (min_value, max_value) floats
- `mask`: flex.bool array with same shape as input image data
- Panel dimensions: tuple of (fast_pixels, slow_pixels) integers

**5. Example Usage Snippet:**
```python
from dxtbx.model.experiment_list import ExperimentListFactory
from dials.array_family import flex

# Load experiment and get detector model
experiments = ExperimentListFactory.from_json_file("experiments.expt")
detector = experiments[0].detector
imageset = experiments[0].imageset

# Get raw image data
raw_data = imageset.get_raw_data(0)  # First frame

# Create trusted range masks
trusted_masks = create_detector_trusted_masks(detector, raw_data)

# Apply masks to data
if isinstance(raw_data, tuple):
    # Multi-panel case
    for panel_id, (panel_data, panel_mask) in enumerate(zip(raw_data, trusted_masks)):
        valid_data = panel_data.select(panel_mask.as_1d())
        n_valid = len(valid_data)
        n_total = len(panel_data)
        print(f"Panel {panel_id}: {n_valid}/{n_total} pixels within trusted range")
        
        # Get trusted range for this panel
        panel = detector[panel_id]
        trusted_min, trusted_max = panel.get_trusted_range()
        print(f"  Trusted range: {trusted_min} to {trusted_max}")
else:
    # Single panel case
    panel_mask = trusted_masks[0]
    valid_data = raw_data.select(panel_mask.as_1d())
    print(f"Single panel: {len(valid_data)}/{len(raw_data)} pixels within trusted range")
```

**6. Integration with Image Processing:**
```python
def apply_trusted_range_filtering(experiments, frame_index=0):
    """
    Complete workflow for applying trusted range masks to image data
    """
    detector = experiments[0].detector
    imageset = experiments[0].imageset
    
    # Get image data
    raw_data = imageset.get_raw_data(frame_index)
    
    # Create trusted masks
    trusted_masks = create_detector_trusted_masks(detector, raw_data)
    
    # Apply masks and collect statistics
    filtered_data = []
    mask_statistics = {}
    
    if isinstance(raw_data, tuple):
        for panel_id, (panel_data, panel_mask) in enumerate(zip(raw_data, trusted_masks)):
            # Apply mask
            masked_data = panel_data.deep_copy()
            masked_data.set_selected(~panel_mask, 0)  # Set invalid pixels to 0
            filtered_data.append(masked_data)
            
            # Collect statistics
            n_valid = flex.sum(panel_mask.as_1d())
            n_total = len(panel_mask)
            panel = detector[panel_id]
            trusted_range = panel.get_trusted_range()
            
            mask_statistics[panel_id] = {
                'n_valid': n_valid,
                'n_total': n_total,
                'fraction_valid': n_valid / n_total,
                'trusted_range': trusted_range
            }
    else:
        # Single panel
        panel_mask = trusted_masks[0]
        masked_data = raw_data.deep_copy()
        masked_data.set_selected(~panel_mask, 0)
        filtered_data.append(masked_data)
        
        mask_statistics[0] = {
            'n_valid': flex.sum(panel_mask.as_1d()),
            'n_total': len(panel_mask),
            'fraction_valid': flex.sum(panel_mask.as_1d()) / len(panel_mask),
            'trusted_range': detector[0].get_trusted_range()
        }
    
    return filtered_data, trusted_masks, mask_statistics

# Usage
filtered_data, masks, stats = apply_trusted_range_filtering(experiments)
for panel_id, panel_stats in stats.items():
    print(f"Panel {panel_id}: {panel_stats['fraction_valid']:.3f} valid fraction")
    print(f"  Trusted range: {panel_stats['trusted_range']}")
```

**7. Notes/Caveats:**
- **Data Types:** Trusted range comparison requires compatible data types (typically int or float)
- **Overload Handling:** Pixels above trusted max are typically overloaded; pixels below trusted min may be noisy
- **Panel Indexing:** Always verify panel indexing consistency when working with multi-panel detectors
- **Memory Efficiency:** Create masks as needed rather than storing all panel masks simultaneously for large detectors

**8. Integration with Diffuse Scattering Pipeline:**
Trusted range masks form the foundation for all subsequent masking operations, ensuring only reliable pixel data is used in geometric corrections and diffuse scattering analysis.

**9. See Also:**
- Section A.6: dials.generate_mask for advanced masking capabilities
- Section A.3: Loading image data for mask application
- Section B.1: Detector model for geometric properties

---

## B.1. Detector Model (experiment.detector)

**1. Purpose:**
Access detector geometry information for coordinate transformations, pixel-to-lab coordinate mapping, and panel identification needed for diffuse scattering analysis across multi-panel detectors.

**2. Primary Python Call(s):**
```python
detector = experiment.detector

# Panel-level operations
for panel_id, panel in enumerate(detector):
    # Basic properties
    image_size = panel.get_image_size()  # (width, height) in pixels
    pixel_size = panel.get_pixel_size()  # (fast, slow) in mm
    
    # Coordinate transformations
    lab_coord = panel.get_pixel_lab_coord((fast_px, slow_px))
    mm_coord = panel.pixel_to_millimeter((fast_px, slow_px))
    px_coord = panel.millimeter_to_pixel((mm_x, mm_y))
    
    # Panel orientation
    fast_axis = panel.get_fast_axis()  # Fast axis direction vector
    slow_axis = panel.get_slow_axis()  # Slow axis direction vector
    origin = panel.get_origin()        # Panel origin in lab frame
```

**3. Key Methods:**
- `get_image_size()` → (int, int): Panel dimensions in pixels (fast, slow)
- `get_pixel_size()` → (float, float): Pixel size in mm (fast, slow)
- `get_pixel_lab_coord(px_coord)` → (float, float, float): Lab coordinates for pixel
- `get_trusted_range()` → (float, float): Valid pixel value range (min, max)
- `get_fast_axis()` → (float, float, float): Fast axis direction vector
- `get_slow_axis()` → (float, float, float): Slow axis direction vector
- `get_origin()` → (float, float, float): Panel origin in lab coordinates
- `detector.get_ray_intersection(s1)` → (int, (float, float)): Panel ID and intersection point
- `detector.get_panel_intersection(s1)` → int: Panel ID for ray intersection (-1 if none)

**4. Return Types:**
- Image coordinates: tuples of (fast, slow) integers or floats
- Lab coordinates: tuples of (x, y, z) floats in mm
- Axis vectors: tuples of (x, y, z) unit direction vectors

**5. Example Usage Snippet:**
```python
detector = experiment.detector
beam_centre_panel, beam_centre_mm = detector.get_ray_intersection(beam.get_s0())

if beam_centre_panel >= 0:
    panel = detector[beam_centre_panel]
    beam_centre_px = panel.millimeter_to_pixel(beam_centre_mm)
    print(f"Beam centre: Panel {beam_centre_panel}, pixel {beam_centre_px}")
```

**6. Notes/Caveats:**
- Multi-panel detectors are accessed by index: `detector[panel_id]`
- Coordinate transformations assume orthogonal detector geometry
- Lab coordinates are in detector coordinate system (not necessarily aligned with beam)
- **Panel identification from pixel coordinates:** No direct `get_panel_for_pixel()` method exists; use ray intersection methods instead
- The A matrix (A = U × B) transforms Miller indices directly to reciprocal lattice vectors in lab frame

**7. Panel Identification Examples:**
```python
# Find which panel a given s1 vector intersects
detector = experiment.detector
beam = experiment.beam

# Calculate s1 for a specific pixel on panel 0
panel = detector[0]
lab_coord = panel.get_pixel_lab_coord((100, 200))
s1 = matrix.col(lab_coord).normalize() / beam.get_wavelength()

# Find which panel this s1 intersects
panel_id = detector.get_panel_intersection(s1)
if panel_id >= 0:
    print(f"Ray intersects panel {panel_id}")
else:
    print("Ray does not intersect any panel")

# Alternative: get panel ID and intersection coordinates
panel_id, intersection_xy = detector.get_ray_intersection(s1)
if panel_id >= 0:
    panel = detector[panel_id]
    pixel_coord = panel.millimeter_to_pixel(intersection_xy)
    print(f"Ray intersects panel {panel_id} at pixel {pixel_coord}")

# Get detector coordinate system vectors
for panel_id, panel in enumerate(detector):
    fast_axis = panel.get_fast_axis()
    slow_axis = panel.get_slow_axis()
    normal = matrix.col(fast_axis).cross(matrix.col(slow_axis))
    print(f"Panel {panel_id} normal vector: {normal}")
```

**8. See Also:**
- Section C.1: Calculating q-vectors using detector geometry
- Section C.4: Geometric corrections requiring detector parameters

---

## B.2. Beam Model (experiment.beam)

**1. Purpose:**
Access incident beam properties for calculating scattering vectors and momentum transfer.

**2. Primary Python Call(s):**
```python
beam = experiment.beam

# Basic properties
wavelength = beam.get_wavelength()      # Wavelength in Angstroms
direction = beam.get_direction()        # Unit direction vector
s0 = beam.get_s0()                     # Incident beam vector (1/λ * direction)
unit_s0 = beam.get_unit_s0()           # Unit incident beam vector

# Polarization properties
polarization_normal = beam.get_polarization_normal()
polarization_fraction = beam.get_polarization_fraction()
```

**3. Key Methods:**
- `get_wavelength()` → float: Wavelength in Angstroms
- `get_s0()` → (float, float, float): Incident beam vector in 1/Angstroms
- `get_direction()` → (float, float, float): Unit direction vector
- `get_polarization_fraction()` → float: Fraction of polarized light

**4. Return Types:**
- Wavelength: float in Angstroms
- Vectors: tuples of (x, y, z) floats
- s0 magnitude: 1/wavelength in 1/Angstroms

**5. Example Usage Snippet:**
```python
beam = experiment.beam
wavelength = beam.get_wavelength()
s0 = beam.get_s0()
print(f"Wavelength: {wavelength:.4f} Å")
print(f"Beam vector s0: {s0}")
print(f"|s0| = {(s0[0]**2 + s0[1]**2 + s0[2]**2)**0.5:.6f} = 1/λ")
```

**6. Notes/Caveats:**
- s0 points in direction of incident beam with magnitude 1/λ
- For time-of-flight experiments, use `get_wavelength_range()` for wavelength distribution
- Polarization methods are available: `get_polarization_normal()` and `get_polarization_fraction()`

**7. Complete Beam Properties Example:**
```python
beam = experiment.beam

# Basic properties
wavelength = beam.get_wavelength()
direction = beam.get_direction() 
s0 = beam.get_s0()

# Polarization properties (from cctbx integration)
polarization_normal = beam.get_polarization_normal()  # Returns (x,y,z) tuple
polarization_fraction = beam.get_polarization_fraction()  # Returns float 0-1

print(f"Wavelength: {wavelength:.4f} Å")
print(f"Incident beam s0: {s0}")
print(f"Polarization: {polarization_fraction:.2f} fraction along {polarization_normal}")

# For Lorentz-polarization corrections
print(f"Polarization normal: {polarization_normal}")
print(f"Polarization fraction: {polarization_fraction}")
```

**8. See Also:**
- Section C.4: Using beam polarization in geometric corrections
- Section C.1: Using s0 vector in q-vector calculations

---

## B.3. Crystal Model (experiment.crystal)

**1. Purpose:**
Access crystal lattice parameters and orientation matrices (from cctbx.uctbx and cctbx.sgtbx) for Miller index calculations, reciprocal space transformations, and diffuse scattering analysis in crystal coordinates.

**2. Primary Python Call(s):**
```python
crystal = experiment.crystal

# Unit cell and symmetry
unit_cell = crystal.get_unit_cell()
space_group = crystal.get_space_group()
a, b, c, alpha, beta, gamma = unit_cell.parameters()

# Orientation matrices
A_matrix = crystal.get_A()  # A = U * B (orientation × metric)
U_matrix = crystal.get_U()  # Orientation matrix
B_matrix = crystal.get_B()  # Metric matrix (reciprocal lattice)

# Real space vectors
real_space_vectors = crystal.get_real_space_vectors()
```

**3. Key Methods:**
- `get_unit_cell()` → cctbx.uctbx.unit_cell: Unit cell object
- `get_A()` → matrix.sqr: Combined orientation and metric matrix
- `get_U()` → matrix.sqr: Orientation matrix
- `get_B()` → matrix.sqr: Reciprocal metric matrix
- `get_space_group()` → cctbx.sgtbx.space_group: Space group object

**4. Return Types:**
- Unit cell: cctbx.uctbx.unit_cell with .parameters() method
- Matrices: scitbx.matrix.sqr objects (3×3)
- Space group: cctbx.sgtbx.space_group with symmetry operations

**5. Example Usage Snippet:**
```python
crystal = experiment.crystal
unit_cell = crystal.get_unit_cell()
space_group = crystal.get_space_group()
A_matrix = crystal.get_A()

print(f"Unit cell: {unit_cell.parameters()}")
print(f"Space group: {space_group.info().symbol_and_number()}")

# Convert Miller index to reciprocal space vector
from scitbx import matrix
hkl = (1, 2, 3)
q_vector = matrix.col(A_matrix) * matrix.col(hkl)
print(f"Miller index {hkl} → q-vector {q_vector}")
```

**6. Notes/Caveats:**
- **A matrix interpretation:** A = U × B transforms Miller indices directly to reciprocal lattice vectors in lab frame
- **Setting consistency:** The crystal model is always in the correct setting for calculations; no direct/reciprocal setting checks needed
- For scan-varying crystals, use `get_A_at_scan_point(i)` for frame-specific matrices
- Real space vectors are in Angstroms, reciprocal vectors in 1/Angstroms
- Space group operations available through `crystal.get_space_group().all_ops()`

**7. Extended Crystal Model Usage:**
```python
crystal = experiment.crystal

# Complete unit cell information
unit_cell = crystal.get_unit_cell()
a, b, c, alpha, beta, gamma = unit_cell.parameters()
volume = unit_cell.volume()
print(f"Unit cell: a={a:.3f}, b={b:.3f}, c={c:.3f} Å")
print(f"Angles: α={alpha:.1f}, β={beta:.1f}, γ={gamma:.1f}°")
print(f"Volume: {volume:.1f} Å³")

# Space group information
space_group = crystal.get_space_group()
symbol = space_group.info().symbol_and_number()
symmetry_ops = space_group.all_ops()
print(f"Space group: {symbol}")
print(f"Number of symmetry operations: {len(symmetry_ops)}")

# Orientation matrices and transformations
U_matrix = crystal.get_U()  # Orientation matrix
B_matrix = crystal.get_B()  # Metric matrix (reciprocal lattice)
A_matrix = crystal.get_A()  # Combined A = U × B

# Real space lattice vectors
real_space_vectors = crystal.get_real_space_vectors()
a_vec, b_vec, c_vec = real_space_vectors
print(f"Real space vectors:")
print(f"a: {a_vec}")
print(f"b: {b_vec}")
print(f"c: {c_vec}")

# Calculate reciprocal lattice vectors for Miller indices
hkl_list = [(1,0,0), (0,1,0), (0,0,1), (1,1,1)]
for hkl in hkl_list:
    q_vector = matrix.col(A_matrix) * matrix.col(hkl)
    d_spacing = unit_cell.d(hkl)
    print(f"Miller index {hkl}: q={q_vector}, d={d_spacing:.3f} Å")
```

**8. See Also:**
- Section C.2: Converting q-vectors to Miller indices using A matrix
- Section C.3: Calculating d-spacings from unit cell parameters

---

## B.4. Goniometer Model (experiment.goniometer)

**1. Purpose:**
Access goniometer geometry for rotation data processing and scan-dependent coordinate transformations.

**2. Primary Python Call(s):**
```python
goniometer = experiment.goniometer

# Rotation properties
rotation_axis = goniometer.get_rotation_axis()
fixed_rotation = goniometer.get_fixed_rotation()
setting_rotation = goniometer.get_setting_rotation()

# Scan-dependent rotations
if scan is not None:
    rotation_matrix = goniometer.get_rotation_matrix_at_scan_point(scan_point)
    angle = goniometer.get_angle_from_rotation_matrix(rotation_matrix)
```

**3. Key Methods:**
- `get_rotation_axis()` → (float, float, float): Rotation axis unit vector
- `get_fixed_rotation()` → matrix.sqr: Fixed rotation matrix
- `get_setting_rotation()` → matrix.sqr: Setting rotation matrix

**4. Return Types:**
- Axis: tuple of (x, y, z) unit vector components
- Matrices: scitbx.matrix.sqr objects (3×3 rotation matrices)

**5. Example Usage Snippet:**
```python
if experiment.goniometer is not None:
    goniometer = experiment.goniometer
    rotation_axis = goniometer.get_rotation_axis()
    print(f"Rotation axis: {rotation_axis}")
    
    # For rotation experiments
    if experiment.scan is not None:
        for i in range(experiment.scan.get_num_images()):
            angle = experiment.scan.get_angle_from_image_index(i)
            print(f"Image {i}: rotation angle {angle:.2f}°")
```

**6. Notes/Caveats:**
- **Existence check required:** Only applicable for rotation experiments (None for still experiments)
- Always check `if experiment.goniometer is not None:` before accessing
- Combined with scan information to calculate frame-specific orientations
- Used in Lorentz correction calculations for rotation experiments

**7. Safe Goniometer Access Pattern:**
```python
# Always check for existence first
if experiment.goniometer is not None:
    goniometer = experiment.goniometer
    rotation_axis = goniometer.get_rotation_axis()
    print(f"Rotation experiment with axis: {rotation_axis}")
    
    # Get rotation matrices for scan points
    if experiment.scan is not None:
        scan = experiment.scan
        for i in range(scan.get_num_images()):
            # Method 1: Use scan to get angle, then goniometer for matrix
            angle = scan.get_angle_from_image_index(i)
            # Method 2: Direct from goniometer if scan points are set
            if hasattr(goniometer, 'get_rotation_matrix_at_scan_point'):
                rotation_matrix = goniometer.get_rotation_matrix_at_scan_point(i)
                print(f"Image {i}: angle={angle:.2f}°")
else:
    print("Still experiment - no goniometer model")
```

**8. See Also:**
- Section C.4: Using goniometer in Lorentz corrections
- Section B.5: Scan model for rotation angle information

---

## B.5. Scan Model (experiment.scan)

**1. Purpose:**
Access scan parameters for rotation experiments including oscillation range and frame timing.

**2. Primary Python Call(s):**
```python
scan = experiment.scan

# Basic scan properties
oscillation = scan.get_oscillation()        # (start_angle, oscillation_width)
image_range = scan.get_image_range()        # (start_image, end_image)
num_images = scan.get_num_images()          

# Frame-specific information
for i in range(num_images):
    angle = scan.get_angle_from_image_index(i)
    exposure_time = scan.get_exposure_times()[i]
```

**3. Key Methods:**
- `get_oscillation()` → (float, float): Start angle and oscillation width in degrees
- `get_image_range()` → (int, int): First and last image numbers
- `get_angle_from_image_index(i)` → float: Rotation angle for image i

**4. Return Types:**
- Angles: floats in degrees
- Image indices: integers
- Times: floats in seconds

**5. Example Usage Snippet:**
```python
if experiment.scan is not None:
    scan = experiment.scan
    start_angle, osc_width = scan.get_oscillation()
    image_range = scan.get_image_range()
    
    print(f"Scan: {osc_width}° oscillation starting at {start_angle}°")
    print(f"Images: {image_range[0]} to {image_range[1]}")
```

**6. Notes/Caveats:**
- **Existence check required:** Only applicable for rotation experiments (None for still experiments)
- Always check `if experiment.scan is not None:` before accessing
- Image indices in DIALS start from 1, not 0
- Exposure times available through `get_exposure_times()` method

**7. Complete Scan Information Access:**
```python
# Always check for existence first
if experiment.scan is not None:
    scan = experiment.scan
    
    # Basic scan properties
    start_angle, osc_width = scan.get_oscillation()
    start_image, end_image = scan.get_image_range()
    num_images = scan.get_num_images()
    
    # Exposure times
    exposure_times = scan.get_exposure_times()
    
    print(f"Scan parameters:")
    print(f"  Oscillation: {osc_width}° starting at {start_angle}°")
    print(f"  Images: {start_image} to {end_image} ({num_images} total)")
    print(f"  Exposure times: {exposure_times[0]:.3f} to {exposure_times[-1]:.3f} s")
    
    # Frame-specific information
    for i in range(min(5, num_images)):  # Show first 5 frames
        angle = scan.get_angle_from_image_index(i)
        exposure = exposure_times[i]
        image_number = start_image + i
        print(f"  Frame {i}: image #{image_number}, angle={angle:.2f}°, exposure={exposure:.3f}s")
else:
    print("Still experiment - no scan model")
```

**8. See Also:**
- Section A.3: Getting exposure times from ImageSet
- Section B.4: Combining with goniometer for rotation calculations
</file>

<file path="libdocs/dials/flex_arrays.md">
# Flex Array Manipulations

This documentation covers `dials.array_family.flex` array operations essential for crystallographic data processing and analysis, plus best practices for DIALS integration.

**Version Information:** Compatible with DIALS 3.x series. Some methods may differ in DIALS 2.x.

**Key Dependencies:**
- `dials.array_family.flex`: Core flex array types and operations
- `dxtbx.flumpy`: NumPy integration utilities
- `scitbx`: Scientific computing utilities
- `cctbx`: Crystallographic computing toolkit

## D.1. Converting Flex Arrays to NumPy Arrays

**1. Purpose:**
Convert DIALS flex arrays to NumPy arrays for compatibility with scientific Python ecosystem and integration with analysis libraries like scipy, matplotlib, and pandas.

**2. Primary Python Call(s):**
```python
from dials.array_family import flex
from dxtbx import flumpy
import numpy as np

# Method 1: Using as_numpy_array() (basic conversion)
flex_array = flex.double([1.0, 2.0, 3.0, 4.0])
numpy_array = flex_array.as_numpy_array()

# Method 2: Using flumpy (recommended for advanced types)
vec3_data = flex.vec3_double([(1,2,3), (4,5,6)])
numpy_array = flumpy.to_numpy(vec3_data)

# For vec3_double, extract components
x, y, z = vec3_data.parts()
x_np = x.as_numpy_array()
y_np = y.as_numpy_array()
z_np = z.as_numpy_array()
```

**3. Key Arguments:**
- No arguments for `as_numpy_array()`
- `flumpy.to_numpy()` handles complex flex types automatically

**4. Return Type:**
- `numpy.ndarray` with appropriate dtype (float64, int32, bool, etc.)

**5. Example Usage Snippet:**
```python
# Convert reflection table columns to NumPy
reflections = flex.reflection_table.from_file("reflections.refl")

# Simple columns
intensities = reflections["intensity.sum.value"].as_numpy_array()
variances = reflections["intensity.sum.variance"].as_numpy_array()

# Vector columns
xyz_cal = reflections["xyzcal.px"]
x, y, z = xyz_cal.parts()
positions = np.column_stack([x.as_numpy_array(), y.as_numpy_array(), z.as_numpy_array()])

print(f"Converted {len(intensities)} intensities to NumPy")
print(f"Position array shape: {positions.shape}")
```

**6. Notes/Caveats:**
- `as_numpy_array()` creates a view when possible, copy when necessary
- Complex types (vec3_double, miller_index) need component extraction
- Use `flumpy` for bidirectional conversion with proper type handling

---

## D.2. Converting NumPy Arrays to Flex Arrays

**1. Purpose:**
Convert NumPy arrays back to DIALS flex arrays for use with DIALS algorithms and maintaining compatibility with DIALS processing workflows.

**2. Primary Python Call(s):**
```python
import numpy as np
from dials.array_family import flex
from dxtbx import flumpy

# Method 1: Using flumpy (recommended)
numpy_array = np.array([1.0, 2.0, 3.0])
flex_array = flumpy.from_numpy(numpy_array)

# Method 2: Direct flex constructors
flex_double = flex.double(numpy_array)
flex_int = flex.int(numpy_array.astype(int))
flex_bool = flex.bool(numpy_array.astype(bool))

# For vector types
positions_np = np.array([[1,2,3], [4,5,6], [7,8,9]], dtype=float)
vec3_flex = flumpy.vec_from_numpy(positions_np)

# Create Miller indices from NumPy
hkl_np = np.array([[1,0,0], [2,0,0], [0,0,1]], dtype=int)
miller_flex = flex.miller_index(hkl_np)
```

**3. Key Arguments:**
- `numpy_array`: Input NumPy array with compatible dtype
- For `flex.miller_index()`: requires integer array with shape (N, 3)

**4. Return Type:**
- Appropriate flex array type (flex.double, flex.int, flex.vec3_double, etc.)

**5. Example Usage Snippet:**
```python
# Create computed data in NumPy and convert to flex
computed_intensities = np.random.exponential(1000, size=10000)
computed_positions = np.random.rand(10000, 3) * 100

# Convert to flex for DIALS
flex_intensities = flumpy.from_numpy(computed_intensities)
flex_positions = flumpy.vec_from_numpy(computed_positions)

# Add to reflection table
new_reflections = flex.reflection_table()
new_reflections["intensity.computed"] = flex_intensities
new_reflections["xyz.computed"] = flex_positions

print(f"Created reflection table with {len(new_reflections)} computed reflections")
```

**6. Notes/Caveats:**
- NumPy array dtype must be compatible with target flex type
- `flumpy` handles type conversion automatically
- Vector types require shape (N, 3) for vec3_double

---

## D.3. Accessing Elements and Properties of flex.reflection_table

**1. Purpose:**
Navigate and manipulate DIALS reflection table data for diffuse scattering analysis, including filtering, statistical analysis, and data export operations.

**2. Primary Python Call(s):**
```python
from dials.array_family import flex

# Create or load reflection table
reflections = flex.reflection_table.from_file("reflections.refl")

# Basic table properties
num_reflections = len(reflections)
column_names = list(reflections.keys())
has_column = "intensity.sum.value" in reflections

# Access columns
miller_indices = reflections["miller_index"]
intensities = reflections["intensity.sum.value"] 
pixel_positions = reflections["xyzcal.px"]

# Column statistics
mean_intensity = flex.mean(intensities)
intensity_range = (flex.min(intensities), flex.max(intensities))

# Table operations
reflections.extend(other_reflections)  # Concatenate tables
subset = reflections[:1000]            # Slice first 1000 rows
reflections.del_selected(bad_mask)     # Delete rows
```

**3. Key Methods:**
- `len(table)` → int: Number of reflections
- `table.keys()` → list: Column names
- `table[column]` → flex array: Access column data
- `table.extend(other)`: Concatenate reflection tables
- `table.select(selection)` → table: Select subset based on boolean mask

**4. Return Types:**
- Columns return appropriate flex array types
- Operations return modified reflection tables

**5. Example Usage Snippet:**
```python
reflections = flex.reflection_table.from_file("integrated.refl")

print(f"Loaded {len(reflections)} reflections")
print(f"Columns: {list(reflections.keys())}")

# Access key data for diffuse scattering
if "intensity.sum.value" in reflections:
    intensities = reflections["intensity.sum.value"]
    strong_mask = intensities > flex.mean(intensities)
    strong_reflections = reflections.select(strong_mask)
    print(f"Found {len(strong_reflections)} strong reflections")

# Work with positions
if "xyzcal.px" in reflections:
    positions = reflections["xyzcal.px"]
    x, y, z = positions.parts()
    print(f"Position range: X({flex.min(x):.1f}-{flex.max(x):.1f}), "
          f"Y({flex.min(y):.1f}-{flex.max(y):.1f})")
```

**6. Notes/Caveats:**
- Column availability depends on processing stage (find_spots, index, integrate, scale)
- Some operations modify the table in-place
- Use boolean masks with `select()` for conditional operations

---

## D.4. Common Operations on Flex Arrays

**1. Purpose:**
Perform mathematical and logical operations on flex arrays for data analysis, statistical calculations, and array manipulation in diffuse scattering processing pipelines.

**2. Primary Python Call(s):**
```python
from dials.array_family import flex
import math

# Mathematical operations
a = flex.double([1, 2, 3, 4, 5])
b = flex.double([2, 3, 4, 5, 6])

# Element-wise arithmetic
result = a + b           # Addition
result = a * 2.0         # Scalar multiplication  
result = a / b           # Element-wise division
result = flex.pow(a, 2)  # Power function

# Mathematical functions
result = flex.sqrt(a)
result = flex.exp(a)
result = flex.log(a)
result = flex.sin(a)

# Statistical operations
mean_val = flex.mean(a)
std_dev = flex.mean_and_variance(a).unweighted_sample_standard_deviation()
min_val = flex.min(a)
max_val = flex.max(a)

# Boolean operations and selection
mask = a > 3.0
selected = a.select(mask)
count = flex.sum(mask.as_1d())
```

**3. Key Methods:**
- Arithmetic: `+`, `-`, `*`, `/` (element-wise)
- Functions: `flex.sqrt()`, `flex.exp()`, `flex.log()`, `flex.sin()`, `flex.cos()`
- Statistics: `flex.mean()`, `flex.min()`, `flex.max()`, `flex.sum()`
- Selection: `array.select(mask)` where mask is flex.bool

**4. Return Types:**
- Mathematical operations return flex arrays of appropriate type
- Statistics return Python scalars
- Selection returns flex array subset

**5. Example Usage Snippet:**
```python
# Analyze intensity distribution
intensities = reflections["intensity.sum.value"]
log_intensities = flex.log(intensities)

# Calculate signal-to-noise ratio
variances = reflections["intensity.sum.variance"]
signal_to_noise = intensities / flex.sqrt(variances)

# Find outliers
mean_snr = flex.mean(signal_to_noise)
std_snr = flex.mean_and_variance(signal_to_noise).unweighted_sample_standard_deviation()
outlier_mask = flex.abs(signal_to_noise - mean_snr) > 3 * std_snr

print(f"Mean S/N: {mean_snr:.2f} ± {std_snr:.2f}")
print(f"Found {flex.sum(outlier_mask.as_1d())} outliers")

# Apply corrections
corrected_intensities = intensities * correction_factors
reflections["intensity.corrected"] = corrected_intensities
```

**6. Notes/Caveats:**
- All arrays in operations must have compatible sizes
- Mathematical functions expect appropriate input ranges (e.g., log(x) requires x > 0)
- Use `as_1d()` to convert boolean masks to size_t for counting

---

## Key Tips and Best Practices

1. **Always check experiment validity** before accessing models:
   ```python
   if experiment.detector is not None and experiment.beam is not None:
       # Safe to proceed with calculations
   ```

2. **Use flumpy for NumPy integration** rather than manual conversion methods:
   ```python
   from dxtbx import flumpy
   numpy_array = flumpy.to_numpy(flex_array)
   flex_array = flumpy.from_numpy(numpy_array)
   ```

3. **Handle multi-panel detectors correctly**:
   ```python
   raw_data = imageset.get_raw_data(0)
   if isinstance(raw_data, tuple):
       for panel_id, panel_data in enumerate(raw_data):
           # Process each panel separately
   ```

4. **Check column existence before accessing**:
   ```python
   if "intensity.sum.value" in reflections:
       intensities = reflections["intensity.sum.value"]
   ```

5. **Use matrix operations for coordinate transformations**:
   ```python
   from scitbx import matrix
   A_matrix = matrix.sqr(crystal.get_A())
   q_vector = A_matrix * matrix.col(hkl)
   ```

6. **Apply geometric corrections properly**:
   ```python
   # Corrections are typically applied as divisors
   corrected_intensity = raw_intensity / lp_correction
   ```

---

## Summary: Integration with Diffuse Scattering Pipeline

This API reference maps directly to the modules outlined in your `plan.md`:

**Module 1.1 (Data Loading):** Sections A.1-A.4 provide all file I/O operations
**Module 2.1.D (Q-space Mapping):** Sections C.1-C.3 for coordinate transformations
**Module 2.2.D (Geometric Corrections):** Section C.4 for all intensity corrections (Solid Angle, Lorentz-Polarization, Detector Efficiency, Air Attenuation)
**Module 3.0.D (Crystal Frame Analysis):** Sections B.3, C.2 for crystal coordinate systems
**Module 3.1.D (Lattice Analysis):** Sections C.2-C.3, D.3 for Miller index operations
**Module 4.1.D (Theoretical Comparison):** Section C.5 for scattering factor calculations

**Common Workflow Pattern:**
```python
# 1. Load experimental data
experiments = ExperimentListFactory.from_json_file("experiments.expt")
reflections = flex.reflection_table.from_file("reflections.refl")
imageset = experiments[0].imageset

# 2. Extract geometry models
detector = experiments[0].detector
beam = experiments[0].beam
crystal = experiments[0].crystal

# 3. Process pixel data with geometric corrections
for panel_id, panel in enumerate(detector):
    for frame_idx in range(len(imageset)):
        raw_data = imageset.get_raw_data(frame_idx)
        panel_data = raw_data[panel_id] if isinstance(raw_data, tuple) else raw_data
        
        # Apply diffuse scattering analysis using APIs from sections C.1-C.4
```

**Official Documentation Links:**
- [DIALS Documentation](https://dials.github.io/documentation/)
- [dxtbx Documentation](https://dxtbx.readthedocs.io/)
- [cctbx Documentation](https://cctbx.sourceforge.net/)

**Error Handling Best Practices:**
- Always check model existence: `if experiment.goniometer is not None:`
- Use try/except blocks for file operations with `dials.util.Sorry` exceptions
- Validate array sizes before mathematical operations
- Handle edge cases in coordinate transformations (beam center, panel edges)

---

## See Also

- **File I/O Operations**: [dials_file_io.md](dials_file_io.md)
- **Detector Models**: [dxtbx_models.md](dxtbx_models.md)
- **Crystallographic Calculations**: [crystallographic_calculations.md](crystallographic_calculations.md)
- **DIALS Scaling**: [dials_scaling.md](dials_scaling.md)
</file>

<file path="libdocs/dials/README.md">
# DIALS Python API Documentation

This directory contains comprehensive documentation for the DIALS/dxtbx/cctbx Python APIs used in crystallographic data processing.

The documentation has been organized into focused modules for easier navigation and reduced context usage:

## Documentation Files

### [dials_file_io.md](dials_file_io.md)
**File I/O and Model Loading (Sections A.0-A.6)**
- `dials.stills_process` Python API
- Loading experiment lists (`.expt` files)
- Loading reflection tables (`.refl` files)  
- CBF/image file handling with dxtbx
- PHIL parameter management
- File format conversions
- Error handling for I/O operations

### [dxtbx_models.md](dxtbx_models.md)
**Detector, Beam, and Crystal Models (Sections B.0-B.5)**
- Detector geometry and panel access
- Beam properties and wavelength handling
- Crystal unit cell and orientation matrices
- Scan and goniometer models
- Coordinate system transformations
- Multi-panel detector support

### [crystallographic_calculations.md](crystallographic_calculations.md)
**Q-Vector and Crystallographic Calculations (Sections C.1-C.8)**
- Unit cell and space group operations
- Miller index calculations and systematic absences
- Direct Q-vector calculation from detector geometry
- Q-vector validation and comparison methods
- Coordinate transformations (detector ↔ lab ↔ reciprocal space)
- Resolution and d-spacing calculations
- Scattering factor and structure factor calculations
- Thermal motion and B-factor corrections
- Peak integration and background estimation

### [dials_scaling.md](dials_scaling.md)
**DIALS Scaling Framework (Section D.0)**
- Basic scaling workflow and model types
- Physical, KB, and array-based scaling models
- Outlier rejection algorithms
- Cross-validation and quality statistics
- Multi-dataset scaling
- Error model refinement

### [flex_arrays.md](flex_arrays.md)
**Flex Array Operations (Sections D.1-D.4)**
- Basic flex array types and creation
- Mathematical and statistical operations
- Logical operations and masking
- Vector and matrix operations
- Advanced manipulations (concatenation, reshaping, interpolation)
- Custom array functions and utilities

## Usage Notes

- **Version Compatibility**: All documentation is compatible with DIALS 3.x series
- **Import Dependencies**: Each file lists required imports at the beginning
- **Code Examples**: All functions include usage examples and error handling
- **Integration**: Examples show how different modules work together

## Quick Reference

For specific tasks, refer to these sections:

- **Loading DIALS data**: Start with `dials_file_io.md`
- **Detector geometry**: See `dxtbx_models.md` 
- **Q-vector calculations**: Use `crystallographic_calculations.md`
- **Data scaling**: Reference `dials_scaling.md`
- **Array operations**: Check `flex_arrays.md`

## Original File

The original comprehensive documentation is preserved as `DIALS_Python_API_Reference.md` for reference, but the split files above are recommended for daily use.

---

**Note**: This documentation covers the Python APIs needed for implementing diffuse scattering data processing modules. For command-line usage of DIALS tools, refer to the official DIALS documentation at https://dials.github.io/.
</file>

<file path="scripts/dev_workflows/README.md">
# Development Workflows for DiffusePipe

This directory contains development and testing workflows that orchestrate multiple pipeline components to provide end-to-end processing and validation capabilities.

**📖 For comprehensive documentation on all visual diagnostic tools, see [docs/VISUAL_DIAGNOSTICS_GUIDE.md](../../docs/VISUAL_DIAGNOSTICS_GUIDE.md)**

## Scripts

### `run_phase2_e2e_visual_check.py` - End-to-End Phase 2 Visual Verification

**Purpose**: Complete end-to-end pipeline that processes a single CBF image through Phase 1 (DIALS processing, masking) and Phase 2 (DataExtractor), then generates comprehensive visual diagnostics to verify correctness.

**What it does:**
1. **Phase 1 - DIALS Processing**: Imports, finds spots, indexes, and refines detector geometry
2. **Phase 1 - Mask Generation**: Creates pixel masks (static + dynamic) and Bragg masks 
3. **Phase 1 - Total Mask**: Combines masks to create final diffuse analysis mask
4. **Phase 2 - Data Extraction**: Extracts diffuse scattering data with pixel corrections
5. **Phase 3 - Visual Diagnostics**: Runs comprehensive visual verification plots

**Key Features:**
- Fully automated pipeline from raw CBF to visual diagnostics
- Configurable DIALS processing parameters
- Support for both spot-based and shoebox-based Bragg masking
- Pixel coordinate tracking for detailed visual analysis
- Comprehensive error handling and logging
- Organized output directory structure

**Usage Examples:**

```bash
# Basic usage with CBF file and PDB reference
python run_phase2_e2e_visual_check.py \
  --cbf-image ../../747/lys_nitr_10_6_0491.cbf \
  --output-base-dir ./e2e_outputs \
  --pdb-path ../../6o2h.pdb

# With custom configurations
python run_phase2_e2e_visual_check.py \
  --cbf-image image.cbf \
  --output-base-dir ./outputs \
  --dials-phil-path custom_dials.phil \
  --static-mask-config '{"beamstop": {"type": "circle", "center_x": 1250, "center_y": 1250, "radius": 50}}' \
  --bragg-mask-config '{"border": 3}' \
  --extraction-config-json '{"pixel_step": 2}' \
  --verbose

# Using shoebox-based Bragg masking
python run_phase2_e2e_visual_check.py \
  --cbf-image image.cbf \
  --output-base-dir ./outputs \
  --use-bragg-mask-option-b \
  --verbose
```

**Command-Line Arguments:**

**Required:**
- `--cbf-image`: Path to input CBF image file
- `--output-base-dir`: Base output directory (unique subdirectory will be created)

**Optional Processing:**
- `--dials-phil-path`: Path to custom DIALS PHIL configuration file
- `--pdb-path`: Path to external PDB file for validation

**Masking Configuration:**
- `--static-mask-config`: JSON string for static mask configuration (beamstop, untrusted regions)
- `--bragg-mask-config`: JSON string for Bragg mask configuration (border size, algorithm)
- `--use-bragg-mask-option-b`: Use shoebox-based Bragg masking instead of spot-based

**Extraction Configuration:**
- `--extraction-config-json`: JSON string for extraction configuration overrides
- `--pixel-step`: Pixel sampling step size for extraction (default from config)
- `--save-pixel-coords`: Save original pixel coordinates in NPZ output (default: True)

**General:**
- `--verbose`: Enable verbose logging

**Output Structure:**

For a CBF file named `lys_nitr_10_6_0491.cbf`, the script creates:

```
output-base-dir/
└── lys_nitr_10_6_0491/
    ├── e2e_visual_check.log                    # Complete pipeline log
    ├── imported.expt                           # DIALS import output
    ├── indexed_initial.expt                    # Initial indexing
    ├── indexed_initial.refl                    # Initial reflections
    ├── indexed_refined_detector.expt           # Final refined experiment
    ├── indexed_refined_detector.refl           # Final refined reflections
    ├── global_pixel_mask.pickle                # Combined pixel mask
    ├── bragg_mask.pickle                       # Bragg peak mask
    ├── total_diffuse_mask.pickle               # Final diffuse analysis mask
    ├── diffuse_data.npz                        # Extracted diffuse data
    └── extraction_diagnostics/                 # Visual diagnostic plots
        ├── diffuse_pixel_overlay.png           # Raw image with diffuse pixels
        ├── q_projection_qx_qy.png              # Q-space projections
        ├── q_projection_qx_qz.png
        ├── q_projection_qy_qz.png
        ├── radial_q_distribution.png           # Intensity vs |Q|
        ├── intensity_histogram.png             # Intensity distributions
        ├── intensity_heatmap_panel_0.png       # Detector heatmap
        ├── sigma_vs_intensity.png              # Error analysis
        ├── isigi_histogram.png                 # I/σ distribution
        ├── intensity_correction_summary.txt    # Correction effects
        └── extraction_diagnostics_summary.txt  # Overall summary
```

**Configuration Examples:**

Static mask configuration (JSON):
```json
{
  "beamstop": {
    "type": "circle",
    "center_x": 1250,
    "center_y": 1250,
    "radius": 50
  },
  "untrusted_rects": [
    {"min_x": 0, "max_x": 10, "min_y": 0, "max_y": 10}
  ],
  "untrusted_panels": [1, 3]
}
```

Bragg mask configuration (JSON):
```json
{
  "border": 3,
  "algorithm": "simple"
}
```

Extraction configuration (JSON):
```json
{
  "pixel_step": 2,
  "min_intensity": 10.0,
  "max_intensity": 100000.0,
  "lp_correction_enabled": true,
  "plot_diagnostics": false
}
```

**Error Handling:**

The script includes comprehensive error handling for each phase:
- **Phase 1 Failures**: DIALS processing errors, mask generation failures
- **Phase 2 Failures**: Data extraction errors, file I/O issues
- **Phase 3 Failures**: Visual diagnostic script errors (non-fatal)

If any phase fails, detailed error information is logged and the script exits gracefully.

**Integration with Visual Diagnostics:**

The script automatically invokes `check_diffuse_extraction.py` with the correct file paths and arguments, providing:
- Raw image overlay with extracted diffuse pixels
- Q-space coverage analysis
- Intensity distribution validation
- Error propagation verification
- Pixel correction effect visualization

**Development Use Cases:**

1. **Phase 2 Implementation Validation**: Verify that DataExtractor correctly processes real crystallographic data
2. **Parameter Optimization**: Test different extraction and masking parameters
3. **Regression Testing**: Generate reference outputs for automated testing
4. **Debugging**: Identify issues in the complete processing pipeline
5. **Documentation**: Generate example outputs for documentation

**Dependencies:**

- DIALS must be properly installed and importable
- All DiffusePipe components must be available
- Sufficient disk space for intermediate files and plots
- CBF input files with proper crystallographic headers

**Performance Notes:**

Processing time depends on:
- CBF image size and detector geometry
- Pixel step size (lower = more pixels = longer processing)
- DIALS processing complexity (number of spots, indexing difficulty)
- Visual diagnostic generation

Typical processing times:
- Small detector (1M pixels): 2-5 minutes
- Large detector (6M pixels): 5-15 minutes
- With pixel_step=1: Longer processing but full resolution
- With pixel_step=2: Faster processing, reduced resolution

## Future Enhancements

Potential improvements for this workflow:
- Multi-panel detector support
- Batch processing of multiple CBF files
- Automated parameter optimization
- Integration with automated testing frameworks
- Performance profiling and optimization
- Support for different detector formats beyond CBF
</file>

<file path="scripts/dev_workflows/run_phase3_e2e_visual_check.py">
#!/usr/bin/env python3
"""
End-to-End Visual Check Script for Phase 3 DiffusePipe Processing

This script orchestrates the complete Phase 1 (DIALS processing, masking), 
Phase 2 (DataExtractor), and Phase 3 (voxelization, relative scaling, merging) 
pipeline for multiple CBF images, then runs visual diagnostics to verify the 
correctness of the voxelization and merging processes.

The script provides an automated pathway to:
1. Process CBF images through DIALS (import, find spots, index, refine)
2. Generate pixel masks (static + dynamic) and Bragg masks
3. Extract diffuse scattering data with pixel corrections
4. Define global voxel grid and bin data into voxels
5. Perform relative scaling of binned observations
6. Merge relatively scaled data into final voxel map
7. Generate comprehensive Phase 3 visual diagnostics

This is particularly useful for:
- Validating Phase 3 implementation on real data
- Debugging voxelization and scaling pipeline issues
- Generating reference outputs for testing
- Development workflow verification

Author: DiffusePipe
"""

import argparse
import json
import logging
import pickle
import subprocess
import sys
from pathlib import Path
from typing import Dict, Any, Optional, List, Tuple

# Add project src to path for imports
sys.path.insert(0, str(Path(__file__).resolve().parent.parent.parent / "src"))

try:
    # DIALS/DXTBX imports
    from dxtbx.imageset import ImageSetFactory
    from dxtbx.model.experiment_list import ExperimentListFactory

    # Project component imports
    from diffusepipe.crystallography.still_processing_and_validation import (
        StillProcessorAndValidatorComponent,
        create_default_config,
        create_default_extraction_config,
    )
    from diffusepipe.types.types_IDL import (
        ExtractionConfig,
        ComponentInputFiles,
    )
    from diffusepipe.masking.pixel_mask_generator import (
        PixelMaskGenerator,
        create_default_static_params,
        create_default_dynamic_params,
    )
    from diffusepipe.masking.bragg_mask_generator import (
        BraggMaskGenerator,
        create_default_bragg_mask_config,
    )
    from diffusepipe.extraction.data_extractor import DataExtractor

except ImportError as e:
    print(f"Error importing required modules: {e}")
    print(
        "Please ensure DIALS is properly installed and the project is set up correctly."
    )
    sys.exit(1)

logger = logging.getLogger(__name__)


def parse_arguments() -> argparse.Namespace:
    """Parse command-line arguments."""
    parser = argparse.ArgumentParser(
        description="End-to-end Phase 3 visual check pipeline",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  # Basic usage with multiple CBF files
  python run_phase3_e2e_visual_check.py \\
    --cbf-image-paths ../../747/lys_nitr_10_6_0491.cbf ../../747/lys_nitr_10_6_0492.cbf \\
    --output-base-dir ./e2e_phase3_outputs \\
    --pdb-path ../../6o2h.pdb

  # With custom configurations
  python run_phase3_e2e_visual_check.py \\
    --cbf-image-paths image1.cbf image2.cbf image3.cbf \\
    --output-base-dir ./outputs \\
    --dials-phil-path custom_dials.phil \\
    --static-mask-config '{"beamstop": {"type": "circle", "center_x": 1250, "center_y": 1250, "radius": 50}}' \\
    --bragg-mask-config '{"border": 3}' \\
    --extraction-config-json '{"pixel_step": 2}' \\
    --relative-scaling-config-json '{"enable_res_smoother": true}' \\
    --grid-config-json '{"ndiv_h": 64, "ndiv_k": 64, "ndiv_l": 64}' \\
    --save-intermediate-phase-outputs \\
    --verbose

  # Using shoebox-based Bragg masking
  python run_phase3_e2e_visual_check.py \\
    --cbf-image-paths image1.cbf image2.cbf \\
    --output-base-dir ./outputs \\
    --use-bragg-mask-option-b \\
    --verbose
        """,
    )

    # Required arguments
    parser.add_argument(
        "--cbf-image-paths", 
        nargs="+", 
        required=True, 
        help="Paths to input CBF image files"
    )
    parser.add_argument(
        "--output-base-dir",
        type=str,
        required=True,
        help="Base output directory (unique subdirectory will be created)",
    )

    # Optional DIALS/processing arguments
    parser.add_argument(
        "--dials-phil-path",
        type=str,
        help="Path to custom DIALS PHIL configuration file",
    )
    parser.add_argument(
        "--pdb-path", type=str, help="Path to external PDB file for validation"
    )

    # Masking configuration arguments
    parser.add_argument(
        "--static-mask-config",
        type=str,
        help="JSON string for static mask configuration (StaticMaskParams)",
    )
    parser.add_argument(
        "--bragg-mask-config", type=str, help="JSON string for Bragg mask configuration"
    )
    parser.add_argument(
        "--use-bragg-mask-option-b",
        action="store_true",
        help="Use shoebox-based Bragg masking (requires shoeboxes from DIALS)",
    )

    # Extraction configuration arguments
    parser.add_argument(
        "--extraction-config-json",
        type=str,
        help="JSON string for extraction configuration overrides",
    )

    # Phase 3 configuration arguments
    parser.add_argument(
        "--relative-scaling-config-json",
        type=str,
        help="JSON string for relative scaling configuration",
    )
    parser.add_argument(
        "--grid-config-json",
        type=str,
        help="JSON string for global voxel grid configuration",
    )

    # Output control arguments
    parser.add_argument(
        "--save-intermediate-phase-outputs",
        action="store_true",
        help="Save intermediate Phase 1 & 2 outputs for inspection",
    )

    # General arguments
    parser.add_argument("--verbose", action="store_true", help="Enable verbose logging")

    return parser.parse_args()


def setup_logging(log_file_path: Path, verbose: bool = False) -> None:
    """Set up logging configuration."""
    log_level = logging.DEBUG if verbose else logging.INFO

    # Create formatter
    formatter = logging.Formatter(
        "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
    )

    # File handler
    file_handler = logging.FileHandler(log_file_path)
    file_handler.setLevel(log_level)
    file_handler.setFormatter(formatter)

    # Console handler
    console_handler = logging.StreamHandler()
    console_handler.setLevel(log_level)
    console_handler.setFormatter(formatter)

    # Configure root logger
    logging.basicConfig(level=log_level, handlers=[file_handler, console_handler])

    logger.info(f"Logging configured. Log file: {log_file_path}")


def parse_json_config(json_str: Optional[str], config_name: str) -> Dict[str, Any]:
    """Parse JSON configuration string."""
    if not json_str:
        return {}

    try:
        config = json.loads(json_str)
        logger.info(f"Parsed {config_name} configuration: {config}")
        return config
    except json.JSONDecodeError as e:
        logger.error(f"Failed to parse {config_name} JSON: {e}")
        raise


def extract_pdb_symmetry(pdb_path: str) -> tuple[Optional[str], Optional[str]]:
    """
    Extract unit cell and space group from PDB file.

    Returns:
        tuple[Optional[str], Optional[str]]: (unit_cell_string, space_group_string)
    """
    try:
        from iotbx import pdb

        pdb_input = pdb.input(file_name=pdb_path)
        crystal_symmetry = pdb_input.crystal_symmetry()

        if crystal_symmetry is None:
            logger.warning(f"No crystal symmetry found in PDB file: {pdb_path}")
            return None, None

        unit_cell = crystal_symmetry.unit_cell()
        space_group = crystal_symmetry.space_group()

        # Format unit cell parameters as comma-separated string
        uc_params = unit_cell.parameters()
        unit_cell_string = f"{uc_params[0]:.3f},{uc_params[1]:.3f},{uc_params[2]:.3f},{uc_params[3]:.2f},{uc_params[4]:.2f},{uc_params[5]:.2f}"

        # Get space group symbol
        space_group_string = space_group.type().lookup_symbol()

        logger.info(
            f"Extracted from PDB: unit_cell={unit_cell_string}, space_group={space_group_string}"
        )
        return unit_cell_string, space_group_string

    except Exception as e:
        logger.warning(f"Failed to extract symmetry from PDB file {pdb_path}: {e}")
        return None, None


def run_phase1_processing_for_multiple_images(
    args: argparse.Namespace, output_dir: Path
) -> List[Dict[str, Any]]:
    """
    Run Phase 1 DIALS processing and masking for multiple CBF images.

    Returns:
        List of dictionaries containing processing results for each image
    """
    logger.info("=== Phase 1: DIALS Processing and Masking (Multiple Images) ===")

    # Extract known symmetry from PDB if provided
    known_unit_cell = None
    known_space_group = None
    if args.pdb_path:
        known_unit_cell, known_space_group = extract_pdb_symmetry(args.pdb_path)

    results = []

    for i, cbf_image_path in enumerate(args.cbf_image_paths):
        logger.info(f"Processing image {i+1}/{len(args.cbf_image_paths)}: {cbf_image_path}")
        
        # Create image-specific output directory
        cbf_path = Path(cbf_image_path)
        image_output_dir = output_dir / f"still_{i:03d}_{cbf_path.stem}"
        image_output_dir.mkdir(parents=True, exist_ok=True)

        try:
            # DIALS Processing & Validation
            processor = StillProcessorAndValidatorComponent()

            # Create DIALS configuration with known symmetry
            dials_config = create_default_config(
                known_unit_cell=known_unit_cell, known_space_group=known_space_group
            )
            if args.dials_phil_path:
                dials_config.stills_process_phil_path = args.dials_phil_path
            if args.use_bragg_mask_option_b:
                dials_config.output_shoeboxes = True

            # Create extraction config for validation
            extraction_config = create_default_extraction_config()

            # Process the still
            still_outcome = processor.process_and_validate_still(
                image_path=cbf_image_path,
                config=dials_config,
                extraction_config=extraction_config,
                external_pdb_path=args.pdb_path,
                output_dir=str(image_output_dir),
            )

            # Check outcome
            if still_outcome.status != "SUCCESS":
                logger.error(f"DIALS processing failed for {cbf_image_path}: {still_outcome.status}")
                logger.error(f"Error details: {still_outcome.message}")
                # Continue with other images instead of raising
                continue

            # Identify output files
            expt_file = image_output_dir / "indexed_refined_detector.expt"
            refl_file = image_output_dir / "indexed_refined_detector.refl"

            # Verify files exist
            if not expt_file.exists() or not refl_file.exists():
                logger.error(f"Expected output files not found for {cbf_image_path}")
                continue

            # Generate masks
            mask_results = run_mask_generation_for_image(
                args, image_output_dir, cbf_image_path, expt_file, refl_file
            )

            # Store results
            result = {
                "cbf_image_path": cbf_image_path,
                "image_output_dir": image_output_dir,
                "expt_file": expt_file,
                "refl_file": refl_file,
                "status": "SUCCESS",
                **mask_results
            }
            results.append(result)

            logger.info(f"Successfully processed {cbf_image_path}")

        except Exception as e:
            logger.error(f"Failed to process {cbf_image_path}: {e}")
            # Continue with other images
            continue

    logger.info(f"Phase 1 completed: {len(results)}/{len(args.cbf_image_paths)} images processed successfully")
    return results


def run_mask_generation_for_image(
    args: argparse.Namespace, 
    image_output_dir: Path, 
    cbf_image_path: str,
    expt_file: Path, 
    refl_file: Path
) -> Dict[str, Path]:
    """
    Generate masks for a single image.

    Returns:
        Dictionary containing mask file paths
    """
    logger.info(f"Generating masks for {cbf_image_path}")

    # Load experiment and reflection data
    experiments = ExperimentListFactory.from_json_file(str(expt_file))
    experiment = experiments[0]
    detector = experiment.detector

    # Load raw CBF image
    imageset = ImageSetFactory.new([cbf_image_path])[0]

    # Pixel Mask Generation
    pixel_generator = PixelMaskGenerator()

    # Parse static mask configuration
    static_config = parse_json_config(args.static_mask_config, "static mask")
    if static_config:
        static_params = create_default_static_params()
        for key, value in static_config.items():
            if hasattr(static_params, key):
                setattr(static_params, key, value)
    else:
        static_params = create_default_static_params()

    # Use default dynamic parameters
    dynamic_params = create_default_dynamic_params()

    # Generate combined pixel mask
    global_pixel_mask_tuple = pixel_generator.generate_combined_pixel_mask(
        detector=detector,
        static_params=static_params,
        representative_images=[imageset],
        dynamic_params=dynamic_params,
    )

    # Save pixel mask
    pixel_mask_path = image_output_dir / "global_pixel_mask.pickle"
    with open(pixel_mask_path, "wb") as f:
        pickle.dump(global_pixel_mask_tuple, f)

    # Bragg Mask Generation
    bragg_generator = BraggMaskGenerator()

    # Parse Bragg mask configuration
    bragg_config_overrides = parse_json_config(args.bragg_mask_config, "Bragg mask")
    bragg_config = create_default_bragg_mask_config()
    bragg_config.update(bragg_config_overrides)

    # Load reflection data
    from dials.array_family import flex
    reflections = flex.reflection_table.from_file(str(refl_file))

    # Generate Bragg mask
    if args.use_bragg_mask_option_b:
        bragg_mask_tuple = bragg_generator.generate_bragg_mask_from_shoeboxes(
            reflections=reflections, detector=detector
        )
    else:
        bragg_mask_tuple = bragg_generator.generate_bragg_mask_from_spots(
            experiment=experiment, reflections=reflections, config=bragg_config
        )

    # Save Bragg mask
    bragg_mask_path = image_output_dir / "bragg_mask.pickle"
    with open(bragg_mask_path, "wb") as f:
        pickle.dump(bragg_mask_tuple, f)

    # Total Diffuse Mask Generation
    total_diffuse_mask_tuple = bragg_generator.get_total_mask_for_still(
        bragg_mask=bragg_mask_tuple,
        global_pixel_mask=global_pixel_mask_tuple,
    )

    # Save total diffuse mask
    total_diffuse_mask_path = image_output_dir / "total_diffuse_mask.pickle"
    with open(total_diffuse_mask_path, "wb") as f:
        pickle.dump(total_diffuse_mask_tuple, f)

    return {
        "pixel_mask_path": pixel_mask_path,
        "bragg_mask_path": bragg_mask_path,
        "total_diffuse_mask_path": total_diffuse_mask_path
    }


def run_phase2_data_extraction(
    args: argparse.Namespace,
    phase1_results: List[Dict[str, Any]],
    output_dir: Path,
) -> List[Dict[str, Any]]:
    """
    Run Phase 2 data extraction for all successfully processed images.

    Returns:
        List of dictionaries with Phase 2 results
    """
    logger.info("=== Phase 2: Data Extraction (Multiple Images) ===")

    phase2_results = []

    for result in phase1_results:
        cbf_image_path = result["cbf_image_path"]
        image_output_dir = result["image_output_dir"]
        expt_file = result["expt_file"]
        total_diffuse_mask_path = result["total_diffuse_mask_path"]

        logger.info(f"Extracting diffuse data from {cbf_image_path}")

        try:
            # Instantiate DataExtractor
            data_extractor = DataExtractor()

            # Create ComponentInputFiles
            component_inputs = ComponentInputFiles(
                cbf_image_path=cbf_image_path,
                dials_expt_path=str(expt_file),
                bragg_mask_path=str(total_diffuse_mask_path),
                external_pdb_path=args.pdb_path,
            )

            # Create ExtractionConfig
            extraction_config = create_default_extraction_config()
            extraction_config.save_original_pixel_coordinates = True

            # Apply JSON overrides if provided
            if args.extraction_config_json:
                json_overrides = parse_json_config(
                    args.extraction_config_json, "extraction config"
                )
                config_dict = extraction_config.model_dump()
                config_dict.update(json_overrides)
                extraction_config = ExtractionConfig(**config_dict)

            # Define output NPZ path
            output_npz_path = image_output_dir / "corrected_diffuse_pixel_data.npz"

            # Extract diffuse data
            data_extractor_outcome = data_extractor.extract_from_still(
                inputs=component_inputs,
                config=extraction_config,
                output_npz_path=str(output_npz_path),
            )

            # Check outcome
            if data_extractor_outcome.status != "SUCCESS":
                logger.error(f"Data extraction failed for {cbf_image_path}: {data_extractor_outcome.status}")
                continue

            # Store results
            phase2_result = {
                **result,  # Include Phase 1 results
                "corrected_npz_path": output_npz_path,
                "phase2_status": "SUCCESS"
            }
            phase2_results.append(phase2_result)

            logger.info(f"Successfully extracted data from {cbf_image_path}")

        except Exception as e:
            logger.error(f"Failed to extract data from {cbf_image_path}: {e}")
            continue

    logger.info(f"Phase 2 completed: {len(phase2_results)} images processed successfully")
    return phase2_results


def run_phase3_voxelization_and_scaling(
    args: argparse.Namespace,
    phase2_results: List[Dict[str, Any]],
    output_dir: Path,
) -> Dict[str, Path]:
    """
    Run Phase 3 voxelization, relative scaling, and merging.

    Returns:
        Dictionary containing Phase 3 output file paths
    """
    logger.info("=== Phase 3: Voxelization, Relative Scaling, and Merging ===")

    # Note: This is a placeholder implementation since actual Phase 3 components
    # would need to be implemented. For now, we'll create mock output files
    # that demonstrate the expected structure.

    logger.warning("Phase 3 implementation is currently a placeholder")
    logger.warning("Creating mock output files for diagnostic testing")

    # Create mock Phase 3 outputs
    phase3_outputs = create_mock_phase3_outputs(phase2_results, output_dir)

    logger.info("Phase 3 completed (mock implementation)")
    return phase3_outputs


def create_mock_phase3_outputs(
    phase2_results: List[Dict[str, Any]], 
    output_dir: Path
) -> Dict[str, Path]:
    """
    Create mock Phase 3 output files for testing the diagnostic system.
    
    Returns:
        Dictionary containing paths to mock output files
    """
    import numpy as np

    # Mock GlobalVoxelGrid definition
    grid_definition = {
        "crystal_avg_ref": {
            "unit_cell_params": [78.9, 78.9, 37.1, 90.0, 90.0, 90.0],
            "space_group": "P43212",
            "setting_matrix_a": [[1.0, 0.0, 0.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0]]
        },
        "hkl_bounds": {
            "h_min": -32, "h_max": 32,
            "k_min": -32, "k_max": 32,
            "l_min": -16, "l_max": 16
        },
        "ndiv_h": 64, "ndiv_k": 64, "ndiv_l": 32,
        "total_voxels": 131072
    }
    
    grid_definition_path = output_dir / "global_voxel_grid_definition.json"
    with open(grid_definition_path, "w") as f:
        json.dump(grid_definition, f, indent=2)

    # Mock ScalingModel refined parameters
    scaling_params = {
        "n_stills": len(phase2_results),
        "per_still_scales": {
            f"still_{i:03d}": {
                "b_i": 1.0 + 0.1 * np.random.randn(),  # Scale factor around 1.0
                "status": "converged"
            }
            for i in range(len(phase2_results))
        },
        "resolution_smoother": {
            "enabled": True,
            "control_points_q": [0.1, 0.2, 0.3, 0.4, 0.5],
            "control_points_a": [1.0, 0.98, 0.95, 0.92, 0.89]
        },
        "convergence_info": {
            "n_iterations": 5,
            "final_r_factor": 0.152
        }
    }
    
    scaling_params_path = output_dir / "refined_scaling_model_params.json"
    with open(scaling_params_path, "w") as f:
        json.dump(scaling_params, f, indent=2)

    # Mock VoxelData_relative
    n_voxels = grid_definition["total_voxels"]
    
    # Create realistic mock data
    np.random.seed(42)  # For reproducibility
    
    # Only populate a fraction of voxels (realistic for diffuse data)
    n_populated = n_voxels // 10
    voxel_indices = np.random.choice(n_voxels, n_populated, replace=False)
    
    # Mock intensities with realistic distribution
    base_intensities = np.random.exponential(scale=100, size=n_populated)
    intensities = np.full(n_voxels, np.nan)
    intensities[voxel_indices] = base_intensities
    
    # Mock sigmas (roughly Poisson-like relationship)
    sigmas = np.full(n_voxels, np.nan)
    sigmas[voxel_indices] = np.sqrt(base_intensities) + 0.1 * base_intensities
    
    # Mock observation counts
    num_observations = np.zeros(n_voxels, dtype=int)
    num_observations[voxel_indices] = np.random.poisson(lam=3, size=n_populated) + 1
    
    # Mock q-vectors for voxel centers
    h_coords = np.random.randint(-32, 33, n_voxels)
    k_coords = np.random.randint(-32, 33, n_voxels)
    l_coords = np.random.randint(-16, 17, n_voxels)
    
    # Convert HKL to q-vectors (simplified)
    q_centers_x = h_coords * 0.05  # Mock conversion factor
    q_centers_y = k_coords * 0.05
    q_centers_z = l_coords * 0.1
    q_magnitudes = np.sqrt(q_centers_x**2 + q_centers_y**2 + q_centers_z**2)
    
    voxel_data_path = output_dir / "voxel_data_relative.npz"
    np.savez_compressed(
        voxel_data_path,
        I_merged_relative=intensities,
        Sigma_merged_relative=sigmas,
        num_observations_in_voxel=num_observations,
        q_center_x=q_centers_x,
        q_center_y=q_centers_y,
        q_center_z=q_centers_z,
        q_magnitude_center=q_magnitudes,
        H_center=h_coords,
        K_center=k_coords,
        L_center=l_coords,
        voxel_indices=np.arange(n_voxels)
    )

    logger.info(f"Created mock grid definition: {grid_definition_path}")
    logger.info(f"Created mock scaling parameters: {scaling_params_path}")
    logger.info(f"Created mock voxel data: {voxel_data_path}")

    return {
        "grid_definition_file": grid_definition_path,
        "scaling_model_params_file": scaling_params_path,
        "voxel_data_file": voxel_data_path
    }


def save_intermediate_outputs_manifest(
    phase2_results: List[Dict[str, Any]], 
    output_dir: Path
) -> None:
    """Save manifest of intermediate outputs if requested."""
    manifest = {
        "phase1_and_phase2_outputs": [
            {
                "cbf_image_path": result["cbf_image_path"],
                "image_output_dir": str(result["image_output_dir"]),
                "expt_file": str(result["expt_file"]),
                "refl_file": str(result["refl_file"]),
                "corrected_npz_path": str(result["corrected_npz_path"]),
                "pixel_mask_path": str(result["pixel_mask_path"]),
                "bragg_mask_path": str(result["bragg_mask_path"]),
                "total_diffuse_mask_path": str(result["total_diffuse_mask_path"])
            }
            for result in phase2_results
        ]
    }
    
    manifest_path = output_dir / "intermediate_outputs_manifest.json"
    with open(manifest_path, "w") as f:
        json.dump(manifest, f, indent=2)
    
    logger.info(f"Saved intermediate outputs manifest: {manifest_path}")


def run_phase3_visual_diagnostics(
    args: argparse.Namespace,
    phase3_outputs: Dict[str, Path],
    phase2_results: List[Dict[str, Any]],
    output_dir: Path,
) -> None:
    """
    Run Phase 3 visual diagnostics check.
    """
    logger.info("=== Phase 3: Visual Diagnostics ===")

    # Construct command for check_phase3_outputs.py
    diagnostics_dir = output_dir / "phase3_diagnostics"

    # Get the script path relative to the project root
    script_path = (
        Path(__file__).resolve().parent.parent
        / "visual_diagnostics"
        / "check_phase3_outputs.py"
    )

    cmd = [
        "python",
        str(script_path),
        "--grid-definition-file",
        str(phase3_outputs["grid_definition_file"]),
        "--scaling-model-params-file",
        str(phase3_outputs["scaling_model_params_file"]),
        "--voxel-data-file",
        str(phase3_outputs["voxel_data_file"]),
        "--output-dir",
        str(diagnostics_dir),
    ]

    # Add optional arguments if intermediate outputs were saved
    if args.save_intermediate_phase_outputs:
        manifest_path = output_dir / "intermediate_outputs_manifest.json"
        if manifest_path.exists():
            cmd.extend(["--experiments-list-file", str(manifest_path)])

    if args.verbose:
        cmd.append("--verbose")

    logger.info(f"Running Phase 3 visual diagnostics: {' '.join(cmd)}")

    # Execute the command
    try:
        result = subprocess.run(
            cmd,
            capture_output=True,
            text=True,
            check=False,
            cwd=str(Path(__file__).resolve().parent.parent.parent),  # Project root
        )

        # Log results
        if result.returncode == 0:
            logger.info("Phase 3 visual diagnostics completed successfully")
        else:
            logger.error(
                f"Phase 3 visual diagnostics failed with return code: {result.returncode}"
            )

        if result.stdout:
            logger.info("Phase 3 diagnostics stdout:")
            for line in result.stdout.strip().split("\n"):
                logger.info(f"  {line}")

        if result.stderr:
            logger.error("Phase 3 diagnostics stderr:")
            for line in result.stderr.strip().split("\n"):
                logger.error(f"  {line}")

        if result.returncode != 0:
            logger.warning("Phase 3 visual diagnostics failed, but continuing...")

    except Exception as e:
        logger.error(f"Failed to run Phase 3 visual diagnostics: {e}")
        logger.warning("Continuing without visual diagnostics...")


def main():
    """Main orchestration function."""
    # Parse arguments
    args = parse_arguments()

    # Validate input files
    for cbf_path in args.cbf_image_paths:
        if not Path(cbf_path).exists():
            logger.error(f"CBF file not found: {cbf_path}")
            sys.exit(1)

    # Create unique output directory
    output_base = Path(args.output_base_dir)
    output_dir = output_base / "phase3_e2e_outputs"
    output_dir.mkdir(parents=True, exist_ok=True)

    # Setup logging
    log_file = output_dir / "phase3_e2e_visual_check.log"
    setup_logging(log_file, args.verbose)

    logger.info("Starting end-to-end Phase 3 visual check pipeline")
    logger.info(f"Input CBF files: {args.cbf_image_paths}")
    logger.info(f"Output directory: {output_dir}")

    try:
        # Phase 1: DIALS Processing and Masking (Multiple Images)
        phase1_results = run_phase1_processing_for_multiple_images(args, output_dir)
        
        if not phase1_results:
            logger.error("No images were successfully processed in Phase 1")
            sys.exit(1)

        # Phase 2: Data Extraction (Multiple Images)
        phase2_results = run_phase2_data_extraction(args, phase1_results, output_dir)
        
        if not phase2_results:
            logger.error("No images were successfully processed in Phase 2")
            sys.exit(1)

        # Save intermediate outputs manifest if requested
        if args.save_intermediate_phase_outputs:
            save_intermediate_outputs_manifest(phase2_results, output_dir)

        # Phase 3: Voxelization, Relative Scaling, and Merging
        phase3_outputs = run_phase3_voxelization_and_scaling(args, phase2_results, output_dir)

        # Phase 3: Visual Diagnostics
        run_phase3_visual_diagnostics(args, phase3_outputs, phase2_results, output_dir)

        logger.info("=== Pipeline Completed Successfully ===")
        logger.info(f"All outputs saved to: {output_dir}")
        logger.info("Generated files:")
        for file_path in sorted(output_dir.rglob("*")):
            if file_path.is_file():
                logger.info(f"  {file_path.relative_to(output_dir)}")

    except Exception as e:
        logger.error(f"Pipeline failed: {e}")
        logger.error("Check the log file for detailed error information")
        sys.exit(1)


if __name__ == "__main__":
    main()
</file>

<file path="src/diffusepipe/adapters/dials_generate_mask_adapter_IDL.md">
// == BEGIN IDL ==
module src.diffusepipe.adapters {

    # @depends_on_resource(type="ExternalTool:DIALS", purpose="Using DIALS masking utilities (dials.util.masking.generate_mask or equivalent)")
    # @depends_on_resource(type="FileSystem", purpose="Writing temporary mask files if needed")
    # @depends_on_type(dxtbx.model.Experiment)
    # @depends_on_type(dials.array_family.flex.reflection_table)
    # @depends_on_type(dials.array_family.flex.bool)

    interface DIALSGenerateMaskAdapter {

        // --- Method: generate_bragg_mask ---
        // Preconditions:
        // - `experiment` must be a valid DIALS Experiment object containing detector geometry and beam information.
        // - `reflections` must be a valid DIALS reflection_table object containing indexed Bragg spots with position data.
        // - If provided, `mask_generation_params` must contain valid key-value pairs for mask generation settings.
        // - The DIALS masking utilities must be available in the Python environment.
        // Postconditions:
        // - If successful (second tuple element is True), returns a tuple of panel masks where each element is a flex.bool array.
        // - The number of masks in the tuple corresponds to the number of detector panels in the experiment.
        // - Each mask array has the same dimensions as its corresponding detector panel.
        // - Pixels marked as True in the mask represent regions occupied by Bragg peaks (to be excluded from diffuse processing).
        // - The third tuple element contains human-readable log messages detailing the mask generation process.
        // - If failed (second tuple element is False), the first element is None.
        // Behavior:
        // - Wraps the DIALS masking functionality to generate Bragg peak exclusion masks.
        // - Uses the `dials.util.masking.generate_mask` utility function or equivalent DIALS masking logic.
        // - Default mask generation parameters are applied if `mask_generation_params` is None:
        //   - Uses reflection positions from the reflection table to define Bragg regions.
        //   - Applies appropriate margin around each reflection to account for peak shape.
        //   - Handles multiple detector panels correctly.
        // - If `mask_generation_params` is provided, merges these with defaults:
        //   - Supports parameters like: border, resolution_range, ice_rings, shadow_angles, etc.
        //   - Parameter names and values should match DIALS masking utility expectations.
        // - Validates the generated mask result:
        //   - Ensures mask dimensions match detector panel dimensions.
        //   - Verifies that reasonable regions are masked (not entire panels, not zero coverage).
        //   - Logs statistics about masked pixel counts per panel.
        // - Returns per-panel boolean masks suitable for downstream diffuse processing exclusion.
        // @raises_error(condition="DIALSError", description="When DIALS masking operations fail or produce invalid results")
        // @raises_error(condition="BraggMaskError", description="When mask generation parameters are invalid or mask validation fails")
        tuple<tuple<object>, boolean, string> generate_bragg_mask(
            object experiment,
            object reflections,
            optional map<string, any> mask_generation_params
        );
    }
}
// == END IDL ==
</file>

<file path="src/diffusepipe/adapters/dxtbx_io_adapter_IDL.md">
// == BEGIN IDL ==
module src.diffusepipe.adapters {

    # @depends_on_resource(type="ExternalTool:DIALS", purpose="Using DXTBX/DIALS Python API for loading and saving experiment/reflection data")
    # @depends_on_resource(type="FileSystem", purpose="Reading and writing experiment (.expt) and reflection (.refl) files")
    # @depends_on_type(dxtbx.model.experiment_list.ExperimentList)
    # @depends_on_type(dials.array_family.flex.reflection_table)

    interface DXTBXIOAdapter {

        // --- Method: load_experiment_list ---
        // Preconditions:
        // - `expt_path` must be a valid path to a readable DIALS experiment JSON file (.expt).
        // - The file must contain valid experiment data compatible with DXTBX format.
        // - The DXTBX Python libraries must be available and properly configured.
        // Postconditions:
        // - Returns a valid DIALS ExperimentList object containing one or more experiments.
        // - The returned object provides access to detector, beam, crystal, and other experimental models.
        // Behavior:
        // - Wraps `dxtbx.model.experiment_list.ExperimentListFactory.from_json_file()`.
        // - Loads the experiment file and parses it into DIALS/DXTBX data structures.
        // - Validates that the loaded data is accessible and well-formed.
        // @raises_error(condition="FileSystemError", description="When the experiment file does not exist, is not readable, or is corrupted")
        // @raises_error(condition="DIALSError", description="When DXTBX fails to parse the experiment file or the format is invalid")
        object load_experiment_list(
            string expt_path
        );

        // --- Method: load_reflection_table ---
        // Preconditions:
        // - `refl_path` must be a valid path to a readable DIALS reflection file (.refl).
        // - The file must contain valid reflection data compatible with DIALS flex format.
        // - The DIALS Python libraries must be available and properly configured.
        // Postconditions:
        // - Returns a valid DIALS reflection_table object containing reflection data.
        // - The returned object provides access to reflection properties (positions, intensities, etc.).
        // Behavior:
        // - Wraps `dials.array_family.flex.reflection_table.from_file()`.
        // - Loads the reflection file and parses it into a DIALS reflection_table data structure.
        // - Validates that the loaded data is accessible and contains expected columns.
        // @raises_error(condition="FileSystemError", description="When the reflection file does not exist, is not readable, or is corrupted")
        // @raises_error(condition="DIALSError", description="When DIALS fails to parse the reflection file or the format is invalid")
        object load_reflection_table(
            string refl_path
        );

        // --- Method: save_experiment_list ---
        // Preconditions:
        // - `experiments` must be a valid DIALS ExperimentList object.
        // - `expt_path` must be a valid file path where the experiment file will be written.
        // - The parent directory of `expt_path` must be writable.
        // - The DXTBX Python libraries must be available and properly configured.
        // Postconditions:
        // - A valid DIALS experiment JSON file is created at `expt_path`.
        // - The file contains all experiment data from the input ExperimentList.
        // - The parent directory is created if it does not exist.
        // Behavior:
        // - Wraps `dxtbx.model.experiment_list.ExperimentListDumper.as_json()`.
        // - Ensures the parent directory exists, creating it if necessary.
        // - Serializes the ExperimentList to JSON format and writes to the specified path.
        // - Validates that the file was written successfully and is readable.
        // @raises_error(condition="DIALSError", description="When DXTBX fails to serialize the experiment data or write the file")
        // @raises_error(condition="FileSystemError", description="When the target path is not writable or directory creation fails")
        void save_experiment_list(
            object experiments,
            string expt_path
        );

        // --- Method: save_reflection_table ---
        // Preconditions:
        // - `reflections` must be a valid DIALS reflection_table object.
        // - `refl_path` must be a valid file path where the reflection file will be written.
        // - The parent directory of `refl_path` must be writable.
        // - The DIALS Python libraries must be available and properly configured.
        // Postconditions:
        // - A valid DIALS reflection file is created at `refl_path`.
        // - The file contains all reflection data from the input reflection_table.
        // - The parent directory is created if it does not exist.
        // Behavior:
        // - Calls the `as_file()` method on the reflection_table object.
        // - Ensures the parent directory exists, creating it if necessary.
        // - Writes the reflection data to the specified path in DIALS native format.
        // - Validates that the file was written successfully and is readable.
        // @raises_error(condition="DIALSError", description="When DIALS fails to serialize the reflection data or write the file")
        // @raises_error(condition="FileSystemError", description="When the target path is not writable or directory creation fails")
        void save_reflection_table(
            object reflections,
            string refl_path
        );
    }
}
// == END IDL ==
</file>

<file path="src/diffusepipe/config/find_spots.phil">
spotfinder {
  threshold {
    algorithm = dispersion
    dispersion {
      sigma_strong = 3 # Or your desired value
      # You can also set other dispersion parameters here if needed,
      # e.g., sigma_background, kernel_size, min_count
    }
  }
}
</file>

<file path="src/diffusepipe/config/refine_detector.phil">
refinement {
  parameterisation {
    detector {
      fix = all
      hierarchy_level = 0 
    }
    beam {
      fix = all
    }
    crystal {
      fix = cell // Keep cell fixed for now
    }
    scan_varying = False 
  }
  reflections {
    outlier {
      algorithm = null
    }
  }
}
</file>

<file path="src/diffusepipe/config/sequence_find_spots_default.phil">
# Base PHIL parameters for dials.find_spots when processing sequence data
# These parameters are optimized for oscillation data and include critical overrides

# Spot finding algorithm - CRITICAL for sequence data
spotfinder {
  # CRITICAL: Use dispersion algorithm (not default)
  threshold {
    algorithm = dispersion
    
    # Dispersion-specific parameters
    dispersion {
      # Default parameters work well for most cases
    }
  }
  
  # CRITICAL: Minimum spot size (not default 2)
  filter {
    min_spot_size = 3
    
    # Additional filtering parameters
    d_min = None
    d_max = None
    ice_rings = False
  }
  
  # Region of interest (if needed)
  region_of_interest = []
  
  # Scan range - use all images by default
  scan_range = []
}

# Output settings
output {
  reflections = "strong.refl"
  shoeboxes = False
  log = "dials.find_spots.log"
  debug_log = "dials.find_spots.debug.log"
}

# Performance settings
nproc = 1
</file>

<file path="src/diffusepipe/config/sequence_import_default.phil">
# Base PHIL parameters for dials.import when processing sequence data
# These parameters are optimized for oscillation data (Angle_increment > 0.0°)

# Input/Output parameters (will be overridden by adapter)
input {
  # CBF file path will be provided by DIALSSequenceProcessAdapter
}

# Geometry settings for sequence data
geometry {
  # CRITICAL: Do not convert sequences to stills - preserve oscillation information
  convert_sequences_to_stills = false
  
  # Allow detector geometry refinement if needed
  refine_detector_geometry = false
}

# Import behavior
import {
  # Handle potential file format variations
  check_format = true
  
  # Template validation
  template = *auto
}

# Logging
output {
  log = "dials.import.log"
  debug_log = "dials.import.debug.log"
}
</file>

<file path="src/diffusepipe/config/sequence_index_default.phil">
# Base PHIL parameters for dials.index when processing sequence data
# These parameters are optimized for oscillation data with critical method selection

# Indexing method - CRITICAL for sequence data
indexing {
  # CRITICAL: Use fft3d method (not fft1d)
  method = fft3d
  
  # Multiple lattice search settings
  multiple_lattice_search {
    max_lattices = 1
  }
  
  # Known symmetry (will be overridden if provided by config)
  known_symmetry {
    unit_cell = None
    space_group = None
  }
  
  # Refinement settings
  refinement_protocol {
    d_min_start = None
    n_macro_cycles = 1
  }
  
  # Basis vector combinations to try
  basis_vector_combinations {
    max_combinations = None
  }
}

# Geometry refinement during indexing
geometry {
  # CRITICAL: Preserve sequence structure
  convert_sequences_to_stills = false
  
  # Detector refinement
  detector_fix = *all in_beam_plane out_beam_plane
  
  # Beam refinement  
  beam_fix = *all in_spindle_plane out_spindle_plane
}

# Output settings
output {
  experiments = "indexed.expt"
  reflections = "indexed.refl"
  log = "dials.index.log"
  debug_log = "dials.index.debug.log"
}
</file>

<file path="src/diffusepipe/config/sequence_integrate_default.phil">
# Base PHIL parameters for dials.integrate when processing sequence data
# These parameters are optimized for oscillation data integration

# Integration algorithm
integration {
  # Integration algorithm selection
  algorithm = *auto 3d summation
  
  # Profile fitting (for better data quality)
  profile_fitting = True
  
  # Background determination
  background {
    algorithm = *auto simple glm robust
    
    # Simple background parameters
    simple {
      outlier_algorithm = *null tukey normal
    }
  }
  
  # Summation integration parameters
  summation {
    # Calculate partialities for quality assessment
    estimate_partiality = True
  }
  
  # Shoebox parameters
  shoebox_size {
    algorithm = *auto bbox
  }
  
  # Prediction parameters
  prediction {
    padding = 1.0
  }
}

# Geometry settings
geometry {
  # CRITICAL: Preserve sequence structure (consistency with other steps)
  convert_sequences_to_stills = false
}

# Profile modeling
profile {
  algorithm = *auto gaussian_rs
}

# Output settings
output {
  experiments = "integrated.expt"
  reflections = "integrated.refl"
  log = "dials.integrate.log"
  debug_log = "dials.integrate.debug.log"
}

# Performance settings
nproc = 1

# Include/exclude settings
include_bad_reference = False
</file>

<file path="src/diffusepipe/crystallography/model_validator_IDL.md">
// == BEGIN IDL ==
module src.diffusepipe.crystallography {

    # @depends_on([src.diffusepipe.crystallography.QConsistencyChecker])
    # @depends_on_resource(type="DIALS/dxtbx", purpose="Using Experiment and reflection_table objects, crystal models")
    # @depends_on_resource(type="FileSystem", purpose="Reading PDB files, writing diagnostic plots")
    # @depends_on_resource(type="ExternalLibrary:iotbx", purpose="PDB file parsing and crystal symmetry analysis")
    # @depends_on_type(src.diffusepipe.types.ExtractionConfig)
    # @depends_on_type(src.diffusepipe.exceptions.PDBError) // For use in @raises_error

    // Container for validation metrics and results
    struct ValidationMetrics {
        // PDB consistency check results
        pdb_cell_passed: optional boolean;         // Result of unit cell comparison (null if not performed)
        pdb_orientation_passed: optional boolean;  // Result of orientation comparison (null if not performed)
        misorientation_angle_vs_pdb: optional float; // Misorientation angle in degrees (null if not calculated)
        
        // Q-vector consistency check results  
        q_consistency_passed: optional boolean;    // Result of Q-vector validation (null if not performed)
        mean_delta_q_mag: optional float;         // Mean |Δq| value in Å⁻¹ (null if not calculated)
        max_delta_q_mag: optional float;          // Maximum |Δq| value in Å⁻¹ (null if not calculated)
        median_delta_q_mag: optional float;       // Median |Δq| value in Å⁻¹ (null if not calculated)
        num_reflections_tested: int;              // Number of reflections used in Q-vector test
        
        // Diagnostic output information
        validation_plots_generated: boolean;      // Whether diagnostic plots were created
        plot_paths: map<string, string>;          // Map of plot type to file path
    }

    interface ModelValidator {

        // --- Method: validate_geometry ---
        // Preconditions:
        // - `experiment` is a valid DIALS Experiment object containing crystal model, beam, and detector.
        // - `reflections` is a valid DIALS reflection_table object.
        // - `external_pdb_path` (if provided) must point to a readable PDB file.
        // - `extraction_config` (if provided) must contain valid tolerance parameters.
        // - `output_dir` (if provided) must be a writable directory path for diagnostic plots.
        // Postconditions:
        // - Returns a tuple: (overall_validation_passed: bool, validation_metrics: ValidationMetrics).
        // - `overall_validation_passed` is True only if Q-vector consistency passes AND PDB checks pass (if performed).
        // - `validation_metrics` contains detailed results from all validation sub-checks.
        // - Diagnostic plots are generated in `output_dir` if specified and matplotlib is available.
        // Behavior:
        // - **Primary Validation:** Performs Q-vector consistency check using QConsistencyChecker with tolerance from `extraction_config.q_consistency_tolerance_angstrom_inv` (default: 0.01 Å⁻¹).
        // - **PDB Validation (if external_pdb_path provided):**
        //   1. Calls `_check_pdb_consistency` to compare unit cell parameters using `extraction_config.cell_length_tol` and `extraction_config.cell_angle_tol`.
        //   2. Compares crystal orientation using `extraction_config.orient_tolerance_deg`.
        // - **Diagnostic Output (if output_dir provided):** Calls `_generate_diagnostic_plots` to create visualization files.
        // - **Overall Pass Criteria:** Q-vector consistency must pass, num_reflections_tested > 0, and PDB checks must not fail (if performed).
        // @raises_error(condition="PDBError", description="If PDB file processing fails or contains invalid crystal symmetry information.")
        tuple<boolean, ValidationMetrics> validate_geometry(
            object experiment,                    // DIALS Experiment object
            object reflections,                   // DIALS reflection_table object
            optional string external_pdb_path,   // Path to reference PDB file for comparison
            optional src.diffusepipe.types.ExtractionConfig extraction_config, // Tolerance parameters
            optional string output_dir           // Directory for diagnostic plot output
        );

        // --- Method: _check_pdb_consistency (Internal) ---
        // Preconditions:
        // - `experiment` contains a valid crystal model.
        // - `pdb_path` points to a readable PDB file with crystal symmetry information.
        // - Tolerance parameters are positive values.
        // Postconditions:
        // - Returns tuple: (cell_similarity_passed: bool, orientation_similarity_passed: bool, misorientation_angle_degrees: optional float).
        // Behavior:
        // - Loads PDB crystal symmetry using iotbx.pdb.
        // - Compares unit cell parameters using CCTBX `is_similar_to` method with provided tolerances.
        // - Calculates misorientation angle between experiment A-matrix and PDB reference orientation.
        // - Handles both direct and inverted hand orientations, returning the smaller angle.
        // - Returns (True, True, None) if PDB lacks crystal symmetry information.
        // @raises_error(condition="PDBError", description="If PDB file cannot be read or processed.")
        static tuple<boolean, boolean, optional float> _check_pdb_consistency(
            object experiment,        // DIALS Experiment object
            string pdb_path,         // Path to PDB file
            float cell_length_tol,   // Relative tolerance for unit cell length comparison
            float cell_angle_tol,    // Absolute tolerance in degrees for unit cell angle comparison
            float orient_tolerance_deg // Tolerance in degrees for orientation comparison
        );

        // --- Method: _check_q_consistency (Internal) ---
        // Preconditions:
        // - `experiment` and `reflections` are valid DIALS objects.
        // - `tolerance` is a positive float in Å⁻¹.
        // Postconditions:
        // - Returns tuple: (q_consistency_passed: bool, statistics: map<string, float>).
        // Behavior:
        // - Delegates to QConsistencyChecker.check_q_consistency() method.
        // - Uses the primary Q-vector validation algorithm comparing q_model vs q_observed.
        tuple<boolean, map<string, float>> _check_q_consistency(
            object experiment,       // DIALS Experiment object
            object reflections,      // DIALS reflection_table object
            float tolerance         // Q-vector consistency tolerance in Å⁻¹
        );

        // --- Method: _generate_diagnostic_plots (Internal) ---
        // Preconditions:
        // - `q_stats` contains valid Q-vector consistency statistics.
        // - `output_dir` is a writable directory path.
        // - Matplotlib is available (graceful degradation if not).
        // Postconditions:
        // - Returns map of plot type names to generated file paths.
        // - Creates simple summary visualization plots for validation results.
        // Behavior:
        // - Generates diagnostic plots showing Q-vector consistency results.
        // - Uses matplotlib with 'Agg' backend for headless operation.
        // - Creates plots with validation statistics and pass/fail indicators.
        // - Handles missing matplotlib gracefully by logging warnings.
        static map<string, string> _generate_diagnostic_plots(
            map<string, float> q_stats,     // Q-vector consistency statistics
            optional string pdb_path,       // PDB path for plot annotations
            string output_dir               // Directory for plot output
        );
    }
}
// == END IDL ==
</file>

<file path="src/diffusepipe/crystallography/q_consistency_checker_IDL.md">
// == BEGIN IDL ==
module src.diffusepipe.crystallography {

    # @depends_on_resource(type="DIALS/dxtbx", purpose="Using Experiment and reflection_table objects for q-vector calculation and validation")
    # @depends_on_type(src.diffusepipe.exceptions.MissingReflectionColumns) // For use in @raises_error
    # @depends_on_type(src.diffusepipe.exceptions.NoValidReflections) // For use in @raises_error

    // Statistics returned by q-vector consistency check
    struct QConsistencyStatistics {
        count: int;          // Number of reflections successfully processed
        mean: optional float;   // Mean |Δq| value in Å⁻¹ (null if no valid reflections)
        median: optional float; // Median |Δq| value in Å⁻¹ (null if no valid reflections)
        max: optional float;    // Maximum |Δq| value in Å⁻¹ (null if no valid reflections)
    }

    interface QConsistencyChecker {

        // --- Method: check_q_consistency ---
        // Preconditions:
        // - `experiment` is a valid DIALS Experiment object containing beam and detector models.
        // - `reflections` is a valid DIALS reflection_table object.
        // - `tolerance` is a positive float representing the acceptable mean |Δq| tolerance in Å⁻¹.
        // - `max_reflections` is a positive integer limiting the number of reflections to sample for performance.
        // Postconditions:
        // - Returns a tuple: (validation_passed: bool, statistics: QConsistencyStatistics).
        // - `validation_passed` is True if both mean |Δq| ≤ tolerance AND max |Δq| ≤ 5 × tolerance.
        // - `statistics.count` indicates the number of reflections successfully processed.
        // Behavior:
        // - Validates that required columns ("miller_index", "panel", "s1") exist in the reflection table.
        // - Selects pixel centroid column in priority order: "xyzobs.px.value" > "xyzcal.px".
        // - Randomly samples up to `max_reflections` reflections from the table for performance.
        // - For each sampled reflection:
        //   1. Calculates `q_model = s1 - s0` using the reflection's s1 vector and beam's s0.
        //   2. Obtains observed pixel coordinates (px, py) from the selected centroid column.
        //   3. Converts pixel coordinates to lab frame using `detector[panel_id].get_pixel_lab_coord()`.
        //   4. Calculates observed scatter vector `s1_obs` as unit direction vector scaled by |s0|.
        //   5. Calculates `q_observed = s1_obs - s0`.
        //   6. Computes `|Δq| = |q_model - q_observed|`.
        // - Aggregates statistics (mean, median, max) of all |Δq| values.
        // - Applies pass/fail criteria: mean ≤ tolerance AND max ≤ 5 × tolerance.
        // @raises_error(condition="MissingReflectionColumns", description="If essential columns ('miller_index', 'panel', 's1') are missing from the reflection table.")
        // @raises_error(condition="NoValidReflections", description="If no reflections can be processed (empty table or all processing attempts fail).")
        tuple<boolean, QConsistencyStatistics> check_q_consistency(
            object experiment,        // DIALS Experiment object
            object reflections,       // DIALS reflection_table object  
            float tolerance,          // Acceptable mean |Δq| tolerance in Å⁻¹ (default: 0.01)
            int max_reflections      // Maximum reflections to sample (default: 500)
        );
    }
}
// == END IDL ==
</file>

<file path="src/diffusepipe/diagnostics/q_consistency_checker.py">
"""
Q-vector consistency checker for geometric validation.

Implements Module 1.S.1.Validation from the plan.
"""

import logging
from typing import Tuple, Optional
from dataclasses import dataclass

logger = logging.getLogger(__name__)


@dataclass
class QConsistencyStatistics:
    """Statistics from q-vector consistency check."""
    count: int
    mean: Optional[float]
    median: Optional[float]
    max: Optional[float]


class ConsistencyChecker:
    """Stub implementation of QConsistencyChecker."""
    
    def check_q_consistency(self, inputs, verbose, working_directory):
        """
        Stub method for Q-vector consistency checking.
        
        This is a placeholder implementation. The full implementation
        would validate q-vectors as described in the IDL.
        """
        logger.info(f"Running Q-vector consistency check in {working_directory}")
        
        # For now, just return success
        return {
            'status': 'SUCCESS',
            'message': 'Q-vector consistency check completed (stub)',
            'artifacts': {}
        }
</file>

<file path="src/diffusepipe/extraction/__init__.py">
# This file makes src/diffusepipe/extraction a Python package.
</file>

<file path="src/diffusepipe/masking/bragg_mask_generator_IDL.md">
// == BEGIN IDL ==
module src.diffusepipe.masking {

    # @depends_on_resource(type="DIALS/dxtbx", purpose="Using Experiment, reflection_table, Detector objects, flex arrays, MaskCode")
    # @depends_on([src.diffusepipe.adapters.DIALSGenerateMaskAdapter]) // For Option A
    # @depends_on_type(src.diffusepipe.exceptions.BraggMaskError) // For use in @raises_error

    interface BraggMaskGenerator {

        // --- Method: generate_bragg_mask_from_spots (Option A for Bragg masking) ---
        // Preconditions:
        // - `experiment` is a valid DIALS Experiment object containing the crystal model and geometry.
        // - `reflections` is a valid DIALS reflection_table containing indexed spot positions.
        // - `config` (if provided) is a dictionary containing valid parameters for `dials.util.masking.generate_mask` (e.g., 'border', 'algorithm').
        // Postconditions:
        // - Returns a tuple of `flex.bool` Bragg mask arrays, one for each panel in the detector.
        // - In the returned Bragg masks, `True` indicates a pixel considered part of a Bragg reflection region.
        // Behavior:
        // - This method implements Option A for Bragg masking as defined in `plan.md`.
        // - It utilizes the `DIALSGenerateMaskAdapter` to invoke `dials.util.masking.generate_mask` or equivalent DIALS functionality.
        // - Generates a mask that covers regions occupied by the indexed Bragg spots found in `reflections`.
        // @raises_error(condition="BraggMaskError", description="If DIALS mask generation via the adapter fails or input objects are invalid.")
        tuple<object> generate_bragg_mask_from_spots( // conceptual return: tuple<flex.bool, ...>
            object experiment, // DIALS Experiment object
            object reflections, // DIALS reflection_table object
            optional map<string, any> config // Parameters for DIALSGenerateMaskAdapter
        );

        // --- Method: generate_bragg_mask_from_shoeboxes (Option B for Bragg masking) ---
        // Preconditions:
        // - `reflections` is a valid DIALS reflection_table that includes a 'shoebox' column. Each shoebox must contain 3D mask data.
        // - `detector` is a valid DIALS Detector object, used to determine panel dimensions for the output masks.
        // Postconditions:
        // - Returns a tuple of `flex.bool` Bragg mask arrays, one for each panel.
        // - `True` in the mask indicates a pixel that falls within the foreground or strong region of any reflection's shoebox.
        // Behavior:
        // - This method implements Option B for Bragg masking as defined in `plan.md`.
        // - Initializes per-panel 2D masks to all False.
        // - Iterates through each reflection in the `reflections` table.
        // - For each reflection, accesses its 3D shoebox mask.
        // - Pixels within the shoebox marked with `MaskCode.Foreground` or `MaskCode.Strong` are projected onto the corresponding 2D panel mask, setting those pixels to True.
        // @raises_error(condition="BraggMaskError", description="If the 'shoebox' column is missing from `reflections`, or if shoebox data is invalid or processing fails.")
        tuple<object> generate_bragg_mask_from_shoeboxes( // conceptual return: tuple<flex.bool, ...>
            object reflections, // DIALS reflection_table with shoeboxes
            object detector     // DIALS Detector object
        );

        // --- Method: get_total_mask_for_still ---
        // Preconditions:
        // - `bragg_mask` is a tuple of `flex.bool` arrays (one per panel), where `True` indicates a Bragg-masked pixel.
        // - `global_pixel_mask` is a tuple of `flex.bool` arrays (one per panel), where `True` indicates a good pixel (passed static/dynamic checks).
        // - Both `bragg_mask` and `global_pixel_mask` must have the same number of panels, and corresponding panels must have compatible dimensions.
        // Postconditions:
        // - Returns a tuple of `flex.bool` total mask arrays, one for each panel.
        // - `True` in the total mask indicates a pixel suitable for diffuse scattering analysis (i.e., it's a good pixel and not part of a Bragg region).
        // Behavior:
        // - For each panel, calculates the total mask using the formula:
        //   `Mask_total_panel = global_pixel_mask_panel AND (NOT bragg_mask_panel)`.
        // @raises_error(condition="BraggMaskError", description="If `bragg_mask` and `global_pixel_mask` have incompatible panel counts or dimensions, or if the logical combination fails.")
        tuple<object> get_total_mask_for_still( // conceptual return: tuple<flex.bool, ...>
            tuple<object> bragg_mask,          // tuple<flex.bool, ...>, Bragg regions = True
            tuple<object> global_pixel_mask    // tuple<flex.bool, ...>, Good pixels = True
        );
    }
}
// == END IDL ==
</file>

<file path="src/diffusepipe/masking/pixel_mask_generator_IDL.md">
// == BEGIN IDL ==
module src.diffusepipe.masking {

    # @depends_on_resource(type="DIALS/dxtbx", purpose="Using Detector and ImageSet objects, flex arrays for mask representation")
    # @depends_on_type(src.diffusepipe.exceptions.MaskGenerationError) // For use in @raises_error

    // Defines a circular region for masking
    struct Circle {
        center_x: float; // Center X coordinate in pixels
        center_y: float; // Center Y coordinate in pixels
        radius: float;   // Radius in pixels
    }

    // Defines a rectangular region for masking
    struct Rectangle {
        min_x: float; // Minimum X coordinate in pixels
        max_x: float; // Maximum X coordinate in pixels
        min_y: float; // Minimum Y coordinate in pixels
        max_y: float; // Maximum Y coordinate in pixels
    }

    // Parameters for static mask generation
    struct StaticMaskParams {
        // Behavior: Defines a beamstop region to be masked. Can be a Circle or Rectangle.
        beamstop: optional union<Circle, Rectangle>;
        // Behavior: A list of rectangular regions to be explicitly masked.
        untrusted_rects: optional list<Rectangle>;
        // Behavior: A list of panel indices (0-based) that should be entirely masked.
        untrusted_panels: optional list<int>;
    }

    // Parameters for dynamic mask generation
    struct DynamicMaskParams {
        // Behavior: Pixels with intensity consistently above this threshold in representative images are masked as hot.
        hot_pixel_thresh: optional float;
        // Behavior: Tolerance for considering a pixel value as negative. Pixels below -abs(negative_pixel_tolerance) are masked.
        negative_pixel_tolerance: float;
        // Behavior: If the fraction of pixels flagged as bad by dynamic checks exceeds this, a warning may be issued.
        max_fraction_bad_pixels: float;
    }

    interface PixelMaskGenerator {

        // --- Method: generate_combined_pixel_mask ---
        // Preconditions:
        // - `detector` is a valid DIALS Detector object.
        // - `static_params` and `dynamic_params` are valid parameter objects.
        // - `representative_images` is a list of valid DIALS ImageSet objects. This list can be empty, in which case the dynamic mask component will effectively be an all-pass mask.
        // Postconditions:
        // - Returns a tuple of `flex.bool` mask arrays, one for each panel in the `detector`.
        // - In the returned masks, `True` indicates a good (unmasked) pixel, and `False` indicates a bad (masked) pixel.
        // Behavior:
        // - Orchestrates the generation of a global pixel mask.
        // - Internally calls `generate_static_mask` to create a mask based on detector geometry and `static_params`.
        // - Internally calls `generate_dynamic_mask` to create a mask based on analysis of `representative_images` and `dynamic_params`.
        // - Combines the static and dynamic masks using a logical AND operation (a pixel is good only if it's good in both static and dynamic masks).
        // @raises_error(condition="MaskGenerationError", description="If any step in the static, dynamic, or combined mask generation process fails.")
        tuple<object> generate_combined_pixel_mask( // Conceptual return: tuple<flex.bool, ...>
            object detector, // DIALS Detector object
            StaticMaskParams static_params,
            list<object> representative_images, // List of DIALS ImageSet objects
            DynamicMaskParams dynamic_params
        );

        // --- Method: generate_static_mask (Potentially a helper, but part of the conceptual interface) ---
        // Preconditions:
        // - `detector` is a valid DIALS Detector object.
        // - `static_params` is a valid StaticMaskParams object.
        // Postconditions:
        // - Returns a tuple of `flex.bool` static mask arrays.
        // Behavior:
        // - Creates a mask based on detector's trusted pixel value range.
        // - Applies masks for any defined `beamstop` in `static_params`.
        // - Applies masks for any `untrusted_rects` in `static_params`.
        // - Applies masks for any `untrusted_panels` in `static_params`.
        // @raises_error(condition="MaskGenerationError", description="If static mask generation fails.")
        tuple<object> generate_static_mask( // conceptual return: tuple<flex.bool, ...>
            object detector, // DIALS Detector object
            StaticMaskParams static_params
        );

        // --- Method: generate_dynamic_mask (Potentially a helper, but part of the conceptual interface) ---
        // Preconditions:
        // - `detector` is a valid DIALS Detector object.
        // - `representative_images` is a list of DIALS ImageSet objects.
        // - `dynamic_params` is a valid DynamicMaskParams object.
        // Postconditions:
        // - Returns a tuple of `flex.bool` dynamic mask arrays.
        // Behavior:
        // - If `representative_images` is empty, returns an all-True mask (all pixels good).
        // - Otherwise, analyzes pixel statistics (e.g., identifying hot pixels above `dynamic_params.hot_pixel_thresh`, negative pixels below `dynamic_params.negative_pixel_tolerance`) across the `representative_images`.
        // - Creates a mask where pixels exhibiting anomalous behavior are flagged as bad.
        // @raises_error(condition="MaskGenerationError", description="If dynamic mask generation fails.")
        tuple<object> generate_dynamic_mask( // conceptual return: tuple<flex.bool, ...>
            object detector, // DIALS Detector object
            list<object> representative_images, // List of DIALS ImageSet objects
            DynamicMaskParams dynamic_params
        );
    }
}
// == END IDL ==
</file>

<file path="src/diffusepipe/merging/merger_IDL.md">
# DiffuseDataMerger IDL

**Module Path:** `src.diffusepipe.merging.merger`

**Dependencies:**
- `@depends_on(numpy)` - Array operations and structured arrays
- `@depends_on(GlobalVoxelGrid)` - Voxel grid definitions
- `@depends_on(DiffuseScalingModel)` - Refined scaling parameters
- `@depends_on(VoxelAccumulator)` - Binned pixel data

## Interface: DiffuseDataMerger

**Purpose:** Apply refined scaling model parameters to all observations and merge them within each voxel of the GlobalVoxelGrid. Produces final relatively-scaled 3D diffuse scattering map with proper error propagation.

### Constructor

```python
def __init__(self, 
            global_voxel_grid: GlobalVoxelGrid) -> None
```

**Preconditions:**
- `global_voxel_grid` is valid and initialized

**Postconditions:**
- Merger configured with grid information
- Ready to process binned data and scaling models

**Behavior:**
- Store reference to voxel grid for coordinate calculations
- Initialize data structures for merged results

### Methods

```python
def merge_scaled_data(self, 
                     binned_pixel_data: dict,
                     scaling_model: DiffuseScalingModel,
                     merge_config: dict) -> VoxelDataRelative
```

**Preconditions:**
- `binned_pixel_data` contains observations organized by voxel
- `scaling_model` has refined parameters from Module 3.S.3
- All observations have valid intensity and uncertainty values

**Postconditions:**
- Returns merged data structure with relatively-scaled intensities
- Error propagation applied correctly for v1 model constraints
- Voxel centers calculated using grid transformations

**Behavior:**
1. For each observation, retrieve refined scaling parameters from model
2. Apply scaling: `I_final_relative = (I_corr - C_i) / M_i`
3. Propagate uncertainties: `Sigma_final_relative = Sigma_corr / abs(M_i)`
4. Verify additive offset C_i is effectively zero for v1 model
5. Perform weighted merge within each voxel using inverse variance weighting
6. Calculate voxel center coordinates and q-vector attributes

**Expected Data Format:**
```python
merge_config = {
    "outlier_rejection": {
        "enabled": bool,              # Apply outlier filtering
        "sigma_threshold": float      # Reject observations >N sigma from voxel mean
    },
    "minimum_observations": int,      # Minimum observations required per voxel
    "weight_method": str              # "inverse_variance" or "uniform"
}

# Returns:
VoxelDataRelative = {
    "voxel_indices": numpy.ndarray,     # Shape (N_voxels,) - voxel indices
    "H_center": numpy.ndarray,          # Shape (N_voxels,) - H coordinates of centers  
    "K_center": numpy.ndarray,          # Shape (N_voxels,) - K coordinates of centers
    "L_center": numpy.ndarray,          # Shape (N_voxels,) - L coordinates of centers
    "q_center_x": numpy.ndarray,        # Shape (N_voxels,) - qx lab frame
    "q_center_y": numpy.ndarray,        # Shape (N_voxels,) - qy lab frame  
    "q_center_z": numpy.ndarray,        # Shape (N_voxels,) - qz lab frame
    "q_magnitude_center": numpy.ndarray, # Shape (N_voxels,) - |q| of centers
    "I_merged_relative": numpy.ndarray,  # Shape (N_voxels,) - merged intensities
    "Sigma_merged_relative": numpy.ndarray, # Shape (N_voxels,) - merged uncertainties
    "num_observations": numpy.ndarray    # Shape (N_voxels,) - observation counts
}
```

```python
def apply_scaling_to_observation(self,
                                observation: dict,
                                scaling_model: DiffuseScalingModel) -> tuple[float, float]
```

**Preconditions:**
- `observation` contains intensity, sigma, still_id, q_vector_lab
- `scaling_model` has refined parameters

**Postconditions:**
- Returns scaled intensity and propagated uncertainty
- Scaling applied according to v1 model formulation

**Behavior:**
1. Extract observation properties (still_id, q_magnitude)
2. Get multiplicative scale M_i and additive offset C_i from model
3. Apply v1 scaling formula: `I_scaled = (I_obs - C_i) / M_i`
4. Propagate uncertainty: `Sigma_scaled = Sigma_obs / abs(M_i)`
5. Verify C_i ≈ 0 for v1 model consistency

**Expected Data Format:**
```python
observation = {
    "intensity": float,
    "sigma": float,
    "still_id": int,
    "q_vector_lab": numpy.ndarray  # Shape (3,)
}

# Returns: (scaled_intensity, scaled_sigma)
(123.45, 12.34)
```

```python
def weighted_merge_voxel(self,
                        scaled_observations: list[tuple],
                        weight_method: str = "inverse_variance") -> tuple[float, float, int]
```

**Preconditions:**
- `scaled_observations` is list of (intensity, sigma) tuples
- All sigma values are positive
- At least one observation provided

**Postconditions:**
- Returns merged intensity, merged uncertainty, and observation count
- Weighting applied according to specified method

**Behavior:**
1. Calculate weights based on method (inverse variance or uniform)
2. Compute weighted average intensity
3. Propagate uncertainties using standard error propagation
4. Handle edge cases (single observation, zero weights)

**Expected Data Format:**
```python
# Input: list of (intensity, sigma) pairs
scaled_observations = [(100.0, 10.0), (120.0, 15.0), (90.0, 8.0)]

# Returns: (merged_intensity, merged_sigma, n_observations)
(105.23, 6.12, 3)
```

```python
def calculate_voxel_coordinates(self,
                               voxel_indices: list[int]) -> dict[str, numpy.ndarray]
```

**Preconditions:**
- `voxel_indices` contains valid indices within grid bounds
- Global voxel grid is properly initialized

**Postconditions:**
- Returns coordinate arrays for voxel centers
- All arrays have same length as input voxel_indices

**Behavior:**
1. For each voxel index, get center HKL coordinates
2. Transform HKL to lab-frame q-vectors using grid A_avg_ref
3. Calculate q-magnitude for each center
4. Return structured coordinate data

**Expected Data Format:**
```python
# Returns:
coordinates = {
    "H_center": numpy.ndarray,          # Fractional H coordinates
    "K_center": numpy.ndarray,          # Fractional K coordinates  
    "L_center": numpy.ndarray,          # Fractional L coordinates
    "q_center_x": numpy.ndarray,        # Lab frame qx
    "q_center_y": numpy.ndarray,        # Lab frame qy
    "q_center_z": numpy.ndarray,        # Lab frame qz
    "q_magnitude_center": numpy.ndarray  # |q| magnitudes
}
```

```python
def get_merge_statistics(self, voxel_data_relative: VoxelDataRelative) -> dict
```

**Preconditions:**
- `voxel_data_relative` is valid merged dataset
- Contains required arrays with consistent lengths

**Postconditions:**
- Returns comprehensive statistics about merged data
- Useful for quality assessment and diagnostics

**Behavior:**
- Calculate intensity and observation distribution statistics  
- Analyze resolution coverage and redundancy
- Compute data completeness metrics

**Expected Data Format:**
```python
statistics = {
    "total_voxels": int,
    "intensity_statistics": {
        "mean": float,
        "std": float,
        "min": float,
        "max": float,
        "median": float
    },
    "observation_statistics": {
        "mean_per_voxel": float,
        "total_observations": int,
        "voxels_with_single_obs": int,
        "max_observations_per_voxel": int
    },
    "resolution_coverage": {
        "q_min": float,
        "q_max": float,
        "mean_q": float
    },
    "data_quality": {
        "mean_sigma_over_intensity": float,
        "high_intensity_voxels": int,  # Above mean + 2*std
        "low_sigma_voxels": int        # Good precision voxels
    }
}
```

### Error Conditions

**@raises_error(condition="InvalidScalingModel", message="Scaling model validation failed")**
- Scaling model missing required components
- Parameters not properly refined

**@raises_error(condition="InconsistentDataFormat", message="Data format validation failed")**
- Missing required fields in binned data
- Array length mismatches

**@raises_error(condition="V1ModelViolation", message="v1 model constraints violated")**
- Additive offset C_i is not effectively zero
- Unexpected scaling model structure

**@raises_error(condition="MergeNumericalError", message="Numerical error in merging")**
- All weights are zero or negative
- Invalid uncertainty propagation

## Implementation Notes

- Use inverse variance weighting as default merging method
- Implement numerical safeguards for edge cases (single observations, zero weights)
- Verify v1 model constraints (C_i ≈ 0) with explicit checks
- Use vectorized operations for efficient coordinate calculations
- Handle memory efficiently for large voxel datasets
- Provide comprehensive statistics for data quality assessment
- Support structured array or dictionary output formats
</file>

<file path="src/diffusepipe/orchestration/pipeline_orchestrator_IDL.md">
// == BEGIN IDL ==
module src.diffusepipe.orchestration {

    # @depends_on_resource(type="ExternalTool:DIALS", purpose="Executing DIALS command-line tools: import, find_spots, index, refine, generate_mask")
    # @depends_on_resource(type="FileSystem", purpose="Managing working directories, reading/writing intermediate files, logs")
    # @depends_on([src.diffusepipe.extraction.DataExtractor], [src.diffusepipe.diagnostics.ConsistencyChecker], [src.diffusepipe.diagnostics.QValueCalculator])
    # @depends_on_type(src.diffusepipe.types.PipelineConfig)
    # @depends_on_type(src.diffusepipe.types.ProcessingOutcome)
    # @depends_on_type(src.diffusepipe.types.FileSet)
    interface PipelineOrchestrator {
        // Preconditions:
        // - `config` contains valid pipeline settings, including paths to PHIL files.
        // - `cbf_files_to_process` is a list of existing, readable CBF file paths.
        // - `external_pdb_for_check` is a path to an existing, readable PDB file.
        // - DIALS environment must be configured and executables accessible from the execution environment.
        // - The root directory for processing (where subdirectories will be created) is implicitly the current working directory or managed by the implementation.
        // Postconditions:
        // - For each input CBF file, a corresponding `ProcessingOutcome` is returned.
        // - Processing attempts are made in dedicated subdirectories for each CBF file.
        // - DIALS intermediate files, diffuse data NPZ files (if successful), and diagnostic outputs are generated in respective subdirectories.
        // - A summary log file is created in the root processing directory detailing the outcomes for all images.
        // Behavior:
        // - Initializes a summary log.
        // - Iterates through each path in `cbf_files_to_process`:
        //   1. Validates the existence of the CBF file.
        //   2. Creates a unique working directory for the current CBF file (e.g., based on CBF filename).
        //   3. Executes the DIALS processing sequence within the working directory:
        //      a. `dials.import` using the CBF file.
        //      b. `dials.find_spots` using `imported.expt` and `config.dials_processing.find_spots_phil_file`, `config.dials_processing.min_spot_size`.
        //      c. `dials.index` using `imported.expt`, `strong.refl`, `config.dials_processing.unit_cell`, and `config.dials_processing.space_group`.
        //      d. `dials.refine` using `indexed_initial.expt`, `indexed_initial.refl`, and `config.dials_processing.refinement_phil_file` (or default refinement strategy if PHIL is not provided).
        //      e. `dials.generate_mask` using the refined experiment and reflection files.
        //   4. Checks for successful completion and existence of output files at each DIALS step. If a step fails, logs error and proceeds to next CBF file.
        //   5. If all DIALS steps are successful:
        //      a. Constructs `FileSet` and `ExtractionParameters` from `config` and DIALS outputs.
        //      b. Invokes `DataExtractor.extract_diffuse_data`.
        //      c. If extraction is successful and `config.run_diagnostics` is true:
        //         i. Invokes `ConsistencyChecker.check_q_consistency`.
        //         ii. Invokes `QValueCalculator.calculate_q_values_for_pixels`.
        //   6. Records the overall outcome for the current CBF file in the summary log and in the list of `ProcessingOutcome`.
        // - Finalizes and saves the summary log.
        // @raises_error(condition="InvalidPipelineConfiguration", description="Essential global configuration in `config` is missing or invalid (e.g., PHIL file paths).")
        // @raises_error(condition="DIALSEnvironmentError", description="DIALS executables are not found or DIALS environment is not correctly set up.")
        // @raises_error(condition="FileSystemError", description="Cannot create working directories or write log files.")
        list<src.diffusepipe.types.ProcessingOutcome> process_cbf_dataset(
            src.diffusepipe.types.PipelineConfig config,
            list<string> cbf_files_to_process,
            string external_pdb_for_check,
            string root_processing_directory // Directory where all processing subdirectories and summary log will be created.
        );
    }
}
// == END IDL ==
</file>

<file path="src/diffusepipe/orchestration/stills_pipeline_orchestrator.py">
"""
StillsPipelineOrchestrator implementation for managing full pipeline workflow.

Orchestrates all phases of diffuse scattering processing including:
- Phase 1: Per-still geometry, indexing, and masking
- Phase 2: Diffuse intensity extraction and correction
- Phase 3: Voxelization, relative scaling, and merging
"""

import os
import logging
import multiprocessing
from pathlib import Path
from typing import List, Dict, Optional, Tuple, Any
from dataclasses import dataclass, field
import json
import numpy as np

from dxtbx.model import ExperimentList
from dials.array_family import flex
from cctbx import sgtbx

from ..types.types_IDL import (
    StillsPipelineConfig,
    StillProcessingOutcome,
    ComponentInputFiles,
    OperationOutcome
)
from ..adapters.dials_stills_process_adapter import DIALSStillsProcessAdapter
from ..adapters.dials_sequence_process_adapter import DIALSSequenceProcessAdapter
from ..adapters.dials_generate_mask_adapter import DIALSGenerateMaskAdapter
from ..extraction.data_extractor import DataExtractor
from ..diagnostics.q_consistency_checker import ConsistencyChecker
from ..diagnostics.q_calculator import QValueCalculator
from ..masking.pixel_mask_generator import PixelMaskGenerator
from ..masking.bragg_mask_generator import BraggMaskGenerator
from ..voxelization.global_voxel_grid import (
    GlobalVoxelGrid, GlobalVoxelGridConfig, CorrectedDiffusePixelData
)
from ..voxelization.voxel_accumulator import VoxelAccumulator
from ..scaling.diffuse_scaling_model import DiffuseScalingModel
from ..merging.merger import DiffuseDataMerger
from ..utils.cbf_utils import CBFUtils
from ..exceptions import (
    ConfigurationError as InvalidConfigurationError,
    DIALSError as DIALSEnvironmentError,
    FileSystemError
)

logger = logging.getLogger(__name__)


class StillsPipelineOrchestrator:
    """
    Orchestrates the complete diffuse scattering processing pipeline.
    
    Manages workflow from raw CBF files through all processing phases:
    - Phase 1: DIALS processing and masking
    - Phase 2: Diffuse extraction and correction
    - Phase 3: Voxelization, scaling, and merging
    """
    
    def __init__(self):
        """Initialize the orchestrator."""
        self.summary_log_entries = []
        self.phase2_outputs = []  # Collect Phase 2 outputs for Phase 3
        self.successful_experiments = []  # Collect successful Experiment objects
        self.pixel_mask = None  # Global pixel mask
        
    def process_stills_batch(self,
                           cbf_image_paths: List[str],
                           config: StillsPipelineConfig,
                           root_output_directory: str) -> List[StillProcessingOutcome]:
        """
        Process a batch of CBF files through the complete pipeline.
        
        Args:
            cbf_image_paths: List of paths to CBF files
            config: Pipeline configuration
            root_output_directory: Output directory for all results
            
        Returns:
            List of StillProcessingOutcome objects
        """
        # Validate inputs
        self._validate_inputs(cbf_image_paths, config, root_output_directory)
        
        # Create output directory
        output_path = Path(root_output_directory)
        output_path.mkdir(parents=True, exist_ok=True)
        
        # Initialize summary log
        summary_log_path = output_path / "pipeline_summary.log"
        self._initialize_summary_log(summary_log_path)
        
        # Phase 1 & 2: Process individual stills
        logger.info(f"Starting Phase 1 & 2 processing for {len(cbf_image_paths)} images")
        outcomes = []
        
        # Generate global pixel mask first
        logger.info("Generating global pixel mask")
        self.pixel_mask = self._generate_global_pixel_mask(cbf_image_paths, config, output_path)
        
        # Process stills (can be parallelized)
        for cbf_path in cbf_image_paths:
            outcome = self._process_single_still(cbf_path, config, output_path)
            outcomes.append(outcome)
            self._update_summary_log(summary_log_path, outcome)
            
            # Collect successful results for Phase 3
            if outcome.status == "SUCCESS_ALL":
                self._collect_phase2_outputs(outcome)
        
        # Phase 3: Voxelization, scaling, and merging
        if len(self.phase2_outputs) > 0 and self._should_run_phase3(config):
            logger.info(f"Starting Phase 3 processing with {len(self.phase2_outputs)} successful stills")
            phase3_output_dir = output_path / "phase3_merged"
            phase3_output_dir.mkdir(exist_ok=True)
            
            try:
                self._run_phase3(config, phase3_output_dir)
                logger.info("Phase 3 completed successfully")
            except Exception as e:
                logger.error(f"Phase 3 failed: {e}")
                # Phase 3 failure doesn't affect individual still outcomes
        else:
            logger.info("Skipping Phase 3 (insufficient data or disabled)")
        
        # Finalize summary
        self._finalize_summary_log(summary_log_path)
        
        return outcomes
    
    def _validate_inputs(self, cbf_paths: List[str], config: StillsPipelineConfig, 
                        output_dir: str):
        """Validate input parameters."""
        # Check CBF files exist
        for cbf_path in cbf_paths:
            if not os.path.exists(cbf_path):
                raise FileSystemError(f"CBF file not found: {cbf_path}")
        
        # Validate configuration
        if config is None:
            raise InvalidConfigurationError("Configuration cannot be None")
        
        # Check output directory is writable
        output_path = Path(output_dir)
        if output_path.exists() and not os.access(output_path, os.W_OK):
            raise FileSystemError(f"Output directory not writable: {output_dir}")
    
    def _initialize_summary_log(self, log_path: Path):
        """Initialize the summary log file."""
        with open(log_path, 'w') as f:
            f.write("DiffusePipe Processing Summary\n")
            f.write("=" * 80 + "\n\n")
    
    def _generate_global_pixel_mask(self, cbf_paths: List[str], 
                                   config: StillsPipelineConfig,
                                   output_path: Path) -> Optional[Any]:
        """Generate global pixel mask from all images."""
        try:
            # Use PixelMaskGenerator to create mask
            # This is a simplified version - actual implementation would process all images
            # For now, return None to use per-image masks
            return None
        except Exception as e:
            logger.warning(f"Failed to generate global pixel mask: {e}")
            return None
    
    def _process_single_still(self, cbf_path: str, config: StillsPipelineConfig,
                            root_output_dir: Path) -> StillProcessingOutcome:
        """Process a single CBF file through Phase 1 & 2."""
        # Create working directory
        cbf_name = Path(cbf_path).stem
        working_dir = root_output_dir / cbf_name
        working_dir.mkdir(exist_ok=True)
        
        # Initialize outcome
        outcome = StillProcessingOutcome(
            input_cbf_path=cbf_path,
            status="FAILURE_DIALS",  # Default to failure
            working_directory=str(working_dir),
            dials_outcome=OperationOutcome(status="FAILURE"),
            extraction_outcome=OperationOutcome(status="FAILURE")
        )
        
        try:
            # Module 1.S.0: Data type detection
            processing_route = self._determine_processing_route(cbf_path, config)
            logger.info(f"Processing {cbf_name} using {processing_route} route")
            
            # Module 1.S.1: DIALS processing
            dials_result = self._run_dials_processing(
                cbf_path, config, working_dir, processing_route
            )
            
            if dials_result['success']:
                outcome.dials_outcome = OperationOutcome(
                    status="SUCCESS",
                    output_artifacts=dials_result['artifacts']
                )
                
                # Module 1.S.3: Bragg mask generation
                bragg_mask_path = self._generate_bragg_mask(
                    dials_result['experiment'],
                    dials_result['reflections'],
                    working_dir
                )
                
                # Module 2.S.1 & 2.S.2: Data extraction
                extraction_result = self._run_data_extraction(
                    cbf_path, dials_result, config, working_dir, bragg_mask_path
                )
                
                if extraction_result['success']:
                    outcome.extraction_outcome = OperationOutcome(
                        status="SUCCESS",
                        output_artifacts=extraction_result['artifacts']
                    )
                    outcome.status = "SUCCESS_ALL"
                    
                    # Store successful results
                    self.successful_experiments.append(dials_result['experiment'])
                else:
                    outcome.status = "FAILURE_EXTRACTION"
                    outcome.extraction_outcome = OperationOutcome(
                        status="FAILURE",
                        message=extraction_result.get('error', 'Unknown extraction error')
                    )
            else:
                outcome.dials_outcome = OperationOutcome(
                    status="FAILURE",
                    message=dials_result.get('error', 'DIALS processing failed')
                )
                
        except Exception as e:
            logger.error(f"Error processing {cbf_name}: {e}")
            outcome.message = str(e)
        
        return outcome
    
    def _determine_processing_route(self, cbf_path: str, 
                                  config: StillsPipelineConfig) -> str:
        """Determine processing route based on CBF data type."""
        # Check forced mode
        forced_mode = config.dials_stills_process_config.force_processing_mode
        if forced_mode in ['stills', 'sequence']:
            return forced_mode
        
        # Auto-detect from CBF header
        try:
            cbf_utils = CBFUtils()
            angle_increment = cbf_utils.get_angle_increment(cbf_path)
            
            if angle_increment is None or abs(angle_increment) < 1e-6:
                return 'stills'
            else:
                return 'sequence'
        except Exception as e:
            logger.warning(f"Failed to detect data type, defaulting to sequence: {e}")
            return 'sequence'
    
    def _run_dials_processing(self, cbf_path: str, config: StillsPipelineConfig,
                            working_dir: Path, processing_route: str) -> Dict:
        """Run DIALS processing using appropriate adapter."""
        try:
            if processing_route == 'stills':
                adapter = DIALSStillsProcessAdapter()
            else:
                adapter = DIALSSequenceProcessAdapter()
            
            # Process the image
            experiment, reflections = adapter.process_still(
                cbf_path, 
                config.dials_stills_process_config,
                str(working_dir)
            )
            
            # Save outputs
            expt_path = working_dir / "integrated.expt"
            refl_path = working_dir / "integrated.refl"
            
            experiment.as_file(str(expt_path))
            reflections.as_file(str(refl_path))
            
            return {
                'success': True,
                'experiment': experiment,
                'reflections': reflections,
                'artifacts': {
                    'experiment_path': str(expt_path),
                    'reflections_path': str(refl_path)
                }
            }
            
        except Exception as e:
            logger.error(f"DIALS processing failed: {e}")
            return {'success': False, 'error': str(e)}
    
    def _generate_bragg_mask(self, experiment: ExperimentList, 
                           reflections: flex.reflection_table,
                           working_dir: Path) -> str:
        """Generate Bragg mask for the still."""
        try:
            generator = BraggMaskGenerator()
            mask = generator.generate_bragg_mask_from_spots(
                experiment[0], reflections
            )
            
            # Save mask
            mask_path = working_dir / "bragg_mask.pickle"
            import pickle
            with open(mask_path, 'wb') as f:
                pickle.dump(mask, f)
            
            return str(mask_path)
            
        except Exception as e:
            logger.error(f"Bragg mask generation failed: {e}")
            raise
    
    def _run_data_extraction(self, cbf_path: str, dials_result: Dict,
                           config: StillsPipelineConfig, working_dir: Path,
                           bragg_mask_path: str) -> Dict:
        """Run diffuse data extraction."""
        try:
            # Prepare inputs
            inputs = ComponentInputFiles(
                cbf_image_path=cbf_path,
                dials_expt_path=dials_result['artifacts']['experiment_path'],
                dials_refl_path=dials_result['artifacts']['reflections_path'],
                bragg_mask_path=bragg_mask_path,
                external_pdb_path=config.extraction_config.external_pdb_path
            )
            
            # Run extraction
            output_npz_path = str(working_dir / "diffuse_data.npz")
            extractor = DataExtractor()
            extractor.extract_from_still(
                inputs,
                config.extraction_config,
                output_npz_path
            )
            
            return {
                'success': True,
                'artifacts': {
                    'diffuse_data_path': output_npz_path
                }
            }
            
        except Exception as e:
            logger.error(f"Data extraction failed: {e}")
            return {'success': False, 'error': str(e)}
    
    def _collect_phase2_outputs(self, outcome: StillProcessingOutcome):
        """Collect successful Phase 2 outputs for Phase 3 processing."""
        if outcome.extraction_outcome.output_artifacts:
            npz_path = outcome.extraction_outcome.output_artifacts.get('diffuse_data_path')
            if npz_path and os.path.exists(npz_path):
                # Load the NPZ file
                data = np.load(npz_path)
                
                # Create CorrectedDiffusePixelData object
                diffuse_data = CorrectedDiffusePixelData(
                    q_vectors=data['q_vectors'],
                    intensities=data['intensities'], 
                    sigmas=data['sigmas'],
                    still_ids=data.get('still_ids', np.zeros(len(data['intensities'])))
                )
                
                self.phase2_outputs.append({
                    'cbf_path': outcome.input_cbf_path,
                    'working_dir': outcome.working_directory,
                    'diffuse_data': diffuse_data,
                    'still_id': len(self.phase2_outputs)  # Sequential ID
                })
    
    def _should_run_phase3(self, config: StillsPipelineConfig) -> bool:
        """Check if Phase 3 should be run."""
        # Phase 3 requires at least some successful Phase 2 outputs
        if len(self.phase2_outputs) < 2:
            logger.warning("Insufficient data for Phase 3 (need at least 2 successful stills)")
            return False
        
        # Check if Phase 3 is configured
        if not hasattr(config, 'relative_scaling_config') or config.relative_scaling_config is None:
            logger.warning("Phase 3 configuration missing")
            return False
        
        return True
    
    def _run_phase3(self, config: StillsPipelineConfig, output_dir: Path):
        """Run Phase 3: Voxelization, scaling, and merging."""
        logger.info("Starting Phase 3: Voxelization, scaling, and merging")
        
        # Extract configuration
        scaling_config = config.relative_scaling_config
        grid_config_dict = scaling_config.grid_config or {}
        
        # Create grid configuration
        grid_config = GlobalVoxelGridConfig(
            d_min_target=grid_config_dict.get('d_min_target', 1.0),
            d_max_target=grid_config_dict.get('d_max_target', 10.0),
            ndiv_h=grid_config_dict.get('ndiv_h', 5),
            ndiv_k=grid_config_dict.get('ndiv_k', 5),
            ndiv_l=grid_config_dict.get('ndiv_l', 5)
        )
        
        # Module 3.S.1: Create global voxel grid
        logger.info("Creating global voxel grid")
        diffuse_data_list = [item['diffuse_data'] for item in self.phase2_outputs]
        
        global_grid = GlobalVoxelGrid(
            self.successful_experiments,
            diffuse_data_list,
            grid_config
        )
        
        diagnostics = global_grid.get_crystal_averaging_diagnostics()
        logger.info(f"Grid created: {diagnostics['total_voxels']} voxels, "
                   f"RMS misorientation: {diagnostics['rms_misorientation_deg']:.2f}°")
        
        # Module 3.S.2: Bin observations into voxels
        logger.info("Binning observations into voxels")
        space_group = self.successful_experiments[0].crystal.get_space_group()
        space_group_info = sgtbx.space_group_info(group=space_group)
        
        accumulator = VoxelAccumulator(
            global_grid,
            space_group_info,
            backend=scaling_config.voxel_accumulator_backend
        )
        
        # Add all observations
        for i, phase2_data in enumerate(self.phase2_outputs):
            diffuse_data = phase2_data['diffuse_data']
            n_binned = accumulator.add_observations(
                phase2_data['still_id'],
                diffuse_data.q_vectors,
                diffuse_data.intensities,
                diffuse_data.sigmas
            )
            logger.debug(f"Still {i}: binned {n_binned} observations")
        
        accumulator_stats = accumulator.get_accumulation_statistics()
        logger.info(f"Total observations binned: {accumulator_stats['total_observations']} "
                   f"in {accumulator_stats['unique_voxels']} voxels")
        
        # Get binned data for scaling
        binned_data = accumulator.get_all_binned_data_for_scaling()
        
        # Module 3.S.3: Relative scaling
        logger.info("Performing relative scaling")
        
        # Create scaling model configuration
        still_ids = [item['still_id'] for item in self.phase2_outputs]
        scaling_model_config = {
            'still_ids': still_ids,
            'per_still_scale': {'enabled': scaling_config.refine_per_still_scale},
            'resolution_smoother': {
                'enabled': scaling_config.refine_resolution_scale_multiplicative,
                'n_control_points': scaling_config.resolution_scale_bins or 3,
                'resolution_range': (0.1, 2.0)  # Default range
            },
            'experimental_components': {
                'panel_scale': {'enabled': False},
                'spatial_scale': {'enabled': False},
                'additive_offset': {'enabled': scaling_config.refine_additive_offset}
            },
            'partiality_threshold': scaling_config.min_partiality_threshold
        }
        
        scaling_model = DiffuseScalingModel(scaling_model_config)
        
        # Perform refinement
        refinement_config = scaling_config.refinement_config or {
            'max_iterations': 10,
            'convergence_tolerance': 1e-4
        }
        
        refined_params, refinement_stats = scaling_model.refine_parameters(
            binned_data,
            {},  # No Bragg reflections for now
            refinement_config
        )
        
        logger.info(f"Scaling refinement completed: {refinement_stats['n_iterations']} iterations, "
                   f"final R-factor: {refinement_stats['final_r_factor']:.6f}")
        
        # Module 3.S.4: Merge scaled data
        logger.info("Merging scaled data")
        merger = DiffuseDataMerger(global_grid)
        
        merge_config = scaling_config.merge_config or {
            'outlier_rejection': {'enabled': True, 'sigma_threshold': 3.0},
            'minimum_observations': 2,
            'weight_method': 'inverse_variance'
        }
        
        voxel_data = merger.merge_scaled_data(
            binned_data,
            scaling_model,
            merge_config
        )
        
        merge_stats = merger.get_merge_statistics(voxel_data)
        logger.info(f"Merging completed: {merge_stats['total_voxels']} voxels, "
                   f"mean intensity: {merge_stats['intensity_statistics']['mean']:.2f}")
        
        # Save merged data
        output_path = output_dir / "merged_diffuse_data.npz"
        merger.save_voxel_data(voxel_data, str(output_path))
        logger.info(f"Saved merged data to {output_path}")
        
        # Save scaling model information
        model_info_path = output_dir / "scaling_model_info.json"
        with open(model_info_path, 'w') as f:
            json.dump(scaling_model.get_model_info(), f, indent=2)
        
        # Save merge statistics
        stats_path = output_dir / "merge_statistics.json"
        with open(stats_path, 'w') as f:
            json.dump(merge_stats, f, indent=2)
        
        logger.info("Phase 3 completed successfully")
    
    def _update_summary_log(self, log_path: Path, outcome: StillProcessingOutcome):
        """Update the summary log with processing outcome."""
        entry = f"{Path(outcome.input_cbf_path).name}: {outcome.status}\n"
        if outcome.message:
            entry += f"  Message: {outcome.message}\n"
        
        with open(log_path, 'a') as f:
            f.write(entry)
    
    def _finalize_summary_log(self, log_path: Path):
        """Finalize the summary log with statistics."""
        with open(log_path, 'a') as f:
            f.write("\n" + "=" * 80 + "\n")
            f.write("Processing Complete\n")
            f.write(f"Total Phase 2 outputs: {len(self.phase2_outputs)}\n")
            if hasattr(self, 'phase3_completed'):
                f.write("Phase 3: Completed\n")
</file>

<file path="src/diffusepipe/scaling/components/__init__.py">
"""
Scaling components for the relative scaling model.
"""
</file>

<file path="src/diffusepipe/scaling/components/per_still_multiplier.py">
"""
PerStillMultiplierComponent for diffuse scaling model.

Implements simple multiplicative scale factor with one parameter per still.
"""

import logging
from typing import Dict, List, Optional

from dials.algorithms.scaling.model.components.scale_components import ScaleComponentBase
from dials.array_family import flex

logger = logging.getLogger(__name__)


class PerStillMultiplierComponent(ScaleComponentBase):
    """
    Simple multiplicative scale factor component with one parameter per still.
    
    This is the core component of the v1 diffuse scaling model, providing
    exactly one refineable parameter (multiplicative scale) per still image.
    """
    
    def __init__(self,
                 active_parameter_manager,
                 still_ids: List[int],
                 initial_values: Optional[Dict[int, float]] = None):
        """
        Initialize per-still multiplier component.
        
        Args:
            active_parameter_manager: DIALS parameter manager for refinement
            still_ids: List of unique still identifiers
            initial_values: Optional dict mapping still_id -> initial scale
        """
        self.still_ids = sorted(still_ids)  # Ensure consistent ordering
        self.n_stills = len(self.still_ids)
        
        # Create mapping from still_id to parameter index
        self.still_id_to_param_idx = {
            still_id: i for i, still_id in enumerate(self.still_ids)
        }
        
        # Set initial parameter values
        if initial_values is None:
            initial_values = {still_id: 1.0 for still_id in self.still_ids}
        
        initial_params = flex.double([
            initial_values.get(still_id, 1.0) for still_id in self.still_ids
        ])
        
        # Initialize base class with parameter values
        super().__init__(initial_params)
        
        # Store parameter manager reference
        self.active_parameter_manager = active_parameter_manager
        
        logger.info(f"PerStillMultiplierComponent initialized for {self.n_stills} stills")
    
    @property
    def n_params(self) -> int:
        """Number of parameters in this component."""
        return self.n_stills
    
    @property
    def parameters(self) -> flex.double:
        """Current parameter values from the parameter manager."""
        # Use the parent class parameters directly
        return self._parameters
    
    def calculate_scales_and_derivatives(self, 
                                       reflection_table,
                                       block_id=None):
        """
        Calculate scale factors and derivatives for reflections.
        
        Args:
            reflection_table: DIALS reflection table or diffuse data structure
            block_id: Unused for this component
            
        Returns:
            tuple: (scales, derivatives) as flex arrays
        """
        # Handle different input types
        if isinstance(reflection_table, dict) and 'still_ids' in reflection_table:
            # Diffuse data dictionary format
            still_ids = reflection_table['still_ids']
            n_reflections = len(still_ids)
        elif hasattr(reflection_table, 'get') and reflection_table.get('still_id') is not None:
            # DIALS flex.reflection_table
            still_ids = reflection_table.get('still_id')
            n_reflections = len(still_ids)
        else:
            raise ValueError("Invalid reflection_table format")
        
        if n_reflections == 0:
            return flex.double(), flex.double()
        
        # Get current parameter values
        current_params = self.parameters
        
        # Calculate scales for each reflection
        scales = flex.double(n_reflections)
        derivatives = flex.double(n_reflections)
        
        for i in range(n_reflections):
            still_id = still_ids[i]
            
            if still_id in self.still_id_to_param_idx:
                param_idx = self.still_id_to_param_idx[still_id]
                scale = current_params[param_idx]
                derivative = 1.0  # d(scale)/d(parameter) = 1.0
            else:
                # Unknown still - use unity scale
                scale = 1.0
                derivative = 0.0
                logger.warning(f"Unknown still_id {still_id}, using unity scale")
            
            scales[i] = scale
            derivatives[i] = derivative
        
        return scales, derivatives
    
    def get_scale_for_still(self, still_id: int) -> float:
        """
        Get current scale factor for a specific still.
        
        Args:
            still_id: Still identifier
            
        Returns:
            Current multiplicative scale factor
        """
        if still_id not in self.still_id_to_param_idx:
            logger.warning(f"Unknown still_id {still_id}")
            return 1.0
        
        param_idx = self.still_id_to_param_idx[still_id]
        current_params = self.parameters
        return current_params[param_idx]
    
    def set_scale_for_still(self, still_id: int, scale: float):
        """
        Set scale factor for a specific still.
        
        Args:
            still_id: Still identifier
            scale: New scale value (should be positive)
        """
        if still_id not in self.still_id_to_param_idx:
            raise ValueError(f"Unknown still_id {still_id}")
        
        if scale <= 0:
            raise ValueError("Scale factor must be positive")
        
        param_idx = self.still_id_to_param_idx[still_id]
        
        # Update parameter directly in the component
        self._parameters[param_idx] = scale
    
    def get_component_info(self) -> Dict:
        """
        Get information about this component.
        
        Returns:
            Dictionary with component details
        """
        current_params = self.parameters
        
        return {
            "component_type": "PerStillMultiplier",
            "n_parameters": self.n_params,
            "n_stills": self.n_stills,
            "still_ids": self.still_ids,
            "current_scales": {
                self.still_ids[i]: float(current_params[i]) 
                for i in range(self.n_stills)
            },
            "scale_statistics": {
                "mean": float(flex.mean(current_params)),
                "std": float(flex.mean_and_variance(current_params).gsl_stats_wsd()),
                "min": float(flex.min(current_params)),
                "max": float(flex.max(current_params))
            }
        }
    
    def update_reflection_data(self, reflection_table):
        """
        Update component state based on reflection data.
        
        This component doesn't need to update state based on reflections.
        """
        pass
    
    def __str__(self) -> str:
        """String representation of component."""
        info = self.get_component_info()
        stats = info["scale_statistics"]
        return (f"PerStillMultiplierComponent({self.n_stills} stills, "
                f"scales: {stats['mean']:.3f}±{stats['std']:.3f})")
    
    def __repr__(self) -> str:
        """Detailed string representation."""
        return (f"PerStillMultiplierComponent(n_stills={self.n_stills}, "
                f"still_ids={self.still_ids})")
</file>

<file path="src/diffusepipe/scaling/diffuse_scaling_model_IDL.md">
# DiffuseScalingModel IDL

**Module Path:** `src.diffusepipe.scaling.diffuse_scaling_model`

**Dependencies:**
- `@depends_on(dials.algorithms.scaling.model.ScalingModelBase)` - Base class for scaling models
- `@depends_on(dials.algorithms.scaling.model.components.scale_components.ScaleComponentBase)` - Component framework
- `@depends_on(dials.algorithms.scaling.active_parameter_managers)` - Parameter management
- `@depends_on(dials.array_family.flex)` - Reflection tables and arrays
- `@depends_on(numpy)` - Array operations for diffuse data

## Interface: DiffuseScalingModel(ScalingModelBase)

**Purpose:** Custom scaling model for relative scaling of diffuse scattering data. Implements v1 parameter-guarded model with per-still multiplicative scales and optional resolution smoothing. Enforces hard limits on model complexity.

### Constructor

```python
def __init__(self, 
            configdict: dict,
            is_scaled: bool = False) -> None
```

**Preconditions:**
- `configdict` contains valid configuration for scaling components
- Total free parameters ≤ MAX_FREE_PARAMS (5 + N_stills)
- Advanced components disabled in v1 unless experimental flag set

**Postconditions:**
- Scaling components initialized according to v1 constraints
- Parameter manager configured with refineable parameters
- Error raised if configuration violates v1 parameter limits

**Behavior:**
1. Validate configuration against v1 parameter limits
2. Initialize enabled components (per-still multiplier, optional resolution smoother)
3. Set up active parameter manager for refinement
4. Configure restraints for smoother parameters if enabled
5. Initialize reference generation capabilities

**Expected Data Format:**
```python
configdict = {
    "per_still_scale": {
        "enabled": bool,           # Should be True for v1
        "initial_values": dict     # still_id -> initial scale (default 1.0)
    },
    "resolution_smoother": {
        "enabled": bool,           # False by default in v1
        "n_control_points": int,   # ≤5 for v1, raises error if exceeded
        "resolution_range": tuple  # (q_min, q_max) for smoother domain
    },
    "experimental_components": {
        "panel_scale": {"enabled": False},      # Hard-disabled in v1
        "spatial_scale": {"enabled": False},    # Hard-disabled in v1
        "additive_offset": {"enabled": False}   # Hard-disabled in v1
    },
    "partiality_threshold": float  # P_min_thresh for Bragg reference (default 0.1)
}
```

### Methods

```python
def refine_parameters(self, 
                     binned_pixel_data: dict,
                     bragg_reflections: dict,
                     refinement_config: dict) -> tuple[dict, dict]
```

**Preconditions:**
- `binned_pixel_data` contains voxel-organized diffuse observations
- `bragg_reflections` contains reflection tables with partiality column
- Model components are properly initialized

**Postconditions:**
- Model parameters refined via iterative minimization
- Convergence achieved or maximum iterations reached
- Returns refined parameters and refinement statistics

**Behavior:**
1. Set up iterative refinement loop with DIALS minimizer
2. Generate Bragg and diffuse references from current parameters
3. Calculate residuals using custom target function
4. Update parameters via Levenberg-Marquardt or similar
5. Check convergence and repeat until criteria met

**Expected Data Format:**
```python
refinement_config = {
    "max_iterations": int,         # Maximum refinement cycles
    "convergence_tolerance": float, # R-factor change threshold
    "minimizer_type": str          # "lm" or "gauss_newton"
}

# Returns:
refined_parameters = {
    still_id: {
        "multiplicative_scale": float,
        "additive_offset": float       # Always 0.0 in v1
    }
}

refinement_statistics = {
    "n_iterations": int,
    "final_r_factor": float,
    "convergence_achieved": bool,
    "parameter_shifts": dict       # Component -> max parameter change
}
```

```python
def get_scales_for_observation(self, 
                              still_id: int,
                              q_magnitude: float,
                              **kwargs) -> tuple[float, float]
```

**Preconditions:**
- `still_id` is valid and has refined parameters
- `q_magnitude` is positive
- Model parameters have been refined

**Postconditions:**
- Returns multiplicative scale and additive offset for observation
- Values computed from current model state

**Behavior:**
1. Retrieve per-still multiplicative scale from component
2. Apply resolution-dependent correction if smoother enabled
3. Calculate additive offset (always 0.0 in v1)
4. Return combined scale factors

**Expected Data Format:**
```python
# Returns: (multiplicative_scale, additive_offset)
(1.23, 0.0)  # additive_offset always 0.0 in v1
```

```python
def generate_references(self, 
                       binned_pixel_data: dict,
                       bragg_reflections: dict) -> tuple[dict, dict]
```

**Preconditions:**
- Current model parameters available
- Input data properly formatted with all required fields

**Postconditions:**
- Returns Bragg and diffuse reference intensities
- References computed using current scale estimates

**Behavior:**
1. Filter Bragg reflections by partiality threshold (P_spot ≥ P_min_thresh)
2. Calculate Bragg reference as weighted average of scaled intensities
3. Calculate diffuse reference for each voxel from scaled observations
4. Apply quality filters and outlier rejection

**Expected Data Format:**
```python
# Returns: (bragg_references, diffuse_references)
bragg_references = {
    hkl_asu: {
        "intensity": float,
        "sigma": float,
        "n_observations": int
    }
}

diffuse_references = {
    voxel_idx: {
        "intensity": float,
        "sigma": float, 
        "n_observations": int
    }
}
```

### Components

**PerStillMultiplierComponent:**
- Exactly one free parameter per still: `b_i`
- Initialized to 1.0 for all stills
- Enforced to be positive during refinement

**ResolutionSmootherComponent (Optional):**
- 1D Gaussian smoother over |q| with ≤5 control points
- Shared by all stills (multiplicative factor)
- Disabled by default in v1 configuration

### Error Conditions

**@raises_error(condition="ParameterLimitExceeded", message="v1 model parameter limit exceeded")**
- Total free parameters > MAX_FREE_PARAMS (5 + N_stills)
- Resolution smoother control points > 5

**@raises_error(condition="DisabledComponentAccess", message="Advanced component disabled in v1")**
- Attempt to enable panel, spatial, or additive components
- Configuration requests forbidden functionality

**@raises_error(condition="RefinementFailure", message="Parameter refinement failed")**
- Minimizer convergence failure
- Invalid parameter values during refinement

## Interface: PerStillMultiplierComponent(ScaleComponentBase)

**Purpose:** Simple multiplicative scale factor component with one parameter per still.

### Constructor

```python
def __init__(self,
            active_parameter_manager: ActiveParameterManager,
            still_ids: list[int],
            initial_values: dict = None) -> None
```

**Preconditions:**
- `still_ids` contains unique identifiers for all stills
- `initial_values` if provided maps still_id to positive float

**Postconditions:**
- One parameter registered per still in parameter manager
- Initial values set (default 1.0)

**Behavior:**
- Add one parameter per still to active parameter manager
- Initialize with unity scale or provided values
- Set up parameter indexing for efficient access

### Methods

```python
def calculate_scales_and_derivatives(self, 
                                   reflection_table: flex.reflection_table,
                                   block_id: int = None) -> tuple[flex.double, flex.double]
```

**Preconditions:**
- Reflection table contains 'still_id' column
- Parameters have been set in parameter manager

**Postconditions:**
- Returns scale factors and derivatives for all reflections
- Arrays match reflection table length

**Behavior:**
- Look up scale parameter for each reflection's still_id
- Return scales and unit derivatives (d(scale)/d(parameter) = 1.0)

## Interface: ResolutionSmootherComponent(ScaleComponentBase)

**Purpose:** 1D Gaussian smoother for resolution-dependent multiplicative scaling.

### Constructor  

```python
def __init__(self,
            active_parameter_manager: ActiveParameterManager,
            n_control_points: int,
            resolution_range: tuple[float, float]) -> None
```

**Preconditions:**
- `n_control_points` ≤ 5 for v1 compliance
- `resolution_range` is (q_min, q_max) with q_min < q_max

**Postconditions:**
- Control points distributed over resolution range
- Smoother initialized with unity values
- Restraints set up for smoothness

**Behavior:**
- Create 1D Gaussian smoother with specified control points
- Distribute control points evenly over |q| range
- Initialize control point values to 1.0 (unity scaling)
- Configure smoothness restraints

### Methods

```python
def calculate_scales_and_derivatives(self,
                                   reflection_table: flex.reflection_table,
                                   block_id: int = None) -> tuple[flex.double, flex.double]
```

**Preconditions:**
- Reflection table contains q-vector information or d-spacing
- Smoother parameters have been set

**Postconditions:**
- Returns resolution-dependent scale factors
- Derivatives computed for parameter refinement

**Behavior:**
- Calculate |q| for each reflection
- Evaluate Gaussian smoother at q-values
- Return scales and derivatives w.r.t. control point parameters

## Implementation Notes

- MAX_FREE_PARAMS = 5 + N_stills enforced at initialization
- All additive components return 0.0 in v1 implementation
- Parameter manager handles constraints and bounds
- Uses DIALS Levenberg-Marquardt minimizer for refinement
- Reference generation filters by P_spot ≥ P_min_thresh
- Convergence based on R-factor change between iterations
</file>

<file path="src/diffusepipe/scaling/diffuse_scaling_model.py">
"""
DiffuseScalingModel implementation for Phase 3 relative scaling.

Custom scaling model using DIALS components with v1 parameter constraints.
"""

import logging
import numpy as np
from typing import Dict, List, Tuple, Any, Optional

from dials.algorithms.scaling.model.model import ScalingModelBase
from dials.algorithms.scaling.active_parameter_managers import active_parameter_manager
from dials.array_family import flex

from .components.per_still_multiplier import PerStillMultiplierComponent
from .components.resolution_smoother import ResolutionSmootherComponent

logger = logging.getLogger(__name__)

# v1 model constraints
MAX_FREE_PARAMS_BASE = 5  # Maximum additional parameters beyond per-still scales


class DiffuseScalingModel(ScalingModelBase):
    """
    Custom scaling model for relative scaling of diffuse scattering data.
    
    Implements v1 parameter-guarded model with per-still multiplicative scales
    and optional resolution smoothing. Enforces hard limits on model complexity.
    """
    
    def __init__(self, configdict: Dict, is_scaled: bool = False):
        """
        Initialize diffuse scaling model.
        
        Args:
            configdict: Configuration dictionary for scaling components
            is_scaled: Whether model has been applied to data
        """
        # Add required keys for DIALS base class
        dials_configdict = dict(configdict)
        dials_configdict.setdefault('corrections', ['scale'])  # Basic scale correction
        
        super().__init__(dials_configdict, is_scaled)
        
        self.diffuse_configdict = configdict
        self._validate_v1_config()
        
        # Initialize parameter manager with empty components
        self.active_parameter_manager = active_parameter_manager({}, [])
        
        # Track components and their parameter counts
        self.diffuse_components = {}
        self.component_param_counts = {}
        
        # Initialize components based on configuration
        self._initialize_components()
        
        # Store refinement state
        self.refinement_statistics = {}
        self.partiality_threshold = self.diffuse_configdict.get('partiality_threshold', 0.1)
        
        logger.info(f"DiffuseScalingModel initialized with {self.n_total_params} parameters")
    
    def _validate_v1_config(self):
        """Validate configuration against v1 constraints."""
        # Check that advanced components are disabled
        experimental = self.diffuse_configdict.get('experimental_components', {})
        
        forbidden_components = ['panel_scale', 'spatial_scale', 'additive_offset']
        for comp in forbidden_components:
            if experimental.get(comp, {}).get('enabled', False):
                raise ValueError(f"v1 model: {comp} component is hard-disabled")
        
        # Check resolution smoother constraints
        res_config = self.diffuse_configdict.get('resolution_smoother', {})
        if res_config.get('enabled', False):
            n_points = res_config.get('n_control_points', 0)
            if n_points > 5:
                raise ValueError(f"v1 model: resolution smoother limited to ≤5 points, got {n_points}")
    
    def _initialize_components(self):
        """Initialize scaling components based on configuration."""
        # Always initialize per-still multiplier (required for v1)
        per_still_config = self.diffuse_configdict.get('per_still_scale', {'enabled': True})
        if per_still_config.get('enabled', True):
            still_ids = self._get_still_ids_from_config()
            initial_values = per_still_config.get('initial_values', None)
            
            self.per_still_component = PerStillMultiplierComponent(
                self.active_parameter_manager, 
                still_ids, 
                initial_values
            )
            self.diffuse_components['per_still'] = self.per_still_component
            self.component_param_counts['per_still'] = len(still_ids)
        
        # Optionally initialize resolution smoother
        res_config = self.diffuse_configdict.get('resolution_smoother', {})
        if res_config.get('enabled', False):
            n_points = res_config.get('n_control_points', 3)
            q_range = res_config.get('resolution_range', (0.1, 2.0))
            
            self.resolution_component = ResolutionSmootherComponent(
                self.active_parameter_manager,
                n_points,
                q_range
            )
            self.diffuse_components['resolution'] = self.resolution_component
            self.component_param_counts['resolution'] = n_points
        
        # Verify total parameter count
        self.n_total_params = sum(self.component_param_counts.values())
        n_stills = self.component_param_counts.get('per_still', 0)
        max_allowed = MAX_FREE_PARAMS_BASE + n_stills
        
        if self.n_total_params > max_allowed:
            raise ValueError(f"v1 model: {self.n_total_params} parameters exceeds "
                           f"limit of {max_allowed} ({MAX_FREE_PARAMS_BASE} + {n_stills} stills)")
    
    def _get_still_ids_from_config(self) -> List[int]:
        """Extract still IDs from configuration or data."""
        # This would normally come from the actual data
        # For now, use configuration or reasonable defaults
        still_ids = self.diffuse_configdict.get('still_ids', [])
        if not still_ids:
            # Default to small number for testing
            still_ids = list(range(3))
            logger.warning(f"No still_ids in config, using default: {still_ids}")
        return still_ids
    
    def configure_components(self, reflection_table, experiment, params):
        """Configure components based on actual data."""
        # This method is called by DIALS framework
        # Update component configurations if needed
        pass
    
    def get_scales(self, data_dict: Dict) -> np.ndarray:
        """
        Get scale factors for observations.
        
        Args:
            data_dict: Dictionary containing observation data
            
        Returns:
            Array of scale factors
        """
        n_obs = len(data_dict.get('still_ids', []))
        if n_obs == 0:
            return np.array([])
        
        # Start with per-still scales
        per_still_scales, _ = self.per_still_component.calculate_scales_and_derivatives(data_dict)
        total_scales = np.array(per_still_scales)
        
        # Apply resolution-dependent corrections if enabled
        if 'resolution' in self.diffuse_components:
            res_scales, _ = self.resolution_component.calculate_scales_and_derivatives(data_dict)
            total_scales *= np.array(res_scales)
        
        return total_scales
    
    def get_scales_for_observation(self, 
                                  still_id: int,
                                  q_magnitude: float,
                                  **kwargs) -> Tuple[float, float]:
        """
        Get multiplicative scale and additive offset for single observation.
        
        Args:
            still_id: Still identifier
            q_magnitude: Magnitude of scattering vector
            **kwargs: Additional parameters (ignored)
            
        Returns:
            Tuple of (multiplicative_scale, additive_offset)
        """
        # Get per-still scale
        multiplicative_scale = self.per_still_component.get_scale_for_still(still_id)
        
        # Apply resolution correction if enabled
        if 'resolution' in self.diffuse_components:
            res_scale = self.resolution_component.get_scale_for_q(q_magnitude)
            multiplicative_scale *= res_scale
        
        # Additive offset is always 0.0 in v1
        additive_offset = 0.0
        
        return multiplicative_scale, additive_offset
    
    def refine_parameters(self, 
                         binned_pixel_data: Dict,
                         bragg_reflections: Dict,
                         refinement_config: Dict) -> Tuple[Dict, Dict]:
        """
        Refine model parameters using iterative minimization.
        
        Args:
            binned_pixel_data: Voxel-organized diffuse observations
            bragg_reflections: Bragg reflection data with partiality
            refinement_config: Refinement settings
            
        Returns:
            Tuple of (refined_parameters, refinement_statistics)
        """
        max_iterations = refinement_config.get('max_iterations', 10)
        convergence_tol = refinement_config.get('convergence_tolerance', 1e-4)
        
        logger.info(f"Starting parameter refinement: {max_iterations} max iterations")
        
        prev_r_factor = float('inf')
        
        for iteration in range(max_iterations):
            logger.debug(f"Refinement iteration {iteration + 1}")
            
            # Generate references with current parameters
            bragg_refs, diffuse_refs = self.generate_references(
                binned_pixel_data, bragg_reflections
            )
            
            # Calculate current R-factor
            r_factor = self._calculate_r_factor(binned_pixel_data, diffuse_refs)
            
            logger.debug(f"Iteration {iteration + 1}: R-factor = {r_factor:.6f}")
            
            # Check convergence
            if abs(prev_r_factor - r_factor) < convergence_tol:
                logger.info(f"Converged after {iteration + 1} iterations")
                break
            
            # Perform one step of parameter refinement
            self._refine_step(binned_pixel_data, diffuse_refs)
            
            prev_r_factor = r_factor
        
        # Extract refined parameters
        refined_params = self._extract_refined_parameters()
        
        # Calculate final statistics
        final_stats = {
            'n_iterations': iteration + 1,
            'final_r_factor': r_factor,
            'convergence_achieved': iteration < max_iterations - 1,
            'parameter_shifts': self._calculate_parameter_shifts()
        }
        
        self.refinement_statistics = final_stats
        logger.info(f"Refinement completed: R-factor = {r_factor:.6f}")
        
        return refined_params, final_stats
    
    def generate_references(self, 
                           binned_pixel_data: Dict,
                           bragg_reflections: Dict) -> Tuple[Dict, Dict]:
        """
        Generate Bragg and diffuse reference intensities.
        
        Args:
            binned_pixel_data: Voxel-organized diffuse data
            bragg_reflections: Bragg reflection data
            
        Returns:
            Tuple of (bragg_references, diffuse_references)
        """
        # Generate diffuse references (main focus for diffuse scaling)
        diffuse_refs = {}
        
        for voxel_idx, voxel_data in binned_pixel_data.items():
            observations = voxel_data['observations']
            if len(observations) == 0:
                continue
            
            # Apply current scaling to observations
            scaled_intensities = []
            weights = []
            
            for obs in observations:
                still_id = obs['still_id']
                q_mag = np.linalg.norm(obs['q_vector_lab'])
                
                mult_scale, add_offset = self.get_scales_for_observation(still_id, q_mag)
                
                # Apply scaling: I_scaled = (I_obs - C) / M
                # For v1: C = 0, so I_scaled = I_obs / M
                scaled_intensity = obs['intensity'] / mult_scale
                weight = 1.0 / (obs['sigma'] / mult_scale)**2
                
                scaled_intensities.append(scaled_intensity)
                weights.append(weight)
            
            # Weighted average for reference
            if weights:
                weighted_intensities = np.array(scaled_intensities) * np.array(weights)
                total_weight = np.sum(weights)
                
                if total_weight > 0:
                    ref_intensity = np.sum(weighted_intensities) / total_weight
                    ref_sigma = 1.0 / np.sqrt(total_weight)
                    
                    diffuse_refs[voxel_idx] = {
                        'intensity': ref_intensity,
                        'sigma': ref_sigma,
                        'n_observations': len(observations)
                    }
        
        # Bragg references (simplified for now)
        bragg_refs = {}
        if bragg_reflections:
            # Implementation would filter by partiality and create references
            # For now, return empty dict
            pass
        
        logger.debug(f"Generated {len(diffuse_refs)} diffuse references")
        
        return bragg_refs, diffuse_refs
    
    def _calculate_r_factor(self, binned_pixel_data: Dict, diffuse_refs: Dict) -> float:
        """Calculate R-factor for current model."""
        numerator = 0.0
        denominator = 0.0
        
        for voxel_idx, voxel_data in binned_pixel_data.items():
            if voxel_idx not in diffuse_refs:
                continue
            
            ref_intensity = diffuse_refs[voxel_idx]['intensity']
            
            for obs in voxel_data['observations']:
                still_id = obs['still_id']
                q_mag = np.linalg.norm(obs['q_vector_lab'])
                
                mult_scale, _ = self.get_scales_for_observation(still_id, q_mag)
                scaled_obs = obs['intensity'] / mult_scale
                
                numerator += abs(scaled_obs - ref_intensity)
                denominator += scaled_obs
        
        if denominator > 0:
            return numerator / denominator
        else:
            return float('inf')
    
    def _refine_step(self, binned_pixel_data: Dict, diffuse_refs: Dict):
        """Perform one step of parameter refinement."""
        # Simplified refinement step
        # Real implementation would use DIALS minimizer
        
        # For now, implement simple gradient descent
        learning_rate = 0.01
        
        # Calculate gradients for per-still parameters
        for still_id in self.per_still_component.still_ids:
            current_scale = self.per_still_component.get_scale_for_still(still_id)
            
            # Calculate approximate gradient
            gradient = self._calculate_gradient_for_still(
                still_id, binned_pixel_data, diffuse_refs
            )
            
            # Update parameter
            new_scale = current_scale - learning_rate * gradient
            new_scale = max(0.1, min(10.0, new_scale))  # Clamp to reasonable range
            
            self.per_still_component.set_scale_for_still(still_id, new_scale)
    
    def _calculate_gradient_for_still(self, still_id: int, 
                                    binned_pixel_data: Dict, 
                                    diffuse_refs: Dict) -> float:
        """Calculate gradient for a still's scale parameter."""
        gradient = 0.0
        count = 0
        
        for voxel_idx, voxel_data in binned_pixel_data.items():
            if voxel_idx not in diffuse_refs:
                continue
            
            ref_intensity = diffuse_refs[voxel_idx]['intensity']
            
            for obs in voxel_data['observations']:
                if obs['still_id'] != still_id:
                    continue
                
                q_mag = np.linalg.norm(obs['q_vector_lab'])
                mult_scale, _ = self.get_scales_for_observation(still_id, q_mag)
                
                scaled_obs = obs['intensity'] / mult_scale
                residual = scaled_obs - ref_intensity
                
                # Gradient of residual w.r.t. scale parameter
                # d/dM [(I/M) - Iref] = -I/M^2
                grad_contrib = -obs['intensity'] / (mult_scale**2)
                gradient += residual * grad_contrib
                count += 1
        
        if count > 0:
            gradient /= count
        
        return gradient
    
    def _extract_refined_parameters(self) -> Dict:
        """Extract refined parameters from components."""
        refined_params = {}
        
        for still_id in self.per_still_component.still_ids:
            refined_params[still_id] = {
                'multiplicative_scale': self.per_still_component.get_scale_for_still(still_id),
                'additive_offset': 0.0  # Always 0.0 in v1
            }
        
        return refined_params
    
    def _calculate_parameter_shifts(self) -> Dict:
        """Calculate parameter shifts from last iteration."""
        # Simplified - would track actual shifts in real implementation
        return {'per_still': 0.001}
    
    def get_model_info(self) -> Dict:
        """Get comprehensive model information."""
        info = {
            'model_type': 'DiffuseScalingModel_v1',
            'n_total_params': self.n_total_params,
            'components': {},
            'refinement_statistics': self.refinement_statistics,
            'partiality_threshold': self.partiality_threshold
        }
        
        for comp_name, component in self.diffuse_components.items():
            info['components'][comp_name] = component.get_component_info()
        
        return info
    
    def __str__(self) -> str:
        """String representation of model."""
        comp_names = list(self.diffuse_components.keys())
        return f"DiffuseScalingModel({self.n_total_params} params, components: {comp_names})"
</file>

<file path="src/diffusepipe/types/__init__.py">
# This file makes src/diffusepipe/types a Python package.
</file>

<file path="src/diffusepipe/utils/cbf_utils_IDL.md">
// == BEGIN IDL ==
module src.diffusepipe.utils {

    # @depends_on_resource(type="ExternalLibrary:dxtbx", purpose="Primary CBF file parsing and scan object access")
    # @depends_on_resource(type="FileSystem", purpose="Reading CBF files for header analysis")
    # @depends_on_resource(type="Logging", purpose="Debug and warning messages for parsing results")

    // Utility interface for CBF header parsing and data type detection
    // Critical for Module 1.S.0: CBF Data Type Detection and Processing Route Selection
    interface CBFHeaderParser {

        // --- Method: get_angle_increment_from_cbf ---
        // Preconditions:
        // - `image_path` must point to a readable CBF file.
        // - The CBF file should have a valid header format (either dxtbx-parseable or standard text format).
        // Postconditions:
        // - Returns the Angle_increment value in degrees, or None if not determinable.
        // - Return value interpretation:
        //   - 0.0: True stills data (no oscillation) → route to DIALSStillsProcessAdapter
        //   - > 0.0: Sequence data (oscillation per frame) → route to DIALSSequenceProcessAdapter  
        //   - None: Could not determine → caller should default to sequence processing (safer)
        // Behavior:
        // - **Two-Phase Parsing Strategy for Robustness:**
        //   1. **Primary Method:** Uses dxtbx.load() to access scan object and get oscillation information.
        //      - More robust and handles various CBF formats correctly.
        //      - Accesses `scan.get_oscillation()[1]` for oscillation width per frame.
        //      - Handles missing scan objects gracefully (treats as stills).
        //   2. **Fallback Method:** Direct CBF header text parsing using regex.
        //      - Used when dxtbx is unavailable or fails.
        //      - Searches for "# Angle_increment <value> deg." pattern in header.
        //      - Handles case-insensitive matching and variable spacing.
        // - **Robust Error Handling:** Gracefully handles missing dxtbx, file read errors, and malformed headers.
        // - **Header-Only Processing:** Stops reading when binary data section is reached for efficiency.
        // - **Logging:** Provides detailed debug information for troubleshooting data type detection issues.
        // @raises_error(condition="IOError", description="When CBF file cannot be read or accessed")
        // @raises_error(condition="Exception", description="When both dxtbx and text parsing methods fail critically")
        static optional float get_angle_increment_from_cbf(
            string image_path    // Path to the CBF file to analyze
        );

        // --- Method: _parse_cbf_header_text (Internal) ---
        // Preconditions:
        // - `image_path` must point to a readable CBF file.
        // Postconditions:
        // - Returns the Angle_increment value parsed from header text, or None if not found.
        // Behavior:
        // - **Fallback Parsing Method:** Used when dxtbx is unavailable or primary method fails.
        // - **Regex Pattern Matching:** Uses flexible regex to match "Angle_increment" lines with variations:
        //   - Case-insensitive matching (Angle_increment, angle_increment, ANGLE_INCREMENT)
        //   - Variable whitespace handling
        //   - Optional "deg." suffix handling
        //   - Support for positive/negative values and decimals
        // - **Efficient File Reading:** Reads CBF files in chunks (16KB) to handle large files efficiently.
        // - **Binary Section Detection:** Stops parsing when reaching the binary data portion of CBF files.
        // - **Error Recovery:** Continues searching if individual line parsing fails.
        // @raises_error(condition="IOError", description="When CBF file cannot be read")
        static optional float _parse_cbf_header_text(
            string image_path    // Path to the CBF file to parse
        );
    }
}
// == END IDL ==
</file>

<file path="src/diffusepipe/voxelization/__init__.py">
"""
Voxelization module for Phase 3 of the diffuse scattering pipeline.

This module provides:
- GlobalVoxelGrid: Define common 3D reciprocal space grid
- VoxelAccumulator: Bin diffuse pixel observations with HDF5 backend
- GlobalVoxelGridConfig: Configuration for grid creation
- CorrectedDiffusePixelData: Data structure for Phase 2 output
"""

from .global_voxel_grid import GlobalVoxelGrid, GlobalVoxelGridConfig, CorrectedDiffusePixelData
from .voxel_accumulator import VoxelAccumulator

__all__ = [
    'GlobalVoxelGrid',
    'GlobalVoxelGridConfig', 
    'CorrectedDiffusePixelData',
    'VoxelAccumulator'
]
</file>

<file path="src/diffusepipe/voxelization/global_voxel_grid_IDL.md">
# GlobalVoxelGrid IDL

**Module Path:** `src.diffusepipe.voxelization.global_voxel_grid`

**Dependencies:** 
- `@depends_on(dxtbx.model.Crystal)` - DIALS crystal models
- `@depends_on(scitbx.matrix)` - Matrix operations for HKL transformations
- `@depends_on(cctbx.uctbx)` - Unit cell averaging utilities
- `@depends_on(numpy)` - Array operations for q-vector analysis

## Interface: GlobalVoxelGrid

**Purpose:** Define a common 3D reciprocal space grid for merging diffuse scattering data from all processed still images. Handles crystal model averaging, HKL range determination, and voxel indexing operations.

### Constructor

```python
def __init__(self, 
            experiment_list: list[Experiment], 
            corrected_diffuse_pixel_data: list[CorrectedDiffusePixelData],
            grid_config: GlobalVoxelGridConfig) -> None
```

**Preconditions:**
- `experiment_list` contains valid `Experiment_dials_i` objects with crystal models
- `corrected_diffuse_pixel_data` contains q-vectors from Phase 2 processing
- `grid_config.d_min_target > 0` and `grid_config.d_max_target > grid_config.d_min_target`
- `grid_config.ndiv_h,k,l` are positive integers

**Postconditions:**
- `crystal_avg_ref` is computed as robust average of all input crystal models
- HKL range covers all diffuse data within resolution limits
- Grid subdivision parameters are stored
- Diagnostic metrics computed for crystal model averaging quality

**Behavior:**
1. Robustly average unit cell parameters using CCTBX utilities
2. Average U matrices using quaternion-based method for rotation matrices  
3. Compute `A_avg_ref = U_avg_ref * B_avg_ref` setting matrix
4. Calculate RMS Δhkl diagnostic for Bragg reflections
5. Calculate RMS misorientation diagnostic between individual U matrices
6. Transform all q-vectors to fractional HKL to determine grid boundaries
7. Store grid parameters and conversion methods

**Expected Data Format:**
```python
GlobalVoxelGridConfig = {
    "d_min_target": float,      # High resolution limit (Å)
    "d_max_target": float,      # Low resolution limit (Å) 
    "ndiv_h": int,              # H subdivisions per unit cell
    "ndiv_k": int,              # K subdivisions per unit cell
    "ndiv_l": int,              # L subdivisions per unit cell
    "max_rms_delta_hkl": float  # Warning threshold for Δhkl RMS (default 0.1)
}

CorrectedDiffusePixelData = {
    "q_vectors": numpy.ndarray,     # Shape (N, 3) lab-frame q-vectors
    "intensities": numpy.ndarray,   # Shape (N,) corrected intensities
    "sigmas": numpy.ndarray,        # Shape (N,) uncertainties
    "still_ids": numpy.ndarray      # Shape (N,) still identifiers
}
```

### Methods

```python
def hkl_to_voxel_idx(self, h: float, k: float, l: float) -> int
```

**Preconditions:** HKL values within grid boundaries
**Postconditions:** Returns unique voxel index for given HKL position
**Behavior:** Maps fractional Miller indices to linear voxel index using grid subdivisions

```python
def voxel_idx_to_hkl_center(self, voxel_idx: int) -> tuple[float, float, float]
```

**Preconditions:** `voxel_idx` is valid index within grid
**Postconditions:** Returns center HKL coordinates of voxel
**Behavior:** Inverse mapping from voxel index to fractional Miller indices

```python  
def get_q_vector_for_voxel_center(self, voxel_idx: int) -> scitbx.matrix.col
```

**Preconditions:** `voxel_idx` is valid index within grid
**Postconditions:** Returns lab-frame q-vector for voxel center
**Behavior:** Transforms voxel center HKL to lab frame using `A_avg_ref`

```python
def get_crystal_averaging_diagnostics(self) -> dict
```

**Preconditions:** Grid has been initialized
**Postconditions:** Returns diagnostic metrics for crystal model averaging quality
**Behavior:** Provides RMS Δhkl, RMS misorientation, and other quality metrics

**Expected Data Format:**
```python
diagnostics = {
    "rms_delta_hkl": float,           # RMS Δhkl for Bragg reflections
    "rms_misorientation_deg": float,   # RMS misorientation in degrees
    "n_crystals_averaged": int,        # Number of input crystal models
    "hkl_range_min": tuple,           # (h_min, k_min, l_min)
    "hkl_range_max": tuple,           # (h_max, k_max, l_max)
    "total_voxels": int               # Total number of voxels in grid
}
```

### Attributes

- `crystal_avg_ref`: dxtbx.model.Crystal - Average reference crystal model
- `A_avg_ref`: scitbx.matrix.sqr - Setting matrix for grid transformations
- `hkl_min`: tuple[int, int, int] - Minimum HKL boundaries  
- `hkl_max`: tuple[int, int, int] - Maximum HKL boundaries
- `ndiv_h, ndiv_k, ndiv_l`: int - Grid subdivisions per unit cell
- `total_voxels`: int - Total number of voxels in grid

### Error Conditions

**@raises_error(condition="InvalidGridConfig", message="Grid configuration validation failed")**
- Grid subdivisions ≤ 0
- Resolution limits invalid (d_min ≥ d_max)

**@raises_error(condition="InsufficientCrystalData", message="Cannot average crystal models")**  
- Empty experiment list
- Crystal models missing or invalid

**@raises_error(condition="ExcessiveCrystalVariation", message="Crystal model averaging quality poor")**
- RMS Δhkl exceeds warning threshold
- RMS misorientation exceeds warning threshold (logged warning, not exception)

## Implementation Notes

- Use CCTBX utilities for robust unit cell averaging
- Implement quaternion-based U matrix averaging for rotation matrices
- All HKL transformations use `A_avg_ref.inverse()` matrix
- Grid boundaries include buffer for resolution limits
- Voxel indexing uses linear mapping for memory efficiency
</file>

<file path="src/diffusepipe/voxelization/global_voxel_grid.py">
"""
GlobalVoxelGrid implementation for Phase 3 voxelization.

Defines common 3D reciprocal space grid for merging diffuse scattering data.
"""

import numpy as np
import logging
from typing import List, Tuple, Dict, Optional
from dataclasses import dataclass

from scitbx import matrix
from cctbx import uctbx
from dxtbx.model import Experiment, Crystal

logger = logging.getLogger(__name__)


@dataclass
class GlobalVoxelGridConfig:
    """Configuration for GlobalVoxelGrid creation."""
    d_min_target: float  # High resolution limit (Å)
    d_max_target: float  # Low resolution limit (Å)  
    ndiv_h: int          # H subdivisions per unit cell
    ndiv_k: int          # K subdivisions per unit cell
    ndiv_l: int          # L subdivisions per unit cell
    max_rms_delta_hkl: float = 0.1  # Warning threshold for Δhkl RMS


@dataclass
class CorrectedDiffusePixelData:
    """Corrected diffuse pixel data from Phase 2."""
    q_vectors: np.ndarray     # Shape (N, 3) lab-frame q-vectors
    intensities: np.ndarray   # Shape (N,) corrected intensities
    sigmas: np.ndarray       # Shape (N,) uncertainties
    still_ids: np.ndarray    # Shape (N,) still identifiers


class GlobalVoxelGrid:
    """
    Global 3D reciprocal space grid for merging diffuse scattering data.
    
    Handles crystal model averaging, HKL range determination, and voxel indexing.
    """
    
    def __init__(self, 
                 experiment_list: List[Experiment],
                 corrected_diffuse_pixel_data: List[CorrectedDiffusePixelData],
                 grid_config: GlobalVoxelGridConfig):
        """
        Initialize GlobalVoxelGrid with crystal averaging and grid definition.
        
        Args:
            experiment_list: List of Experiment objects with crystal models
            corrected_diffuse_pixel_data: List of corrected diffuse data from Phase 2
            grid_config: Grid configuration parameters
        """
        self.config = grid_config
        self._validate_config()
        self._validate_inputs(experiment_list, corrected_diffuse_pixel_data)
        
        # Extract crystal models
        self.crystal_models = [exp.crystal for exp in experiment_list]
        self.diffuse_data = corrected_diffuse_pixel_data
        
        # Average crystal models
        logger.info(f"Averaging {len(self.crystal_models)} crystal models")
        self._average_crystal_models()
        
        # Determine HKL range from diffuse data
        logger.info("Determining HKL range from diffuse data")
        self._determine_hkl_range()
        
        # Calculate total voxels
        self.total_voxels = ((self.hkl_max[0] - self.hkl_min[0] + 1) * 
                           (self.hkl_max[1] - self.hkl_min[1] + 1) * 
                           (self.hkl_max[2] - self.hkl_min[2] + 1))
        
        logger.info(f"Grid initialized: {self.total_voxels} voxels, "
                   f"HKL range: {self.hkl_min} to {self.hkl_max}")
    
    def _validate_config(self):
        """Validate grid configuration parameters."""
        if self.config.d_min_target <= 0:
            raise ValueError("d_min_target must be positive")
        if self.config.d_max_target <= self.config.d_min_target:
            raise ValueError("d_max_target must be greater than d_min_target")
        if any(ndiv <= 0 for ndiv in [self.config.ndiv_h, self.config.ndiv_k, self.config.ndiv_l]):
            raise ValueError("Grid subdivisions must be positive integers")
    
    def _validate_inputs(self, experiment_list, diffuse_data):
        """Validate input data."""
        if not experiment_list:
            raise ValueError("experiment_list cannot be empty")
        if not diffuse_data:
            raise ValueError("corrected_diffuse_pixel_data cannot be empty")
        
        for exp in experiment_list:
            if exp.crystal is None:
                raise ValueError("All experiments must have crystal models")
    
    def _average_crystal_models(self):
        """Average crystal models to create reference crystal."""
        # Average unit cells using CCTBX utilities
        unit_cells = [crystal.get_unit_cell() for crystal in self.crystal_models]
        logger.info(f"Unit cell parameters before averaging:")
        for i, uc in enumerate(unit_cells[:3]):  # Show first 3
            logger.info(f"  Crystal {i}: {uc.parameters()}")
        
        # Use CCTBX to average unit cells robustly
        avg_unit_cell = self._average_unit_cells(unit_cells)
        logger.info(f"Average unit cell: {avg_unit_cell.parameters()}")
        
        # Average U matrices
        u_matrices = [matrix.sqr(crystal.get_U()) for crystal in self.crystal_models]
        self._check_orientation_spread(u_matrices)
        u_avg_ref = self._average_u_matrices(u_matrices)
        
        # Calculate B matrix from averaged unit cell
        b_avg_ref = matrix.sqr(avg_unit_cell.fractionalization_matrix()).transpose()
        
        # Final setting matrix
        self.A_avg_ref = u_avg_ref * b_avg_ref
        
        # Create reference crystal model
        orth_matrix = matrix.sqr(avg_unit_cell.orthogonalization_matrix())
        self.crystal_avg_ref = Crystal(
            real_space_a=orth_matrix * matrix.col((1, 0, 0)),
            real_space_b=orth_matrix * matrix.col((0, 1, 0)),
            real_space_c=orth_matrix * matrix.col((0, 0, 1)),
            space_group=self.crystal_models[0].get_space_group()
        )
        self.crystal_avg_ref.set_U(u_avg_ref)
        
        # Calculate diagnostics
        self._calculate_averaging_diagnostics()
    
    def _average_unit_cells(self, unit_cells):
        """Average unit cell parameters using CCTBX utilities."""
        # Convert to parameters and average
        params_list = [uc.parameters() for uc in unit_cells]
        params_array = np.array(params_list)
        
        # Simple arithmetic mean for now - could use more robust method
        avg_params = np.mean(params_array, axis=0)
        
        return uctbx.unit_cell(tuple(avg_params))
    
    def _check_orientation_spread(self, u_matrices):
        """Check and warn about orientation spread between crystals."""
        if len(u_matrices) < 2:
            return
        
        u_ref = u_matrices[0]
        misorientations = []
        
        for u_i in u_matrices[1:]:
            # Calculate misorientation angle using trace of rotation matrix
            rotation_matrix = u_i * u_ref.transpose()
            trace = rotation_matrix.trace()
            # Handle numerical precision issues
            trace = max(-1.0, min(3.0, trace))
            angle_rad = np.arccos((trace - 1.0) / 2.0)
            angle_deg = np.degrees(angle_rad)
            misorientations.append(angle_deg)
        
        rms_misorientation = np.sqrt(np.mean(np.array(misorientations)**2))
        self.rms_misorientation_deg = rms_misorientation
        
        if rms_misorientation > 5.0:  # 5 degree threshold
            logger.warning(f"Large RMS misorientation: {rms_misorientation:.2f}° - "
                         "may cause smearing in merged diffuse map")
        else:
            logger.info(f"RMS misorientation: {rms_misorientation:.2f}°")
    
    def _average_u_matrices(self, u_matrices):
        """Average U matrices using quaternion-based method."""
        # For simplicity, use arithmetic mean (assumes small deviations)
        # Production code should use proper quaternion averaging
        if len(u_matrices) == 0:
            raise ValueError("Cannot average empty list of U matrices")
        
        # Start with first matrix
        u_sum = u_matrices[0]
        for u_matrix in u_matrices[1:]:
            u_sum = u_sum + u_matrix
        u_avg = u_sum * (1.0 / len(u_matrices))
        
        # Orthogonalize the result using SVD-like approach
        # This is a simplified version - proper implementation would use quaternions
        return self._orthogonalize_matrix(u_avg)
    
    def _orthogonalize_matrix(self, matrix_approx):
        """Orthogonalize a nearly-orthogonal matrix."""
        # Convert to numpy for SVD
        mat_np = np.array(matrix_approx).reshape(3, 3)
        u, s, vt = np.linalg.svd(mat_np)
        
        # Ensure proper rotation (det = +1)
        orthogonal = u @ vt
        if np.linalg.det(orthogonal) < 0:
            u[:, -1] *= -1
            orthogonal = u @ vt
        
        return matrix.sqr(orthogonal.flatten())
    
    def _determine_hkl_range(self):
        """Determine HKL range from diffuse data and resolution limits."""
        all_q_vectors = []
        for data in self.diffuse_data:
            all_q_vectors.append(data.q_vectors)
        
        if not all_q_vectors:
            raise ValueError("No diffuse data provided for HKL range determination")
        
        combined_q_vectors = np.vstack(all_q_vectors)
        logger.info(f"Processing {len(combined_q_vectors)} q-vectors for HKL range")
        
        # Transform to fractional HKL using average crystal
        A_inv = self.A_avg_ref.inverse()
        hkl_fractional = []
        
        for q_vec in combined_q_vectors:
            q_matrix = matrix.col(q_vec)
            hkl_frac = A_inv * q_matrix
            hkl_fractional.append(hkl_frac.elems)
        
        hkl_array = np.array(hkl_fractional)
        
        # Apply resolution filters
        q_magnitudes = np.linalg.norm(combined_q_vectors, axis=1)
        d_spacings = 1.0 / (q_magnitudes + 1e-10)  # Avoid division by zero
        
        valid_mask = ((d_spacings >= self.config.d_min_target) & 
                     (d_spacings <= self.config.d_max_target))
        
        if not np.any(valid_mask):
            raise ValueError("No data within specified resolution limits")
        
        filtered_hkl = hkl_array[valid_mask]
        logger.info(f"After resolution filtering: {len(filtered_hkl)} observations")
        
        # Determine integer HKL boundaries with buffer
        hkl_min_frac = np.min(filtered_hkl, axis=0)
        hkl_max_frac = np.max(filtered_hkl, axis=0)
        
        # Convert to integer boundaries with subdivision buffer
        buffer = 1  # Buffer for grid edges
        ndiv_list = [self.config.ndiv_h, self.config.ndiv_k, self.config.ndiv_l]
        self.hkl_min = tuple(int(np.floor(hkl_min_frac[i] * ndiv_list[i]) - buffer)
                           for i in range(3))
        self.hkl_max = tuple(int(np.ceil(hkl_max_frac[i] * ndiv_list[i]) + buffer) 
                           for i in range(3))
        
        logger.info(f"HKL range determined: {self.hkl_min} to {self.hkl_max}")
    
    def _calculate_averaging_diagnostics(self):
        """Calculate diagnostic metrics for crystal averaging quality."""
        # For now, just store what we have
        self.diagnostics = {
            "rms_misorientation_deg": getattr(self, 'rms_misorientation_deg', 0.0),
            "n_crystals_averaged": len(self.crystal_models),
            "total_diffuse_observations": sum(len(data.q_vectors) for data in self.diffuse_data)
        }
        
        # TODO: Calculate RMS Δhkl for Bragg reflections when available
        self.diagnostics["rms_delta_hkl"] = 0.0  # Placeholder
    
    def hkl_to_voxel_idx(self, h: float, k: float, l: float) -> int:
        """Map fractional Miller indices to linear voxel index."""
        # Convert to subdivision coordinates
        h_sub = int(np.round(h * self.config.ndiv_h))
        k_sub = int(np.round(k * self.config.ndiv_k))
        l_sub = int(np.round(l * self.config.ndiv_l))
        
        # Shift to positive indices
        h_idx = h_sub - self.hkl_min[0]
        k_idx = k_sub - self.hkl_min[1]  
        l_idx = l_sub - self.hkl_min[2]
        
        # Linear index
        h_range = self.hkl_max[0] - self.hkl_min[0] + 1
        k_range = self.hkl_max[1] - self.hkl_min[1] + 1
        
        return l_idx * (h_range * k_range) + k_idx * h_range + h_idx
    
    def voxel_idx_to_hkl_center(self, voxel_idx: int) -> Tuple[float, float, float]:
        """Map voxel index to center HKL coordinates."""
        h_range = self.hkl_max[0] - self.hkl_min[0] + 1
        k_range = self.hkl_max[1] - self.hkl_min[1] + 1
        
        # Inverse linear mapping
        l_idx = voxel_idx // (h_range * k_range)
        remainder = voxel_idx % (h_range * k_range)
        k_idx = remainder // h_range
        h_idx = remainder % h_range
        
        # Convert back to HKL coordinates
        h_sub = h_idx + self.hkl_min[0]
        k_sub = k_idx + self.hkl_min[1]
        l_sub = l_idx + self.hkl_min[2]
        
        # Convert subdivision coordinates to fractional HKL
        h = h_sub / self.config.ndiv_h
        k = k_sub / self.config.ndiv_k
        l = l_sub / self.config.ndiv_l
        
        return h, k, l
    
    def get_q_vector_for_voxel_center(self, voxel_idx: int) -> matrix.col:
        """Get lab-frame q-vector for voxel center."""
        h, k, l = self.voxel_idx_to_hkl_center(voxel_idx)
        hkl = matrix.col((h, k, l))
        q_vector = self.A_avg_ref * hkl
        return q_vector
    
    def get_crystal_averaging_diagnostics(self) -> Dict:
        """Return diagnostic metrics for crystal model averaging quality."""
        return {
            **self.diagnostics,
            "hkl_range_min": self.hkl_min,
            "hkl_range_max": self.hkl_max,
            "total_voxels": self.total_voxels,
            "grid_subdivisions": (self.config.ndiv_h, self.config.ndiv_k, self.config.ndiv_l)
        }
</file>

<file path="src/diffusepipe/voxelization/voxel_accumulator_IDL.md">
# VoxelAccumulator IDL

**Module Path:** `src.diffusepipe.voxelization.voxel_accumulator`

**Dependencies:**
- `@depends_on(h5py)` - HDF5 file handling for large-scale data storage
- `@depends_on(numpy)` - Array operations and data structures
- `@depends_on(cctbx.sgtbx)` - Space group operations for ASU mapping
- `@depends_on(GlobalVoxelGrid)` - Grid definitions and HKL transformations

## Interface: VoxelAccumulator

**Purpose:** Bin corrected diffuse pixel observations into voxels with HDF5 backend for memory-efficient storage. Handles HKL transformation, ASU mapping, and incremental accumulation of observations for scaling and merging.

### Constructor

```python
def __init__(self, 
            global_voxel_grid: GlobalVoxelGrid,
            space_group_info: cctbx.sgtbx.space_group_info,
            backend: str = "hdf5",
            storage_path: Optional[str] = None) -> None
```

**Preconditions:**
- `global_voxel_grid` is valid and initialized
- `space_group_info` contains valid space group for ASU mapping
- `backend` is either "memory" or "hdf5"
- `storage_path` is writable directory if backend="hdf5"

**Postconditions:**
- Storage backend initialized (HDF5 file created if needed)
- Ready to accumulate voxel observations
- ASU mapping operations configured

**Behavior:**
1. Initialize storage backend (in-memory dict or HDF5 with zstd compression)
2. Configure space group symmetry operations for ASU mapping
3. Pre-allocate HDF5 datasets if using file backend
4. Set up efficient indexing for voxel data retrieval

### Methods

```python
def add_observations(self, 
                    still_id: int,
                    q_vectors_lab: numpy.ndarray,
                    intensities: numpy.ndarray, 
                    sigmas: numpy.ndarray) -> int
```

**Preconditions:**
- All input arrays have same length
- `q_vectors_lab` shape is (N, 3) - lab frame q-vectors
- `intensities` and `sigmas` are positive
- `still_id` is valid identifier

**Postconditions:**
- Observations binned to appropriate voxels 
- ASU symmetry applied correctly
- Returns number of observations successfully binned

**Behavior:**
1. Transform `q_vectors_lab` to fractional HKL using `global_voxel_grid.A_avg_ref.inverse()`
2. Map HKL to asymmetric unit using space group operations
3. Determine voxel indices using `global_voxel_grid.hkl_to_voxel_idx()`
4. Store `(intensity, sigma, still_id, q_lab)` for each voxel
5. Update accumulation statistics if using Welford's algorithm

**Expected Data Format:**
```python
# Input arrays
q_vectors_lab: numpy.ndarray     # Shape (N, 3) - lab frame q-vectors
intensities: numpy.ndarray       # Shape (N,) - corrected intensities
sigmas: numpy.ndarray           # Shape (N,) - intensity uncertainties
```

```python
def get_observations_for_voxel(self, voxel_idx: int) -> dict
```

**Preconditions:** `voxel_idx` is valid voxel index
**Postconditions:** Returns all observations for specified voxel
**Behavior:** Retrieves stored observations from backend for given voxel

**Expected Data Format:**
```python
voxel_observations = {
    "intensities": numpy.ndarray,     # All intensities for this voxel
    "sigmas": numpy.ndarray,         # Corresponding uncertainties  
    "still_ids": numpy.ndarray,      # Still identifiers
    "q_vectors_lab": numpy.ndarray,  # Original lab-frame q-vectors
    "n_observations": int            # Number of observations
}
```

```python
def get_all_binned_data_for_scaling(self) -> dict
```

**Preconditions:** Accumulation is complete
**Postconditions:** Returns complete binned dataset for scaling
**Behavior:** Assembles all voxel data in format required by scaling algorithms

**Expected Data Format:**
```python
binned_data_global = {
    voxel_idx: {
        "observations": list[ObservationTuple],  # List of (I, sigma, still_id, q_lab)
        "n_observations": int,
        "voxel_center_hkl": tuple[float, float, float],
        "voxel_center_q": numpy.ndarray  # Shape (3,) lab-frame q-vector
    }
    # ... for all voxels with data
}

ObservationTuple = {
    "intensity": float,
    "sigma": float, 
    "still_id": int,
    "q_vector_lab": numpy.ndarray  # Shape (3,)
}
```

```python
def get_accumulation_statistics(self) -> dict
```

**Preconditions:** Some observations have been added
**Postconditions:** Returns statistics about accumulated data
**Behavior:** Provides summary statistics for data quality assessment

**Expected Data Format:**
```python
statistics = {
    "total_observations": int,
    "unique_voxels": int,
    "observations_per_voxel_stats": {
        "mean": float,
        "std": float,
        "min": int,
        "max": int
    },
    "still_distribution": dict[int, int],  # still_id -> n_observations
    "resolution_range": {
        "d_min": float,  # Minimum d-spacing observed
        "d_max": float   # Maximum d-spacing observed  
    }
}
```

```python
def finalize(self) -> None
```

**Preconditions:** All observations have been added
**Postconditions:** Storage backend optimized and ready for access
**Behavior:** Closes HDF5 file handles, optimizes storage, prepares for scaling phase

### Attributes

- `global_voxel_grid`: GlobalVoxelGrid - Grid definition and transformations
- `space_group`: cctbx.sgtbx.space_group - Space group for ASU mapping
- `backend`: str - Storage backend type ("memory" or "hdf5")
- `storage_path`: str - Path to HDF5 file if using file backend
- `n_total_observations`: int - Total observations accumulated
- `n_unique_voxels`: int - Number of voxels with data

### Error Conditions

**@raises_error(condition="InvalidVoxelIndex", message="Voxel index out of bounds")**
- Voxel index outside grid boundaries
- Negative voxel indices

**@raises_error(condition="StorageBackendError", message="Backend storage operation failed")**
- HDF5 file creation/write errors
- Disk space issues for large datasets

**@raises_error(condition="DataConsistencyError", message="Input array dimensions mismatch")**
- Array length mismatches between q_vectors, intensities, sigmas
- Invalid array shapes

**@raises_error(condition="SymmetryError", message="ASU mapping failed")**  
- Space group operations failed
- Invalid HKL coordinates for symmetry operations

## Implementation Notes

- Use HDF5 with zstd compression for large datasets
- Implement chunked storage for efficient random access
- Pre-allocate datasets when possible for performance
- Use memory backend for small test datasets
- Store q_vectors in original lab frame for traceability
- ASU mapping uses CCTBX space group operations
- Voxel indices calculated using GlobalVoxelGrid methods
- Support both incremental addition and batch processing
</file>

<file path="src/diffusepipe/voxelization/voxel_accumulator.py">
"""
VoxelAccumulator implementation for Phase 3 binning.

Bins corrected diffuse pixel observations into voxels with HDF5 backend.
"""

import numpy as np
import logging
import tempfile
import os
from typing import Dict, List, Optional, Tuple, Any
from dataclasses import dataclass
from collections import defaultdict

try:
    import h5py
    HDF5_AVAILABLE = True
except ImportError:
    HDF5_AVAILABLE = False
    h5py = None

from scitbx import matrix
from cctbx import sgtbx

from .global_voxel_grid import GlobalVoxelGrid

logger = logging.getLogger(__name__)


@dataclass 
class ObservationData:
    """Single observation data structure."""
    intensity: float
    sigma: float
    still_id: int
    q_vector_lab: np.ndarray  # Shape (3,)


class VoxelAccumulator:
    """
    Bins corrected diffuse pixel observations into voxels.
    
    Supports both in-memory and HDF5 backends for memory management.
    Handles HKL transformation and ASU mapping.
    """
    
    def __init__(self,
                 global_voxel_grid: GlobalVoxelGrid,
                 space_group_info: sgtbx.space_group_info,
                 backend: str = "memory",
                 storage_path: Optional[str] = None):
        """
        Initialize VoxelAccumulator.
        
        Args:
            global_voxel_grid: Grid definition for voxel mapping
            space_group_info: Space group for ASU mapping
            backend: "memory" or "hdf5" for storage
            storage_path: Path for HDF5 file if backend="hdf5"
        """
        self.global_voxel_grid = global_voxel_grid
        self.space_group = space_group_info.group()
        self.backend = backend
        self.storage_path = storage_path
        
        # Statistics
        self.n_total_observations = 0
        self.n_unique_voxels = 0
        self._still_observation_counts = defaultdict(int)
        
        # Initialize storage backend
        self._initialize_storage()
        
        logger.info(f"VoxelAccumulator initialized with {backend} backend")
    
    def _initialize_storage(self):
        """Initialize the storage backend."""
        if self.backend == "memory":
            self._voxel_data = defaultdict(list)
        elif self.backend == "hdf5":
            if not HDF5_AVAILABLE:
                raise RuntimeError("h5py not available for HDF5 backend")
            self._initialize_hdf5_storage()
        else:
            raise ValueError(f"Unknown backend: {self.backend}")
    
    def _initialize_hdf5_storage(self):
        """Initialize HDF5 storage backend."""
        if self.storage_path is None:
            # Create temporary file
            temp_dir = tempfile.mkdtemp()
            self.storage_path = os.path.join(temp_dir, "voxel_accumulator.h5")
        
        self.h5_file = h5py.File(self.storage_path, 'w')
        
        # Create main datasets with chunking and compression
        max_obs_estimate = 1000000  # Estimate for dataset size
        
        # Main observation storage
        self.h5_intensities = self.h5_file.create_dataset(
            'intensities', (0,), maxshape=(None,), dtype='f4',
            chunks=True, compression='gzip', shuffle=True
        )
        self.h5_sigmas = self.h5_file.create_dataset(
            'sigmas', (0,), maxshape=(None,), dtype='f4', 
            chunks=True, compression='gzip', shuffle=True
        )
        self.h5_still_ids = self.h5_file.create_dataset(
            'still_ids', (0,), maxshape=(None,), dtype='i4',
            chunks=True, compression='gzip', shuffle=True
        )
        self.h5_voxel_indices = self.h5_file.create_dataset(
            'voxel_indices', (0,), maxshape=(None,), dtype='i8',
            chunks=True, compression='gzip', shuffle=True
        )
        self.h5_q_vectors = self.h5_file.create_dataset(
            'q_vectors', (0, 3), maxshape=(None, 3), dtype='f4',
            chunks=True, compression='gzip', shuffle=True
        )
        
        # Voxel index mapping for efficient access
        self._voxel_ranges = {}  # voxel_idx -> (start, end) in arrays
        self._current_obs_count = 0
        
        logger.info(f"HDF5 storage initialized at {self.storage_path}")
    
    def add_observations(self, 
                        still_id: int,
                        q_vectors_lab: np.ndarray,
                        intensities: np.ndarray,
                        sigmas: np.ndarray) -> int:
        """
        Add observations for a still to appropriate voxels.
        
        Args:
            still_id: Identifier for the still
            q_vectors_lab: Lab-frame q-vectors, shape (N, 3)
            intensities: Corrected intensities, shape (N,)
            sigmas: Intensity uncertainties, shape (N,)
            
        Returns:
            Number of observations successfully binned
        """
        if len(q_vectors_lab) != len(intensities) or len(intensities) != len(sigmas):
            raise ValueError("Array length mismatch in input data")
        
        n_input = len(q_vectors_lab)
        if n_input == 0:
            return 0
        
        logger.debug(f"Adding {n_input} observations from still {still_id}")
        
        # Transform q-vectors to fractional HKL
        A_inv = self.global_voxel_grid.A_avg_ref.inverse()
        hkl_fractional = []
        
        for q_vec in q_vectors_lab:
            q_matrix = matrix.col(q_vec)
            hkl_frac = A_inv * q_matrix
            hkl_fractional.append(hkl_frac.elems)
        
        hkl_array = np.array(hkl_fractional)
        
        # Apply ASU mapping
        hkl_asu = self._map_to_asu(hkl_array)
        
        # Get voxel indices
        voxel_indices = []
        valid_mask = []
        
        for i, (h, k, l) in enumerate(hkl_asu):
            try:
                voxel_idx = self.global_voxel_grid.hkl_to_voxel_idx(h, k, l)
                voxel_indices.append(voxel_idx)
                valid_mask.append(True)
            except (ValueError, IndexError):
                # Outside grid boundaries
                valid_mask.append(False)
        
        valid_mask = np.array(valid_mask)
        n_valid = np.sum(valid_mask)
        
        if n_valid == 0:
            logger.warning(f"No valid observations for still {still_id}")
            return 0
        
        # Filter to valid observations
        valid_q_vectors = q_vectors_lab[valid_mask]
        valid_intensities = intensities[valid_mask]
        valid_sigmas = sigmas[valid_mask]
        valid_voxel_indices = np.array(voxel_indices)[valid_mask]
        
        # Store observations
        if self.backend == "memory":
            self._store_observations_memory(
                still_id, valid_voxel_indices, valid_intensities, 
                valid_sigmas, valid_q_vectors
            )
        else:
            self._store_observations_hdf5(
                still_id, valid_voxel_indices, valid_intensities,
                valid_sigmas, valid_q_vectors  
            )
        
        # Update statistics
        self.n_total_observations += n_valid
        self._still_observation_counts[still_id] += n_valid
        
        logger.debug(f"Successfully binned {n_valid}/{n_input} observations")
        return n_valid
    
    def _map_to_asu(self, hkl_array: np.ndarray) -> np.ndarray:
        """Map HKL indices to asymmetric unit."""
        hkl_asu = []
        
        for h, k, l in hkl_array:
            # Convert to miller index
            miller_index = (int(round(h)), int(round(k)), int(round(l)))
            
            # For P1 space group, ASU mapping is identity
            # For other space groups, would use proper ASU mapping
            asu_hkl = miller_index
            hkl_asu.append(asu_hkl)
        
        return np.array(hkl_asu, dtype=float)
    
    def _store_observations_memory(self, still_id: int, voxel_indices: np.ndarray,
                                 intensities: np.ndarray, sigmas: np.ndarray,
                                 q_vectors: np.ndarray):
        """Store observations in memory backend."""
        for i, voxel_idx in enumerate(voxel_indices):
            obs = ObservationData(
                intensity=intensities[i],
                sigma=sigmas[i],
                still_id=still_id,
                q_vector_lab=q_vectors[i]
            )
            self._voxel_data[voxel_idx].append(obs)
    
    def _store_observations_hdf5(self, still_id: int, voxel_indices: np.ndarray,
                                intensities: np.ndarray, sigmas: np.ndarray,
                                q_vectors: np.ndarray):
        """Store observations in HDF5 backend."""
        n_new = len(voxel_indices)
        start_idx = self._current_obs_count
        end_idx = start_idx + n_new
        
        # Resize datasets
        self.h5_intensities.resize((end_idx,))
        self.h5_sigmas.resize((end_idx,))
        self.h5_still_ids.resize((end_idx,))
        self.h5_voxel_indices.resize((end_idx,))
        self.h5_q_vectors.resize((end_idx, 3))
        
        # Store data
        self.h5_intensities[start_idx:end_idx] = intensities
        self.h5_sigmas[start_idx:end_idx] = sigmas
        self.h5_still_ids[start_idx:end_idx] = still_id
        self.h5_voxel_indices[start_idx:end_idx] = voxel_indices
        self.h5_q_vectors[start_idx:end_idx] = q_vectors
        
        self._current_obs_count = end_idx
    
    def get_observations_for_voxel(self, voxel_idx: int) -> Dict[str, np.ndarray]:
        """Get all observations for a specific voxel."""
        if self.backend == "memory":
            return self._get_voxel_observations_memory(voxel_idx)
        else:
            return self._get_voxel_observations_hdf5(voxel_idx)
    
    def _get_voxel_observations_memory(self, voxel_idx: int) -> Dict[str, np.ndarray]:
        """Get voxel observations from memory backend."""
        observations = self._voxel_data.get(voxel_idx, [])
        
        if not observations:
            return {
                "intensities": np.array([]),
                "sigmas": np.array([]),
                "still_ids": np.array([]),
                "q_vectors_lab": np.array([]).reshape(0, 3),
                "n_observations": 0
            }
        
        intensities = np.array([obs.intensity for obs in observations])
        sigmas = np.array([obs.sigma for obs in observations])
        still_ids = np.array([obs.still_id for obs in observations])
        q_vectors = np.array([obs.q_vector_lab for obs in observations])
        
        return {
            "intensities": intensities,
            "sigmas": sigmas,
            "still_ids": still_ids,
            "q_vectors_lab": q_vectors,
            "n_observations": len(observations)
        }
    
    def _get_voxel_observations_hdf5(self, voxel_idx: int) -> Dict[str, np.ndarray]:
        """Get voxel observations from HDF5 backend."""
        # Find all indices for this voxel
        voxel_mask = self.h5_voxel_indices[:] == voxel_idx
        indices = np.where(voxel_mask)[0]
        
        if len(indices) == 0:
            return {
                "intensities": np.array([]),
                "sigmas": np.array([]),
                "still_ids": np.array([]),
                "q_vectors_lab": np.array([]).reshape(0, 3),
                "n_observations": 0
            }
        
        return {
            "intensities": self.h5_intensities[indices],
            "sigmas": self.h5_sigmas[indices],
            "still_ids": self.h5_still_ids[indices],
            "q_vectors_lab": self.h5_q_vectors[indices],
            "n_observations": len(indices)
        }
    
    def get_all_binned_data_for_scaling(self) -> Dict[int, Dict[str, Any]]:
        """Get complete binned dataset for scaling algorithms."""
        binned_data = {}
        
        if self.backend == "memory":
            unique_voxels = list(self._voxel_data.keys())
        else:
            # Get unique voxel indices from HDF5
            unique_voxels = np.unique(self.h5_voxel_indices[:])
        
        self.n_unique_voxels = len(unique_voxels)
        
        for voxel_idx in unique_voxels:
            voxel_obs = self.get_observations_for_voxel(voxel_idx)
            
            if voxel_obs["n_observations"] > 0:
                # Get voxel center coordinates
                h, k, l = self.global_voxel_grid.voxel_idx_to_hkl_center(voxel_idx)
                q_center = self.global_voxel_grid.get_q_vector_for_voxel_center(voxel_idx)
                
                # Format observations for scaling
                observations = []
                for i in range(voxel_obs["n_observations"]):
                    obs = {
                        "intensity": voxel_obs["intensities"][i],
                        "sigma": voxel_obs["sigmas"][i],
                        "still_id": voxel_obs["still_ids"][i],
                        "q_vector_lab": voxel_obs["q_vectors_lab"][i]
                    }
                    observations.append(obs)
                
                binned_data[voxel_idx] = {
                    "observations": observations,
                    "n_observations": voxel_obs["n_observations"],
                    "voxel_center_hkl": (h, k, l),
                    "voxel_center_q": np.array(q_center.elems)
                }
        
        logger.info(f"Retrieved binned data for {len(binned_data)} voxels")
        return binned_data
    
    def get_accumulation_statistics(self) -> Dict[str, Any]:
        """Get statistics about accumulated data."""
        if self.backend == "memory":
            voxel_obs_counts = [len(obs_list) for obs_list in self._voxel_data.values()]
            unique_voxels = len(self._voxel_data)
        else:
            unique_voxels = len(np.unique(self.h5_voxel_indices[:]))
            # Calculate observations per voxel
            voxel_indices, counts = np.unique(self.h5_voxel_indices[:], return_counts=True)
            voxel_obs_counts = counts.tolist()
        
        if voxel_obs_counts:
            obs_stats = {
                "mean": np.mean(voxel_obs_counts),
                "std": np.std(voxel_obs_counts),
                "min": int(np.min(voxel_obs_counts)),
                "max": int(np.max(voxel_obs_counts))
            }
        else:
            obs_stats = {"mean": 0, "std": 0, "min": 0, "max": 0}
        
        return {
            "total_observations": self.n_total_observations,
            "unique_voxels": unique_voxels,
            "observations_per_voxel_stats": obs_stats,
            "still_distribution": dict(self._still_observation_counts),
            "backend": self.backend
        }
    
    def finalize(self):
        """Finalize storage and prepare for access."""
        if self.backend == "hdf5" and hasattr(self, 'h5_file'):
            self.h5_file.flush()
            logger.info("HDF5 storage finalized")
        
        stats = self.get_accumulation_statistics()
        logger.info(f"VoxelAccumulator finalized: {stats['total_observations']} observations "
                   f"in {stats['unique_voxels']} voxels")
    
    def __del__(self):
        """Cleanup HDF5 resources."""
        if hasattr(self, 'h5_file') and self.h5_file:
            try:
                self.h5_file.close()
            except:
                pass
</file>

<file path="src/diffusepipe/__init__.py">
# This file makes src/diffusepipe a Python package.
</file>

<file path="src/diffusepipe/corrections_IDL.md">
// == BEGIN IDL ==
module src.diffusepipe.corrections {

    # @depends_on_resource(type="ExternalLibrary:numpy", purpose="Numerical calculations for correction factors")
    # @depends_on_resource(type="Logging", purpose="Debug logging of correction calculations")

    // Utility module providing helper functions for pixel correction factors
    // as specified in Module 2.S.2 of the plan (rule 0.7: all corrections as multipliers)
    interface CorrectionsHelper {

        // --- Method: apply_corrections ---
        // Preconditions:
        // - All correction factor parameters must be positive finite numbers.
        // - `raw_I` must be a non-negative intensity value.
        // Postconditions:
        // - Returns the corrected intensity with all factors applied as multipliers.
        // - The result equals `raw_I * lp_mult * qe_mult * sa_mult * air_mult`.
        // Behavior:
        // - **Centralizes Correction Logic:** Implements the multiplicative correction formula from Module 2.S.2.
        // - **All Multipliers Convention:** Enforces the project convention that all corrections are expressed as multipliers (rule 0.7).
        // - **Debug Logging:** Logs individual correction values for debugging and validation.
        // - **Mathematical Formula:** TotalCorrection_mult = LP_mult × QE_mult × SA_mult × Air_mult
        // @raises_error(condition="ValueError", description="When any correction factor is non-positive or non-finite")
        static float apply_corrections(
            float raw_I,        // Raw intensity value
            float lp_mult,      // Lorentz-Polarization correction multiplier (1/LP_divisor)
            float qe_mult,      // Quantum Efficiency correction multiplier
            float sa_mult,      // Solid Angle correction multiplier (1/solid_angle)
            float air_mult      // Air Attenuation correction multiplier (1/attenuation)
        );

        // --- Method: calculate_analytic_pixel_corrections_45deg ---
        // Preconditions:
        // - `raw_intensity` must be non-negative.
        // - `wavelength_angstrom` must be positive (typical X-ray wavelengths: 0.5-2.0 Å).
        // - `detector_distance_mm` must be positive.
        // - `pixel_size_mm` must be positive.
        // - `air_path_length_mm` (if provided) must be non-negative.
        // Postconditions:
        // - Returns tuple: (corrected_intensity, correction_factors_dict).
        // - `correction_factors_dict` contains individual correction factors for validation.
        // - The corrected intensity should recover analytic intensity within 1% (regression test requirement).
        // Behavior:
        // - **Regression Test Function:** Provides known analytic values for testing the correction pipeline.
        // - **45° Scattering Geometry:** Calculates corrections for a pixel at exactly 45° scattering angle.
        // - **Lorentz-Polarization:** Uses formula LP = 1/(sin²θ × (1 + cos²(2θ))) for unpolarized X-rays.
        // - **Solid Angle:** Calculates geometric solid angle considering pixel position and size.
        // - **Air Attenuation:** Applies Beer-Lambert law with approximate μ_air for testing purposes.
        // - **QE Correction:** Assumes ideal detector (QE = 1.0) for analytical simplicity.
        // - **Validation Target:** This function must recover analytic intensity to <1% accuracy for regression testing.
        static tuple<float, map<string, float>> calculate_analytic_pixel_corrections_45deg(
            float raw_intensity,              // Raw pixel intensity
            float wavelength_angstrom,        // X-ray wavelength in Angstroms
            float detector_distance_mm,       // Sample-to-detector distance in mm
            float pixel_size_mm,              // Pixel size in mm (assumed square)
            optional float air_path_length_mm // Air path length in mm for attenuation calculation
        );

        // --- Method: validate_correction_factors ---
        // Preconditions:
        // - `correction_factors` must be a map containing correction factor values.
        // - `tolerance` must be a positive float.
        // Postconditions:
        // - Returns tuple: (is_valid, error_message).
        // - `is_valid` is true only if all correction factors pass reasonableness checks.
        // - `error_message` provides specific details about any validation failures.
        // Behavior:
        // - **Reasonableness Checks:** Validates that correction factors fall within expected physical ranges.
        // - **Positivity Check:** All factors must be positive.
        // - **LP Range Check:** LP correction should be between 0.1 and 10.0 (geometric limits).
        // - **QE Range Check:** QE correction should be between 0.1 and 2.0 (realistic detector efficiency range).
        // - **SA Range Check:** Solid angle correction should not be extreme (1e-6 to 1e6).
        // - **Air Range Check:** Air attenuation should be close to 1.0 (0.9 to 1.2 for typical geometries).
        // - **Error Reporting:** Provides specific error messages for debugging.
        static tuple<boolean, string> validate_correction_factors(
            map<string, float> correction_factors, // Dictionary of correction factors to validate
            optional float tolerance               // Tolerance for validation checks (default: 0.01)
        );

        // --- Method: create_synthetic_experiment_for_testing ---
        // Preconditions:
        // - Mock testing framework must be available.
        // Postconditions:
        // - Returns a mock DIALS Experiment object with proper geometric parameters.
        // - The mock object supports necessary methods for correction calculations.
        // Behavior:
        // - **Test Fixture Creation:** Creates synthetic DIALS-like objects for unit testing.
        // - **45° Geometry Setup:** Positions detector and beam for 45° scattering angle testing.
        // - **Proper Mock Structure:** Implements necessary DIALS API methods with realistic values.
        // - **Regression Test Support:** Provides controlled geometry for validating correction calculations.
        // - **Beam Configuration:** Sets wavelength and direction vector appropriate for testing.
        // - **Detector Configuration:** Sets pixel size, position, and axes for realistic geometry.
        static object create_synthetic_experiment_for_testing();
    }
}
// == END IDL ==
</file>

<file path="tests/diagnostics/test_q_calculator.py">
"""Tests for QValueCalculator."""

import pytest
import numpy as np
import tempfile
import os
from unittest.mock import Mock, patch, MagicMock

from diffusepipe.diagnostics.q_calculator import QValueCalculator
from diffusepipe.types.types_IDL import ComponentInputFiles, OperationOutcome


@pytest.fixture
def calculator():
    """Create a QValueCalculator instance for testing."""
    return QValueCalculator()


@pytest.fixture
def mock_inputs():
    """Create mock ComponentInputFiles."""
    return ComponentInputFiles(
        dials_expt_path="/path/to/test.expt"
    )


@pytest.fixture
def mock_experiment():
    """Create a mock DIALS experiment with beam and detector."""
    experiment = MagicMock()
    
    # Mock beam model
    beam = MagicMock()
    beam.get_wavelength.return_value = 1.0  # 1 Angstrom
    beam.get_s0.return_value = [0.0, 0.0, 1.0]  # Beam along +z direction
    experiment.beam = beam
    
    # Mock detector with single panel
    panel = MagicMock()
    panel.get_image_size.return_value = (100, 100)  # 100x100 pixels
    
    # Mock get_pixel_lab_coord to return predictable coordinates
    def mock_get_pixel_lab_coord(pixel_coord):
        fast_idx, slow_idx = pixel_coord
        # Return coordinates that simulate a simple flat detector
        # positioned at z = 100mm with pixel size 0.1mm
        x = (fast_idx - 50) * 0.1  # Center at x=0
        y = (slow_idx - 50) * 0.1  # Center at y=0
        z = 100.0  # Fixed distance
        return [x, y, z]
    
    panel.get_pixel_lab_coord.side_effect = mock_get_pixel_lab_coord
    
    detector = MagicMock()
    detector.__len__.return_value = 1
    detector.__iter__.return_value = iter([panel])
    detector.__getitem__.return_value = panel
    experiment.detector = detector
    
    return experiment


@pytest.fixture
def mock_experiment_list(mock_experiment):
    """Create a mock DIALS experiment list."""
    exp_list = MagicMock()
    exp_list.__len__.return_value = 1
    exp_list.__getitem__.return_value = mock_experiment
    return exp_list


class TestQValueCalculator:
    """Test cases for QValueCalculator."""

    def test_init(self, calculator):
        """Test calculator initialization."""
        assert calculator is not None

    def test_calculate_q_map_missing_expt_path(self, calculator):
        """Test calculation with missing experiment path."""
        inputs = ComponentInputFiles()  # No dials_expt_path
        
        result = calculator.calculate_q_map(inputs, "test_output")
        
        assert result.status == "FAILURE"
        assert result.error_code == "InputFileError"
        assert "not provided" in result.message

    def test_calculate_q_map_file_not_found(self, calculator):
        """Test calculation with non-existent experiment file."""
        inputs = ComponentInputFiles(dials_expt_path="/nonexistent/file.expt")
        
        result = calculator.calculate_q_map(inputs, "test_output")
        
        assert result.status == "FAILURE"
        assert result.error_code == "InputFileError"
        assert "not found" in result.message

    @patch('diffusepipe.diagnostics.q_calculator.os.path.exists')
    @patch('dxtbx.model.ExperimentList.from_file')
    @patch('numpy.save')
    def test_calculate_q_map_success_single_panel(
        self, mock_np_save, mock_from_file, mock_exists, calculator, mock_inputs, mock_experiment_list
    ):
        """Test successful q-map calculation for single panel detector."""
        # Setup mocks
        mock_exists.return_value = True
        mock_from_file.return_value = mock_experiment_list
        
        # Execute
        result = calculator.calculate_q_map(mock_inputs, "test_output")
        
        # Verify
        assert result.status == "SUCCESS"
        assert "Successfully generated q-maps" in result.message
        
        # Check output artifacts
        assert "qx_map_path" in result.output_artifacts
        assert "qy_map_path" in result.output_artifacts
        assert "qz_map_path" in result.output_artifacts
        
        # Verify np.save was called for each component
        assert mock_np_save.call_count == 3
        
        # Check file paths
        assert "test_output_qx.npy" in result.output_artifacts["qx_map_path"]
        assert "test_output_qy.npy" in result.output_artifacts["qy_map_path"]
        assert "test_output_qz.npy" in result.output_artifacts["qz_map_path"]

    @patch('diffusepipe.diagnostics.q_calculator.os.path.exists')
    @patch('dxtbx.model.ExperimentList.from_file')
    @patch('numpy.save')
    def test_calculate_q_map_success_multi_panel(
        self, mock_np_save, mock_from_file, mock_exists, calculator, mock_inputs
    ):
        """Test successful q-map calculation for multi-panel detector."""
        # Setup multi-panel experiment
        experiment = MagicMock()
        beam = MagicMock()
        beam.get_wavelength.return_value = 1.0
        beam.get_s0.return_value = [0.0, 0.0, 1.0]
        experiment.beam = beam
        
        # Create two panels
        panel1 = MagicMock()
        panel1.get_image_size.return_value = (50, 50)
        panel1.get_pixel_lab_coord.return_value = [0.0, 0.0, 100.0]
        
        panel2 = MagicMock()
        panel2.get_image_size.return_value = (50, 50)
        panel2.get_pixel_lab_coord.return_value = [0.0, 0.0, 100.0]
        
        detector = MagicMock()
        detector.__len__.return_value = 2
        detector.__iter__.return_value = iter([panel1, panel2])
        experiment.detector = detector
        
        exp_list = MagicMock()
        exp_list.__len__.return_value = 1
        exp_list.__getitem__.return_value = experiment
        
        mock_exists.return_value = True
        mock_from_file.return_value = exp_list
        
        # Execute
        result = calculator.calculate_q_map(mock_inputs, "test_output")
        
        # Verify
        assert result.status == "SUCCESS"
        
        # Check that panel-specific paths are created
        assert "panel0_qx_map_path" in result.output_artifacts
        assert "panel1_qx_map_path" in result.output_artifacts
        
        # Verify np.save was called for both panels (3 components × 2 panels = 6)
        assert mock_np_save.call_count == 6

    @patch('diffusepipe.diagnostics.q_calculator.os.path.exists')
    @patch('dxtbx.model.ExperimentList.from_file')
    def test_calculate_q_map_empty_experiment_list(
        self, mock_from_file, mock_exists, calculator, mock_inputs
    ):
        """Test calculation with empty experiment list."""
        mock_exists.return_value = True
        empty_exp_list = MagicMock()
        empty_exp_list.__len__.return_value = 0
        mock_from_file.return_value = empty_exp_list
        
        result = calculator.calculate_q_map(mock_inputs, "test_output")
        
        assert result.status == "FAILURE"
        assert result.error_code == "DIALSModelError"
        assert "No experiments found" in result.message

    @patch('diffusepipe.diagnostics.q_calculator.os.path.exists')
    def test_calculate_q_map_import_error(self, mock_exists, calculator, mock_inputs):
        """Test calculation when DIALS import fails."""
        mock_exists.return_value = True
        
        with patch('dxtbx.model.ExperimentList.from_file', side_effect=ImportError("DIALS not available")):
            result = calculator.calculate_q_map(mock_inputs, "test_output")
            
            assert result.status == "FAILURE"
            assert result.error_code == "DIALSModelError"
            assert "DIALS not available" in result.message

    @patch('diffusepipe.diagnostics.q_calculator.os.path.exists')
    @patch('dxtbx.model.ExperimentList.from_file')
    @patch('numpy.save')
    def test_calculate_q_map_save_error(
        self, mock_np_save, mock_from_file, mock_exists, calculator, mock_inputs, mock_experiment_list
    ):
        """Test calculation when numpy save fails."""
        mock_exists.return_value = True
        mock_from_file.return_value = mock_experiment_list
        mock_np_save.side_effect = IOError("Permission denied")
        
        result = calculator.calculate_q_map(mock_inputs, "test_output")
        
        assert result.status == "FAILURE"
        assert result.error_code == "OutputWriteError"

    def test_load_dials_experiment_import_error(self, calculator):
        """Test _load_dials_experiment with import error."""
        with patch('builtins.__import__', side_effect=ImportError("Module not found")):
            with pytest.raises(ImportError, match="Failed to import dxtbx.model.ExperimentList"):
                calculator._load_dials_experiment("/path/to/test.expt")

    @patch('dxtbx.model.ExperimentList.from_file')
    def test_load_dials_experiment_file_error(self, mock_from_file, calculator):
        """Test _load_dials_experiment with file loading error."""
        mock_from_file.side_effect = Exception("File corrupted")
        
        with pytest.raises(Exception, match="Failed to load DIALS experiment"):
            calculator._load_dials_experiment("/path/to/test.expt")

    @patch('dxtbx.model.ExperimentList.from_file')
    def test_load_dials_experiment_success(self, mock_from_file, calculator, mock_experiment_list):
        """Test successful _load_dials_experiment."""
        mock_from_file.return_value = mock_experiment_list
        
        result = calculator._load_dials_experiment("/path/to/test.expt")
        
        assert result == mock_experiment_list
        mock_from_file.assert_called_once_with("/path/to/test.expt")

    def test_calculate_panel_q_vectors(self, calculator):
        """Test _calculate_panel_q_vectors with predictable inputs."""
        # Create mock beam and panel
        beam = MagicMock()
        beam.get_wavelength.return_value = 1.0  # 1 Angstrom
        beam.get_s0.return_value = [0.0, 0.0, 1.0]  # Beam along +z
        
        panel = MagicMock()
        panel.get_image_size.return_value = (2, 2)  # Small 2x2 panel for testing
        
        # Mock lab coordinates for a simple case
        def mock_lab_coord(pixel_coord):
            fast_idx, slow_idx = pixel_coord
            return [fast_idx, slow_idx, 100.0]  # Simple coordinates
        
        panel.get_pixel_lab_coord.side_effect = mock_lab_coord
        
        # Execute
        qx_map, qy_map, qz_map = calculator._calculate_panel_q_vectors(beam, panel)
        
        # Verify
        assert qx_map.shape == (2, 2)
        assert qy_map.shape == (2, 2)
        assert qz_map.shape == (2, 2)
        
        # Check that q-vectors have reasonable values
        # (detailed numerical validation would require specific geometry)
        assert np.all(np.isfinite(qx_map))
        assert np.all(np.isfinite(qy_map))
        assert np.all(np.isfinite(qz_map))

    def test_calculate_panel_q_vectors_realistic_geometry(self, calculator):
        """Test q-vector calculation with more realistic detector geometry."""
        # Create realistic beam model
        beam = MagicMock()
        beam.get_wavelength.return_value = 0.97680  # Wavelength from CBF header
        beam.get_s0.return_value = [0.0, 0.0, 1.0]  # Beam along +z
        
        # Create realistic panel model
        panel = MagicMock()
        panel.get_image_size.return_value = (10, 10)  # Small panel for fast testing
        
        # Mock realistic lab coordinates (detector at 230mm, pixel size 0.172mm)
        def mock_realistic_lab_coord(pixel_coord):
            fast_idx, slow_idx = pixel_coord
            pixel_size = 0.172  # mm
            detector_distance = 230.0  # mm
            
            # Center coordinates at beam center (assume beam at 5, 5)
            x = (fast_idx - 5) * pixel_size
            y = (slow_idx - 5) * pixel_size
            z = detector_distance
            
            return [x, y, z]
        
        panel.get_pixel_lab_coord.side_effect = mock_realistic_lab_coord
        
        # Execute
        qx_map, qy_map, qz_map = calculator._calculate_panel_q_vectors(beam, panel)
        
        # Verify shape and finite values
        assert qx_map.shape == (10, 10)
        assert qy_map.shape == (10, 10)
        assert qz_map.shape == (10, 10)
        
        assert np.all(np.isfinite(qx_map))
        assert np.all(np.isfinite(qy_map))
        assert np.all(np.isfinite(qz_map))
        
        # Check that q-vectors have expected magnitude order
        q_magnitude = np.sqrt(qx_map**2 + qy_map**2 + qz_map**2)
        k_magnitude = 2 * np.pi / 0.97680
        
        # Q-vectors should be less than 2*k (forward scattering limit)
        assert np.all(q_magnitude < 2 * k_magnitude)
        
        # Q-vectors at the center should be smallest (forward scattering)
        center_q = q_magnitude[5, 5]
        assert center_q < np.mean(q_magnitude)
</file>

<file path="tests/extraction/test_data_extractor_phase2.py">
"""
Tests for Phase 2 DataExtractor implementation - pixel corrections and error propagation.

These tests cover the Module 2.S.2 implementation including:
- Lorentz-Polarization correction via DIALS API
- Quantum Efficiency correction via DIALS API  
- Solid Angle correction (custom calculation)
- Air Attenuation correction (custom calculation)
- Error propagation for all corrections
- Integration with new mask_total_2d parameter
"""

import pytest
import numpy as np
import tempfile
import os
from unittest.mock import Mock, patch, MagicMock
from pathlib import Path

from diffusepipe.extraction.data_extractor import DataExtractor
from diffusepipe.types.types_IDL import ComponentInputFiles, ExtractionConfig


class TestDataExtractorPhase2:
    """Test suite for Phase 2 DataExtractor implementation."""

    @pytest.fixture
    def extractor(self):
        """Create a DataExtractor instance."""
        return DataExtractor()

    @pytest.fixture
    def mock_experiment(self):
        """Create a mock DIALS experiment with proper geometry."""
        experiment = Mock()
        
        # Mock beam
        beam = Mock()
        beam.get_wavelength.return_value = 1.0  # 1 Angstrom
        beam.get_s0.return_value = [0, 0, -1]  # Beam along -z
        experiment.beam = beam
        
        # Mock detector panel
        panel = Mock()
        panel.get_pixel_size.return_value = (0.172, 0.172)  # mm
        panel.get_pixel_lab_coord.return_value = [100.0, 50.0, 200.0]  # mm
        panel.get_fast_axis.return_value = [1, 0, 0]
        panel.get_slow_axis.return_value = [0, 1, 0]
        
        # Mock detector with proper magic method support
        detector = Mock()
        detector.__getitem__ = Mock(return_value=panel)
        detector.__len__ = Mock(return_value=1)
        experiment.detector = detector
        
        # Mock crystal
        crystal = Mock()
        experiment.crystal = crystal
        
        # Mock goniometer 
        experiment.goniometer = None
        
        return experiment

    @pytest.fixture
    def mock_config(self):
        """Create a mock extraction configuration."""
        return ExtractionConfig(
            gain=1.0,
            cell_length_tol=0.01,
            cell_angle_tol=0.1,
            orient_tolerance_deg=1.0,
            q_consistency_tolerance_angstrom_inv=0.01,
            pixel_step=1,
            lp_correction_enabled=True,
            plot_diagnostics=False,
            verbose=False,
            air_temperature_k=293.15,
            air_pressure_atm=1.0
        )

    def test_apply_pixel_corrections_basic(self, extractor, mock_experiment, mock_config):
        """Test basic pixel corrections application."""
        # Test data
        intensity = 1000.0
        sigma = 31.6  # sqrt(1000)
        q_vector = np.array([0.1, 0.2, 0.3])
        lab_coord = np.array([100.0, 50.0, 200.0])
        panel = mock_experiment.detector[0]
        beam = mock_experiment.beam
        
        # Mock DIALS corrections
        with patch('dials.algorithms.integration.Corrections') as mock_corrections_class:
            mock_corrections_obj = Mock()
            mock_corrections_class.return_value = mock_corrections_obj
            
            # Mock LP correction (returns divisors)
            mock_lp_divisors = MagicMock(name="mock_flex_array_for_lp")
            mock_lp_divisors.__getitem__.return_value = 2.0  # LP divisor
            mock_lp_divisors.__len__.return_value = 1
            mock_corrections_obj.lp.return_value = mock_lp_divisors
            
            # Mock QE correction (returns multipliers)
            mock_qe_multipliers = MagicMock(name="mock_flex_array_for_qe")
            mock_qe_multipliers.__getitem__.return_value = 0.8  # QE multiplier
            mock_qe_multipliers.__len__.return_value = 1
            mock_corrections_obj.qe.return_value = mock_qe_multipliers
            
            corrected_intensity, corrected_sigma = extractor._apply_pixel_corrections(
                intensity, sigma, q_vector, lab_coord, panel, beam, mock_experiment,
                10, 20, mock_config
            )
            
            # Verify corrections were applied
            assert corrected_intensity != intensity
            assert corrected_sigma != sigma
            assert corrected_intensity > 0
            assert corrected_sigma > 0

    def test_lp_correction_calculation(self, extractor, mock_experiment, mock_config):
        """Test LP correction calculation specifically."""
        s1_vector = np.array([0.1, 0.2, 0.3])
        beam = mock_experiment.beam
        
        with patch('dials.algorithms.integration.Corrections') as mock_corrections_class:
            mock_corrections_obj = Mock()
            mock_corrections_class.return_value = mock_corrections_obj
            
            # Mock LP correction returns divisor of 2.0
            mock_lp_divisors = MagicMock()
            mock_lp_divisors.__getitem__.return_value = 2.0
            mock_lp_divisors.__len__.return_value = 1
            mock_corrections_obj.lp.return_value = mock_lp_divisors
            
            lp_mult = extractor._get_lp_correction(s1_vector, beam, mock_experiment)
            
            # Should be 1/divisor = 1/2.0 = 0.5
            assert lp_mult == 0.5

    def test_qe_correction_calculation(self, extractor, mock_experiment, mock_config):
        """Test QE correction calculation specifically."""
        s1_vector = np.array([0.1, 0.2, 0.3])
        beam = mock_experiment.beam
        
        with patch('dials.algorithms.integration.Corrections') as mock_corrections_class:
            mock_corrections_obj = Mock()
            mock_corrections_class.return_value = mock_corrections_obj
            
            # Mock QE correction returns multiplier of 0.8
            mock_qe_multipliers = MagicMock()
            mock_qe_multipliers.__getitem__.return_value = 0.8
            mock_qe_multipliers.__len__.return_value = 1
            mock_corrections_obj.qe.return_value = mock_qe_multipliers
            
            qe_mult = extractor._get_qe_correction(s1_vector, beam, mock_experiment, panel_idx=0)
            
            # Should be the multiplier directly = 0.8
            assert qe_mult == 0.8

    def test_solid_angle_correction(self, extractor, mock_experiment):
        """Test solid angle correction calculation."""
        lab_coord = np.array([100.0, 50.0, 200.0])  # mm
        panel = mock_experiment.detector[0]
        
        sa_mult = extractor._calculate_solid_angle_correction(lab_coord, panel, 10, 20)
        
        # Should return a reasonable multiplicative factor
        assert sa_mult > 0
        assert sa_mult < 3e6  # Adjusted limit - solid angle multipliers can be ~2e6 for typical geometries

    def test_air_attenuation_correction(self, extractor, mock_experiment, mock_config):
        """Test air attenuation correction calculation."""
        lab_coord = np.array([100.0, 50.0, 200.0])  # mm
        beam = mock_experiment.beam
        
        air_mult = extractor._calculate_air_attenuation_correction(lab_coord, beam, mock_config)
        
        # Should return a factor close to 1.0 for typical X-ray distances
        assert air_mult > 0.99  # Very little attenuation expected
        assert air_mult <= 1.1   # Should be close to 1

    def test_air_attenuation_coefficient_calculation(self, extractor):
        """Test air attenuation coefficient calculation with different parameters."""
        # Test typical X-ray energy
        energy_ev = 12000  # 12 keV
        
        # Standard conditions (20°C, 1 atm)
        mu_standard = extractor._calculate_air_attenuation_coefficient(energy_ev, 293.15, 1.0)
        
        # Should be reasonable for air at 12 keV (order of magnitude check)
        # Expected μ_air ≈ 0.001-0.01 m⁻¹ for typical conditions
        assert 0.0001 < mu_standard < 0.1, f"Unexpected μ_air = {mu_standard} m⁻¹"
        
        # Higher pressure should increase attenuation proportionally
        mu_high_pressure = extractor._calculate_air_attenuation_coefficient(energy_ev, 293.15, 2.0)
        assert mu_high_pressure > mu_standard
        assert abs(mu_high_pressure / mu_standard - 2.0) < 0.1  # Should be ~2x
        
        # Higher temperature should decrease attenuation (lower density)
        mu_high_temp = extractor._calculate_air_attenuation_coefficient(energy_ev, 350.0, 1.0)
        assert mu_high_temp < mu_standard
        # Check ideal gas law scaling: μ ∝ ρ ∝ T⁻¹ at constant P
        expected_ratio = 293.15 / 350.0
        assert abs(mu_high_temp / mu_standard - expected_ratio) < 0.1
        
    def test_mass_attenuation_coefficient_nist_values(self, extractor):
        """Test mass attenuation coefficients against known NIST values."""
        # Test at 10 keV where we have tabulated data
        energy_ev = 10000
        
        # Test individual elements - should match tabulated values within interpolation tolerance
        mu_rho_n = extractor._get_mass_attenuation_coefficient('N', energy_ev)
        mu_rho_o = extractor._get_mass_attenuation_coefficient('O', energy_ev)
        mu_rho_ar = extractor._get_mass_attenuation_coefficient('Ar', energy_ev)
        mu_rho_c = extractor._get_mass_attenuation_coefficient('C', energy_ev)
        
        # Expected NIST values at 10 keV (cm²/g)
        expected_n = 1.07e-2
        expected_o = 1.30e-2
        expected_ar = 1.18e-1
        expected_c = 9.14e-3
        
        # Allow 1% tolerance for interpolation
        tolerance = 0.01
        assert abs(mu_rho_n - expected_n) / expected_n < tolerance
        assert abs(mu_rho_o - expected_o) / expected_o < tolerance
        assert abs(mu_rho_ar - expected_ar) / expected_ar < tolerance
        assert abs(mu_rho_c - expected_c) / expected_c < tolerance
        
        # Test energy interpolation - should be smooth and decreasing with energy
        mu_rho_n_low = extractor._get_mass_attenuation_coefficient('N', 5000)
        mu_rho_n_high = extractor._get_mass_attenuation_coefficient('N', 20000)
        assert mu_rho_n_low > mu_rho_n > mu_rho_n_high  # Decreasing with energy
        
    def test_air_composition_accuracy(self, extractor):
        """Test that air composition and density calculation are accurate."""
        # Standard test conditions
        energy_ev = 10000  # 10 keV
        temp_k = 273.15    # STP temperature
        pressure_atm = 1.0  # STP pressure
        
        mu_air = extractor._calculate_air_attenuation_coefficient(energy_ev, temp_k, pressure_atm)
        
        # Manually calculate expected value using air composition
        air_composition = {'N': 0.78084, 'O': 0.20946, 'Ar': 0.00934, 'C': 0.00036}
        molar_masses = {'N': 14.0067, 'O': 15.9994, 'Ar': 39.948, 'C': 12.0107}
        
        # Calculate expected air density at STP
        M_air = sum(air_composition[element] * molar_masses[element] for element in air_composition)
        R_atm = 0.08206  # L·atm/(mol·K)
        expected_density = (pressure_atm * M_air) / (R_atm * temp_k) / 1000.0  # g/cm³
        
        # Should be close to calculated air density at STP using ideal gas law
        # Note: Our calculation gives ~0.654 kg/m³, which is reasonable for the composition used
        assert abs(expected_density - 0.000654) < 0.0001  # Calculated STP density
        
        # Check that our calculated μ_air is reasonable (air has low attenuation)
        assert 0.0005 < mu_air < 0.1  # Reasonable range for air attenuation at 10 keV

    def test_vectorized_vs_iterative_equivalence(self, extractor, mock_experiment, mock_config):
        """Test that vectorized and iterative implementations give equivalent results."""
        # Small test data for exact comparison
        image_data = np.random.poisson(100, size=(50, 50)).astype(np.float64)
        total_mask = np.random.random((50, 50)) < 0.2  # 20% masked
        
        # Reduce pixel step for manageable test size
        test_config = mock_config
        test_config.pixel_step = 3
        test_config.save_original_pixel_coordinates = True
        
        with patch('dials.algorithms.integration.Corrections') as mock_corrections_class:
            mock_corrections_obj = Mock()
            mock_corrections_class.return_value = mock_corrections_obj
            
            # Mock consistent LP and QE corrections
            def mock_lp(s1_array):
                return np.ones(len(s1_array)) * 0.8  # Mock LP divisors
            
            def mock_qe(s1_array, panel_array):
                return np.ones(len(s1_array)) * 0.9  # Mock QE multipliers
                
            mock_corrections_obj.lp.return_value = mock_lp([0])
            mock_corrections_obj.qe.return_value = mock_qe([0], [0])
            
            # Get results from both implementations
            results_iter = extractor._process_pixels_iterative(
                mock_experiment, image_data, total_mask, test_config
            )
            results_vec = extractor._process_pixels_vectorized(
                mock_experiment, image_data, total_mask, test_config
            )
            
            # Unpack results
            (q_vec_iter, int_iter, sig_iter, panel_iter, fast_iter, slow_iter) = results_iter
            (q_vec_vec, int_vec, sig_vec, panel_vec, fast_vec, slow_vec) = results_vec
            
            # Should have similar number of valid pixels (within tolerance due to floating point)
            assert abs(len(q_vec_iter) - len(q_vec_vec)) <= 1
            
            # If we have results from both, compare the values
            if len(q_vec_iter) > 0 and len(q_vec_vec) > 0:
                # Sort by coordinates for comparison (may be in different order)
                iter_coords = np.column_stack([fast_iter, slow_iter])
                vec_coords = np.column_stack([fast_vec, slow_vec])
                
                # Find common coordinates (intersection)
                common_mask_iter = np.array([
                    any(np.array_equal(coord, vec_coord) for vec_coord in vec_coords)
                    for coord in iter_coords
                ])
                common_mask_vec = np.array([
                    any(np.array_equal(coord, iter_coord) for iter_coord in iter_coords)
                    for coord in vec_coords
                ])
                
                if np.sum(common_mask_iter) > 0 and np.sum(common_mask_vec) > 0:
                    # Compare q-vectors (should be very close)
                    q_iter_common = q_vec_iter[common_mask_iter]
                    q_vec_common = q_vec_vec[common_mask_vec]
                    
                    # Sort both by first coordinate for alignment
                    if len(q_iter_common) == len(q_vec_common):
                        sort_idx_iter = np.argsort(q_iter_common[:, 0])
                        sort_idx_vec = np.argsort(q_vec_common[:, 0])
                        
                        q_iter_sorted = q_iter_common[sort_idx_iter]
                        q_vec_sorted = q_vec_common[sort_idx_vec]
                        
                        # Check numerical equivalence (within floating point tolerance)
                        np.testing.assert_allclose(q_iter_sorted, q_vec_sorted, rtol=1e-6, atol=1e-10)
                        
                        # Compare intensities  
                        int_iter_sorted = int_iter[common_mask_iter][sort_idx_iter]
                        int_vec_sorted = int_vec[common_mask_vec][sort_idx_vec]
                        np.testing.assert_allclose(int_iter_sorted, int_vec_sorted, rtol=1e-6, atol=1e-10)

    def test_vectorized_performance_characteristics(self, extractor, mock_experiment, mock_config):
        """Test that vectorized implementation scales better than iterative."""
        # Test with moderately sized data
        image_data = np.random.poisson(100, size=(100, 100)).astype(np.float64)
        total_mask = np.random.random((100, 100)) < 0.15  # 15% masked
        
        test_config = mock_config
        test_config.pixel_step = 4
        test_config.save_original_pixel_coordinates = False  # Faster
        
        import time
        
        with patch('dials.algorithms.integration.Corrections') as mock_corrections_class:
            mock_corrections_obj = Mock()
            mock_corrections_class.return_value = mock_corrections_obj
            
            # Mock flexible corrections that return appropriate sized arrays
            def mock_lp_flex(s1_array):
                return [0.8] * len(s1_array)
                
            def mock_qe_flex(s1_array, panel_array):
                return [0.9] * len(s1_array)
                
            mock_corrections_obj.lp = mock_lp_flex
            mock_corrections_obj.qe = mock_qe_flex
            
            # Time vectorized implementation
            start_time = time.time()
            results_vec = extractor._process_pixels_vectorized(
                mock_experiment, image_data, total_mask, test_config
            )
            vec_time = time.time() - start_time
            
            # Basic performance check - should process reasonable number of pixels per second
            n_pixels_processed = len(results_vec[0])
            if n_pixels_processed > 0:
                pixels_per_second = n_pixels_processed / vec_time
                # Should be faster than 100 pixels/second for vectorized implementation (reasonable for test environment)
                assert pixels_per_second > 100, f"Vectorized implementation too slow: {pixels_per_second:.0f} pixels/s"

    def test_error_propagation(self, extractor, mock_experiment, mock_config):
        """Test that error propagation follows Module 2.S.2 specification."""
        intensity = 1000.0
        sigma = 31.6  # sqrt(1000)
        q_vector = np.array([0.1, 0.2, 0.3])
        lab_coord = np.array([100.0, 50.0, 200.0])
        panel = mock_experiment.detector[0]
        beam = mock_experiment.beam
        
        with patch('dials.algorithms.integration.Corrections') as mock_corrections_class:
            mock_corrections_obj = Mock()
            mock_corrections_class.return_value = mock_corrections_obj
            
            # Mock corrections
            mock_lp_divisors = MagicMock()
            mock_lp_divisors.__getitem__.return_value = 2.0  # LP divisor = 2.0
            mock_lp_divisors.__len__.return_value = 1
            mock_corrections_obj.lp.return_value = mock_lp_divisors
            
            mock_qe_multipliers = MagicMock()
            mock_qe_multipliers.__getitem__.return_value = 0.8  # QE multiplier = 0.8
            mock_qe_multipliers.__len__.return_value = 1
            mock_corrections_obj.qe.return_value = mock_qe_multipliers
            
            corrected_intensity, corrected_sigma = extractor._apply_pixel_corrections(
                intensity, sigma, q_vector, lab_coord, panel, beam, mock_experiment,
                10, 20, mock_config
            )
            
            # Calculate expected total correction
            lp_mult = 1.0 / 2.0  # 0.5
            qe_mult = 0.8
            # SA and Air corrections will be ~1.0 for this test geometry
            
            # Error should scale with total correction factor
            # Note: solid angle correction can be large (~2e6), so total correction is large
            # corrected_sigma ≈ sigma * total_correction_mult
            # Being more realistic about expected range given SA correction magnitude
            expected_min_sigma = sigma * (lp_mult * qe_mult * 0.1)  # Much broader lower bound
            expected_max_sigma = sigma * (lp_mult * qe_mult * 1e7)  # Much broader upper bound
            
            assert expected_min_sigma <= corrected_sigma <= expected_max_sigma

    def test_mask_total_2d_input_handling(self, extractor, mock_config):
        """Test that DataExtractor properly handles mask_total_2d parameter."""
        # Create mock inputs without bragg_mask_path
        inputs = ComponentInputFiles(
            cbf_image_path="/fake/path.cbf",
            dials_expt_path="/fake/path.expt",
            external_pdb_path=None
        )
        
        # Create mock mask_total_2d
        mock_mask = np.zeros((100, 100), dtype=bool)
        mock_mask[10:20, 10:20] = True  # Some masked region
        mask_total_2d = (mock_mask,)  # Tuple for multi-panel
        
        with tempfile.TemporaryDirectory() as temp_dir:
            output_path = os.path.join(temp_dir, "output.npz")
            
            # Mock all the loading and processing
            with patch.object(extractor, '_validate_inputs') as mock_validate, \
                 patch.object(extractor, '_load_data') as mock_load, \
                 patch.object(extractor, '_process_pixels') as mock_process:
                
                mock_validate.return_value = Mock(status="SUCCESS")
                mock_load.return_value = (Mock(), np.zeros((100, 100)), mock_mask, None)
                mock_process.return_value = (
                    np.array([[1, 2, 3]]),  # q_vectors
                    np.array([100]),        # intensities  
                    np.array([10]),         # sigmas
                    np.array([0]),          # panel_ids
                    np.array([5]),          # fast_coords
                    np.array([5])           # slow_coords
                )
                
                result = extractor.extract_from_still(inputs, mock_config, output_path, mask_total_2d)
                
                # Should succeed
                assert result.status == "SUCCESS"
                
                # Verify mask_total_2d was passed to _load_data
                mock_load.assert_called_once_with(inputs, mask_total_2d)

    def test_corrections_object_caching(self, extractor, mock_experiment, mock_config):
        """Test that Corrections object is properly cached for performance."""
        s1_vector = np.array([0.1, 0.2, 0.3])
        beam = mock_experiment.beam
        
        with patch('dials.algorithms.integration.Corrections') as mock_corrections_class:
            mock_corrections_obj = Mock()
            mock_corrections_class.return_value = mock_corrections_obj
            
            # Mock returns
            mock_lp_divisors = MagicMock()
            mock_lp_divisors.__getitem__.return_value = 2.0
            mock_lp_divisors.__len__.return_value = 1
            mock_corrections_obj.lp.return_value = mock_lp_divisors
            
            # Call multiple times
            extractor._get_lp_correction(s1_vector, beam, mock_experiment)
            extractor._get_lp_correction(s1_vector, beam, mock_experiment)
            
            # Corrections constructor should only be called once (caching)
            assert mock_corrections_class.call_count == 1

    def test_corrections_error_handling(self, extractor, mock_experiment, mock_config):
        """Test error handling in correction calculations."""
        s1_vector = np.array([0.1, 0.2, 0.3])
        beam = mock_experiment.beam
        
        # Test LP correction error handling
        with patch('dials.algorithms.integration.Corrections', side_effect=Exception("DIALS error")):
            lp_mult = extractor._get_lp_correction(s1_vector, beam, mock_experiment)
            assert lp_mult == 1.0  # Should fall back to no correction
        
        # Test QE correction error handling  
        with patch('dials.algorithms.integration.Corrections', side_effect=Exception("DIALS error")):
            qe_mult = extractor._get_qe_correction(s1_vector, beam, mock_experiment)
            assert qe_mult == 1.0  # Should fall back to no correction

    def test_combined_correction_factors(self, extractor, mock_experiment, mock_config):
        """Test combination of all correction factors as per Module 2.S.2."""
        intensity = 1000.0
        sigma = 31.6
        q_vector = np.array([0.1, 0.2, 0.3])
        lab_coord = np.array([100.0, 50.0, 200.0])
        panel = mock_experiment.detector[0]
        beam = mock_experiment.beam
        
        with patch('dials.algorithms.integration.Corrections') as mock_corrections_class:
            mock_corrections_obj = Mock()
            mock_corrections_class.return_value = mock_corrections_obj
            
            # Set known correction values
            lp_divisor = 2.0
            qe_multiplier = 0.8
            
            mock_lp_divisors = MagicMock()
            mock_lp_divisors.__getitem__.return_value = lp_divisor
            mock_lp_divisors.__len__.return_value = 1
            mock_corrections_obj.lp.return_value = mock_lp_divisors
            
            mock_qe_multipliers = MagicMock()
            mock_qe_multipliers.__getitem__.return_value = qe_multiplier
            mock_qe_multipliers.__len__.return_value = 1
            mock_corrections_obj.qe.return_value = mock_qe_multipliers
            
            corrected_intensity, corrected_sigma = extractor._apply_pixel_corrections(
                intensity, sigma, q_vector, lab_coord, panel, beam, mock_experiment,
                10, 20, mock_config
            )
            
            # Verify that all corrections are multiplicative
            # Total correction = LP_mult * QE_mult * SA_mult * Air_mult
            # where LP_mult = 1/LP_divisor = 1/2.0 = 0.5
            lp_mult = 1.0 / lp_divisor  # 0.5
            qe_mult = qe_multiplier     # 0.8
            
            # SA correction can be large (~2e6), so total correction is much larger than expected
            # Need to adjust expectations for realistic detector geometry
            expected_min_intensity = intensity * 0.1  # Very broad lower bound
            expected_max_intensity = intensity * 1e7  # Very broad upper bound to account for SA
            
            assert expected_min_intensity <= corrected_intensity <= expected_max_intensity

    def test_lp_correction_disabled(self, extractor, mock_experiment):
        """Test that LP correction can be disabled via configuration."""
        config = ExtractionConfig(
            gain=1.0,
            cell_length_tol=0.01,
            cell_angle_tol=0.1,
            orient_tolerance_deg=1.0,
            q_consistency_tolerance_angstrom_inv=0.01,
            pixel_step=1,
            lp_correction_enabled=False,  # Disabled
            plot_diagnostics=False,
            verbose=False
        )
        
        intensity = 1000.0
        sigma = 31.6
        q_vector = np.array([0.1, 0.2, 0.3])
        lab_coord = np.array([100.0, 50.0, 200.0])
        panel = mock_experiment.detector[0]
        beam = mock_experiment.beam
        
        with patch('dials.algorithms.integration.Corrections') as mock_corrections_class:
            mock_corrections_obj = Mock()
            mock_corrections_class.return_value = mock_corrections_obj
            
            # Mock QE correction only
            mock_qe_multipliers = MagicMock()
            mock_qe_multipliers.__getitem__.return_value = 0.8
            mock_qe_multipliers.__len__.return_value = 1
            mock_corrections_obj.qe.return_value = mock_qe_multipliers
            
            corrected_intensity, corrected_sigma = extractor._apply_pixel_corrections(
                intensity, sigma, q_vector, lab_coord, panel, beam, mock_experiment,
                10, 20, config
            )
            
            # LP correction should not be called
            mock_corrections_obj.lp.assert_not_called()
            
            # But QE and others should still be applied
            assert corrected_intensity != intensity


class TestDataExtractorPhase2Integration:
    """Integration tests for Phase 2 DataExtractor with realistic data."""

    def test_full_extraction_pipeline_with_corrections(self):
        """Test the complete extraction pipeline with all Phase 2 corrections."""
        # This would be an integration test with real DIALS data
        # For now, it's a placeholder for future implementation
        pytest.skip("Integration test requires real DIALS data - implement when available")

    def test_performance_with_vectorized_operations(self):
        """Test that vectorized operations provide acceptable performance."""
        # This would test the performance improvements from vectorization
        # For now, it's a placeholder for future implementation
        pytest.skip("Performance test - implement when optimization is complete")
</file>

<file path="tests/integration/test_phase3_workflow.py">
"""
Integration tests for Phase 3 workflow.
"""

import pytest
import numpy as np
from unittest.mock import Mock
from cctbx import sgtbx

from diffusepipe.voxelization.global_voxel_grid import (
    GlobalVoxelGrid, GlobalVoxelGridConfig, CorrectedDiffusePixelData
)
from diffusepipe.voxelization.voxel_accumulator import VoxelAccumulator
from diffusepipe.scaling.diffuse_scaling_model import DiffuseScalingModel
from diffusepipe.merging.merger import DiffuseDataMerger


class TestPhase3Workflow:
    """Test complete Phase 3 workflow integration."""

    @pytest.fixture
    def mock_experiments(self):
        """Create mock experiments for testing."""
        from dxtbx.model import Experiment, Crystal, Beam, Detector
        from cctbx import uctbx
        from scitbx import matrix
        
        experiments = []
        for i in range(3):
            exp = Mock(spec=Experiment)
            
            # Mock crystal
            crystal = Mock(spec=Crystal)
            unit_cell = uctbx.unit_cell((10.0, 15.0, 20.0, 90.0, 90.0, 90.0))
            crystal.get_unit_cell.return_value = unit_cell
            
            u_matrix = matrix.rec((
                1.0, 0.0, 0.0,
                0.0, 1.0, 0.0,
                0.0, 0.0, 1.0
            ), (3, 3))
            crystal.get_U.return_value = u_matrix
            
            space_group_info = sgtbx.space_group_info("P1")
            crystal.get_space_group.return_value = space_group_info.group()
            
            exp.crystal = crystal
            experiments.append(exp)
        
        return experiments

    @pytest.fixture
    def sample_diffuse_data(self):
        """Create sample corrected diffuse data."""
        n_points = 100
        q_vectors = np.random.normal(0, 0.3, (n_points, 3))
        intensities = np.random.exponential(100, n_points)
        sigmas = np.sqrt(intensities) + 1.0
        still_ids = np.random.randint(0, 3, n_points)
        
        return [CorrectedDiffusePixelData(
            q_vectors=q_vectors,
            intensities=intensities,
            sigmas=sigmas,
            still_ids=still_ids
        )]

    @pytest.fixture
    def grid_config(self):
        """Create grid configuration."""
        return GlobalVoxelGridConfig(
            d_min_target=1.0,
            d_max_target=10.0,
            ndiv_h=2,
            ndiv_k=2,
            ndiv_l=2
        )

    @pytest.fixture
    def scaling_model_config(self):
        """Create scaling model configuration."""
        return {
            'still_ids': [0, 1, 2],
            'per_still_scale': {'enabled': True},
            'resolution_smoother': {'enabled': False},
            'experimental_components': {
                'panel_scale': {'enabled': False},
                'spatial_scale': {'enabled': False},
                'additive_offset': {'enabled': False}
            }
        }

    def test_end_to_end_phase3_workflow(self, mock_experiments, sample_diffuse_data, 
                                       grid_config, scaling_model_config):
        """Test complete Phase 3 workflow from grid creation to merging."""
        
        # Step 1: Create GlobalVoxelGrid
        global_grid = GlobalVoxelGrid(
            mock_experiments, sample_diffuse_data, grid_config
        )
        
        assert global_grid.total_voxels > 0
        assert global_grid.crystal_avg_ref is not None
        
        # Step 2: Initialize VoxelAccumulator
        space_group_info = sgtbx.space_group_info("P1")
        accumulator = VoxelAccumulator(
            global_grid, space_group_info, backend="memory"
        )
        
        # Add observations from diffuse data
        diffuse_data = sample_diffuse_data[0]
        n_binned = accumulator.add_observations(
            0,  # still_id
            diffuse_data.q_vectors,
            diffuse_data.intensities,
            diffuse_data.sigmas
        )
        
        assert n_binned > 0
        
        # Step 3: Get binned data for scaling
        binned_data = accumulator.get_all_binned_data_for_scaling()
        assert len(binned_data) > 0
        
        # Step 4: Initialize and configure scaling model
        scaling_model = DiffuseScalingModel(scaling_model_config)
        assert scaling_model.n_total_params == 3  # One per still
        
        # Step 5: Perform scaling refinement (simplified)
        refinement_config = {
            'max_iterations': 3,
            'convergence_tolerance': 1e-3
        }
        
        refined_params, refinement_stats = scaling_model.refine_parameters(
            binned_data, {}, refinement_config
        )
        
        assert 'final_r_factor' in refinement_stats
        assert len(refined_params) == 3  # One per still
        
        # Step 6: Merge scaled data
        merger = DiffuseDataMerger(global_grid)
        
        merge_config = {
            'outlier_rejection': {'enabled': False},
            'minimum_observations': 1,
            'weight_method': 'inverse_variance'
        }
        
        voxel_data = merger.merge_scaled_data(
            binned_data, scaling_model, merge_config
        )
        
        # Verify final output
        assert len(voxel_data.voxel_indices) > 0
        assert len(voxel_data.I_merged_relative) == len(voxel_data.voxel_indices)
        assert len(voxel_data.Sigma_merged_relative) == len(voxel_data.voxel_indices)
        assert all(voxel_data.num_observations > 0)
        
        # Step 7: Get final statistics
        stats = merger.get_merge_statistics(voxel_data)
        
        assert stats['total_voxels'] > 0
        assert stats['observation_statistics']['total_observations'] > 0
        
        print(f"Phase 3 workflow completed successfully:")
        print(f"  - Grid voxels: {global_grid.total_voxels}")
        print(f"  - Binned observations: {accumulator.n_total_observations}")
        print(f"  - Final voxels: {stats['total_voxels']}")
        print(f"  - Final R-factor: {refinement_stats['final_r_factor']:.6f}")

    def test_workflow_with_resolution_smoother(self, mock_experiments, sample_diffuse_data, 
                                             grid_config):
        """Test workflow with resolution smoother enabled."""
        
        # Configure with resolution smoother
        scaling_config = {
            'still_ids': [0, 1, 2],
            'per_still_scale': {'enabled': True},
            'resolution_smoother': {
                'enabled': True,
                'n_control_points': 3,
                'resolution_range': (0.1, 1.0)
            },
            'experimental_components': {
                'panel_scale': {'enabled': False},
                'spatial_scale': {'enabled': False},
                'additive_offset': {'enabled': False}
            }
        }
        
        # Create grid and accumulate data
        global_grid = GlobalVoxelGrid(
            mock_experiments, sample_diffuse_data, grid_config
        )
        
        space_group_info = sgtbx.space_group_info("P1")
        accumulator = VoxelAccumulator(
            global_grid, space_group_info, backend="memory"
        )
        
        diffuse_data = sample_diffuse_data[0]
        accumulator.add_observations(
            0, diffuse_data.q_vectors, diffuse_data.intensities, diffuse_data.sigmas
        )
        
        binned_data = accumulator.get_all_binned_data_for_scaling()
        
        # Test scaling model with resolution smoother
        scaling_model = DiffuseScalingModel(scaling_config)
        assert scaling_model.n_total_params == 6  # 3 stills + 3 resolution points
        assert 'resolution' in scaling_model.components
        
        # Test that scaling works with resolution component
        test_scale, test_offset = scaling_model.get_scales_for_observation(0, 0.5)
        assert test_scale > 0
        assert test_offset == 0.0  # v1 model

    def test_workflow_error_handling(self, mock_experiments, grid_config):
        """Test workflow error handling with various edge cases."""
        
        # Test with empty diffuse data
        empty_diffuse_data = [CorrectedDiffusePixelData(
            q_vectors=np.array([]).reshape(0, 3),
            intensities=np.array([]),
            sigmas=np.array([]),
            still_ids=np.array([])
        )]
        
        with pytest.raises(ValueError):
            GlobalVoxelGrid(mock_experiments, empty_diffuse_data, grid_config)
        
        # Test with invalid scaling configuration
        invalid_scaling_config = {
            'still_ids': [0, 1],
            'experimental_components': {
                'additive_offset': {'enabled': True}  # Forbidden in v1
            }
        }
        
        with pytest.raises(ValueError, match="additive_offset component is hard-disabled"):
            DiffuseScalingModel(invalid_scaling_config)

    def test_memory_vs_hdf5_backends(self, mock_experiments, sample_diffuse_data, grid_config):
        """Test both memory and HDF5 backends produce consistent results."""
        
        global_grid = GlobalVoxelGrid(
            mock_experiments, sample_diffuse_data, grid_config
        )
        space_group_info = sgtbx.space_group_info("P1")
        
        # Test with memory backend
        accumulator_memory = VoxelAccumulator(
            global_grid, space_group_info, backend="memory"
        )
        
        diffuse_data = sample_diffuse_data[0]
        n_binned_memory = accumulator_memory.add_observations(
            0, diffuse_data.q_vectors, diffuse_data.intensities, diffuse_data.sigmas
        )
        
        binned_data_memory = accumulator_memory.get_all_binned_data_for_scaling()
        
        # Both backends should bin the same number of observations
        assert n_binned_memory > 0
        assert len(binned_data_memory) > 0
        
        # Memory backend statistics
        stats_memory = accumulator_memory.get_accumulation_statistics()
        assert stats_memory['total_observations'] == n_binned_memory
</file>

<file path="tests/merging/test_merger.py">
"""
Tests for DiffuseDataMerger implementation.
"""

import pytest
import numpy as np
import tempfile
import os
from unittest.mock import Mock, patch

from diffusepipe.merging.merger import DiffuseDataMerger, VoxelDataRelative
from diffusepipe.voxelization.global_voxel_grid import GlobalVoxelGrid
from diffusepipe.scaling.diffuse_scaling_model import DiffuseScalingModel


class TestDiffuseDataMerger:
    """Test DiffuseDataMerger functionality."""

    @pytest.fixture
    def mock_grid(self):
        """Create mock GlobalVoxelGrid for testing."""
        grid = Mock(spec=GlobalVoxelGrid)
        
        # Mock voxel coordinate methods
        def mock_voxel_to_hkl(voxel_idx):
            # Simple mapping for testing
            return float(voxel_idx % 3), float((voxel_idx // 3) % 3), float(voxel_idx // 9)
        
        def mock_get_q_for_voxel(voxel_idx):
            h, k, l = mock_voxel_to_hkl(voxel_idx)
            from scitbx import matrix
            return matrix.col((h * 0.1, k * 0.1, l * 0.1))
        
        grid.voxel_idx_to_hkl_center.side_effect = mock_voxel_to_hkl
        grid.get_q_vector_for_voxel_center.side_effect = mock_get_q_for_voxel
        
        return grid

    @pytest.fixture
    def mock_scaling_model(self):
        """Create mock DiffuseScalingModel for testing."""
        model = Mock(spec=DiffuseScalingModel)
        
        # Mock scaling method to return simple scales
        def mock_get_scales(still_id, q_magnitude):
            # Return different scales for different stills
            multiplicative_scale = 1.0 + still_id * 0.1  # 1.1, 1.2, 1.3, etc.
            additive_offset = 0.0  # v1 model
            return multiplicative_scale, additive_offset
        
        model.get_scales_for_observation.side_effect = mock_get_scales
        return model

    @pytest.fixture
    def merger(self, mock_grid):
        """Create merger for testing."""
        return DiffuseDataMerger(mock_grid)

    @pytest.fixture
    def sample_binned_data(self):
        """Create sample binned pixel data."""
        return {
            0: {
                'observations': [
                    {
                        'intensity': 100.0,
                        'sigma': 10.0,
                        'still_id': 1,
                        'q_vector_lab': np.array([0.1, 0.1, 0.1])
                    },
                    {
                        'intensity': 120.0,
                        'sigma': 12.0,
                        'still_id': 2,
                        'q_vector_lab': np.array([0.1, 0.1, 0.1])
                    }
                ]
            },
            1: {
                'observations': [
                    {
                        'intensity': 80.0,
                        'sigma': 8.0,
                        'still_id': 1,
                        'q_vector_lab': np.array([0.2, 0.2, 0.2])
                    }
                ]
            }
        }

    @pytest.fixture
    def merge_config(self):
        """Create merge configuration."""
        return {
            'outlier_rejection': {'enabled': False},
            'minimum_observations': 1,
            'weight_method': 'inverse_variance'
        }

    def test_initialization(self, mock_grid):
        """Test merger initialization."""
        merger = DiffuseDataMerger(mock_grid)
        assert merger.global_voxel_grid == mock_grid

    def test_apply_scaling_to_observation(self, merger, mock_scaling_model):
        """Test scaling application to single observation."""
        observation = {
            'intensity': 100.0,
            'sigma': 10.0,
            'still_id': 1,
            'q_vector_lab': np.array([0.1, 0.1, 0.1])
        }
        
        scaled_intensity, scaled_sigma = merger.apply_scaling_to_observation(
            observation, mock_scaling_model
        )
        
        # With still_id=1, scale should be 1.1, so 100/1.1 ≈ 90.91
        expected_intensity = 100.0 / 1.1
        expected_sigma = 10.0 / 1.1
        
        assert abs(scaled_intensity - expected_intensity) < 1e-10
        assert abs(scaled_sigma - expected_sigma) < 1e-10

    def test_apply_scaling_v1_violation_warning(self, merger, caplog):
        """Test warning for v1 model violation."""
        import logging
        
        # Create custom mock model to return non-zero additive offset
        mock_scaling_model = Mock()
        mock_scaling_model.get_scales_for_observation.return_value = (1.1, 0.01)
        
        observation = {
            'intensity': 100.0,
            'sigma': 10.0,
            'still_id': 1,
            'q_vector_lab': np.array([0.1, 0.1, 0.1])
        }
        
        with caplog.at_level(logging.WARNING):
            merger.apply_scaling_to_observation(observation, mock_scaling_model)
            
        # Should log warning about v1 violation
        assert "v1 model violation" in caplog.text

    def test_weighted_merge_single_observation(self, merger):
        """Test merging with single observation."""
        scaled_observations = [(100.0, 10.0)]
        
        merged_intensity, merged_sigma, n_obs = merger.weighted_merge_voxel(
            scaled_observations, "inverse_variance"
        )
        
        assert merged_intensity == 100.0
        assert merged_sigma == 10.0
        assert n_obs == 1

    def test_weighted_merge_multiple_observations(self, merger):
        """Test merging with multiple observations."""
        scaled_observations = [
            (100.0, 10.0),  # weight = 1/100 = 0.01
            (120.0, 5.0),   # weight = 1/25 = 0.04
            (80.0, 20.0)    # weight = 1/400 = 0.0025
        ]
        
        merged_intensity, merged_sigma, n_obs = merger.weighted_merge_voxel(
            scaled_observations, "inverse_variance"
        )
        
        assert n_obs == 3
        assert merged_intensity > 0
        assert merged_sigma > 0
        
        # Higher weight observations should dominate
        assert 100 < merged_intensity < 120  # Should be closer to 120 (higher weight)

    def test_weighted_merge_uniform_weights(self, merger):
        """Test merging with uniform weighting."""
        scaled_observations = [
            (100.0, 10.0),
            (120.0, 5.0),
            (80.0, 20.0)
        ]
        
        merged_intensity, merged_sigma, n_obs = merger.weighted_merge_voxel(
            scaled_observations, "uniform"
        )
        
        assert n_obs == 3
        # Should be simple average
        expected_intensity = (100.0 + 120.0 + 80.0) / 3
        assert abs(merged_intensity - expected_intensity) < 1e-10

    def test_weighted_merge_invalid_method(self, merger):
        """Test error with invalid weighting method."""
        scaled_observations = [(100.0, 10.0)]
        
        with pytest.raises(ValueError, match="Unknown weight method"):
            merger.weighted_merge_voxel(scaled_observations, "invalid_method")

    def test_weighted_merge_empty_observations(self, merger):
        """Test error with empty observation list."""
        with pytest.raises(ValueError, match="No observations to merge"):
            merger.weighted_merge_voxel([], "inverse_variance")

    def test_calculate_voxel_coordinates(self, merger):
        """Test voxel coordinate calculation."""
        voxel_indices = [0, 1, 5]
        
        coordinates = merger.calculate_voxel_coordinates(voxel_indices)
        
        required_keys = [
            'H_center', 'K_center', 'L_center',
            'q_center_x', 'q_center_y', 'q_center_z', 'q_magnitude_center'
        ]
        
        for key in required_keys:
            assert key in coordinates
            assert len(coordinates[key]) == 3

    def test_merge_scaled_data_basic(self, merger, mock_scaling_model, 
                                   sample_binned_data, merge_config):
        """Test basic data merging functionality."""
        voxel_data = merger.merge_scaled_data(
            sample_binned_data, mock_scaling_model, merge_config
        )
        
        assert isinstance(voxel_data, VoxelDataRelative)
        assert len(voxel_data.voxel_indices) == 2  # Two voxels in sample data
        assert len(voxel_data.I_merged_relative) == 2
        assert len(voxel_data.Sigma_merged_relative) == 2
        assert all(voxel_data.num_observations > 0)

    def test_merge_scaled_data_minimum_observations(self, merger, mock_scaling_model, 
                                                  sample_binned_data):
        """Test minimum observations filtering."""
        config = {
            'minimum_observations': 2,  # Require at least 2 observations
            'weight_method': 'inverse_variance'
        }
        
        voxel_data = merger.merge_scaled_data(
            sample_binned_data, mock_scaling_model, config
        )
        
        # Only voxel 0 has 2 observations, voxel 1 has only 1
        assert len(voxel_data.voxel_indices) == 1
        assert voxel_data.voxel_indices[0] == 0

    def test_merge_scaled_data_outlier_rejection(self, merger, mock_scaling_model):
        """Test outlier rejection functionality."""
        # Create data with obvious outlier
        binned_data = {
            0: {
                'observations': [
                    {
                        'intensity': 100.0,
                        'sigma': 10.0,
                        'still_id': 1,
                        'q_vector_lab': np.array([0.1, 0.1, 0.1])
                    },
                    {
                        'intensity': 110.0,
                        'sigma': 11.0,
                        'still_id': 1,
                        'q_vector_lab': np.array([0.1, 0.1, 0.1])
                    },
                    {
                        'intensity': 1000.0,  # Outlier
                        'sigma': 100.0,
                        'still_id': 1,
                        'q_vector_lab': np.array([0.1, 0.1, 0.1])
                    }
                ]
            }
        }
        
        config = {
            'outlier_rejection': {
                'enabled': True,
                'sigma_threshold': 2.0
            },
            'minimum_observations': 1,
            'weight_method': 'inverse_variance'
        }
        
        voxel_data = merger.merge_scaled_data(
            binned_data, mock_scaling_model, config
        )
        
        # Should have removed the outlier and merged remaining observations
        assert len(voxel_data.voxel_indices) == 1
        # Merged intensity should be much less than 1000 (outlier excluded)
        assert voxel_data.I_merged_relative[0] < 200

    def test_get_merge_statistics(self, merger):
        """Test statistics calculation."""
        # Create sample voxel data
        voxel_data = VoxelDataRelative(
            voxel_indices=np.array([0, 1, 2]),
            H_center=np.array([0.0, 1.0, 2.0]),
            K_center=np.array([0.0, 1.0, 2.0]),
            L_center=np.array([0.0, 1.0, 2.0]),
            q_center_x=np.array([0.1, 0.2, 0.3]),
            q_center_y=np.array([0.1, 0.2, 0.3]),
            q_center_z=np.array([0.1, 0.2, 0.3]),
            q_magnitude_center=np.array([0.17, 0.35, 0.52]),
            I_merged_relative=np.array([100.0, 200.0, 150.0]),
            Sigma_merged_relative=np.array([10.0, 20.0, 15.0]),
            num_observations=np.array([3, 5, 2])
        )
        
        stats = merger.get_merge_statistics(voxel_data)
        
        required_keys = [
            'total_voxels', 'intensity_statistics', 'observation_statistics',
            'resolution_coverage', 'data_quality'
        ]
        
        for key in required_keys:
            assert key in stats
        
        assert stats['total_voxels'] == 3
        assert stats['observation_statistics']['total_observations'] == 10
        assert stats['intensity_statistics']['mean'] == 150.0

    def test_save_voxel_data_npz(self, merger):
        """Test saving voxel data to NPZ format."""
        voxel_data = VoxelDataRelative(
            voxel_indices=np.array([0, 1]),
            H_center=np.array([0.0, 1.0]),
            K_center=np.array([0.0, 1.0]),
            L_center=np.array([0.0, 1.0]),
            q_center_x=np.array([0.1, 0.2]),
            q_center_y=np.array([0.1, 0.2]),
            q_center_z=np.array([0.1, 0.2]),
            q_magnitude_center=np.array([0.17, 0.35]),
            I_merged_relative=np.array([100.0, 200.0]),
            Sigma_merged_relative=np.array([10.0, 20.0]),
            num_observations=np.array([3, 5])
        )
        
        with tempfile.TemporaryDirectory() as temp_dir:
            output_path = os.path.join(temp_dir, "test_voxel_data.npz")
            
            merger.save_voxel_data(voxel_data, output_path, format="npz")
            
            assert os.path.exists(output_path)
            
            # Verify data can be loaded
            loaded_data = np.load(output_path)
            assert 'voxel_indices' in loaded_data
            assert 'I_merged_relative' in loaded_data
            assert len(loaded_data['voxel_indices']) == 2

    def test_save_voxel_data_invalid_format(self, merger):
        """Test error with invalid save format."""
        voxel_data = VoxelDataRelative(
            voxel_indices=np.array([0]),
            H_center=np.array([0.0]),
            K_center=np.array([0.0]),
            L_center=np.array([0.0]),
            q_center_x=np.array([0.1]),
            q_center_y=np.array([0.1]),
            q_center_z=np.array([0.1]),
            q_magnitude_center=np.array([0.17]),
            I_merged_relative=np.array([100.0]),
            Sigma_merged_relative=np.array([10.0]),
            num_observations=np.array([3])
        )
        
        with pytest.raises(ValueError, match="Unsupported format"):
            merger.save_voxel_data(voxel_data, "test.xyz", format="invalid")
</file>

<file path="tests/orchestration/test_stills_pipeline_orchestrator.py">
"""
Tests for StillsPipelineOrchestrator including Phase 3 integration.
"""

import pytest
import tempfile
import os
import numpy as np
from unittest.mock import Mock, patch, MagicMock
from pathlib import Path

from diffusepipe.orchestration.stills_pipeline_orchestrator import StillsPipelineOrchestrator
from diffusepipe.types.types_IDL import (
    StillsPipelineConfig,
    DIALSStillsProcessConfig,
    ExtractionConfig,
    RelativeScalingConfig,
    StillProcessingOutcome,
    OperationOutcome
)
from diffusepipe.exceptions import FileSystemError, ConfigurationError


class TestStillsPipelineOrchestrator:
    """Test StillsPipelineOrchestrator functionality."""
    
    @pytest.fixture
    def basic_config(self):
        """Create basic pipeline configuration."""
        return StillsPipelineConfig(
            dials_stills_process_config=DIALSStillsProcessConfig(
                stills_process_phil_path=None,
                force_processing_mode='stills',
                calculate_partiality=True
            ),
            extraction_config=ExtractionConfig(
                gain=1.0,
                cell_length_tol=0.05,
                cell_angle_tol=2.0,
                orient_tolerance_deg=2.0,
                q_consistency_tolerance_angstrom_inv=0.01,
                pixel_step=1,
                lp_correction_enabled=True,
                plot_diagnostics=False,
                verbose=False,
                external_pdb_path=None
            ),
            relative_scaling_config=RelativeScalingConfig(
                refine_per_still_scale=True,
                refine_resolution_scale_multiplicative=False,
                refine_additive_offset=False,
                min_partiality_threshold=0.1,
                grid_config={
                    'd_min_target': 1.0,
                    'd_max_target': 10.0,
                    'ndiv_h': 3,
                    'ndiv_k': 3,
                    'ndiv_l': 3
                }
            ),
            run_consistency_checker=False,
            run_q_calculator=False
        )
    
    @pytest.fixture
    def orchestrator(self):
        """Create orchestrator instance."""
        return StillsPipelineOrchestrator()
    
    def test_initialization(self, orchestrator):
        """Test orchestrator initialization."""
        assert orchestrator.summary_log_entries == []
        assert orchestrator.phase2_outputs == []
        assert orchestrator.successful_experiments == []
        assert orchestrator.pixel_mask is None
    
    def test_validate_inputs_missing_cbf(self, orchestrator, basic_config):
        """Test input validation with missing CBF file."""
        cbf_paths = ['/nonexistent/file.cbf']
        
        with pytest.raises(FileSystemError, match="CBF file not found"):
            orchestrator._validate_inputs(cbf_paths, basic_config, '/tmp')
    
    def test_validate_inputs_none_config(self, orchestrator):
        """Test input validation with None configuration."""
        cbf_paths = []
        
        with pytest.raises(ConfigurationError, match="Configuration cannot be None"):
            orchestrator._validate_inputs(cbf_paths, None, '/tmp')
    
    def test_determine_processing_route_forced(self, orchestrator, basic_config):
        """Test processing route determination with forced mode."""
        # Test forced stills mode
        basic_config.dials_stills_process_config.force_processing_mode = 'stills'
        route = orchestrator._determine_processing_route('dummy.cbf', basic_config)
        assert route == 'stills'
        
        # Test forced sequence mode
        basic_config.dials_stills_process_config.force_processing_mode = 'sequence'
        route = orchestrator._determine_processing_route('dummy.cbf', basic_config)
        assert route == 'sequence'
    
    @patch('diffusepipe.orchestration.stills_pipeline_orchestrator.CBFUtils')
    def test_determine_processing_route_auto(self, mock_cbf_utils, orchestrator, basic_config):
        """Test automatic processing route determination."""
        basic_config.dials_stills_process_config.force_processing_mode = None
        
        # Mock CBF utils
        mock_utils_instance = Mock()
        mock_cbf_utils.return_value = mock_utils_instance
        
        # Test stills detection (angle_increment = 0)
        mock_utils_instance.get_angle_increment.return_value = 0.0
        route = orchestrator._determine_processing_route('dummy.cbf', basic_config)
        assert route == 'stills'
        
        # Test sequence detection (angle_increment > 0)
        mock_utils_instance.get_angle_increment.return_value = 0.1
        route = orchestrator._determine_processing_route('dummy.cbf', basic_config)
        assert route == 'sequence'
    
    def test_should_run_phase3_insufficient_data(self, orchestrator, basic_config):
        """Test Phase 3 skip with insufficient data."""
        # Only one successful output
        orchestrator.phase2_outputs = [{'dummy': 'data'}]
        
        assert not orchestrator._should_run_phase3(basic_config)
    
    def test_should_run_phase3_missing_config(self, orchestrator):
        """Test Phase 3 skip with missing configuration."""
        # Enough data but no config
        orchestrator.phase2_outputs = [{'dummy': 'data1'}, {'dummy': 'data2'}]
        
        config = StillsPipelineConfig(
            dials_stills_process_config=DIALSStillsProcessConfig(),
            extraction_config=ExtractionConfig(gain=1.0, cell_length_tol=0.05, 
                                             cell_angle_tol=2.0, orient_tolerance_deg=2.0,
                                             q_consistency_tolerance_angstrom_inv=0.01,
                                             pixel_step=1, lp_correction_enabled=True,
                                             plot_diagnostics=False, verbose=False,
                                             external_pdb_path=None),
            relative_scaling_config=None,  # Missing Phase 3 config
            run_consistency_checker=False,
            run_q_calculator=False
        )
        
        assert not orchestrator._should_run_phase3(config)
    
    def test_should_run_phase3_ready(self, orchestrator, basic_config):
        """Test Phase 3 readiness check with valid conditions."""
        # Sufficient data and config
        orchestrator.phase2_outputs = [
            {'dummy': 'data1'}, 
            {'dummy': 'data2'},
            {'dummy': 'data3'}
        ]
        
        assert orchestrator._should_run_phase3(basic_config)
    
    def test_collect_phase2_outputs(self, orchestrator):
        """Test collection of Phase 2 outputs."""
        with tempfile.TemporaryDirectory() as temp_dir:
            # Create mock NPZ file
            npz_path = os.path.join(temp_dir, 'diffuse_data.npz')
            np.savez(npz_path,
                    q_vectors=np.random.randn(100, 3),
                    intensities=np.random.exponential(100, 100),
                    sigmas=np.random.uniform(1, 10, 100))
            
            # Create mock outcome
            outcome = StillProcessingOutcome(
                input_cbf_path='/dummy/path.cbf',
                status='SUCCESS_ALL',
                working_directory=temp_dir,
                dials_outcome=OperationOutcome(status='SUCCESS'),
                extraction_outcome=OperationOutcome(
                    status='SUCCESS',
                    output_artifacts={'diffuse_data_path': npz_path}
                )
            )
            
            # Collect outputs
            orchestrator._collect_phase2_outputs(outcome)
            
            assert len(orchestrator.phase2_outputs) == 1
            assert 'diffuse_data' in orchestrator.phase2_outputs[0]
            assert orchestrator.phase2_outputs[0]['still_id'] == 0
    
    @patch('diffusepipe.orchestration.stills_pipeline_orchestrator.DIALSStillsProcessAdapter')
    @patch('diffusepipe.orchestration.stills_pipeline_orchestrator.DataExtractor')
    @patch.object(StillsPipelineOrchestrator, '_generate_bragg_mask', return_value="/fake/mask.pickle")
    def test_process_single_still_success(self, mock_generate_mask, mock_extractor, 
                                        mock_adapter, orchestrator, basic_config):
        """Test successful processing of single still."""
        with tempfile.TemporaryDirectory() as temp_dir:
            # Mock DIALS adapter
            mock_adapter_instance = Mock()
            mock_adapter.return_value = mock_adapter_instance
            
            # Mock experiment and reflections
            mock_experiment = Mock()
            mock_reflections = Mock()
            mock_adapter_instance.process_still.return_value = (
                mock_experiment, mock_reflections
            )
            
            # Mock data extractor
            mock_extractor_instance = Mock()
            mock_extractor.return_value = mock_extractor_instance
            
            # Process still
            outcome = orchestrator._process_single_still(
                'dummy.cbf', basic_config, Path(temp_dir)
            )
            
            assert outcome.status == 'SUCCESS_ALL'
            assert outcome.dials_outcome.status == 'SUCCESS'
            assert outcome.extraction_outcome.status == 'SUCCESS'
    
    @patch('os.path.exists', return_value=True)
    @patch('diffusepipe.orchestration.stills_pipeline_orchestrator.Path.mkdir')
    def test_process_stills_batch_no_phase3(self, mock_mkdir, mock_exists, orchestrator, basic_config):
        """Test batch processing without Phase 3."""
        with tempfile.TemporaryDirectory() as temp_dir:
            # Disable Phase 3
            basic_config.relative_scaling_config = None
            
            # Mock successful processing
            with patch.object(orchestrator, '_process_single_still') as mock_process:
                mock_outcome = StillProcessingOutcome(
                    input_cbf_path='dummy.cbf',
                    status='FAILURE_DIALS',
                    working_directory=temp_dir,
                    dials_outcome=OperationOutcome(status='FAILURE'),
                    extraction_outcome=OperationOutcome(status='FAILURE')
                )
                mock_process.return_value = mock_outcome
                
                # Process batch
                outcomes = orchestrator.process_stills_batch(
                    ['dummy.cbf'], basic_config, temp_dir
                )
                
                assert len(outcomes) == 1
                assert outcomes[0].status == 'FAILURE_DIALS'
    
    def test_integration_with_phase3_components(self):
        """Test that Phase 3 components can be imported and instantiated."""
        # This test verifies that all Phase 3 imports work correctly
        from diffusepipe.voxelization.global_voxel_grid import (
            GlobalVoxelGrid, GlobalVoxelGridConfig, CorrectedDiffusePixelData
        )
        from diffusepipe.voxelization.voxel_accumulator import VoxelAccumulator
        from diffusepipe.scaling.diffuse_scaling_model import DiffuseScalingModel
        from diffusepipe.merging.merger import DiffuseDataMerger
        
        # Verify classes can be referenced
        assert GlobalVoxelGrid is not None
        assert VoxelAccumulator is not None
        assert DiffuseScalingModel is not None
        assert DiffuseDataMerger is not None
</file>

<file path="tests/test_corrections_regression.py">
"""
Regression tests for correction factors as specified in Module 2.S.2.

This test module implements the regression test requirement from the plan:
"A regression test must be implemented that, for a synthetic experiment and a few 
selected pixel positions (including off-Bragg positions), compares the individual 
LP_divisor and QE_multiplier values obtained from the DIALS Corrections adapter 
against trusted reference values or a separate, careful analytic calculation."
"""

import numpy as np
from unittest.mock import Mock, patch, MagicMock

from diffusepipe.corrections import (
    apply_corrections,
    calculate_analytic_pixel_corrections_45deg,
    validate_correction_factors,
    create_synthetic_experiment_for_testing,
)
from diffusepipe.extraction.data_extractor import DataExtractor


class TestCorrectionsRegression:
    """Regression tests for correction factor calculations."""

    def test_apply_corrections_helper(self):
        """Test the apply_corrections helper function."""
        raw_intensity = 1000.0
        lp_mult = 0.5
        qe_mult = 0.8
        sa_mult = 1.2
        air_mult = 1.05

        corrected_intensity = apply_corrections(
            raw_intensity, lp_mult, qe_mult, sa_mult, air_mult
        )

        expected = raw_intensity * lp_mult * qe_mult * sa_mult * air_mult
        assert abs(corrected_intensity - expected) < 1e-10

    def test_analytic_45deg_pixel_corrections(self):
        """Test analytic correction calculation for 45° pixel."""
        raw_intensity = 1000.0
        wavelength = 1.0  # Angstrom
        detector_distance = 200.0  # mm
        pixel_size = 0.172  # mm

        corrected_intensity, factors = calculate_analytic_pixel_corrections_45deg(
            raw_intensity, wavelength, detector_distance, pixel_size
        )

        # Verify factors are reasonable
        assert 0.1 <= factors["lp_mult"] <= 10.0
        assert 0.5 <= factors["qe_mult"] <= 2.0  # Should be 1.0 for ideal detector
        assert factors["sa_mult"] > 0
        assert 0.9 <= factors["air_mult"] <= 1.1  # Should be ~1.0

        # Verify total correction is applied
        expected_total = (
            factors["lp_mult"]
            * factors["qe_mult"]
            * factors["sa_mult"]
            * factors["air_mult"]
        )
        assert abs(factors["total_mult"] - expected_total) < 1e-10

        expected_corrected = raw_intensity * expected_total
        assert abs(corrected_intensity - expected_corrected) < 1e-10

    def test_lp_correction_45deg_analytic_vs_implementation(self):
        """
        Regression test comparing DataExtractor LP correction with analytic calculation.

        This implements the specific regression test requirement from Module 2.S.2.
        """
        # Create synthetic experiment for 45° pixel
        experiment = create_synthetic_experiment_for_testing()
        extractor = DataExtractor()

        # Calculate s1 vector for 45° scattering
        wavelength = 1.0  # Angstrom

        # For 45° scattering: lab_coord = [141.4, 0, 200] gives 45° angle
        lab_coord = np.array([141.4, 0.0, 200.0])
        scatter_direction = lab_coord / np.linalg.norm(lab_coord)
        s1_vector = scatter_direction * (1.0 / wavelength)  # |s1| = 1/λ

        # Calculate analytic LP correction for 45°
        theta = np.pi / 4  # 45 degrees
        sin_theta = np.sin(theta)
        cos_2theta = np.cos(2 * theta)
        lp_divisor_analytic = sin_theta**2 * (1 + cos_2theta**2)
        lp_mult_analytic = 1.0 / lp_divisor_analytic

        # Get LP correction from DataExtractor implementation
        with (
            patch("dials.algorithms.integration.Corrections") as mock_corrections_class,
            patch("dials.array_family.flex") as mock_flex,
        ):
            mock_corrections_obj = Mock()
            mock_corrections_class.return_value = mock_corrections_obj

            # Mock flex array creation
            mock_flex.vec3_double.return_value = [None]
            mock_flex.size_t.return_value = [0]

            # Set the mock to return the analytic divisor
            from unittest.mock import MagicMock

            mock_lp_divisors = MagicMock()
            mock_lp_divisors.__getitem__.return_value = lp_divisor_analytic
            mock_lp_divisors.__len__.return_value = 1
            mock_corrections_obj.lp.return_value = mock_lp_divisors

            lp_mult_implementation = extractor._get_lp_correction(
                s1_vector, experiment.beam, experiment
            )

            # Compare implementation with analytic (should match within 1%)
            relative_error = (
                abs(lp_mult_implementation - lp_mult_analytic) / lp_mult_analytic
            )
            assert relative_error < 0.01, (
                f"LP correction mismatch: analytic={lp_mult_analytic:.6f}, "
                f"implementation={lp_mult_implementation:.6f}, error={relative_error:.3%}"
            )

    def test_correction_factors_validation(self):
        """Test the correction factor validation function."""
        # Valid factors
        valid_factors = {
            "lp_mult": 1.5,
            "qe_mult": 0.8,
            "sa_mult": 1000.0,
            "air_mult": 1.02,
        }

        is_valid, message = validate_correction_factors(valid_factors)
        assert is_valid
        assert "reasonable" in message

        # Invalid factors - negative LP
        invalid_factors = valid_factors.copy()
        invalid_factors["lp_mult"] = -0.5

        is_valid, message = validate_correction_factors(invalid_factors)
        assert not is_valid
        assert "not positive" in message

        # Invalid factors - extreme QE
        invalid_factors = valid_factors.copy()
        invalid_factors["qe_mult"] = 10.0

        is_valid, message = validate_correction_factors(invalid_factors)
        assert not is_valid
        assert "unreasonable" in message

    def test_multiple_pixel_positions_regression(self):
        """Test correction factors for multiple pixel positions."""
        extractor = DataExtractor()
        experiment = create_synthetic_experiment_for_testing()

        # Test positions: on-axis, 30°, 45°, 60°
        test_positions = [
            (0.0, 0.0, 200.0),  # On-axis (0°)
            (100.0, 0.0, 173.2),  # ~30°
            (141.4, 0.0, 141.4),  # 45°
            (173.2, 0.0, 100.0),  # ~60°
        ]

        wavelength = 1.0

        with patch(
            "dials.algorithms.integration.Corrections"
        ) as mock_corrections_class:
            mock_corrections_obj = Mock()
            mock_corrections_class.return_value = mock_corrections_obj

            for i, (x, y, z) in enumerate(test_positions):
                lab_coord = np.array([x, y, z])
                scatter_direction = lab_coord / np.linalg.norm(lab_coord)
                s1_vector = scatter_direction * (1.0 / wavelength)

                # Calculate expected scattering angle
                beam_direction = np.array([0, 0, -1])
                cos_theta = -np.dot(scatter_direction, beam_direction)
                theta = np.arccos(cos_theta)

                # Calculate analytic LP factor
                sin_theta = np.sin(theta)
                cos_2theta = np.cos(2 * theta)
                lp_divisor_expected = sin_theta**2 * (1 + cos_2theta**2)

                # Handle potential division by zero for on-axis scattering
                if abs(lp_divisor_expected) < 1e-9:
                    lp_mult_expected = float(
                        "inf"
                    )  # Very large correction for forward scattering
                else:
                    lp_mult_expected = 1.0 / lp_divisor_expected

                # Mock DIALS to return expected value
                mock_lp_divisors = MagicMock(name="mock_flex_array_for_lp")
                mock_lp_divisors.__getitem__.return_value = lp_divisor_expected
                mock_lp_divisors.__len__.return_value = 1
                mock_corrections_obj.lp.return_value = mock_lp_divisors

                # Test LP correction
                lp_mult = extractor._get_lp_correction(
                    s1_vector, experiment.beam, experiment
                )

                # Handle infinity case for on-axis scattering
                if np.isinf(lp_mult_expected):
                    # For on-axis scattering with zero divisor, the data extractor should return 1.0 as fallback
                    assert lp_mult == 1.0, (
                        f"Position {i} (θ={np.degrees(theta):.1f}°): "
                        f"Expected fallback LP correction 1.0 for zero divisor, got {lp_mult}"
                    )
                else:
                    relative_error = abs(lp_mult - lp_mult_expected) / lp_mult_expected
                    assert relative_error < 0.01, (
                        f"Position {i} (θ={np.degrees(theta):.1f}°): "
                        f"LP error {relative_error:.3%}"
                    )

                # Test that LP correction is reasonable for this angle (except for on-axis)
                if not np.isinf(lp_mult_expected):
                    assert (
                        0.1 <= lp_mult <= 10.0
                    ), f"Unreasonable LP correction: {lp_mult}"

    def test_solid_angle_correction_known_geometry(self):
        """Test solid angle correction for known geometric configuration."""
        extractor = DataExtractor()
        experiment = create_synthetic_experiment_for_testing()
        panel = experiment.detector[0]

        # Test with known geometry: pixel at (100, 50, 200) mm
        lab_coord = np.array([100.0, 50.0, 200.0])

        sa_mult = extractor._calculate_solid_angle_correction(lab_coord, panel, 10, 20)

        # Calculate expected solid angle
        pixel_size = 0.172  # mm
        pixel_area = pixel_size**2
        distance = np.linalg.norm(lab_coord)

        # Get panel normal (should be [0, 0, 1] for this mock)
        fast_axis = np.array([1, 0, 0])
        slow_axis = np.array([0, 1, 0])
        normal = np.cross(fast_axis, slow_axis)  # [0, 0, 1]

        scatter_direction = lab_coord / distance
        cos_theta = abs(np.dot(normal, scatter_direction))

        expected_solid_angle = (pixel_area * cos_theta) / (distance**2)
        expected_sa_mult = 1.0 / expected_solid_angle

        # Allow for small numerical differences
        relative_error = abs(sa_mult - expected_sa_mult) / expected_sa_mult
        assert relative_error < 0.01, (
            f"Solid angle correction mismatch: expected={expected_sa_mult:.6f}, "
            f"actual={sa_mult:.6f}, error={relative_error:.3%}"
        )

    def test_air_attenuation_known_conditions(self):
        """Test air attenuation correction for known conditions."""
        extractor = DataExtractor()
        experiment = create_synthetic_experiment_for_testing()

        # Mock config with known conditions
        config = Mock()
        config.air_temperature_k = 293.15  # 20°C
        config.air_pressure_atm = 1.0  # 1 atm

        # Test with short path (should have minimal attenuation)
        lab_coord_short = np.array([100.0, 0.0, 100.0])  # ~141 mm path
        air_mult_short = extractor._calculate_air_attenuation_correction(
            lab_coord_short, experiment.beam, config
        )

        # Should be very close to 1.0 for short paths
        assert 0.99 <= air_mult_short <= 1.01

        # Test with longer path (should have slightly more attenuation)
        lab_coord_long = np.array([500.0, 0.0, 500.0])  # ~707 mm path
        air_mult_long = extractor._calculate_air_attenuation_correction(
            lab_coord_long, experiment.beam, config
        )

        # Should still be close to 1.0 but slightly higher than short path
        assert air_mult_long >= air_mult_short
        assert 0.98 <= air_mult_long <= 1.05

    def test_end_to_end_correction_pipeline_45deg(self):
        """End-to-end test of correction pipeline for 45° pixel."""
        extractor = DataExtractor()
        experiment = create_synthetic_experiment_for_testing()

        # Mock config
        config = Mock()
        config.lp_correction_enabled = True
        config.air_temperature_k = 293.15
        config.air_pressure_atm = 1.0

        # Test data for 45° pixel
        intensity = 1000.0
        sigma = np.sqrt(intensity)
        q_vector = np.array([0.1, 0.1, 0.1])  # Arbitrary q-vector
        lab_coord = np.array([141.4, 0.0, 200.0])  # 45° position
        panel = experiment.detector[0]
        beam = experiment.beam

        with (
            patch("dials.algorithms.integration.Corrections") as mock_corrections_class,
            patch("dials.array_family.flex") as mock_flex,
        ):
            mock_corrections_obj = Mock()
            mock_corrections_class.return_value = mock_corrections_obj

            # Mock flex array creation
            mock_flex.vec3_double.return_value = [None]
            mock_flex.size_t.return_value = [0]

            # Use analytic values for 45° pixel
            theta = np.pi / 4
            sin_theta = np.sin(theta)
            cos_2theta = np.cos(2 * theta)
            lp_divisor = sin_theta**2 * (1 + cos_2theta**2)

            from unittest.mock import MagicMock

            mock_lp_divisors = MagicMock()
            mock_lp_divisors.__getitem__.return_value = lp_divisor
            mock_lp_divisors.__len__.return_value = 1
            mock_corrections_obj.lp.return_value = mock_lp_divisors

            # Mock QE as ideal detector
            mock_qe_multipliers = MagicMock()
            mock_qe_multipliers.__getitem__.return_value = 1.0
            mock_qe_multipliers.__len__.return_value = 1
            mock_corrections_obj.qe.return_value = mock_qe_multipliers

            corrected_intensity, corrected_sigma = extractor._apply_pixel_corrections(
                intensity,
                sigma,
                q_vector,
                lab_coord,
                panel,
                beam,
                experiment,
                10,
                20,
                config,
            )

            # Verify corrections were applied
            assert corrected_intensity != intensity
            assert corrected_sigma != sigma

            # Total correction should include LP correction
            # The correction is much larger due to solid angle correction
            assert (
                corrected_intensity > intensity
            )  # Corrected should be larger than original
            assert corrected_sigma > sigma  # Corrected sigma should also be larger
</file>

<file path="tests/test_phase2_core_functionality.py">
"""
Core functionality tests for Phase 2 implementation.

Tests the essential Phase 2 functionality without requiring DIALS dependencies,
focusing on the correction calculation logic and data flow.
"""

import pytest
import numpy as np
from unittest.mock import Mock

from diffusepipe.extraction.data_extractor import DataExtractor
from diffusepipe.types.types_IDL import ExtractionConfig
from diffusepipe.corrections import (
    apply_corrections,
    calculate_analytic_pixel_corrections_45deg,
    validate_correction_factors,
)


class TestPhase2CoreFunctionality:
    """Test core Phase 2 functionality."""

    def test_correction_helper_function(self):
        """Test the apply_corrections helper function."""
        raw_intensity = 1000.0
        lp_mult = 0.5
        qe_mult = 0.8
        sa_mult = 1.2
        air_mult = 1.05
        
        corrected = apply_corrections(raw_intensity, lp_mult, qe_mult, sa_mult, air_mult)
        expected = raw_intensity * lp_mult * qe_mult * sa_mult * air_mult
        
        assert abs(corrected - expected) < 1e-10
        assert corrected == 1000.0 * 0.5 * 0.8 * 1.2 * 1.05  # 504.0

    def test_solid_angle_correction_calculation(self):
        """Test solid angle correction calculation with mock panel."""
        extractor = DataExtractor()
        
        # Create mock panel
        panel = Mock()
        panel.get_pixel_size.return_value = (0.172, 0.172)  # mm
        panel.get_fast_axis.return_value = [1, 0, 0]
        panel.get_slow_axis.return_value = [0, 1, 0]
        
        # Test pixel position
        lab_coord = np.array([100.0, 50.0, 200.0])  # mm
        
        sa_mult = extractor._calculate_solid_angle_correction(lab_coord, panel, 10, 20)
        
        # Should return reasonable correction factor
        assert sa_mult > 0
        assert sa_mult < 1e7  # Sanity check - can be large for small pixels
        
        # Manual calculation to verify
        pixel_area = 0.172 * 0.172  # mm²
        r = np.linalg.norm(lab_coord)
        normal = np.array([0, 0, 1])  # z-axis from cross product of x and y
        scatter_direction = lab_coord / r
        cos_theta = abs(np.dot(normal, scatter_direction))
        expected_solid_angle = (pixel_area * cos_theta) / (r * r)
        expected_sa_mult = 1.0 / expected_solid_angle
        
        assert abs(sa_mult - expected_sa_mult) / expected_sa_mult < 0.01

    def test_air_attenuation_coefficient_with_parameters(self):
        """Test air attenuation coefficient calculation with different parameters."""
        extractor = DataExtractor()
        
        energy_ev = 12000  # 12 keV
        
        # Standard conditions
        mu_standard = extractor._calculate_air_attenuation_coefficient(energy_ev, 293.15, 1.0)
        
        # Higher pressure should increase attenuation
        mu_high_pressure = extractor._calculate_air_attenuation_coefficient(energy_ev, 293.15, 2.0)
        assert mu_high_pressure > mu_standard
        
        # Higher temperature should decrease attenuation  
        mu_high_temp = extractor._calculate_air_attenuation_coefficient(energy_ev, 350.0, 1.0)
        assert mu_high_temp < mu_standard
        
        # All should be reasonable values
        assert 0.0001 < mu_standard < 1.0
        assert 0.0001 < mu_high_pressure < 2.0
        assert 0.0001 < mu_high_temp < 1.0

    def test_air_attenuation_correction_calculation(self):
        """Test air attenuation correction calculation with mock beam."""
        extractor = DataExtractor()
        
        # Create mock beam
        beam = Mock()
        beam.get_wavelength.return_value = 1.0  # 1 Angstrom
        
        # Create mock config
        config = Mock()
        config.air_temperature_k = 293.15
        config.air_pressure_atm = 1.0
        
        # Test short path (minimal attenuation)
        lab_coord_short = np.array([100.0, 0.0, 100.0])
        air_mult_short = extractor._calculate_air_attenuation_correction(lab_coord_short, beam, config)
        
        # Should be close to 1.0 for short paths
        assert 0.99 <= air_mult_short <= 1.01
        
        # Test longer path
        lab_coord_long = np.array([500.0, 0.0, 500.0])
        air_mult_long = extractor._calculate_air_attenuation_correction(lab_coord_long, beam, config)
        
        # Should be slightly higher (more correction) for longer path
        assert air_mult_long >= air_mult_short
        assert 0.98 <= air_mult_long <= 1.05

    def test_mask_total_2d_parameter_handling(self):
        """Test that DataExtractor properly handles the new mask_total_2d parameter."""
        extractor = DataExtractor()
        
        # Test validation with mask_total_2d provided
        from diffusepipe.types.types_IDL import ComponentInputFiles
        
        inputs = ComponentInputFiles(
            cbf_image_path="/fake/path.cbf",
            dials_expt_path="/fake/path.expt"
            # Note: no bragg_mask_path
        )
        
        config = ExtractionConfig(
            gain=1.0,
            cell_length_tol=0.01,
            cell_angle_tol=0.1,
            orient_tolerance_deg=1.0,
            q_consistency_tolerance_angstrom_inv=0.01,
            pixel_step=1,
            lp_correction_enabled=True,
            plot_diagnostics=False,
            verbose=False
        )
        
        # Create mock mask
        mock_mask = np.zeros((100, 100), dtype=bool)
        mask_total_2d = (mock_mask,)
        
        # This should pass validation since mask_total_2d is provided
        result = extractor._validate_inputs(inputs, config, "/tmp/output.npz", mask_total_2d)
        
        # Note: This will fail due to file existence checks, but the mask validation should pass
        # The important thing is it doesn't fail due to missing bragg_mask_path
        assert "mask_total_2d not passed" not in result.message

    def test_analytic_45deg_correction_calculation(self):
        """Test the analytic 45° correction calculation."""
        raw_intensity = 1000.0
        wavelength = 1.0  # Angstrom
        detector_distance = 200.0  # mm
        pixel_size = 0.172  # mm
        
        corrected_intensity, factors = calculate_analytic_pixel_corrections_45deg(
            raw_intensity, wavelength, detector_distance, pixel_size
        )
        
        # Verify factors are reasonable
        assert 0.1 <= factors['lp_mult'] <= 10.0
        assert factors['qe_mult'] == 1.0  # Ideal detector
        assert factors['sa_mult'] > 0
        assert factors['air_mult'] == 1.0  # No air path specified
        
        # Verify correction was applied
        expected_total = factors['lp_mult'] * factors['qe_mult'] * factors['sa_mult'] * factors['air_mult']
        assert abs(factors['total_mult'] - expected_total) < 1e-10
        assert abs(corrected_intensity - raw_intensity * expected_total) < 1e-10

    def test_correction_factors_validation(self):
        """Test correction factor validation logic."""
        # Valid factors
        valid_factors = {
            'lp_mult': 1.5,
            'qe_mult': 0.8,
            'sa_mult': 1000.0,
            'air_mult': 1.02
        }
        
        is_valid, message = validate_correction_factors(valid_factors)
        assert is_valid
        assert "reasonable" in message
        
        # Test various invalid cases
        invalid_cases = [
            ({'lp_mult': -0.5, 'qe_mult': 0.8, 'sa_mult': 1.0, 'air_mult': 1.0}, "not positive"),
            ({'lp_mult': 15.0, 'qe_mult': 0.8, 'sa_mult': 1.0, 'air_mult': 1.0}, "unreasonable"),
            ({'lp_mult': 1.0, 'qe_mult': 5.0, 'sa_mult': 1.0, 'air_mult': 1.0}, "unreasonable"),
            ({'lp_mult': 1.0, 'qe_mult': 0.8, 'sa_mult': 1e7, 'air_mult': 1.0}, "unreasonable"),
            ({'lp_mult': 1.0, 'qe_mult': 0.8, 'sa_mult': 1.0, 'air_mult': 2.0}, "unreasonable"),
        ]
        
        for invalid_factors, expected_error in invalid_cases:
            is_valid, message = validate_correction_factors(invalid_factors)
            assert not is_valid
            assert expected_error in message

    def test_configuration_parameters_added(self):
        """Test that new configuration parameters are properly handled."""
        # Test that ExtractionConfig accepts the new air parameters
        config = ExtractionConfig(
            gain=1.0,
            cell_length_tol=0.01,
            cell_angle_tol=0.1,
            orient_tolerance_deg=1.0,
            q_consistency_tolerance_angstrom_inv=0.01,
            pixel_step=1,
            lp_correction_enabled=True,
            plot_diagnostics=False,
            verbose=False,
            air_temperature_k=300.0,  # Custom temperature
            air_pressure_atm=0.8      # Custom pressure
        )
        
        assert config.air_temperature_k == 300.0
        assert config.air_pressure_atm == 0.8
        
        # Test defaults
        config_default = ExtractionConfig(
            gain=1.0,
            cell_length_tol=0.01,
            cell_angle_tol=0.1,
            orient_tolerance_deg=1.0,
            q_consistency_tolerance_angstrom_inv=0.01,
            pixel_step=1,
            lp_correction_enabled=True,
            plot_diagnostics=False,
            verbose=False
        )
        
        assert config_default.air_temperature_k == 293.15  # Default 20°C
        assert config_default.air_pressure_atm == 1.0       # Default 1 atm
</file>

<file path="meisburger.md">
**Assumptions:**

*   We are following the rotation data processing pipeline outlined by the MATLAB package.
*   `I_raw(px, py, f)`: Raw intensity at detector pixel `(px, py)` for frame `f`.
*   `t_exp(f)`: Exposure time for frame `f`.
*   `t_bkg_total`: Total exposure time for the summed background image.
*   `(H, K, L)`: Integer Miller indices of a reciprocal lattice point (RLP).
*   `(h, k, l)`: Continuous coordinates in reciprocal space (often fractional Miller indices).
*   `s`: Scattering vector, `s = k_scattered - k_incident`. `|s| = 2sin(θ)/λ`.
*   `v`: Index for a voxel in the 3D reciprocal space grid.
*   `p`: Index for a pixel on the 2D detector.
*   `w`: Index for a wedge of data.

---

**Phase 1: Geometry Definition and Initial Data Preparation**

**Step 0: Define Experimental Geometry and Image Series (Conceptual Pre-step)**
*   **Action:** Load/define `geom.DiffractionExperiment` (DE) and `io.ImageSeries` (IS).
*   **Math:** Not an explicit calculation on intensities, but defines parameters used later.
    *   `DE_w`: `DiffractionExperiment` object for wedge `w` (includes `Crystal_w`, `Detector_w`, `Source_w`, `Spindle_w`).
    *   `IS_w`: `ImageSeries` object for wedge `w`.

**Step 1: Voxel Grid Definition and Filtering (`+proc/@Batch/filter.m`)**

*   **1a. Pixel Masking:**
    *   **Action:** Identify bad/untrusted detector pixels.
    *   **Math:** `Mask_pixel(px, py)` = boolean (true if pixel is good).
        *   `Mask_pixel = Mask_static & Mask_beamstop & Mask_gaps & Mask_virtual_corrected & (I_raw_first_frame >= 0)`
*   **1b. HKL Prediction & Grid Initialization (per wedge `w`):**
    *   **Action:** Determine reciprocal space coverage for `DE_w` and define `grid.Sub3dRef` object `Grid_w`.
    *   **Math (conceptual):**
        *   For each frame `f` in `IS_w`, for each detector pixel `(px, py)`:
            `(h(px,py,f), k(px,py,f), l(px,py,f)) = DE_w.frame2hkl(f)` using pixel `(px,py)`.
        *   `Grid_w.ref`: Set of unique integer `(H,K,L)` covered.
        *   `Grid_w.ndiv`: Subdivisions `[n_h, n_k, n_l]`.
*   **1c. Count Histogramming (per wedge `w`, for `Grid_w`):**
    *   **Action:** For each voxel `v` in `Grid_w`, count how many pixels `p` mapping to it have a certain intensity `I_raw(p,f)`.
    *   **Math:**
        *   `idx_voxel(p,f) = Grid_w.hkl2index(h(p,f), k(p,f), l(p,f))` (maps pixel `p` at frame `f` to a voxel index).
        *   `CountHist_w(v, count_val+1) = sum_{p,f where idx_voxel(p,f)=v and I_raw(p,f)=count_val} (1)` if `I_raw(p,f) <= maxCount`.
        *   `Overflow_w(v) = sum_{p,f where idx_voxel(p,f)=v and I_raw(p,f)>maxCount} (I_raw(p,f))`.
*   **1d. Statistical Filtering (`proc.Filter` on `CountHist_w`):**
    *   **Action:** Identify outlier voxels (likely Bragg peaks).
    *   **Math (conceptual for voxel `v` in wedge `w`):**
        *   `Lambda_v = WeightedMedian_{v' in Neighborhood(v)} ( MeanCountRate(v') )`
        *   `P_poisson(c | Lambda_v) = exp(-Lambda_v) * Lambda_v^c / c!`
        *   `P_observed(c | Neighborhood(v)) = CountHist_Neighborhood(v, c+1) / sum(CountHist_Neighborhood(v, :))`
        *   `DKL_v = sum_c P_observed(c) * log(P_observed(c) / P_poisson(c | Lambda_v))`
        *   Iteratively identify `isOutlier_w(v)` based on `DKL_v` and change in DKL upon removal.
    *   `Grid_w.voxelMask(v)` = `isOutlier_w(v)` OR (`Overflow_w(v) > threshold`).

---

**Phase 2: Intensity Integration and Correction**

**Step 2: Intensity Integration into Voxels (`+proc/@Batch/integrate.m`)**

*   **Action (per wedge `w`):** Sum intensities from `IS_w` into voxels of `Grid_w`, respecting `Grid_w.voxelMask`.
*   **Math (for voxel `v` in wedge `w`):**
    *   `Counts_w(v) = sum_{p,f where idx_voxel(p,f)=v AND NOT Grid_w.voxelMask(v)} (I_raw(p,f) * Mask_pixel(p))`
    *   `Pixels_w(v) = sum_{p,f where idx_voxel(p,f)=v AND NOT Grid_w.voxelMask(v)} (Mask_pixel(p))`
    *   `N_frame_weighted_w(v) = sum_{p,f where idx_voxel(p,f)=v AND NOT Grid_w.voxelMask(v)} (f * Mask_pixel(p))`
    *   `iz_avg_w(v) = N_frame_weighted_w(v) / Pixels_w(v)`
    *   Similar sums for `CountsExcl_w(v)`, `PixelsExcl_w(v)` if `Grid_w.voxelMask(v)` is true.
    *   **Output:** `bin{w} = table(Counts_w, Pixels_w, iz_avg_w)`

**Step 3: Calculation of Correction Factors (`+proc/@Batch/correct.m`)**

*   **3a. Per-Pixel Geometric Corrections (per wedge `w`, for `Detector_w`, `Source_w`):**
    *   **Action:** Calculate standard corrections for each pixel `p`.
    *   **Math:**
        *   `SolidAngle_w(p) = geom.Corrections.solidAngle(...)`
        *   `Polarization_w(p) = geom.Corrections.polarization(...)`
        *   `Efficiency_w(p) = geom.Corrections.efficiency(...)`
        *   `Attenuation_w(p) = geom.Corrections.attenuation(...)` (typically for air)
        *   `d3s_w(p) = geom.Corrections.d3s(Source_w, Detector_w, Spindle_w)` (swept volume per pixel per frame)
        *   `(sx_w(p), sy_w(p), sz_w(p)) = DE_w.s()` using pixel `p`.
*   **3b. Background Image Processing (per wedge `w`, if `readBackground`):**
    *   **Action:** Read and sum dedicated background images corresponding to `IS_w`.
    *   **Math:**
        *   `I_bkg_sum_w(p) = sum_{f_bkg in IS_bkg_w} (I_raw_bkg(p, f_bkg))`
        *   `t_bkg_total_w = sum_{f_bkg in IS_bkg_w} (t_exp_bkg(f_bkg))`
*   **3c. Averaging Corrections per Voxel (per wedge `w`):**
    *   **Action:** Average per-pixel corrections over all pixels `p` contributing to each voxel `v`.
    *   **Math (for a generic correction `CF(p)` and voxel `v`):**
        *   `Multiplicity_w(v,p)`: Number of times pixel `p` maps to voxel `v` in wedge `w` (from `Integrater.pixelMultiplicity`).
        *   `CF_avg_w(v) = [sum_p (Multiplicity_w(v,p) * CF(p))] / [sum_p Multiplicity_w(v,p)]`
        *   This is done for `SolidAngle_avg_w(v)`, `Polarization_avg_w(v)`, etc., and also for:
            *   `sx_avg_w(v)`, `sy_avg_w(v)`, `sz_avg_w(v)`
            *   `ix_avg_w(v)`, `iy_avg_w(v)` (average detector pixel contributing)
            *   `d3s_avg_w(v)`
            *   `Bkg_avg_counts_w(v) = [sum_p (Multiplicity_w(v,p) * I_bkg_sum_w(p))] / [sum_p Multiplicity_w(v,p)]`
            *   `BkgErr_avg_counts_w(v) = sqrt[sum_p (Multiplicity_w(v,p)^2 * I_bkg_sum_w(p))] / [sum_p Multiplicity_w(v,p)]` (error of weighted mean)
    *   **Output:** `corr{w}` table containing these averaged corrections, `dt_w` (avg exposure of signal frames), `BkgDt_w` (total background exposure).

---

**Phase 3: Scaling (Relative and Absolute) and Merging**

**Step 4: Initial Export and Combination of Batches (`+proc/@Batch/export.m`, then `+proc/@Batch/combine.m`)**

*   **4a. Per-Wedge Intensity Calculation (`exportScript`):**
    *   **Action:** Calculate initial intensity per voxel before inter-wedge scaling.
    *   **Math (for voxel `v` in wedge `w`):**
        *   `TotalGeomCorr_w(v) = SolidAngle_avg_w(v) * Polarization_avg_w(v) * Efficiency_avg_w(v) * Attenuation_avg_w(v)`
        *   `Intensity_raw_w(v) = Counts_w(v) / (Pixels_w(v) * TotalGeomCorr_w(v) * dt_w)`
        *   `Sigma_raw_w(v) = sqrt(Counts_w(v)) / (Pixels_w(v) * TotalGeomCorr_w(v) * dt_w)`
        *   `BkgIntensity_raw_w(v) = Bkg_avg_counts_w(v) / (TotalGeomCorr_w(v) * BkgDt_w)` (assuming same geom. corr. for bkg)
        *   `BkgSigma_raw_w(v) = BkgErr_avg_counts_w(v) / (TotalGeomCorr_w(v) * BkgDt_w)`
        *   `I_obs_w(v) = Intensity_raw_w(v) - BkgIntensity_raw_w(v)`
        *   `Sigma_obs_w(v) = sqrt(Sigma_raw_w(v)^2 + BkgSigma_raw_w(v)^2)`
        *   `Fraction_w(v) = (d3s_avg_w(v) * Pixels_w(v)) / GridVoxelVolume_w(v)`
    *   **Output:** `diffuseTable_w` with `(H,K,L)`, `I_obs_w`, `Sigma_obs_w`, `sx_avg_w`, `sy_avg_w`, `sz_avg_w`, `ix_avg_w`, `iy_avg_w`, `iz_avg_w`, `Fraction_w`, `wedge_idx=w`.
*   **4b. Combine Batch Data (`combineScript`):**
    *   **Action:** Concatenate all `diffuseTable_w` into one `DiffuseTable_combined`.
    *   **Math:**
        *   `(H_asu, K_asu, L_asu)_v = AverageCrystal.hkl2asu((H,K,L)_v)`
        *   `s_mag_v = sqrt(sx_avg_v^2 + sy_avg_v^2 + sz_avg_v^2)`
        *   `panel_idx_v = AverageDetector.chipIndex(ix_avg_v, iy_avg_v)`
    *   **Output:** `DiffuseTable_combined` (with new columns), `ScalingModel_initial_list` (one per original batch, initialized).

**Step 5: Relative Scaling (`+proc/@Batch/scale.m`)**

*   **Action:** Refine `ScalingModel_initial_list` parameters (`a,b,c,d` control points) iteratively.
*   **Math (conceptual for one iteration and one batch `j`'s `ScalingModel_j`):**
    *   `I_merged_reference(H_asu,K_asu,L_asu)` is the current best estimate of merged intensities (from `MultiBatchScaler.merge()`).
    *   For each observation `obs` in batch `j` (mapping to `HKL_asu`):
        *   `a_obs = ScalingModel_j.aVal(ix_obs, iy_obs, iz_obs)`
        *   `b_obs = ScalingModel_j.bVal(iz_obs)`
        *   `c_obs = ScalingModel_j.cVal(s_mag_obs, iz_obs)`
        *   `d_obs = ScalingModel_j.dVal(panel_idx_obs)`
        *   `Scale_mult_obs = a_obs * b_obs * d_obs`
        *   `Offset_add_obs = c_obs / b_obs`
        *   `I_predicted_obs = Scale_mult_obs * (I_merged_reference(HKL_asu) + Offset_add_obs)`
        *   **Fitting `b` (example):** Minimize `sum_obs [ (I_obs(obs)/ (a_obs*d_obs) - (Offset_add_obs_using_b_current_guess + I_merged_reference(HKL_asu))) / (Sigma_obs(obs)/(a_obs*d_obs)) ]^2` by adjusting control points of `b`. Regularization terms (smoothness) are added to the minimization target. Similar for `a, c, d`.
    *   **Output:** `ScalingModel_refined_list`.

**Step 6: Merging Relatively Scaled Data (`+proc/@Batch/merge.m`)**

*   **Action:** Apply refined scales and merge.
*   **Math (for each observation `obs` in `DiffuseTable_combined` belonging to batch `j`):**
    *   `a_obs, b_obs, c_obs, d_obs` from `ScalingModel_refined_list(j)`.
    *   `Scale_mult_obs = a_obs * b_obs * d_obs`
    *   `Offset_add_obs = c_obs / b_obs`
    *   `I_scaled_obs = I_obs(obs) / Scale_mult_obs - Offset_add_obs`
    *   `Sigma_scaled_obs = Sigma_obs(obs) / Scale_mult_obs`
    *   **Merge (for each unique `HKL_asu`):**
        *   `w_obs = 1 / Sigma_scaled_obs^2`
        *   `I_merged(HKL_asu) = sum(I_scaled_obs * w_obs) / sum(w_obs)`
        *   `Sigma_merged(HKL_asu) = sqrt(1 / sum(w_obs))`
    *   (Outlier rejection can be done by re-weighting based on residuals against `I_merged` and re-merging).
    *   **Output:** `hklMerge_relative` table.

---

**Phase 4: Absolute Scaling**

**Step 7: Absolute Scaling (`+proc/@Batch/rescale.m`)**

*   **7a. Theoretical Scattering Calculation:**
    *   **Action:** Calculate theoretical coherent and incoherent scattering from `unitCellInventory`.
    *   **Math (for given `s`):**
        *   `f_0_atom(s, Z) = model.atom.ScatteringFactor(atom_symbol).f_coh(s)`
        *   `I_incoh_atomic(s, Z) = model.atom.ScatteringFactor(atom_symbol).I_incoh(s)`
        *   `I_coh_UC(s) = sum_atoms_in_UC (occupancy_atom * f_0_atom(s, Z_atom)^2)`
        *   `I_incoh_UC(s) = sum_atoms_in_UC (occupancy_atom * I_incoh_atomic(s, Z_atom))`
        *   `(Optionally) Ibond_UC(s) = ...` (from bonding interference terms)
        *   `N_elec_UC = sum_atoms_in_UC (occupancy_atom * Z_atom)`
*   **7b. Radial Averaging of Observed and Theoretical Data:**
    *   **Action:** Use `proc.script.StatisticsVsRadius`.
    *   **Math:**
        *   `I_obs_total_avg(s_shell) = RadiallyAvg(I_merged_diffuse(HKL_asu) + I_merged_Bragg(HKL_asu))` (from `hklMerge_relative` and `hklMergeBragg_relative`).
        *   `I_theo_total_avg(s_shell) = RadiallyAvg(I_coh_UC(s) [+ Ibond_UC(s)] + I_incoh_UC(s))`
*   **7c. Determine Absolute Scale Factor:**
    *   **Action:** Compare cumulative sums.
    *   **Math:**
        *   `V_cell = Crystal.UnitCell.vCell`
        *   `Cumul_I_obs(s_max_cutoff) = V_cell * sum_{s_shell <= s_max_cutoff} ( I_obs_total_avg(s_shell) * Volume_shell(s_shell) )`
        *   `Cumul_I_theo(s_max_cutoff) = -N_elec_UC^2 + V_cell * sum_{s_shell <= s_max_cutoff} ( I_theo_total_avg(s_shell) * Volume_shell(s_shell) )` (Forward scattering theorem term `-N_elec_UC^2`)
        *   `Scale_Absolute = Cumul_I_theo(scutoff) / Cumul_I_obs(scutoff)`
*   **7d. Apply Absolute Scale and Subtract Incoherent:**
    *   **Action:** Create final diffuse map.
    *   **Math (for each `HKL_asu` in `hklMerge_relative`):**
        *   `I_abs_diffuse(HKL_asu) = I_merged_diffuse(HKL_asu) * Scale_Absolute - I_incoh_UC(s_at_HKL_asu)`
        *   `Sigma_abs_diffuse(HKL_asu) = Sigma_merged_diffuse(HKL_asu) * Scale_Absolute`
    *   **Output:** `hklMerge_absolute` table (the final diffuse map data).
*   **7e. Update ScalingModel (for consistency if used later):**
    *   `ScalingModel_final(j).b = ScalingModel_refined_list(j).b / Scale_Absolute`
    *   `ScalingModel_final(j).c = ScalingModel_refined_list(j).c + I_incoh_UC_on_c_grid * (ScalingModel_refined_list(j).b / Scale_Absolute)`

This detailed trace shows the transformation of raw pixel intensities through various corrections, averaging, scaling (relative and absolute), and finally incoherent subtraction to yield the diffuse scattering map on an absolute scale. Each step involves specific mathematical operations and relies on the geometric and physical models defined within the package.
</file>

<file path="plan2.md">
**`plan2.md`**

**Supporting Components and Advanced Utilities Plan**

**Nomenclature:**
*This document uses nomenclature consistent with `plan.md`. Specific new terms related to Phase 3 outputs will be defined within their respective module descriptions.*

*   `VoxelData_relative`: The primary output of Phase 3, a data structure (e.g., NumPy structured array or `flex.reflection_table`) where each row represents a voxel. Contains `(voxel_idx, H_center, K_center, L_center, q_center_x, q_center_y, q_center_z, |q|_center, I_merged_relative, Sigma_merged_relative, num_observations_in_voxel)`.
*   `GlobalVoxelGrid_obj`: The Python object instance defining the 3D reciprocal space grid.
*   `ScalingModel_refined_params`: A data structure (e.g., dictionary or list of Pydantic models) holding the refined parameters of the `DiffuseScalingModel` (e.g., per-still `b_i` scales, resolution smoother `a(|q|)` parameters).

---

**0. Introduction and Purpose**

This document, `plan2.md`, outlines the design and implementation requirements for supporting components and advanced utilities that complement the core processing pipeline defined in `plan.md`.

**Precedence:** `plan2.md` has **lower precedence** than `plan.md`. In case of conflicts regarding core pipeline logic, `plan.md` is the authoritative source. `plan2.md` focuses on auxiliary systems like advanced diagnostics, specialized data analysis tools, or alternative workflow orchestrators that build upon the core pipeline outputs.

This initial version of `plan2.md` will focus on creating a **Phase 3 Visual Check System**, analogous to the visual diagnostics already implemented for earlier phases.

---

**Section P3.VC: Phase 3 Visual Check System**

**Overview:**
The Phase 3 Visual Check System aims to provide developers and users with tools to visually inspect and validate the outputs of the voxelization, relative scaling, and merging steps (Modules 3.S.1 - 3.S.4). It will consist of an orchestration script to run the full pipeline up to Phase 3 and generate inputs for a dedicated diagnostic script, which will then produce plots and summary reports.

This system will be modeled on the existing `run_phase2_e2e_visual_check.py` and `scripts/visual_diagnostics/check_diffuse_extraction.py` framework.

---

**Module P3.VC.O: Orchestration Script for Phase 3 Visual Checks**

*   **Script Name:** `scripts/dev_workflows/run_phase3_e2e_visual_check.py`
*   **Action:** Orchestrate the complete DiffusePipe pipeline from raw CBF image(s) through all of Phase 1, Phase 2, and Phase 3. It will then invoke the Phase 3 diagnostic script (`check_phase3_outputs.py`) with the generated outputs.
*   **Input (Command Line Arguments):**
    *   `--cbf-image-paths`: List of paths to input CBF image files (allowing multiple stills for realistic Phase 3 testing).
    *   `--output-base-dir`: Base output directory for all intermediate files and final diagnostics.
    *   `--pdb-path` (optional): Path to external PDB file for Phase 1 validation.
    *   `--dials-phil-path` (optional): Custom DIALS PHIL for Phase 1 processing.
    *   `--static-mask-config` (optional): JSON string/file for static mask parameters (Phase 1).
    *   `--bragg-mask-config` (optional): JSON string/file for Bragg mask parameters (Phase 1).
    *   `--use-bragg-mask-option-b` (optional): Flag to use shoebox-based Bragg masking.
    *   `--extraction-config-json` (optional): JSON string/file for `ExtractionConfig` overrides (Phase 2).
    *   `--relative-scaling-config-json` (optional): JSON string/file for `RelativeScalingConfig` overrides (Phase 3).
    *   `--grid-config-json` (optional): JSON string/file for `GlobalVoxelGrid` parameters (e.g., `d_min_target`, `ndiv_hkl`) (Phase 3).
    *   `--save-intermediate-phase-outputs`: Flag to explicitly save key outputs from Phase 1 and 2 (e.g., `Experiment_dials_i` list, `CorrectedDiffusePixelData_i` list as NPZ files) that might be needed by `check_phase3_outputs.py`.
    *   `--verbose`: Enable verbose logging.
*   **Process:**
    1.  **Setup:** Create a unique output subdirectory within `--output-base-dir` (e.g., based on the first CBF filename or a timestamp). Setup logging.
    2.  **Run Phase 1 (Modules 1.S.0 - 1.S.3):** For each CBF file:
        *   Perform DIALS processing (still/sequence as detected).
        *   Perform geometric validation.
        *   Generate pixel masks and Bragg masks.
        *   Generate total diffuse mask.
        *   Store/collect `Experiment_dials_i` objects and paths to `Mask_total_2D_i` (or the mask objects themselves).
    3.  **Run Phase 2 (Modules 2.S.1 - 2.S.2):** For each successfully processed still from Phase 1:
        *   Instantiate `DataExtractor`.
        *   Extract diffuse data (`q_vector, I_corrected, Sigma_corrected`, etc.).
        *   Save the output per-still NPZ file (`CorrectedDiffusePixelData_i.npz`) containing these arrays into the working directory. Collect paths to these files.
    4.  **Run Phase 3 (Modules 3.S.1 - 3.S.4):**
        *   **Module 3.S.1 (Grid Definition):**
            *   Collect all `Experiment_dials_i` objects.
            *   Load all `CorrectedDiffusePixelData_i.npz` files to get q-vector ranges.
            *   Instantiate and build `GlobalVoxelGrid_obj` using configurations.
            *   (Optional) Serialize `GlobalVoxelGrid_obj` definition (e.g., to JSON/pickle) for the diagnostic script. Path: `global_voxel_grid_definition.json`.
        *   **Module 3.S.2 (Binning):**
            *   Instantiate `VoxelAccumulator` (with HDF5 backend in the working directory).
            *   Iterate through `CorrectedDiffusePixelData_i.npz` files, binning observations into `VoxelAccumulator`.
            *   Generate `ScalingModel_initial_list`.
        *   **Module 3.S.3 (Relative Scaling):**
            *   Instantiate `DiffuseScalingModel`.
            *   Perform relative scaling using data from `VoxelAccumulator` and `Reflections_dials_i` (for Bragg ref).
            *   Store `ScalingModel_refined_params`. Serialize parameters (e.g., to JSON). Path: `refined_scaling_model_params.json`.
        *   **Module 3.S.4 (Merging):**
            *   Apply refined scales and merge data.
            *   Generate and save `VoxelData_relative` (e.g., as `voxel_data_relative.npz` or `voxel_data_relative.hdf5`).
    5.  **Invoke Phase 3 Diagnostic Script:**
        *   Construct command for `scripts/visual_diagnostics/check_phase3_outputs.py`.
        *   Pass paths to:
            *   `global_voxel_grid_definition.json` (or equivalent)
            *   `refined_scaling_model_params.json`
            *   `voxel_data_relative.npz` (or `.hdf5`)
            *   Optionally, paths to the collection of `Experiment_dials_i` (if needed for context) and `CorrectedDiffusePixelData_i.npz` files.
            *   Output directory for diagnostic plots (e.g., `phase3_diagnostics/` within the main output dir).
        *   Execute the script.
*   **Output:**
    *   All intermediate files from Phases 1, 2, and 3 saved in the unique output subdirectory.
    *   Log file for the entire orchestration.
    *   A dedicated subdirectory (e.g., `phase3_diagnostics/`) containing plots and reports from `check_phase3_outputs.py`.
*   **Relevant `libdocs/dials/` API:** This script will use high-level project components which internally use DIALS APIs. No direct DIALS API calls from this script.

---

**Module P3.VC.D: Phase 3 Diagnostic Script**

*   **Script Name:** `scripts/visual_diagnostics/check_phase3_outputs.py`
*   **Action:** Load outputs from Phase 3 (grid definition, scaling model, merged voxel data) and generate visualizations and summary statistics to verify correctness.
*   **Input (Command Line Arguments):**
    *   `--grid-definition-file`: Path to serialized `GlobalVoxelGrid_obj` definition.
    *   `--scaling-model-params-file`: Path to serialized `ScalingModel_refined_params`.
    *   `--voxel-data-file`: Path to the `VoxelData_relative` file (NPZ or HDF5).
    *   `--output-dir`: Directory to save diagnostic plots and reports.
    *   `--experiments-list-file` (optional): Path to a file listing paths to individual `Experiment_dials_i.expt` files (if needed for still-specific context).
    *   `--corrected-pixel-data-dir` (optional): Path to directory containing per-still `CorrectedDiffusePixelData_i.npz` files (if comparison to pre-binned data is desired).
    *   `--max-plot-points` (optional): Max points for scatter plots.
    *   `--verbose`: Enable verbose logging.
*   **Process & Generated Diagnostics (Plots & Text Summaries):**
    1.  **Load Inputs:** Deserialize/load all required input files.
    2.  **Global Voxel Grid Summary (`grid_summary.txt`, `grid_visualization_conceptual.png`):**
        *   Text: Report `Crystal_avg_ref` parameters (unit cell, space group), calculated HKL bounds of the grid, voxel dimensions (`1/ndiv_h`, etc.), total number of voxels.
        *   Plot (Conceptual): A 3D scatter plot showing a subset of `q_vector` points (from optional `--corrected-pixel-data-dir`) overlaid with the bounding box of the `GlobalVoxelGrid` in q-space to visualize coverage.
    3.  **Voxel Occupancy/Redundancy Plots (`voxel_occupancy_slice_L0.png`, `voxel_occupancy_histogram.png`):**
        *   Requires `num_observations_in_voxel` from `VoxelData_relative`.
        *   Plot: 2D heatmap slices (e.g., H-K plane at L=0, H-L at K=0, K-L at H=0) of `num_observations_in_voxel`. Colormap indicating redundancy.
        *   Plot: Histogram of `num_observations_in_voxel` values.
        *   Text: Min, max, mean, median redundancy. Percentage of voxels with redundancy < N.
    4.  **Relative Scaling Model Parameter Plots (`scaling_model_params.png`, `scaling_residuals.png`):**
        *   Requires `ScalingModel_refined_params`.
        *   Plot (if applicable): `b_i` (per-still scales) vs. still index or `p_order(i)`.
        *   Plot (if applicable): `a(|q|)` (resolution smoother parameters/curve).
        *   Text: Summary of refined parameters.
        *   (Advanced, if residuals saved by main pipeline): Plot histogram of residuals from scaling refinement.
    5.  **Merged Voxel Data Visualization (`merged_intensity_slice_L0.png`, `merged_isigi_slice_L0.png`, `merged_radial_avg.png`, `merged_intensity_histogram.png`):**
        *   Requires `VoxelData_relative`.
        *   **Reciprocal Space Slices:**
            *   Plot: 2D heatmap slices (e.g., H-K at L=0) of `I_merged_relative` (log scale).
            *   Plot: 2D heatmap slices of `Sigma_merged_relative`.
            *   Plot: 2D heatmap slices of `I_merged_relative / Sigma_merged_relative`.
        *   **Radial Average Plot:**
            *   Calculate `|q|_center` for each voxel.
            *   Plot `I_merged_relative` vs. `|q|_center` (scatter or binned average).
        *   **Intensity Histogram:**
            *   Plot histogram of `I_merged_relative` values (linear and log y-scale).
    6.  **Summary Report (`phase3_diagnostics_summary.txt`):**
        *   Text file summarizing key statistics from all plots, input file names, configurations used (if available).
*   **Output Files:** PNG plot files and TXT summary files in the specified `--output-dir`.
*   **Relevant `libdocs/dials/` API:**
    *   For loading `Experiment_dials_i` (if passed): `dials_file_io.md` (A.1).
    *   For potentially re-calculating q-vectors from HKL centers: `dxtbx_models.md` (B.3 for `crystal.get_A()`) and `crystallographic_calculations.md` (C.2 implied `q = A * hkl`).
    *   For array manipulations (if loading NPZ and working with NumPy): `flex_arrays.md` concepts might be mapped to NumPy equivalents.
    *   Primarily uses project-specific data structures and plotting libraries (matplotlib).

---

**Module P3.VC.U: Plotting Utilities (Enhancements to `scripts/visual_diagnostics/plot_utils.py`)**

*   **Action:** Extend `plot_utils.py` with functions needed for Phase 3 diagnostics.
*   **New/Enhanced Functions:**
    1.  `plot_3d_grid_slice(grid_data_3d, slice_dim, slice_idx, title, output_path, cmap, norm, xlabel, ylabel)`:
        *   Input: 3D NumPy array (`grid_data_3d`), dimension to slice along (`'H'`, `'K'`, or `'L'`), index for the slice.
        *   Behavior: Extracts a 2D slice and plots it as a heatmap using `imshow`. Handles axis labeling based on slice.
    2.  `plot_radial_average(q_magnitudes, intensities, num_bins, title, output_path)`:
        *   Input: Arrays of q-magnitudes and corresponding intensities.
        *   Behavior: Bins intensities by q-magnitude, calculates mean intensity per bin, plots mean intensity vs. q-bin center. Includes error bars if sigmas are provided.
    3.  `plot_parameter_vs_index(param_values, index_values, title, param_label, index_label, output_path)`:
        *   For plotting scaling parameters like `b_i` vs. still index.
    4.  `plot_smoother_curve(smoother_params_or_evaluator, q_range, title, output_path)`:
        *   For plotting the resolution smoother `a(|q|)`.
*   **Relevant `libdocs/dials/` API:** Not directly calling DIALS APIs, but uses matplotlib for plotting data derived from DIALS/project outputs.

---

**Module P3.VC.T: Testing the Phase 3 Visual Check System**

*   **Action:** Create tests for `run_phase3_e2e_visual_check.py` and `check_phase3_outputs.py`.
*   **Process:**
    1.  **Test `run_phase3_e2e_visual_check.py`:**
        *   Requires mock components for Phase 1, 2, and 3 that produce dummy output files in the expected formats.
        *   Verify that the orchestrator calls each phase correctly.
        *   Verify that it correctly invokes `check_phase3_outputs.py` with the paths to generated files.
    2.  **Test `check_phase3_outputs.py`:**
        *   Create synthetic input files (`global_voxel_grid_definition.json`, `refined_scaling_model_params.json`, `voxel_data_relative.npz`) with known characteristics.
        *   Run the script with these inputs.
        *   Assert that all expected plot files and summary files are generated.
        *   (Optional, advanced) For simple synthetic data, potentially load generated plots and check for key features or summary statistics.
*   **Relevant `libdocs/dials/` API:** None directly, focuses on testing script logic and file I/O.

---
</file>

<file path=".claude/commands/dpayload.md">
prepare a self contained 'debug payload' file containing:

- description of the issue
- all context information / files needed to understand the issue and relevant parts of the codebase
- all files that might be the source of the bug and / or relevant to debugging 
- if appropriate, a description of which debugging approaches we already tried, and why it didn't work

Instead of including the literal contents of each file that you decided to include, specify them with 
jinja style template syntax:

{rel/path/to/file.py}
{another/file.md}

Don't include data files with binary sections (such as cbf).


Remember to include api documentation context (lives under libdocs/dials/), but ONLY in the format of excerpted relevant sections, not jinja entries (this is to reduce size, since some documentation files are large). Use subagents to parse such big files.

Include these files:
./phase1_demo_output/validation_failure_report.txt
./phase1_demo_output/validation_report.txt
</file>

<file path="dev_scripts/debug_q_vector_suite/debug_q_fix.py">
#!/usr/bin/env python3

"""
Debug script to test Q-vector coordinate frame transformation
and compare with the consistency_checker.py approach.
"""

import sys

sys.path.append("src")

from dxtbx.model.experiment_list import ExperimentListFactory
from dials.array_family import flex
import numpy as np
from scitbx import matrix


def test_q_transformation(expt_file, refl_file):
    """Test Q-vector transformation using both methods."""
    print(f"Testing Q-vector transformation with {expt_file}")

    # Load data
    experiments = ExperimentListFactory.from_json_file(expt_file)
    reflections = flex.reflection_table.from_file(refl_file)
    experiment = experiments[0]

    print(f"Loaded {len(reflections)} reflections")

    # Test first few reflections
    n_test = min(5, len(reflections))
    print(f"Testing first {n_test} reflections:")

    for i in range(n_test):
        hkl = reflections["miller_index"][i]
        print(f"\nReflection {i}: HKL = {hkl}")

        # Method 1: Direct crystal method (problematic)
        try:
            q_crystal_method = experiment.crystal.hkl_to_reciprocal_space_vec(hkl)
            q_crystal = np.array(q_crystal_method.elems)
            print(
                f"  Crystal method: {q_crystal} (mag: {np.linalg.norm(q_crystal):.4f})"
            )
        except AttributeError:
            q_crystal = None
            print("  Crystal method: Not available")

        # Method 2: Lab frame transformation (fixed)
        hkl_vec = matrix.col(hkl)
        A = matrix.sqr(experiment.crystal.get_A())
        S = matrix.sqr(experiment.goniometer.get_setting_rotation())
        F = matrix.sqr(experiment.goniometer.get_fixed_rotation())
        C = matrix.sqr((1, 0, 0, 0, 0, -1, 0, 1, 0))
        R_lab = C * S * F
        q_lab_method = R_lab * A * hkl_vec
        q_lab = np.array(q_lab_method.elems)
        print(f"  Lab method:     {q_lab} (mag: {np.linalg.norm(q_lab):.4f})")

        # Method 3: Just A*hkl (no rotation)
        A_array = np.array(A.elems).reshape(3, 3)
        hkl_array = np.array(hkl)
        q_A_only = A_array @ hkl_array
        print(f"  A*hkl only:     {q_A_only} (mag: {np.linalg.norm(q_A_only):.4f})")

        # Compare methods
        if q_crystal is not None:
            diff_crystal_lab = np.linalg.norm(q_crystal - q_lab)
            diff_crystal_A = np.linalg.norm(q_crystal - q_A_only)
            print(f"  |crystal - lab|: {diff_crystal_lab:.4f}")
            print(f"  |crystal - A*hkl|: {diff_crystal_A:.4f}")

        diff_lab_A = np.linalg.norm(q_lab - q_A_only)
        print(f"  |lab - A*hkl|:   {diff_lab_A:.4f}")

        # Calculate q_pixel for comparison
        if "xyzcal.mm" in reflections:
            try:
                panel_id = int(reflections["panel"][i])
                mm_pos = reflections["xyzcal.mm"][i]
                x_mm, y_mm = mm_pos[0], mm_pos[1]

                panel = experiment.detector[panel_id]
                px_coords = panel.millimeter_to_pixel((x_mm, y_mm))

                # Calculate q_pixel using the same method as in consistency checker
                s0 = np.array(experiment.beam.get_s0())
                k_magnitude = np.linalg.norm(s0)
                P_lab = np.array(panel.get_pixel_lab_coord(px_coords))
                D_scattered = P_lab
                D_scattered_norm = np.linalg.norm(D_scattered)
                s1_lab = D_scattered / D_scattered_norm
                k_out = s1_lab * k_magnitude
                q_pixel = k_out - s0

                print(
                    f"  q_pixel:        {q_pixel} (mag: {np.linalg.norm(q_pixel):.4f})"
                )
                print(f"  |lab - pixel|:   {np.linalg.norm(q_lab - q_pixel):.4f}")
                if q_crystal is not None:
                    print(
                        f"  |crystal - pixel|: {np.linalg.norm(q_crystal - q_pixel):.4f}"
                    )

            except Exception as e:
                print(f"  q_pixel calculation failed: {e}")


if __name__ == "__main__":
    print("=" * 60)
    print("DEBUGGING Q-VECTOR COORDINATE TRANSFORMATION")
    print("=" * 60)

    # Test both datasets
    print("\n1. Testing WORKING dataset (lys_nitr_8_2_0110):")
    test_q_transformation(
        "lys_nitr_8_2_0110_dials_processing/indexed_refined_detector.expt",
        "lys_nitr_8_2_0110_dials_processing/indexed_refined_detector.refl",
    )

    print("\n" + "=" * 60)
    print("\n2. Testing PROBLEMATIC dataset (lys_nitr_10_6_0491):")
    test_q_transformation(
        "lys_nitr_10_6_0491_dials_processing/indexed_refined_detector.expt",
        "lys_nitr_10_6_0491_dials_processing/indexed_refined_detector.refl",
    )
</file>

<file path="dev_scripts/debug_q_vector_suite/debug_q_validation.py">
#!/usr/bin/env python3
"""
Debug script to directly test Q-vector validation on processed DIALS data.
"""

import sys
import logging
from pathlib import Path

# Add project src to path
sys.path.insert(0, str(Path(__file__).parent / "src"))

from diffusepipe.crystallography.still_processing_and_validation import (
    StillProcessorAndValidatorComponent,
    create_default_config,
    create_default_extraction_config,
)

# Set up debug logging
logging.basicConfig(
    level=logging.DEBUG, format="%(asctime)s - %(name)s - %(levelname)s - %(message)s"
)
logger = logging.getLogger(__name__)


def debug_q_validation():
    """Debug the Q-vector validation on a real processed image."""
    print("=" * 60)
    print("DEBUG Q-VECTOR VALIDATION")
    print("=" * 60)

    image_path = "747/lys_nitr_10_6_0491.cbf"

    # Create configurations
    dials_config = create_default_config(
        enable_partiality=True,
        enable_shoeboxes=True,
        known_unit_cell="27.424,32.134,34.513,88.66,108.46,111.88",
        known_space_group="P 1",
    )
    extraction_config = create_default_extraction_config()

    # Initialize processor
    processor = StillProcessorAndValidatorComponent()

    print(f"\n1. Processing image: {image_path}")
    outcome = processor.process_and_validate_still(
        image_path=image_path,
        config=dials_config,
        extraction_config=extraction_config,
        external_pdb_path="6o2h.pdb",
    )

    print(f"\n2. Processing result: {outcome.status}")

    if outcome.output_artifacts:
        experiment = outcome.output_artifacts.get("experiment")
        reflections = outcome.output_artifacts.get("reflections")

        if experiment and reflections:
            print(f"\n3. Direct Q-vector validation test:")

            # Create a validator and test directly
            validator = processor.validator

            # Test with debug logging
            tolerance = extraction_config.q_consistency_tolerance_angstrom_inv
            print(f"   Using tolerance: {tolerance} Å⁻¹")

            passed, stats = validator._check_q_consistency(
                experiment=experiment, reflections=reflections, tolerance=tolerance
            )

            print(f"\n4. Validation Results:")
            print(f"   Passed: {passed}")
            print(f"   Stats: {stats}")

            # Also inspect the reflection table directly
            print(f"\n5. Reflection Table Inspection:")
            try:
                print(f"   Type: {type(reflections)}")
                print(f"   Length: {len(reflections)}")
                if hasattr(reflections, "keys"):
                    keys = list(reflections.keys())
                    print(f"   Available columns: {keys}")

                    # Check for required columns
                    required = ["miller_index", "panel"]
                    for col in required:
                        if col in reflections:
                            sample = (
                                reflections[col][:3] if len(reflections) > 0 else []
                            )
                            print(f"   {col} (sample): {sample}")
                        else:
                            print(f"   {col}: MISSING")

                    # Check position columns
                    pos_cols = [
                        "xyzcal.mm",
                        "xyzobs.mm.value",
                        "xyzcal.px",
                        "xyzobs.px.value",
                    ]
                    for col in pos_cols:
                        if col in reflections:
                            sample = (
                                reflections[col][:3] if len(reflections) > 0 else []
                            )
                            print(f"   {col} (sample): {sample}")

            except Exception as e:
                print(f"   Error inspecting reflections: {e}")
        else:
            print("\n3. No experiment/reflections available for direct testing")
    else:
        print("\n3. No output artifacts available")


if __name__ == "__main__":
    debug_q_validation()
</file>

<file path="dev_scripts/debug_q_vector_suite/debug_simple_q_check.py">
#!/usr/bin/env python3
"""
Simple debug script to isolate the Q-vector validation issue.
"""

import sys
import logging
from pathlib import Path
import numpy as np

# Add project src to path
sys.path.insert(0, str(Path(__file__).parent / "src"))

# Set up debug logging
logging.basicConfig(level=logging.DEBUG, format="%(levelname)s - %(message)s")


def debug_q_consistency_simple():
    """Debug the Q-vector consistency check step by step."""
    print("=" * 60)
    print("SIMPLE Q-VECTOR DEBUG")
    print("=" * 60)

    from diffusepipe.crystallography.still_processing_and_validation import (
        StillProcessorAndValidatorComponent,
        create_default_config,
        create_default_extraction_config,
    )

    # Process the image
    processor = StillProcessorAndValidatorComponent()
    config = create_default_config(
        enable_partiality=True,
        known_unit_cell="27.424,32.134,34.513,88.66,108.46,111.88",
        known_space_group="P 1",
    )
    extraction_config = create_default_extraction_config()

    outcome = processor.process_and_validate_still(
        image_path="747/lys_nitr_10_6_0491.cbf",
        config=config,
        extraction_config=extraction_config,
        external_pdb_path="6o2h.pdb",
    )

    experiment = outcome.output_artifacts.get("experiment")
    reflections = outcome.output_artifacts.get("reflections")

    print(f"\n1. Reflection table inspection:")
    print(f"   Type: {type(reflections)}")
    print(f"   Length: {len(reflections)}")

    # Check columns
    required_cols = ["miller_index", "panel"]
    for col in required_cols:
        present = col in reflections
        print(f"   {col}: {'✓ present' if present else '❌ missing'}")

    pos_cols = ["xyzcal.mm", "xyzobs.mm.value", "xyzcal.px", "xyzobs.px.value"]
    print(f"\n2. Position columns:")
    for col in pos_cols:
        present = col in reflections
        print(f"   {col}: {'✓ present' if present else '❌ missing'}")

    # Try to access the data
    print(f"\n3. Trying to access data:")
    try:
        n_total = len(reflections)
        print(f"   Total reflections: {n_total}")

        # Get first reflection data
        if n_total > 0:
            hkl = reflections["miller_index"][0]
            panel_id = reflections["panel"][0]

            print(f"   First reflection:")
            print(f"     Miller index: {hkl}")
            print(f"     Panel: {panel_id}")

            # Try position data
            if "xyzcal.mm" in reflections:
                pos_mm = reflections["xyzcal.mm"][0]
                print(f"     xyzcal.mm: {pos_mm}")

            # Test q_bragg calculation
            try:
                q_bragg_scitbx = experiment.crystal.hkl_to_reciprocal_space_vec(hkl)
                q_bragg = np.array(q_bragg_scitbx.elems)
                print(f"     q_bragg: {q_bragg} (mag: {np.linalg.norm(q_bragg):.4f})")
            except Exception as e:
                print(f"     q_bragg calculation failed: {e}")

            # Test pixel coordinate conversion
            try:
                if "xyzcal.mm" in reflections:
                    mm_pos = reflections["xyzcal.mm"][0]
                    x_mm, y_mm = mm_pos[0], mm_pos[1]

                    panel = experiment.detector[int(round(panel_id))]
                    pixel_coords = panel.millimeter_to_pixel((x_mm, y_mm))
                    print(f"     mm to pixel: ({x_mm}, {y_mm}) -> {pixel_coords}")

                    # Test lab coordinates
                    lab_coord_scitbx = panel.get_pixel_lab_coord(pixel_coords)
                    lab_coord = np.array(lab_coord_scitbx.elems)
                    print(f"     lab coordinates: {lab_coord}")

                    # Test q_pixel calculation
                    s0 = np.array(experiment.beam.get_s0())
                    k_magnitude = np.linalg.norm(s0)
                    scattered_direction_norm = np.linalg.norm(lab_coord)
                    s1_unit = lab_coord / scattered_direction_norm
                    s1 = s1_unit * k_magnitude
                    q_pixel = s1 - s0

                    print(f"     s0: {s0}")
                    print(f"     s1: {s1}")
                    print(
                        f"     q_pixel: {q_pixel} (mag: {np.linalg.norm(q_pixel):.4f})"
                    )

                    # Calculate difference
                    delta_q = q_bragg - q_pixel
                    delta_q_mag = np.linalg.norm(delta_q)
                    print(f"     |Δq|: {delta_q_mag:.6f} Å⁻¹")

            except Exception as e:
                print(f"     Coordinate calculation failed: {e}")
                import traceback

                traceback.print_exc()

    except Exception as e:
        print(f"   Data access failed: {e}")
        import traceback

        traceback.print_exc()


if __name__ == "__main__":
    debug_q_consistency_simple()
</file>

<file path="dev_scripts/debug_q_vector_suite/simple_debug_test.py">
#!/usr/bin/env python3

import sys

sys.path.append("src")

import logging

logging.basicConfig(
    level=logging.DEBUG, format="%(name)s - %(levelname)s - %(message)s"
)

# Enable debug logging for validation
logger = logging.getLogger(
    "diffusepipe.crystallography.still_processing_and_validation"
)
logger.setLevel(logging.DEBUG)

print("Testing if we can import and run the validation...")

try:
    from diffusepipe.crystallography.still_processing_and_validation import (
        ModelValidator,
    )
    from diffusepipe.types.types_IDL import ExtractionConfig

    print("✓ Imports successful")

    # Try to create validator
    validator = ModelValidator()
    print("✓ Validator created")

    # Test if we can access the processed data directly
    from dxtbx.model.experiment_list import ExperimentListFactory
    from dials.array_family import flex

    print("Loading experiment from processed data...")
    experiment = ExperimentListFactory.from_json_file(
        "lys_nitr_10_6_0491_dials_processing/indexed_refined_detector.expt"
    )[0]
    reflections = flex.reflection_table.from_file(
        "lys_nitr_10_6_0491_dials_processing/indexed_refined_detector.refl"
    )

    print(f"Loaded experiment with {len(reflections)} reflections")
    print("Testing the patched _check_q_consistency method directly...")

    # This should trigger our debug logging
    ok, stats = validator._check_q_consistency(experiment, reflections, tolerance=0.05)

    print(f"Direct validation result: {ok}")
    print(f"Stats: {stats}")

except Exception as e:
    print(f"Error: {e}")
    import traceback

    traceback.print_exc()
</file>

<file path="dev_scripts/debug_q_vector_suite/test_coordinate_matrix.py">
#!/usr/bin/env python3

import sys

sys.path.append("src")

import numpy as np
from scitbx import matrix

print("TESTING DIFFERENT COORDINATE TRANSFORMATION APPROACHES")
print("=" * 60)

# Load one dataset to test with
from dxtbx.model.experiment_list import ExperimentListFactory
from dials.array_family import flex

experiment = ExperimentListFactory.from_json_file(
    "lys_nitr_10_6_0491_dials_processing/indexed_refined_detector.expt"
)[0]
reflections = flex.reflection_table.from_file(
    "lys_nitr_10_6_0491_dials_processing/indexed_refined_detector.refl"
)

# Get first reflection for testing
hkl = reflections["miller_index"][0]
print(f"Testing with first reflection: HKL = {hkl}")

# Get matrices
A = matrix.sqr(experiment.crystal.get_A())
S = matrix.sqr(experiment.goniometer.get_setting_rotation())
F = matrix.sqr(experiment.goniometer.get_fixed_rotation())

print(f"A matrix: {A.elems}")
print(f"S matrix: {S.elems}")
print(f"F matrix: {F.elems}")

# Test different C matrices and transformations
test_cases = [
    ("Our current C", matrix.sqr((1, 0, 0, 0, 0, -1, 0, 1, 0))),
    ("Identity (no C)", matrix.sqr((1, 0, 0, 0, 1, 0, 0, 0, 1))),
    ("Y/Z flip only", matrix.sqr((1, 0, 0, 0, 0, 1, 0, 1, 0))),
    ("Different C", matrix.sqr((1, 0, 0, 0, -1, 0, 0, 0, -1))),
]

hkl_vec = matrix.col(hkl)

for name, C in test_cases:
    print(f"\n{name}:")
    print(f"  C matrix: {C.elems}")

    # Test different transformation orders
    transformations = [
        ("C*S*F*A", C * S * F * A),
        ("S*F*C*A", S * F * C * A),
        ("A only", A),
        ("C*A only", C * A),
    ]

    for trans_name, trans_matrix in transformations:
        q_result = trans_matrix * hkl_vec
        q_array = np.array(q_result.elems)
        print(f"    {trans_name}: {q_array} (mag: {np.linalg.norm(q_array):.4f})")

# Test what the built-in DIALS method gives us (if available)
print(f"\nBuilt-in DIALS method:")
try:
    q_dials = experiment.crystal.hkl_to_reciprocal_space_vec(hkl)
    q_dials_array = np.array(q_dials.elems)
    print(
        f"  hkl_to_reciprocal_space_vec: {q_dials_array} (mag: {np.linalg.norm(q_dials_array):.4f})"
    )
except AttributeError:
    print("  hkl_to_reciprocal_space_vec not available")

# Calculate q_pixel for comparison
panel_id = int(reflections["panel"][0])
mm_pos = reflections["xyzcal.mm"][0]
x_mm, y_mm = mm_pos[0], mm_pos[1]

panel = experiment.detector[panel_id]
px_coords = panel.millimeter_to_pixel((x_mm, y_mm))

s0 = np.array(experiment.beam.get_s0())
k_magnitude = np.linalg.norm(s0)
P_lab = np.array(panel.get_pixel_lab_coord(px_coords))
D_scattered = P_lab
D_scattered_norm = np.linalg.norm(D_scattered)
s1_lab = D_scattered / D_scattered_norm
k_out = s1_lab * k_magnitude
q_pixel = k_out - s0

print(f"\nq_pixel reference: {q_pixel} (mag: {np.linalg.norm(q_pixel):.4f})")
</file>

<file path="dev_scripts/debug_q_vector_suite/test_debug_logging.py">
#!/usr/bin/env python3

import sys

sys.path.append("src")

import logging

# Set debug logging for the validation module
logging.getLogger(
    "diffusepipe.crystallography.still_processing_and_validation"
).setLevel(logging.DEBUG)
logging.basicConfig(
    level=logging.DEBUG, format="%(name)s - %(levelname)s - %(message)s"
)

# Run one failing image with debug logging
from diffusepipe.crystallography.still_processing_and_validation import (
    StillProcessorAndValidatorComponent,
)
from diffusepipe.adapters.dials_sequence_process_adapter import (
    DIALSSequenceProcessAdapter,
)

print("=" * 60)
print("STEP 1: CONFIRMING PATCHED CODE PATH IS EXECUTED")
print("=" * 60)

processor = StillProcessorAndValidatorComponent()

print("\nRunning pipeline with debug logging...")
result = processor.process_and_validate_still(
    "747/lys_nitr_10_6_0491.cbf", {"pdb_file": "6o2h.pdb"}, extraction_config=None
)

print(f"\nResult status: {result.status}")
print(f"Q-consistency passed: {result.validation_metrics.get('q_consistency_passed')}")
print(f"Mean delta q: {result.validation_metrics.get('mean_delta_q_mag')}")
</file>

<file path="dev_scripts/debug_q_vector_suite/test_q_fix.py">
#!/usr/bin/env python3
"""
Quick test of the Q-vector calculation fixes.
"""

import sys
import logging
from pathlib import Path
import numpy as np

# Add project src to path
sys.path.insert(0, str(Path(__file__).parent / "src"))

# Set up debug logging
logging.basicConfig(level=logging.DEBUG, format="%(levelname)s - %(message)s")


def test_q_vector_fixes():
    """Test that our Q-vector API fixes work."""
    print("=" * 60)
    print("TESTING Q-VECTOR CALCULATION FIXES")
    print("=" * 60)

    from diffusepipe.crystallography.still_processing_and_validation import (
        StillProcessorAndValidatorComponent,
        create_default_config,
        create_default_extraction_config,
    )

    # Process the image
    processor = StillProcessorAndValidatorComponent()
    config = create_default_config(
        enable_partiality=True,
        known_unit_cell="27.424,32.134,34.513,88.66,108.46,111.88",
        known_space_group="P 1",
    )
    extraction_config = create_default_extraction_config()

    outcome = processor.process_and_validate_still(
        image_path="747/lys_nitr_10_6_0491.cbf",
        config=config,
        extraction_config=extraction_config,
        external_pdb_path="6o2h.pdb",
    )

    print(f"\nProcessing outcome: {outcome.status}")
    print(f"Message: {outcome.message}")

    if outcome.output_artifacts:
        validation_metrics = outcome.output_artifacts.get("validation_metrics", {})
        print(f"\nValidation Results:")
        print(
            f"  Q-consistency passed: {validation_metrics.get('q_consistency_passed')}"
        )
        print(
            f"  Reflections tested: {validation_metrics.get('num_reflections_tested')}"
        )
        print(f"  Mean |Δq|: {validation_metrics.get('mean_delta_q_mag')}")
        print(f"  Max |Δq|: {validation_metrics.get('max_delta_q_mag')}")
        print(f"  Median |Δq|: {validation_metrics.get('median_delta_q_mag')}")

    return outcome


if __name__ == "__main__":
    test_q_vector_fixes()
</file>

<file path="dev_scripts/check_reflection_columns.py">
#!/usr/bin/env python3

import sys

sys.path.append("src")

import numpy as np

print("CHECKING WHAT Q-VECTORS ARE ALREADY AVAILABLE IN REFLECTION TABLE")
print("=" * 70)

from dxtbx.model.experiment_list import ExperimentListFactory
from dials.array_family import flex

experiment = ExperimentListFactory.from_json_file(
    "lys_nitr_10_6_0491_dials_processing/indexed_refined_detector.expt"
)[0]
reflections = flex.reflection_table.from_file(
    "lys_nitr_10_6_0491_dials_processing/indexed_refined_detector.refl"
)

print(f"Available columns in reflection table:")
for col in reflections.keys():
    print(f"  {col}")

print(f"\nChecking if DIALS already calculated q-vectors...")

# Check if we have rlp (reciprocal lattice point) or other q-vector columns
if "rlp" in reflections:
    print(f"Found 'rlp' column (reciprocal lattice point)")
    rlp_0 = reflections["rlp"][0]
    print(f"  First rlp: {rlp_0}")

if "s1" in reflections:
    print(f"Found 's1' column (scattered beam vector)")
    s1_0 = np.array(reflections["s1"][0])
    s0_0 = np.array(experiment.beam.get_s0())
    q_from_s1 = s1_0 - s0_0
    print(f"  First s1: {s1_0}")
    print(f"  s0: {s0_0}")
    print(f"  q = s1 - s0: {q_from_s1} (mag: {np.linalg.norm(q_from_s1):.4f})")

# Calculate our reference q_pixel for comparison
hkl = reflections["miller_index"][0]
panel_id = int(reflections["panel"][0])
mm_pos = reflections["xyzcal.mm"][0]
x_mm, y_mm = mm_pos[0], mm_pos[1]

panel = experiment.detector[panel_id]
px_coords = panel.millimeter_to_pixel((x_mm, y_mm))

s0 = np.array(experiment.beam.get_s0())
k_magnitude = np.linalg.norm(s0)
P_lab = np.array(panel.get_pixel_lab_coord(px_coords))
D_scattered = P_lab
D_scattered_norm = np.linalg.norm(D_scattered)
s1_lab = D_scattered / D_scattered_norm
k_out = s1_lab * k_magnitude
q_pixel = k_out - s0

print(f"\nFor comparison:")
print(f"  Our calculated q_pixel: {q_pixel} (mag: {np.linalg.norm(q_pixel):.4f})")
print(f"  HKL: {hkl}")

# Check if s1 from table matches our calculation
if "s1" in reflections:
    s1_table = np.array(reflections["s1"][0])
    print(f"  s1 from table: {s1_table}")
    print(f"  s1 we calculated: {s1_lab * k_magnitude}")
    print(f"  s1 difference: {np.linalg.norm(s1_table - s1_lab * k_magnitude):.6f}")

    # Maybe DIALS s1 - s0 is already the right q-vector!
    q_from_dials = s1_table - s0
    print(
        f"  q from DIALS (s1-s0): {q_from_dials} (mag: {np.linalg.norm(q_from_dials):.4f})"
    )

    print(f"  |q_pixel - q_dials|: {np.linalg.norm(q_pixel - q_from_dials):.6f}")
</file>

<file path="dev_scripts/compare_coordinates.py">
#!/usr/bin/env python3

import sys

sys.path.append("src")

import logging

logging.basicConfig(level=logging.INFO)

print("COMPARING COORDINATE TRANSFORMATIONS BETWEEN DATASETS")
print("=" * 60)

from diffusepipe.crystallography.still_processing_and_validation import ModelValidator

# Test both datasets with the same validator
datasets = [
    ("WORKING", "lys_nitr_8_2_0110_dials_processing"),
    ("PROBLEMATIC", "lys_nitr_10_6_0491_dials_processing"),
]

for name, dataset_dir in datasets:
    print(f"\n{name} DATASET ({dataset_dir}):")
    print("-" * 50)

    try:
        from dxtbx.model.experiment_list import ExperimentListFactory
        from dials.array_family import flex

        experiment = ExperimentListFactory.from_json_file(
            f"{dataset_dir}/indexed_refined_detector.expt"
        )[0]
        reflections = flex.reflection_table.from_file(
            f"{dataset_dir}/indexed_refined_detector.refl"
        )

        validator = ModelValidator()

        # Test with higher tolerance to see actual errors
        ok, stats = validator._check_q_consistency(
            experiment, reflections, tolerance=0.5
        )

        print(f"Validation passed: {ok}")
        print(f"Mean |Δq|: {stats['mean']:.4f} Å⁻¹")
        print(f"Max |Δq|: {stats['max']:.4f} Å⁻¹")

        # Let's check the coordinate transformation matrices are identical
        from scitbx import matrix

        A = matrix.sqr(experiment.crystal.get_A())
        S = matrix.sqr(experiment.goniometer.get_setting_rotation())
        F = matrix.sqr(experiment.goniometer.get_fixed_rotation())
        C = matrix.sqr((1, 0, 0, 0, 0, -1, 0, 1, 0))
        R_lab = C * S * F

        print(f"R_lab transformation matrix: {R_lab.elems}")

    except Exception as e:
        print(f"Error: {e}")
</file>

<file path="dev_scripts/debug_manual_vs_stills.py">
#!/usr/bin/env python3
"""
Debug script to compare manual DIALS workflow vs stills_process
"""

import tempfile
import shutil
import subprocess
import sys
from pathlib import Path


def run_manual_workflow(cbf_path, output_dir):
    """Run the exact manual DIALS workflow that works"""
    print(f"=== MANUAL WORKFLOW in {output_dir} ===")

    # Change to output directory
    original_cwd = Path.cwd()
    output_path = Path(output_dir)
    output_path.mkdir(exist_ok=True)

    try:
        import os

        os.chdir(output_path)

        # Step 1: Import
        print("Step 1: dials.import")
        result = subprocess.run(
            ["dials.import", str(Path(original_cwd) / cbf_path)],
            capture_output=True,
            text=True,
        )
        print(f"Import stdout: {result.stdout}")
        if result.stderr:
            print(f"Import stderr: {result.stderr}")

        # Step 2: Find spots
        print("Step 2: dials.find_spots")
        result = subprocess.run(
            [
                "dials.find_spots",
                "imported.expt",
                "spotfinder.filter.min_spot_size=3",
                "spotfinder.threshold.algorithm=dispersion",
            ],
            capture_output=True,
            text=True,
        )
        print(f"Find_spots stdout: {result.stdout}")
        if result.stderr:
            print(f"Find_spots stderr: {result.stderr}")

        # Step 3: Index with known symmetry
        print("Step 3: dials.index")
        result = subprocess.run(
            [
                "dials.index",
                "imported.expt",
                "strong.refl",
                'indexing.known_symmetry.space_group="P 1"',
                "indexing.known_symmetry.unit_cell=27.424,32.134,34.513,88.66,108.46,111.88",
                'output.experiments="indexed_manual.expt"',
                'output.reflections="indexed_manual.refl"',
            ],
            capture_output=True,
            text=True,
        )
        print(f"Index stdout: {result.stdout}")
        if result.stderr:
            print(f"Index stderr: {result.stderr}")

        # Check results
        if Path("indexed_manual.expt").exists():
            print("✅ Manual workflow succeeded!")
            return True
        else:
            print("❌ Manual workflow failed!")
            return False

    finally:
        os.chdir(original_cwd)


def run_stills_process_workflow(cbf_path, output_dir):
    """Run stills_process with our current configuration"""
    print(f"=== STILLS_PROCESS WORKFLOW in {output_dir} ===")

    output_path = Path(output_dir)
    output_path.mkdir(exist_ok=True)

    # Add project src to path for testing
    sys.path.insert(0, str(Path(__file__).parent / "src"))

    try:
        from diffusepipe.crystallography.still_processing_and_validation import (
            StillProcessorAndValidatorComponent,
            create_default_config,
        )

        # Set up configuration
        dials_config = create_default_config(
            enable_partiality=True,
            enable_shoeboxes=True,
            known_unit_cell="27.424,32.134,34.513,88.66,108.46,111.88",
            known_space_group="P 1",
        )

        # Initialize processor
        processor = StillProcessorAndValidatorComponent()

        # Process the still using our adapter
        result = processor.adapter.process_still(
            image_path=cbf_path, config=dials_config
        )

        experiment, reflections, success, log_messages = result

        if success:
            print("✅ stills_process succeeded!")
            print(f"Experiment: {experiment}")
            print(f"Reflections: {len(reflections) if reflections else 0}")
            return True
        else:
            print("❌ stills_process failed!")
            print(f"Log: {log_messages}")
            return False

    except Exception as e:
        print(f"❌ stills_process error: {e}")
        return False


def main():
    cbf_path = "747/lys_nitr_10_6_0491.cbf"

    if not Path(cbf_path).exists():
        print(f"CBF file not found: {cbf_path}")
        return

    # Test manual workflow
    with tempfile.TemporaryDirectory() as temp_dir:
        manual_dir = Path(temp_dir) / "manual"
        manual_success = run_manual_workflow(cbf_path, manual_dir)

    # Test stills_process workflow
    with tempfile.TemporaryDirectory() as temp_dir:
        stills_dir = Path(temp_dir) / "stills"
        stills_success = run_stills_process_workflow(cbf_path, stills_dir)

    print("\n=== COMPARISON SUMMARY ===")
    print(f"Manual workflow: {'✅ SUCCESS' if manual_success else '❌ FAILED'}")
    print(f"stills_process:  {'✅ SUCCESS' if stills_success else '❌ FAILED'}")

    if manual_success and not stills_success:
        print("\n🔍 DIAGNOSIS: Manual workflow succeeds but stills_process fails")
        print("   This confirms there's a difference in the approaches.")
        print("   Need to identify what stills_process is doing differently.")
    elif not manual_success and not stills_success:
        print("\n🔍 DIAGNOSIS: Both workflows fail")
        print("   The issue may be with the data or environment setup.")
    elif manual_success and stills_success:
        print("\n🔍 DIAGNOSIS: Both workflows succeed")
        print("   The issue may have been resolved.")


if __name__ == "__main__":
    main()
</file>

<file path="docs/ARCHITECTURE/types.md">
# System-Wide Shared Type Definitions [Type:DiffusePipe:1.0]

> This document is the authoritative source for system-wide shared data structures and type definitions used across the DiffusePipe crystallography processing pipeline.
>
> These types are defined in the IDL specification file `src/diffusepipe/types/types_IDL.md` and are referenced by multiple components throughout the pipeline.
>
> Cross-reference: For implementation details and complete IDL specifications, see `src/diffusepipe/types/types_IDL.md`.

---

## Core Operation Types

### OperationOutcome [Type:DiffusePipe:OperationOutcome:1.0]

Generic outcome for operations within components, providing standardized return type for component methods.

```python
# Implementation: Pydantic BaseModel in src/diffusepipe/types/
class OperationOutcome:
    status: str                          # Must be "SUCCESS", "FAILURE", or "WARNING"
    message: Optional[str]               # Human-readable message about the outcome
    error_code: Optional[str]            # Machine-readable code for specific error types
    output_artifacts: Optional[Dict[str, str]]  # Map of artifact names to file paths
```

**Usage**: Returned by all major component methods to indicate success/failure with structured details.

### StillProcessingOutcome [Type:DiffusePipe:StillProcessingOutcome:1.0]

Comprehensive outcome for processing a single still image through the entire pipeline.

```python
class StillProcessingOutcome:
    input_cbf_path: str                  # Path to the original CBF file
    status: str                          # Pipeline status (see Status Values below)
    message: Optional[str]               # Overall processing message
    working_directory: str               # Path to dedicated working directory
    dials_outcome: OperationOutcome      # DIALS processing results
    extraction_outcome: OperationOutcome # DataExtractor results
    consistency_outcome: Optional[OperationOutcome]  # ConsistencyChecker results (if run)
    q_calc_outcome: Optional[OperationOutcome]       # QValueCalculator results (if run)
```

**Status Values**:
- `"SUCCESS_ALL"`: All stages completed successfully
- `"SUCCESS_DIALS_ONLY"`: DIALS succeeded, subsequent stages failed
- `"SUCCESS_EXTRACTION_ONLY"`: DIALS and extraction succeeded, diagnostics failed
- `"FAILURE_DIALS"`: DIALS processing failed
- `"FAILURE_EXTRACTION"`: DIALS succeeded but extraction failed
- `"FAILURE_DIAGNOSTICS"`: Processing succeeded but diagnostics failed

---

## Configuration Types

### DIALSStillsProcessConfig [Type:DiffusePipe:DIALSStillsProcessConfig:1.0]

Configuration for DIALS crystallographic processing, supporting both stills and sequence data.

```python
class DIALSStillsProcessConfig:
    stills_process_phil_path: Optional[str]          # Path to PHIL configuration file
    force_processing_mode: Optional[str]             # "stills", "sequence", or None for auto-detection
    sequence_processing_phil_overrides: Optional[List[str]]  # PHIL overrides for sequence processing
    data_type_detection_enabled: Optional[bool]     # Enable automatic data type detection (default: True)
    known_unit_cell: Optional[str]                  # Unit cell parameters for indexing
    known_space_group: Optional[str]                # Space group for indexing
    spotfinder_threshold_algorithm: Optional[str]   # Spot finding algorithm (e.g., "dispersion")
    min_spot_area: Optional[int]                    # Minimum spot area for detection
    output_shoeboxes: Optional[bool]                # Save shoeboxes for Bragg masking
    calculate_partiality: Optional[bool]            # Calculate partialities (default: True)
```

**Key Behavior**: 
- `force_processing_mode` overrides automatic CBF header detection
- `sequence_processing_phil_overrides` applied only when sequence processing is used
- Partialities used as quality threshold, not quantitative divisors

### ExtractionConfig [Type:DiffusePipe:ExtractionConfig:1.0]

Configuration for diffuse scattering data extraction and correction.

```python
class ExtractionConfig:
    # Resolution and intensity filtering
    min_res: Optional[float]                         # Low-resolution limit (max d-spacing in Å)
    max_res: Optional[float]                         # High-resolution limit (min d-spacing in Å)
    min_intensity: Optional[float]                   # Minimum pixel intensity threshold
    max_intensity: Optional[float]                   # Maximum pixel intensity threshold
    
    # Processing parameters
    gain: float                                      # Detector gain factor (required)
    pixel_step: int                                  # Process every Nth pixel (required)
    
    # Geometric validation tolerances
    cell_length_tol: float                           # Cell length tolerance (fractional)
    cell_angle_tol: float                            # Cell angle tolerance (degrees)
    orient_tolerance_deg: float                      # Orientation tolerance (degrees)
    q_consistency_tolerance_angstrom_inv: float      # Q-vector consistency tolerance (Å⁻¹)
    
    # Correction and background options
    lp_correction_enabled: bool                      # Apply Lorentz-Polarization correction
    subtract_measured_background_path: Optional[str] # Path to background map
    subtract_constant_background_value: Optional[float]  # Constant background value
    
    # Output and diagnostics
    plot_diagnostics: bool                           # Generate diagnostic plots
    verbose: bool                                    # Enable verbose logging
```

**Key Validation Tolerances**:
- `q_consistency_tolerance_angstrom_inv`: Used in Module 1.S.1.Validation for geometric model validation
- Typical values: `cell_length_tol=0.01` (1%), `q_consistency_tolerance_angstrom_inv=0.01` (0.01 Å⁻¹)

### RelativeScalingConfig [Type:DiffusePipe:RelativeScalingConfig:1.0]

Configuration for the relative scaling model (future implementation).

```python
class RelativeScalingConfig:
    refine_per_still_scale: bool                     # Refine multiplicative scale per still (default: True)
    refine_resolution_scale_multiplicative: bool     # Refine resolution-dependent scaling (default: False)
    resolution_scale_bins: Optional[int]            # Number of resolution bins if enabled
    refine_additive_offset: bool                     # MUST be False for v1 implementation
    min_partiality_threshold: float                  # P_spot threshold for Bragg reference (default: 0.1)
```

**Critical Note**: `refine_additive_offset` must be `False` in v1 to avoid parameter correlation issues.

---

## Pipeline Configuration

### StillsPipelineConfig [Type:DiffusePipe:StillsPipelineConfig:1.0]

Overall pipeline configuration encapsulating all processing stages.

```python
class StillsPipelineConfig:
    dials_stills_process_config: DIALSStillsProcessConfig  # DIALS processing settings
    extraction_config: ExtractionConfig                   # Diffuse extraction settings
    relative_scaling_config: RelativeScalingConfig        # Future scaling settings
    run_consistency_checker: bool                          # Enable q-vector consistency checking
    run_q_calculator: bool                                 # Enable q-map calculation
```

---

## File Management Types

### ComponentInputFiles [Type:DiffusePipe:ComponentInputFiles:1.0]

Standardized file path container for component dependencies.

```python
class ComponentInputFiles:
    cbf_image_path: Optional[str]        # Primary CBF image file
    dials_expt_path: Optional[str]       # DIALS experiment list (.expt)
    dials_refl_path: Optional[str]       # DIALS reflection table (.refl)
    bragg_mask_path: Optional[str]       # DIALS-generated Bragg mask (.pickle)
    external_pdb_path: Optional[str]     # External PDB for validation
```

**Usage Pattern**: Fields are optional to allow flexibility for different components. Components validate required fields in their preconditions.

---

## Processing Route Constants

### Data Type Detection [Type:DiffusePipe:ProcessingConstants:1.0]

```python
# Processing route identifiers (Module 1.S.0)
PROCESSING_ROUTE_STILLS = "stills"      # For Angle_increment = 0.0°
PROCESSING_ROUTE_SEQUENCE = "sequence"  # For Angle_increment > 0.0°

# Status constants
SUCCESS_STATUS = "SUCCESS"
FAILURE_STATUS = "FAILURE" 
WARNING_STATUS = "WARNING"
```

---

## Type Dependencies and Relationships

### Cross-Component Usage

1. **OperationOutcome**: Used by all major components (adapters, extractors, validators)
2. **ComponentInputFiles**: Standard input pattern for file-dependent components
3. **Configuration Types**: Passed down through orchestrator to individual components
4. **StillProcessingOutcome**: Aggregates results from all pipeline stages

### Implementation Notes

- All types implemented as Pydantic BaseModel classes for validation
- Optional fields use `Optional[Type]` annotation
- Required fields have no default values and must be explicitly provided
- Configuration validation occurs at component initialization

---

For detailed behavioral specifications and implementation requirements, refer to the complete IDL definitions in `src/diffusepipe/types/types_IDL.md`.
</file>

<file path="libdocs/dials/dials_scaling.md">
# DIALS Scaling Framework

This documentation covers the DIALS scaling framework for intensity correction, outlier rejection, and systematic error modeling in crystallographic data processing.

**Version Information:** Compatible with DIALS 3.x series. Some methods may differ in DIALS 2.x.

**Key Dependencies:**
- `dials.algorithms.scaling`: Main scaling framework
- `dials.array_family.flex`: Reflection tables and array operations
- `cctbx`: Unit cells, space groups, symmetry operations

## D.0. dials.algorithms.scaling Python Framework (Major)

**1. Purpose:**
Comprehensive framework for implementing custom scaling models, components, and refinement procedures for relative scaling of diffraction data in Module 3.S.3. Provides the infrastructure for building sophisticated scaling algorithms that can handle multi-crystal datasets and custom parameterizations.

**2. Primary Python Call(s):**
```python
from dials.algorithms.scaling.model.model import ScalingModelBase
from dials.algorithms.scaling.model.components.scale_components import ScaleComponentBase
from dials.algorithms.scaling.model.components.smooth_scale_components import (
    GaussianSmoother1D, GaussianSmoother2D, GaussianSmoother3D
)
from dials.algorithms.scaling.active_parameter_managers import (
    multi_active_parameter_manager,
    active_parameter_manager
)
from dials.algorithms.scaling.scaling_refiner import scaling_refinery
from dials.algorithms.scaling.target_function import ScalingTargetFunction

# Base classes for custom scaling models
class CustomScalingModel(ScalingModelBase):
    def __init__(self, configdict, is_scaled=False):
        super().__init__(configdict, is_scaled)
        # Initialize custom components
        
class CustomScaleComponent(ScaleComponentBase):
    def __init__(self, active_parameter_manager, params):
        super().__init__(active_parameter_manager, params)
        # Initialize component-specific parameters
```

**3. Key Framework Classes:**

**ScalingModelBase:**
- `__init__(self, configdict, is_scaled=False)`: Initialize scaling model
  - `configdict`: Dictionary with model configuration parameters
  - `is_scaled`: Boolean indicating if model has been applied to data
- `configure_components(self, reflection_table, experiment, params)`: Setup model components
- `_components`: Dictionary storing individual scale components
- `components`: Property returning list of active components
- `get_scales(self, reflection_table)` → flex.double: Calculate scale factors
- `get_inverse_scales(self, reflection_table)` → flex.double: Calculate inverse scales

**ScaleComponentBase:**
- `__init__(self, active_parameter_manager, params)`: Initialize component
- `calculate_scales_and_derivatives(self, reflection_table, block_id=None)`: Core scaling calculation
- `parameters`: Property accessing current parameter values
- `n_params`: Property returning number of parameters
- `update_reflection_data(self, reflection_table)`: Update component state

**GaussianSmoother Classes:**
- `GaussianSmoother1D(n_parameters, value_range)`: 1D smoothing component
- `GaussianSmoother2D(n_x_params, n_y_params, x_range, y_range)`: 2D smoothing
- `GaussianSmoother3D(n_x_params, n_y_params, n_z_params, x_range, y_range, z_range)`: 3D smoothing
- `value_error_for_location(locations)` → (values, errors): Evaluate smoother at coordinates

**4. Return Types:**
- `get_scales()`: flex.double array of scale factors
- `calculate_scales_and_derivatives()`: tuple of (scales, derivatives) flex arrays
- `value_error_for_location()`: tuple of (flex.double values, flex.double errors)

**5. Example Usage - Custom Scaling Model:**
```python
from dials.algorithms.scaling.model.model import ScalingModelBase
from dials.algorithms.scaling.model.components.scale_components import ScaleComponentBase
from dials.algorithms.scaling.active_parameter_managers import active_parameter_manager
from dials.array_family import flex
from scitbx import matrix

class SimpleMultiplicativeComponent(ScaleComponentBase):
    """Simple multiplicative scale factor component"""
    
    def __init__(self, active_parameter_manager, initial_scale=1.0):
        # Initialize with single parameter
        super().__init__(active_parameter_manager)
        self._initial_scale = initial_scale
        
        # Add parameter to manager
        self.active_parameter_manager.add_parameters(
            flex.double([initial_scale])  # Single scale parameter
        )
    
    @property
    def n_params(self):
        return 1
    
    @property  
    def parameters(self):
        return self.active_parameter_manager.get_parameters()[:self.n_params]
    
    def calculate_scales_and_derivatives(self, reflection_table, block_id=None):
        """
        Calculate scales and derivatives for all reflections
        
        Returns:
            tuple: (scales, derivatives) where derivatives[i,j] = ∂scale_i/∂param_j
        """
        n_refl = len(reflection_table)
        scale_param = self.parameters[0]
        
        # All reflections get the same scale factor
        scales = flex.double(n_refl, scale_param)
        
        # Derivatives: ∂scale_i/∂param_0 = 1 for all i
        derivatives = flex.double(flex.grid(n_refl, 1), 1.0)
        
        return scales, derivatives
    
    def update_reflection_data(self, reflection_table):
        """Update component with new reflection data if needed"""
        pass

class ResolutionDependentComponent(ScaleComponentBase):
    """Scale component that varies with resolution"""
    
    def __init__(self, active_parameter_manager, n_resolution_bins=10, d_min=1.0, d_max=50.0):
        super().__init__(active_parameter_manager)
        self.n_resolution_bins = n_resolution_bins
        self.d_min = d_min
        self.d_max = d_max
        
        # Initialize parameters (one per resolution bin)
        initial_params = flex.double([1.0] * n_resolution_bins)
        self.active_parameter_manager.add_parameters(initial_params)
        
        # Store bin edges
        import math
        log_d_min = math.log(d_min)
        log_d_max = math.log(d_max)
        self.log_d_edges = [log_d_min + i * (log_d_max - log_d_min) / n_resolution_bins 
                           for i in range(n_resolution_bins + 1)]
    
    @property
    def n_params(self):
        return self.n_resolution_bins
    
    @property
    def parameters(self):
        return self.active_parameter_manager.get_parameters()[:self.n_params]
    
    def calculate_scales_and_derivatives(self, reflection_table, block_id=None):
        """Calculate resolution-dependent scales"""
        import math
        
        # Get d-spacings
        if "d" not in reflection_table:
            raise ValueError("Reflection table must contain 'd' column for resolution-dependent scaling")
        
        d_spacings = reflection_table["d"]
        n_refl = len(d_spacings)
        
        scales = flex.double(n_refl)
        derivatives = flex.double(flex.grid(n_refl, self.n_params), 0.0)
        
        # Assign scales based on resolution bins
        for i, d_val in enumerate(d_spacings):
            if d_val <= 0:
                scales[i] = 1.0
                continue
                
            log_d = math.log(d_val)
            
            # Find appropriate bin
            bin_idx = 0
            for j in range(len(self.log_d_edges) - 1):
                if self.log_d_edges[j] <= log_d < self.log_d_edges[j + 1]:
                    bin_idx = j
                    break
            
            # Ensure bin_idx is valid
            bin_idx = max(0, min(bin_idx, self.n_params - 1))
            
            # Set scale and derivative
            scales[i] = self.parameters[bin_idx]
            derivatives[i, bin_idx] = 1.0
        
        return scales, derivatives

class CustomRelativeScalingModel(ScalingModelBase):
    """Custom scaling model for Module 3.S.3 relative scaling"""
    
    def __init__(self, configdict, is_scaled=False):
        super().__init__(configdict, is_scaled)
        self._components = {}
        
    def configure_components(self, reflection_table, experiment, params):
        """Configure scaling components based on data and parameters"""
        
        # Set up parameter manager
        self.active_parameter_manager = active_parameter_manager()
        
        # Add multiplicative component
        self._components["multiplicative"] = SimpleMultiplicativeComponent(
            self.active_parameter_manager, initial_scale=1.0
        )
        
        # Add resolution-dependent component if requested
        if hasattr(params, 'resolution_dependent') and params.resolution_dependent:
            self._components["resolution"] = ResolutionDependentComponent(
                self.active_parameter_manager, 
                n_resolution_bins=params.n_resolution_bins,
                d_min=params.d_min, 
                d_max=params.d_max
            )
        
        # Calculate d-spacings if needed
        if "resolution" in self._components and "d" not in reflection_table:
            # Add d-spacing calculation
            from cctbx import uctbx
            unit_cell = experiment.crystal.get_unit_cell()
            miller_indices = reflection_table["miller_index"]
            d_spacings = flex.double([unit_cell.d(hkl) for hkl in miller_indices])
            reflection_table["d"] = d_spacings
    
    @property
    def components(self):
        return list(self._components.values())
    
    def get_scales(self, reflection_table):
        """Calculate combined scale factors from all components"""
        if not self._components:
            return flex.double(len(reflection_table), 1.0)
        
        # Start with unit scales
        total_scales = flex.double(len(reflection_table), 1.0)
        
        # Multiply scales from all components
        for component in self.components:
            component_scales, _ = component.calculate_scales_and_derivatives(reflection_table)
            total_scales *= component_scales
        
        return total_scales
    
    def get_inverse_scales(self, reflection_table):
        """Calculate inverse scale factors"""
        scales = self.get_scales(reflection_table)
        return 1.0 / scales
```

**6. Advanced Framework Usage - Parameter Management and Refinement:**
```python
from dials.algorithms.scaling.active_parameter_managers import multi_active_parameter_manager
from dials.algorithms.scaling.scaling_refiner import scaling_refinery
from dials.algorithms.scaling.target_function import ScalingTargetFunction
from scitbx.lstbx import normal_eqns

class CustomScalingTargetFunction:
    """Custom target function for scaling refinement"""
    
    def __init__(self, scaling_models, reflection_tables):
        self.scaling_models = scaling_models
        self.reflection_tables = reflection_tables
    
    def compute_residuals_and_gradients(self):
        """
        Compute residuals and gradients for least squares refinement
        
        Returns:
            tuple: (residuals, jacobian) for least squares optimization
        """
        from dials.array_family import flex
        import math
        
        all_residuals = flex.double()
        all_gradients = []
        
        for model, refl_table in zip(self.scaling_models, self.reflection_tables):
            if len(refl_table) == 0:
                continue
                
            # Get observed intensities and calculate expected from model
            obs_intensities = refl_table["intensity.sum.value"]
            obs_variances = refl_table["intensity.sum.variance"]
            
            # Get scales from model
            scales = model.get_scales(refl_table)
            
            # Example residual: (I_obs - scale * I_ref) / sigma
            # For relative scaling, we might compare intensities between datasets
            
            # Simple example: residual based on deviation from mean intensity
            mean_intensity = flex.mean(obs_intensities)
            expected_intensities = scales * mean_intensity
            
            # Calculate residuals
            residuals = flex.double()
            gradients_matrix = []
            
            for i, (obs, exp, var) in enumerate(zip(obs_intensities, expected_intensities, obs_variances)):
                if var > 0:
                    sigma = math.sqrt(var)
                    residual = (obs - exp) / sigma
                    residuals.append(residual)
                    
                    # Calculate gradients with respect to model parameters
                    # ∂residual/∂param = -(∂exp/∂param) / sigma = -(∂(scale*I_ref)/∂param) / sigma
                    
                    # Get parameter gradients from components
                    component_gradients = flex.double()
                    for component in model.components:
                        _, derivatives = component.calculate_scales_and_derivatives(refl_table)
                        # ∂exp/∂param = I_ref * ∂scale/∂param
                        param_gradient = -mean_intensity * derivatives[i, :] / sigma
                        component_gradients.extend(param_gradient)
                    
                    gradients_matrix.append(component_gradients)
            
            all_residuals.extend(residuals)
            all_gradients.extend(gradients_matrix)
        
        # Convert gradients to proper format for least squares
        if all_gradients:
            n_residuals = len(all_residuals)
            n_params = len(all_gradients[0]) if all_gradients else 0
            jacobian = flex.double(flex.grid(n_residuals, n_params))
            
            for i, grad_row in enumerate(all_gradients):
                for j, grad_val in enumerate(grad_row):
                    jacobian[i, j] = grad_val
        else:
            jacobian = flex.double(flex.grid(0, 0))
        
        return all_residuals, jacobian

def run_scaling_refinement(scaling_models, reflection_tables, max_iterations=10):
    """
    Run scaling parameter refinement using DIALS scaling framework
    """
    from scitbx.lstbx import normal_eqns_solving
    
    # Create target function
    target_function = CustomScalingTargetFunction(scaling_models, reflection_tables)
    
    # Set up parameter manager for all models
    param_manager = multi_active_parameter_manager()
    for model in scaling_models:
        if hasattr(model, 'active_parameter_manager'):
            param_manager.add_parameter_manager(model.active_parameter_manager)
    
    # Refinement loop
    for iteration in range(max_iterations):
        print(f"Refinement iteration {iteration + 1}")
        
        # Calculate residuals and gradients
        residuals, jacobian = target_function.compute_residuals_and_gradients()
        
        if len(residuals) == 0:
            print("No data for refinement")
            break
        
        # Set up normal equations
        normal_eqns = normal_eqns_solving.levenberg_marquardt_iterations(
            residuals=residuals,
            jacobian=jacobian,
            step_threshold=1e-6,
            gradient_threshold=1e-6
        )
        
        # Solve for parameter shifts
        try:
            normal_eqns.build_and_solve()
            parameter_shifts = normal_eqns.solution()
            
            # Apply parameter shifts
            current_params = param_manager.get_parameters()
            new_params = current_params + parameter_shifts
            param_manager.set_parameters(new_params)
            
            # Check convergence
            shift_magnitude = flex.max(flex.abs(parameter_shifts))
            residual_rms = math.sqrt(flex.mean(residuals * residuals))
            
            print(f"  RMS residual: {residual_rms:.6f}")
            print(f"  Max parameter shift: {shift_magnitude:.6f}")
            
            if shift_magnitude < 1e-6:
                print("Refinement converged")
                break
                
        except Exception as e:
            print(f"Refinement failed at iteration {iteration + 1}: {e}")
            break
    
    return scaling_models

# Usage example
def create_and_refine_scaling_model(experiments, reflection_tables):
    """Complete example of creating and refining a custom scaling model"""
    
    # Create scaling models for each dataset
    scaling_models = []
    for i, (expt, refl_table) in enumerate(zip(experiments, reflection_tables)):
        
        # Configuration for this model
        configdict = {
            'model_type': 'custom_relative',
            'resolution_dependent': True,
            'n_resolution_bins': 10,
            'd_min': 1.5,
            'd_max': 50.0
        }
        
        # Create model
        model = CustomRelativeScalingModel(configdict, is_scaled=False)
        
        # Configure components based on data
        from types import SimpleNamespace
        params = SimpleNamespace()
        params.resolution_dependent = True
        params.n_resolution_bins = 10
        params.d_min = 1.5
        params.d_max = 50.0
        
        model.configure_components(refl_table, expt, params)
        scaling_models.append(model)
        
        print(f"Model {i}: {len(model.components)} components, "
              f"{sum(c.n_params for c in model.components)} parameters")
    
    # Run refinement
    refined_models = run_scaling_refinement(scaling_models, reflection_tables)
    
    # Apply scaling to data
    for model, refl_table in zip(refined_models, reflection_tables):
        scales = model.get_scales(refl_table)
        
        # Apply scales to intensities
        refl_table["intensity.scaled.value"] = refl_table["intensity.sum.value"] * scales
        refl_table["intensity.scaled.variance"] = refl_table["intensity.sum.variance"] * (scales * scales)
        
        print(f"Applied scaling: scale range {flex.min(scales):.3f} - {flex.max(scales):.3f}")
    
    return refined_models, reflection_tables
```

**7. Gaussian Smoothing Components Usage:**
```python
from dials.algorithms.scaling.model.components.smooth_scale_components import GaussianSmoother1D

class SmoothScaleComponent(ScaleComponentBase):
    """Component using Gaussian smoothing for parameter regularization"""
    
    def __init__(self, active_parameter_manager, coordinate_values, n_control_points=20):
        super().__init__(active_parameter_manager)
        
        # Set up coordinate range
        self.coord_min = flex.min(coordinate_values) 
        self.coord_max = flex.max(coordinate_values)
        self.coord_range = (self.coord_min, self.coord_max)
        
        # Create Gaussian smoother
        self.smoother = GaussianSmoother1D(
            n_parameters=n_control_points,
            value_range=self.coord_range
        )
        
        # Initialize control point values
        initial_values = flex.double([1.0] * n_control_points)
        self.active_parameter_manager.add_parameters(initial_values)
        
        # Store coordinate values for evaluation
        self.coordinate_values = coordinate_values
    
    @property
    def n_params(self):
        return self.smoother.n_parameters
    
    def calculate_scales_and_derivatives(self, reflection_table, block_id=None):
        """Calculate smoothed scales using Gaussian interpolation"""
        
        # Get current parameter values
        control_values = self.parameters
        
        # Set control point values in smoother
        self.smoother.set_parameters(control_values)
        
        # Evaluate smoother at coordinate positions
        scales, errors = self.smoother.value_error_for_location(self.coordinate_values)
        
        # Calculate derivatives: ∂scale_i/∂control_j
        derivatives = flex.double(flex.grid(len(scales), self.n_params), 0.0)
        
        # Use analytical derivatives from smoother weight matrices
        # DIALS GaussianSmoother classes provide analytical derivatives through weight matrices
        # This is more accurate and efficient than finite differences
        for i, coord in enumerate(self.coordinate_values):
            # Get analytical weights and values at coordinate location
            weight_result = self.smoother.value_weight(coord)
            weights = weight_result.get_weight()  # Weight matrix for this coordinate
            
            # Weights provide direct derivatives: ∂scale_i/∂control_j = weight[i,j]
            for j in range(self.n_params):
                derivatives[i, j] = weights[j]  # Direct analytical derivative
        
        return scales, derivatives
```

**8. Notes/Caveats:**
- **Framework Complexity:** The scaling framework is sophisticated; start with simple components before building complex models
- **Parameter Management:** Always use `active_parameter_manager` for proper parameter handling and refinement
- **Data Requirements:** Ensure reflection tables contain required columns (intensity, variance, Miller indices)
- **Memory Usage:** Large datasets with many parameters can consume significant memory during refinement
- **Convergence:** Monitor refinement convergence carefully; poor initial guesses can lead to non-convergence
- **Component Ordering:** The order of components in `_components` affects the final scaling calculation
- **Analytical Derivatives:** GaussianSmoother classes provide analytical derivatives through weight matrices from `value_weight()` methods, eliminating the need for finite difference calculations

**9. Integration with Diffuse Scattering Pipeline:**
This framework enables implementation of Module 3.S.3 relative scaling by providing:
- Custom scaling models tailored to diffuse scattering requirements  
- Flexible parameter management for multi-dataset scaling
- Robust refinement algorithms for optimizing scale factors
- Integration with DIALS reflection table infrastructure

**10. See Also:**
- **File I/O Operations**: [dials_file_io.md](dials_file_io.md)
- **Detector Models**: [dxtbx_models.md](dxtbx_models.md)
- **Crystallographic Calculations**: [crystallographic_calculations.md](crystallographic_calculations.md)
- **Array Operations**: [flex_arrays.md](flex_arrays.md)
- DIALS scaling documentation for additional built-in scaling models
</file>

<file path="scripts/visual_diagnostics/__init__.py">
# Visual diagnostics for Phase 0 and Phase 1 outputs
</file>

<file path="scripts/visual_diagnostics/check_dials_processing.py">
#!/usr/bin/env python3
"""
Visual diagnostic script for DIALS processing outputs (Module 1.S.1).

This script loads raw still images and their corresponding DIALS processing outputs
(experiments and reflections) and generates visualizations to verify that spot finding,
indexing, and refinement worked correctly.
"""

import sys
import argparse
import logging
from pathlib import Path
from typing import List, Tuple, Optional

# Add project src to path for imports
sys.path.insert(0, str(Path(__file__).parent.parent.parent / "src"))

from plot_utils import (
    plot_detector_image,
    plot_spot_overlay,
    ensure_output_dir,
    close_all_figures,
    setup_logging_for_plots,
)

logger = logging.getLogger(__name__)


def load_raw_image(image_path: str):
    """
    Load raw still image using dxtbx.

    Args:
        image_path: Path to raw image file (e.g., CBF)

    Returns:
        ImageSet object from dxtbx
    """
    try:
        from dxtbx.model.experiment_list import ExperimentListFactory
        from dxtbx.imageset import ImageSetFactory

        # Load image as ImageSet
        image_set = ImageSetFactory.new([image_path])[0]
        return image_set

    except ImportError as e:
        logger.error(f"Failed to import dxtbx: {e}")
        logger.info("Install DIALS to use this script with real data")
        return None
    except Exception as e:
        logger.error(f"Failed to load raw image {image_path}: {e}")
        return None


def load_dials_outputs(expt_path: str, refl_path: str):
    """
    Load DIALS experiment and reflection files.

    Args:
        expt_path: Path to .expt file
        refl_path: Path to .refl file

    Returns:
        Tuple of (experiments, reflections) or (None, None) if failed
    """
    try:
        from dxtbx.model.experiment_list import ExperimentListFactory
        from dials.array_family import flex

        # Load experiments
        experiments = ExperimentListFactory.from_json_file(expt_path)

        # Load reflections
        reflections = flex.reflection_table.from_file(refl_path)

        logger.info(
            f"Loaded {len(experiments)} experiments and {len(reflections)} reflections"
        )
        return experiments, reflections

    except ImportError as e:
        logger.error(f"Failed to import DIALS components: {e}")
        return None, None
    except Exception as e:
        logger.error(f"Failed to load DIALS outputs: {e}")
        return None, None


def extract_spot_positions(
    reflections, column_name: str = "xyzobs.px.value"
) -> List[Tuple[float, float]]:
    """
    Extract spot positions from reflection table.

    Args:
        reflections: DIALS reflection table
        column_name: Name of column containing xyz pixel coordinates

    Returns:
        List of (x, y) pixel coordinates
    """
    try:
        if not reflections.has_key(column_name):
            logger.warning(f"Reflection table missing column {column_name}")
            return []

        xyz_px = reflections[column_name]
        positions = []

        for i in range(len(xyz_px)):
            x, y, z = xyz_px[i]
            positions.append((x, y))

        logger.info(f"Extracted {len(positions)} spot positions from {column_name}")
        return positions

    except Exception as e:
        logger.error(f"Failed to extract spot positions: {e}")
        return []


def create_dials_visualizations(
    image_set, experiments, reflections, output_dir: str
) -> bool:
    """
    Create visualizations for DIALS processing results.

    Args:
        image_set: Raw image data
        experiments: DIALS experiments
        reflections: DIALS reflections
        output_dir: Directory to save plots

    Returns:
        True if successful, False otherwise
    """
    try:
        output_path = ensure_output_dir(output_dir)

        # Get raw image data (first image, first panel)
        raw_data = image_set.get_raw_data(0)
        if len(raw_data) > 1:
            logger.info(
                f"Multi-panel detector detected ({len(raw_data)} panels), using first panel"
            )
        image_data = raw_data[0]

        # Plot 1: Raw image alone
        logger.info("Creating raw image plot...")
        plot_detector_image(
            image_data,
            title="Raw Still Image",
            output_path=str(output_path / "raw_image.png"),
            log_scale=True,
        )

        # Plot 2: Raw image with observed spots
        logger.info("Creating observed spots overlay...")
        observed_positions = extract_spot_positions(reflections, "xyzobs.px.value")

        if observed_positions:
            plot_spot_overlay(
                image_data,
                observed_positions,
                title="Raw Image with Observed Spots",
                output_path=str(output_path / "image_with_observed_spots.png"),
                log_scale=True,
                spot_color="red",
                spot_size=30,
            )

        # Plot 3: Raw image with predicted spots (if available)
        logger.info("Creating predicted spots overlay...")
        predicted_positions = extract_spot_positions(reflections, "xyzcal.px.value")

        if predicted_positions:
            plot_spot_overlay(
                image_data,
                predicted_positions,
                title="Raw Image with Predicted Spots",
                output_path=str(output_path / "image_with_predicted_spots.png"),
                log_scale=True,
                spot_color="blue",
                spot_size=30,
            )

        # Plot 4: Raw image with both observed and predicted spots
        if observed_positions and predicted_positions:
            logger.info("Creating combined spots overlay...")
            plot_spot_overlay(
                image_data,
                observed_positions,
                title="Raw Image with Observed (red) and Predicted (blue) Spots",
                output_path=str(output_path / "image_with_both_spots.png"),
                log_scale=True,
                spot_color="red",
                spot_size=25,
                predicted_positions=predicted_positions,
                predicted_color="blue",
            )

        # Generate information summary
        info_file = output_path / "dials_processing_info.txt"
        with open(info_file, "w") as f:
            f.write("DIALS Processing Visual Check Results\n")
            f.write("=" * 40 + "\n\n")

            f.write(f"Number of experiments: {len(experiments)}\n")
            f.write(f"Number of reflections: {len(reflections)}\n")
            f.write(f"Observed spot positions: {len(observed_positions)}\n")
            f.write(f"Predicted spot positions: {len(predicted_positions)}\n\n")

            if experiments:
                exp = experiments[0]
                f.write("First Experiment Details:\n")
                f.write(
                    f"  Crystal space group: {exp.crystal.get_space_group().info()}\n"
                )
                f.write(f"  Unit cell: {exp.crystal.get_unit_cell()}\n")
                f.write(f"  Detector panels: {len(exp.detector)}\n")
                f.write(f"  Beam wavelength: {exp.beam.get_wavelength():.4f} Å\n")

            if reflections.has_key("partiality"):
                partialities = reflections["partiality"]
                f.write(f"\nPartiality statistics:\n")
                f.write(f"  Mean: {partialities.mean():.3f}\n")
                f.write(f"  Min: {partialities.min():.3f}\n")
                f.write(f"  Max: {partialities.max():.3f}\n")

        logger.info(f"DIALS processing info saved to {info_file}")

        # Suggest DIALS command for reciprocal lattice view
        logger.info("\nFor interactive reciprocal lattice view, run:")
        logger.info(
            f"dials.reciprocal_lattice_viewer {experiments._experiments[0]._filename} {reflections._filename}"
        )

        return True

    except Exception as e:
        logger.error(f"Failed to create DIALS visualizations: {e}")
        return False


def main():
    """Main function for DIALS processing visual checks."""
    parser = argparse.ArgumentParser(
        description="Visual diagnostic for DIALS processing outputs",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  # Check DIALS processing results
  python check_dials_processing.py --raw-image image.cbf --expt processed.expt --refl processed.refl --output-dir dials_check

  # Using existing test data
  python check_dials_processing.py --raw-image ../../747/lys_nitr_10_6_0491.cbf \\
    --expt ../../lys_nitr_10_6_0491_dials_processing/indexed_refined_detector.expt \\
    --refl ../../lys_nitr_10_6_0491_dials_processing/indexed_refined_detector.refl \\
    --output-dir dials_visual_check
        """,
    )

    parser.add_argument(
        "--raw-image", required=True, help="Path to raw still image file (e.g., CBF)"
    )

    parser.add_argument(
        "--expt", required=True, help="Path to DIALS experiment file (.expt)"
    )

    parser.add_argument(
        "--refl", required=True, help="Path to DIALS reflection file (.refl)"
    )

    parser.add_argument(
        "--output-dir",
        default="dials_visual_check",
        help="Output directory for plots (default: dials_visual_check)",
    )

    parser.add_argument("--verbose", action="store_true", help="Enable verbose logging")

    args = parser.parse_args()

    # Set up logging
    setup_logging_for_plots()
    if args.verbose:
        logging.getLogger().setLevel(logging.DEBUG)

    logger.info("Starting DIALS processing visual check...")

    # Validate input files
    for file_path in [args.raw_image, args.expt, args.refl]:
        if not Path(file_path).exists():
            logger.error(f"Input file not found: {file_path}")
            return 1

    try:
        # Load raw image
        logger.info(f"Loading raw image: {args.raw_image}")
        image_set = load_raw_image(args.raw_image)
        if image_set is None:
            logger.error("Failed to load raw image")
            return 1

        # Load DIALS outputs
        logger.info(f"Loading DIALS outputs: {args.expt}, {args.refl}")
        experiments, reflections = load_dials_outputs(args.expt, args.refl)
        if experiments is None or reflections is None:
            logger.error("Failed to load DIALS outputs")
            return 1

        # Create visualizations
        logger.info(f"Creating visualizations in: {args.output_dir}")
        success = create_dials_visualizations(
            image_set, experiments, reflections, args.output_dir
        )

        if success:
            logger.info(f"✅ DIALS processing visual check completed successfully!")
            logger.info(f"📁 Check output files in: {args.output_dir}")
            return 0
        else:
            logger.error("❌ DIALS processing visual check failed")
            return 1

    except Exception as e:
        logger.error(f"Unexpected error: {e}")
        return 1

    finally:
        close_all_figures()


if __name__ == "__main__":
    sys.exit(main())
</file>

<file path="scripts/visual_diagnostics/check_pixel_masks.py">
#!/usr/bin/env python3
"""
Visual diagnostic script for pixel mask generation (Module 1.S.2).

This script generates static and dynamic pixel masks and creates visualizations
to verify that masking is working correctly for different detector regions and
anomalous pixels.
"""

import sys
import argparse
import logging
import json
from pathlib import Path
from typing import Dict, Any, Optional

# Add project src to path for imports
sys.path.insert(0, str(Path(__file__).parent.parent.parent / "src"))

from plot_utils import (
    plot_detector_image,
    plot_multi_panel_comparison,
    ensure_output_dir,
    close_all_figures,
    setup_logging_for_plots,
)

# Import diffusepipe components
from diffusepipe.masking.pixel_mask_generator import (
    PixelMaskGenerator,
    StaticMaskParams,
    DynamicMaskParams,
    Circle,
    Rectangle,
    create_default_static_params,
    create_default_dynamic_params,
)

logger = logging.getLogger(__name__)


def load_detector_from_expt(expt_path: str):
    """
    Load detector model from DIALS experiment file.

    Args:
        expt_path: Path to .expt file

    Returns:
        DIALS Detector object or None if failed
    """
    try:
        from dxtbx.model.experiment_list import ExperimentListFactory

        experiments = ExperimentListFactory.from_json_file(expt_path)
        detector = experiments[0].detector

        logger.info(f"Loaded detector with {len(detector)} panels")
        return detector

    except ImportError as e:
        logger.error(f"Failed to import DIALS: {e}")
        return None
    except Exception as e:
        logger.error(f"Failed to load detector from {expt_path}: {e}")
        return None


def load_representative_images(image_paths: list):
    """
    Load representative images for dynamic mask generation.

    Args:
        image_paths: List of paths to image files

    Returns:
        List of ImageSet objects
    """
    try:
        from dxtbx.imageset import ImageSetFactory

        image_sets = []
        for image_path in image_paths:
            if Path(image_path).exists():
                image_set = ImageSetFactory.new([image_path])[0]
                image_sets.append(image_set)
                logger.info(f"Loaded image: {image_path}")
            else:
                logger.warning(f"Image not found: {image_path}")

        return image_sets

    except ImportError as e:
        logger.error(f"Failed to import dxtbx: {e}")
        return []
    except Exception as e:
        logger.error(f"Failed to load images: {e}")
        return []


def parse_mask_config(config_str: str) -> Dict[str, Any]:
    """
    Parse mask configuration from JSON string or file.

    Args:
        config_str: JSON string or path to JSON file

    Returns:
        Configuration dictionary
    """
    try:
        # Try to parse as JSON string first
        if config_str.strip().startswith("{"):
            return json.loads(config_str)

        # Otherwise treat as file path
        config_path = Path(config_str)
        if config_path.exists():
            with open(config_path, "r") as f:
                return json.load(f)
        else:
            logger.warning(f"Config file not found: {config_str}")
            return {}

    except Exception as e:
        logger.error(f"Failed to parse mask config: {e}")
        return {}


def create_static_mask_params(config: Dict[str, Any]) -> StaticMaskParams:
    """
    Create StaticMaskParams from configuration dictionary.

    Args:
        config: Configuration dictionary

    Returns:
        StaticMaskParams object
    """
    beamstop = None
    if "beamstop" in config:
        bs_config = config["beamstop"]
        if bs_config.get("type") == "circle":
            beamstop = Circle(
                center_x=bs_config["center_x"],
                center_y=bs_config["center_y"],
                radius=bs_config["radius"],
            )
        elif bs_config.get("type") == "rectangle":
            beamstop = Rectangle(
                min_x=bs_config["min_x"],
                max_x=bs_config["max_x"],
                min_y=bs_config["min_y"],
                max_y=bs_config["max_y"],
            )

    untrusted_rects = None
    if "untrusted_rects" in config:
        untrusted_rects = []
        for rect_config in config["untrusted_rects"]:
            rect = Rectangle(
                min_x=rect_config["min_x"],
                max_x=rect_config["max_x"],
                min_y=rect_config["min_y"],
                max_y=rect_config["max_y"],
            )
            untrusted_rects.append(rect)

    untrusted_panels = config.get("untrusted_panels")

    return StaticMaskParams(
        beamstop=beamstop,
        untrusted_rects=untrusted_rects,
        untrusted_panels=untrusted_panels,
    )


def create_dynamic_mask_params(config: Dict[str, Any]) -> DynamicMaskParams:
    """
    Create DynamicMaskParams from configuration dictionary.

    Args:
        config: Configuration dictionary

    Returns:
        DynamicMaskParams object
    """
    return DynamicMaskParams(
        hot_pixel_thresh=config.get("hot_pixel_thresh", 1e6),
        negative_pixel_tolerance=config.get("negative_pixel_tolerance", 0.0),
        max_fraction_bad_pixels=config.get("max_fraction_bad_pixels", 0.1),
    )


def create_pixel_mask_visualizations(
    detector,
    representative_images: list,
    static_params: StaticMaskParams,
    dynamic_params: DynamicMaskParams,
    output_dir: str,
) -> bool:
    """
    Create visualizations for pixel mask generation.

    Args:
        detector: DIALS Detector object
        representative_images: List of ImageSet objects for dynamic analysis
        static_params: Static mask parameters
        dynamic_params: Dynamic mask parameters
        output_dir: Directory to save plots

    Returns:
        True if successful, False otherwise
    """
    try:
        output_path = ensure_output_dir(output_dir)
        generator = PixelMaskGenerator()

        # Generate masks
        logger.info("Generating static mask...")
        static_mask = generator.generate_static_mask(detector, static_params)

        logger.info("Generating dynamic mask...")
        dynamic_mask = generator.generate_dynamic_mask(
            detector, representative_images, dynamic_params
        )

        logger.info("Generating combined mask...")
        combined_mask = generator.generate_combined_pixel_mask(
            detector, static_params, representative_images, dynamic_params
        )

        # Create visualizations for each panel
        for panel_idx in range(len(detector)):
            logger.info(f"Creating visualizations for panel {panel_idx}...")

            # Get panel masks
            static_panel = static_mask[panel_idx]
            dynamic_panel = dynamic_mask[panel_idx]
            combined_panel = combined_mask[panel_idx]

            # Plot individual masks
            plot_detector_image(
                static_panel,
                title=f"Panel {panel_idx}: Static Mask",
                output_path=str(output_path / f"panel_{panel_idx}_static_mask.png"),
                cmap="RdYlBu",
            )

            plot_detector_image(
                dynamic_panel,
                title=f"Panel {panel_idx}: Dynamic Mask",
                output_path=str(output_path / f"panel_{panel_idx}_dynamic_mask.png"),
                cmap="RdYlBu",
            )

            plot_detector_image(
                combined_panel,
                title=f"Panel {panel_idx}: Combined Mask",
                output_path=str(output_path / f"panel_{panel_idx}_combined_mask.png"),
                cmap="RdYlBu",
            )

            # Plot comparison
            plot_multi_panel_comparison(
                [static_panel, dynamic_panel, combined_panel],
                [
                    f"Panel {panel_idx}: Static",
                    f"Panel {panel_idx}: Dynamic",
                    f"Panel {panel_idx}: Combined",
                ],
                output_path=str(output_path / f"panel_{panel_idx}_mask_comparison.png"),
                figsize=(18, 6),
                cmap="RdYlBu",
            )

        # Generate summary information
        info_file = output_path / "pixel_mask_info.txt"
        with open(info_file, "w") as f:
            f.write("Pixel Mask Generation Visual Check Results\n")
            f.write("=" * 45 + "\n\n")

            f.write(f"Number of detector panels: {len(detector)}\n\n")

            # Statistics for each panel
            for panel_idx in range(len(detector)):
                panel_size = detector[panel_idx].get_image_size()
                total_pixels = panel_size[0] * panel_size[1]

                static_good = static_mask[panel_idx].count(True)
                dynamic_good = dynamic_mask[panel_idx].count(True)
                combined_good = combined_mask[panel_idx].count(True)

                f.write(
                    f"Panel {panel_idx} ({panel_size[0]}x{panel_size[1]} pixels):\n"
                )
                f.write(f"  Total pixels: {total_pixels}\n")
                f.write(
                    f"  Static mask good pixels: {static_good} ({static_good/total_pixels*100:.1f}%)\n"
                )
                f.write(
                    f"  Dynamic mask good pixels: {dynamic_good} ({dynamic_good/total_pixels*100:.1f}%)\n"
                )
                f.write(
                    f"  Combined mask good pixels: {combined_good} ({combined_good/total_pixels*100:.1f}%)\n"
                )
                f.write(
                    f"  Static mask rejected: {total_pixels - static_good} pixels\n"
                )
                f.write(
                    f"  Dynamic mask rejected: {total_pixels - dynamic_good} pixels\n"
                )
                f.write(f"  Total rejected: {total_pixels - combined_good} pixels\n\n")

            # Configuration summary
            f.write("Configuration used:\n")
            f.write(f"  Static mask parameters:\n")
            f.write(f"    Beamstop: {static_params.beamstop}\n")
            f.write(
                f"    Untrusted rectangles: {len(static_params.untrusted_rects) if static_params.untrusted_rects else 0}\n"
            )
            f.write(f"    Untrusted panels: {static_params.untrusted_panels}\n")
            f.write(f"  Dynamic mask parameters:\n")
            f.write(f"    Hot pixel threshold: {dynamic_params.hot_pixel_thresh}\n")
            f.write(
                f"    Negative pixel tolerance: {dynamic_params.negative_pixel_tolerance}\n"
            )
            f.write(
                f"    Max bad pixel fraction: {dynamic_params.max_fraction_bad_pixels}\n"
            )

        logger.info(f"Pixel mask info saved to {info_file}")
        return True

    except Exception as e:
        logger.error(f"Failed to create pixel mask visualizations: {e}")
        return False


def main():
    """Main function for pixel mask visual checks."""
    parser = argparse.ArgumentParser(
        description="Visual diagnostic for pixel mask generation",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  # Basic pixel mask check with default parameters
  python check_pixel_masks.py --expt detector.expt --images image1.cbf image2.cbf --output-dir pixel_mask_check
  
  # With custom static mask configuration
  python check_pixel_masks.py --expt detector.expt --images image1.cbf \\
    --static-config '{"beamstop": {"type": "circle", "center_x": 1250, "center_y": 1250, "radius": 50}}' \\
    --output-dir pixel_mask_check
  
  # Using existing test data
  python check_pixel_masks.py \\
    --expt ../../lys_nitr_10_6_0491_dials_processing/indexed_refined_detector.expt \\
    --images ../../747/lys_nitr_10_6_0491.cbf \\
    --output-dir pixel_mask_visual_check
        """,
    )

    parser.add_argument(
        "--expt",
        required=True,
        help="Path to DIALS experiment file (.expt) containing detector model",
    )

    parser.add_argument(
        "--images",
        nargs="+",
        required=True,
        help="Path(s) to representative image files for dynamic mask generation",
    )

    parser.add_argument(
        "--static-config", help="Static mask configuration (JSON string or file path)"
    )

    parser.add_argument(
        "--dynamic-config", help="Dynamic mask configuration (JSON string or file path)"
    )

    parser.add_argument(
        "--output-dir",
        default="pixel_mask_visual_check",
        help="Output directory for plots (default: pixel_mask_visual_check)",
    )

    parser.add_argument("--verbose", action="store_true", help="Enable verbose logging")

    args = parser.parse_args()

    # Set up logging
    setup_logging_for_plots()
    if args.verbose:
        logging.getLogger().setLevel(logging.DEBUG)

    logger.info("Starting pixel mask visual check...")

    # Validate input files
    if not Path(args.expt).exists():
        logger.error(f"Experiment file not found: {args.expt}")
        return 1

    try:
        # Load detector
        logger.info(f"Loading detector from: {args.expt}")
        detector = load_detector_from_expt(args.expt)
        if detector is None:
            logger.error("Failed to load detector")
            return 1

        # Load representative images
        logger.info(f"Loading representative images: {args.images}")
        representative_images = load_representative_images(args.images)
        if not representative_images:
            logger.warning("No images loaded, dynamic mask will be empty")

        # Parse configurations
        static_config = {}
        dynamic_config = {}

        if args.static_config:
            static_config = parse_mask_config(args.static_config)

        if args.dynamic_config:
            dynamic_config = parse_mask_config(args.dynamic_config)

        # Create mask parameters
        static_params = (
            create_static_mask_params(static_config)
            if static_config
            else create_default_static_params()
        )
        dynamic_params = (
            create_dynamic_mask_params(dynamic_config)
            if dynamic_config
            else create_default_dynamic_params()
        )

        logger.info(f"Static mask config: {static_params}")
        logger.info(f"Dynamic mask config: {dynamic_params}")

        # Create visualizations
        logger.info(f"Creating visualizations in: {args.output_dir}")
        success = create_pixel_mask_visualizations(
            detector,
            representative_images,
            static_params,
            dynamic_params,
            args.output_dir,
        )

        if success:
            logger.info(f"✅ Pixel mask visual check completed successfully!")
            logger.info(f"📁 Check output files in: {args.output_dir}")
            return 0
        else:
            logger.error("❌ Pixel mask visual check failed")
            return 1

    except Exception as e:
        logger.error(f"Unexpected error: {e}")
        return 1

    finally:
        close_all_figures()


if __name__ == "__main__":
    sys.exit(main())
</file>

<file path="scripts/visual_diagnostics/check_total_mask.py">
#!/usr/bin/env python3
"""
Visual diagnostic script for total mask generation (Module 1.S.3).

This script generates Bragg masks and combines them with pixel masks to create
total masks for diffuse scattering analysis. It provides visualizations to verify
that Bragg peak regions are properly masked and diffuse regions are preserved.
"""

import sys
import argparse
import logging
import json
from pathlib import Path
from typing import Dict, Any, Optional

# Add project src to path for imports
sys.path.insert(0, str(Path(__file__).parent.parent.parent / "src"))

from plot_utils import (
    plot_detector_image,
    plot_mask_overlay,
    plot_multi_panel_comparison,
    ensure_output_dir,
    close_all_figures,
    setup_logging_for_plots,
)

# Import diffusepipe components
from diffusepipe.masking.bragg_mask_generator import (
    BraggMaskGenerator,
    create_default_bragg_mask_config,
    validate_mask_compatibility,
)
from diffusepipe.masking.pixel_mask_generator import (
    PixelMaskGenerator,
    create_default_static_params,
    create_default_dynamic_params,
)

logger = logging.getLogger(__name__)


def load_dials_data(image_path: str, expt_path: str, refl_path: str):
    """
    Load raw image and DIALS processing outputs.

    Args:
        image_path: Path to raw image file
        expt_path: Path to .expt file
        refl_path: Path to .refl file

    Returns:
        Tuple of (image_set, experiments, reflections) or (None, None, None) if failed
    """
    try:
        from dxtbx.imageset import ImageSetFactory
        from dxtbx.model.experiment_list import ExperimentListFactory
        from dials.array_family import flex

        # Load raw image
        image_set = ImageSetFactory.new([image_path])[0]

        # Load experiments
        experiments = ExperimentListFactory.from_json_file(expt_path)

        # Load reflections
        reflections = flex.reflection_table.from_file(refl_path)

        logger.info(
            f"Loaded image, {len(experiments)} experiments, and {len(reflections)} reflections"
        )
        return image_set, experiments, reflections

    except ImportError as e:
        logger.error(f"Failed to import DIALS components: {e}")
        return None, None, None
    except Exception as e:
        logger.error(f"Failed to load DIALS data: {e}")
        return None, None, None


def parse_bragg_config(config_str: str) -> Dict[str, Any]:
    """
    Parse Bragg mask configuration from JSON string or file.

    Args:
        config_str: JSON string or path to JSON file

    Returns:
        Configuration dictionary
    """
    try:
        # Try to parse as JSON string first
        if config_str and config_str.strip().startswith("{"):
            return json.loads(config_str)

        # Otherwise treat as file path
        if config_str:
            config_path = Path(config_str)
            if config_path.exists():
                with open(config_path, "r") as f:
                    return json.load(f)

        # Return default if no config provided
        return create_default_bragg_mask_config()

    except Exception as e:
        logger.error(f"Failed to parse Bragg mask config: {e}")
        return create_default_bragg_mask_config()


def generate_pixel_mask(detector, representative_images: list):
    """
    Generate global pixel mask using default parameters.

    Args:
        detector: DIALS Detector object
        representative_images: List of ImageSet objects

    Returns:
        Combined pixel mask or None if failed
    """
    try:
        generator = PixelMaskGenerator()
        static_params = create_default_static_params()
        dynamic_params = create_default_dynamic_params()

        pixel_mask = generator.generate_combined_pixel_mask(
            detector, static_params, representative_images, dynamic_params
        )

        logger.info("Generated global pixel mask")
        return pixel_mask

    except Exception as e:
        logger.error(f"Failed to generate pixel mask: {e}")
        return None


def create_total_mask_visualizations(
    image_set,
    experiments,
    reflections,
    pixel_mask,
    bragg_config: Dict[str, Any],
    output_dir: str,
    use_option_b: bool = False,
) -> bool:
    """
    Create visualizations for total mask generation.

    Args:
        image_set: Raw image data
        experiments: DIALS experiments
        reflections: DIALS reflections
        pixel_mask: Global pixel mask
        bragg_config: Bragg mask generation configuration
        output_dir: Directory to save plots
        use_option_b: Whether to use shoebox-based Bragg masking (Option B)

    Returns:
        True if successful, False otherwise
    """
    try:
        output_path = ensure_output_dir(output_dir)
        generator = BraggMaskGenerator()

        # Get raw image data (first image, first panel)
        raw_data = image_set.get_raw_data(0)
        if len(raw_data) > 1:
            logger.info(
                f"Multi-panel detector detected ({len(raw_data)} panels), using first panel"
            )
        image_data = raw_data[0]

        # Generate Bragg mask
        experiment = experiments[0]

        if use_option_b:
            logger.info("Generating Bragg mask using shoebox data (Option B)...")
            bragg_mask = generator.generate_bragg_mask_from_shoeboxes(
                reflections, experiment.detector
            )
        else:
            logger.info("Generating Bragg mask using dials.generate_mask (Option A)...")
            bragg_mask = generator.generate_bragg_mask_from_spots(
                experiment, reflections, bragg_config
            )

        # Generate total mask
        logger.info("Generating total mask...")
        total_mask = generator.get_total_mask_for_still(bragg_mask, pixel_mask)

        # Validate mask compatibility
        if not validate_mask_compatibility(bragg_mask, pixel_mask):
            logger.error("Bragg and pixel masks are incompatible")
            return False

        # Create visualizations for first panel
        panel_idx = 0
        bragg_panel = bragg_mask[panel_idx]
        pixel_panel = pixel_mask[panel_idx]
        total_panel = total_mask[panel_idx]

        # Plot 1: Raw image alone
        logger.info("Creating raw image plot...")
        plot_detector_image(
            image_data,
            title="Raw Still Image",
            output_path=str(output_path / "raw_image.png"),
            log_scale=True,
        )

        # Plot 2: Raw image with Bragg mask overlay
        logger.info("Creating Bragg mask overlay...")
        plot_mask_overlay(
            image_data,
            bragg_panel,
            title="Raw Image with Bragg Mask (red = masked Bragg regions)",
            output_path=str(output_path / "image_with_bragg_mask.png"),
            mask_color="red",
            mask_alpha=0.4,
            log_scale=True,
        )

        # Plot 3: Raw image with total mask overlay (diffuse regions)
        logger.info("Creating total mask overlay...")
        plot_mask_overlay(
            image_data,
            total_panel,
            title="Raw Image with Total Mask (blue = good diffuse pixels)",
            output_path=str(output_path / "image_with_total_diffuse_mask.png"),
            mask_color="blue",
            mask_alpha=0.3,
            log_scale=True,
        )

        # Plot 4: Masked image showing only diffuse pixels
        logger.info("Creating diffuse-only image...")
        try:
            # Convert masks to numpy arrays for multiplication
            import numpy as np

            if hasattr(image_data, "as_numpy_array"):
                img_array = image_data.as_numpy_array()
            else:
                img_array = np.array(image_data)

            if hasattr(total_panel, "as_numpy_array"):
                mask_array = total_panel.as_numpy_array()
            else:
                mask_array = np.array(total_panel)

            # Handle 1D arrays
            if len(img_array.shape) == 1 and hasattr(image_data, "accessor"):
                accessor = image_data.accessor()
                height, width = accessor.all()
                img_array = img_array.reshape(height, width)
                mask_array = mask_array.reshape(height, width)

            # Create masked image
            masked_image = img_array * mask_array.astype(float)

            plot_detector_image(
                masked_image,
                title="Diffuse Pixels Only (after total masking)",
                output_path=str(output_path / "image_diffuse_pixels_only.png"),
                log_scale=True,
            )

        except Exception as e:
            logger.warning(f"Failed to create diffuse-only image: {e}")

        # Plot 5: Individual masks comparison
        logger.info("Creating mask comparison...")
        plot_multi_panel_comparison(
            [pixel_panel, bragg_panel, total_panel],
            ["Global Pixel Mask", "Bragg Mask", "Total Mask (diffuse regions)"],
            output_path=str(output_path / "mask_comparison.png"),
            figsize=(18, 6),
            cmap="RdYlBu",
        )

        # Create individual mask plots
        plot_detector_image(
            pixel_panel,
            title="Global Pixel Mask",
            output_path=str(output_path / "global_pixel_mask.png"),
            cmap="RdYlBu",
        )

        plot_detector_image(
            bragg_panel,
            title="Bragg Mask",
            output_path=str(output_path / "bragg_mask.png"),
            cmap="RdYlBu",
        )

        plot_detector_image(
            total_panel,
            title="Total Mask (Diffuse Regions)",
            output_path=str(output_path / "total_diffuse_mask.png"),
            cmap="RdYlBu",
        )

        # Generate summary information
        info_file = output_path / "total_mask_info.txt"
        with open(info_file, "w") as f:
            f.write("Total Mask Generation Visual Check Results\n")
            f.write("=" * 43 + "\n\n")

            # Get panel size
            panel_size = experiment.detector[panel_idx].get_image_size()
            total_pixels = panel_size[0] * panel_size[1]

            # Count pixels in each mask
            pixel_good = pixel_panel.count(True)
            bragg_masked = bragg_panel.count(True)
            total_good = total_panel.count(True)

            f.write(f"Panel {panel_idx} ({panel_size[0]}x{panel_size[1]} pixels):\n")
            f.write(f"  Total pixels: {total_pixels}\n")
            f.write(
                f"  Global pixel mask good: {pixel_good} ({pixel_good/total_pixels*100:.1f}%)\n"
            )
            f.write(
                f"  Bragg mask flagged: {bragg_masked} ({bragg_masked/total_pixels*100:.1f}%)\n"
            )
            f.write(
                f"  Total mask good (diffuse): {total_good} ({total_good/total_pixels*100:.1f}%)\n"
            )
            f.write(
                f"  Pixels excluded: {total_pixels - total_good} ({(total_pixels-total_good)/total_pixels*100:.1f}%)\n\n"
            )

            # Reflection statistics
            if reflections:
                f.write(f"Reflection statistics:\n")
                f.write(f"  Total reflections: {len(reflections)}\n")

                if reflections.has_key("partiality"):
                    partialities = reflections["partiality"]
                    f.write(f"  Partiality mean: {partialities.mean():.3f}\n")
                    f.write(
                        f"  Partiality range: {partialities.min():.3f} - {partialities.max():.3f}\n"
                    )

                if reflections.has_key("intensity.sum.value"):
                    intensities = reflections["intensity.sum.value"]
                    f.write(f"  Intensity mean: {intensities.mean():.1f}\n")
                    f.write(
                        f"  Intensity range: {intensities.min():.1f} - {intensities.max():.1f}\n"
                    )

            f.write(
                f"\nBragg mask generation method: {'Option B (shoeboxes)' if use_option_b else 'Option A (dials.generate_mask)'}\n"
            )
            f.write(f"Bragg mask configuration: {bragg_config}\n")

        logger.info(f"Total mask info saved to {info_file}")
        return True

    except Exception as e:
        logger.error(f"Failed to create total mask visualizations: {e}")
        return False


def main():
    """Main function for total mask visual checks."""
    parser = argparse.ArgumentParser(
        description="Visual diagnostic for total mask generation (Bragg + pixel masks)",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  # Basic total mask check
  python check_total_mask.py --raw-image image.cbf --expt processed.expt --refl processed.refl --output-dir total_mask_check
  
  # Using shoebox-based Bragg masking (Option B)
  python check_total_mask.py --raw-image image.cbf --expt processed.expt --refl processed.refl \\
    --use-option-b --output-dir total_mask_check
  
  # With custom Bragg mask configuration
  python check_total_mask.py --raw-image image.cbf --expt processed.expt --refl processed.refl \\
    --bragg-config '{"border": 3, "algorithm": "simple"}' --output-dir total_mask_check
  
  # Using existing test data
  python check_total_mask.py \\
    --raw-image ../../747/lys_nitr_10_6_0491.cbf \\
    --expt ../../lys_nitr_10_6_0491_dials_processing/indexed_refined_detector.expt \\
    --refl ../../lys_nitr_10_6_0491_dials_processing/indexed_refined_detector.refl \\
    --output-dir total_mask_visual_check
        """,
    )

    parser.add_argument(
        "--raw-image", required=True, help="Path to raw still image file (e.g., CBF)"
    )

    parser.add_argument(
        "--expt", required=True, help="Path to DIALS experiment file (.expt)"
    )

    parser.add_argument(
        "--refl", required=True, help="Path to DIALS reflection file (.refl)"
    )

    parser.add_argument(
        "--bragg-config", help="Bragg mask configuration (JSON string or file path)"
    )

    parser.add_argument(
        "--use-option-b",
        action="store_true",
        help="Use shoebox-based Bragg masking (Option B) instead of dials.generate_mask (Option A)",
    )

    parser.add_argument(
        "--output-dir",
        default="total_mask_visual_check",
        help="Output directory for plots (default: total_mask_visual_check)",
    )

    parser.add_argument("--verbose", action="store_true", help="Enable verbose logging")

    args = parser.parse_args()

    # Set up logging
    setup_logging_for_plots()
    if args.verbose:
        logging.getLogger().setLevel(logging.DEBUG)

    logger.info("Starting total mask visual check...")

    # Validate input files
    for file_path in [args.raw_image, args.expt, args.refl]:
        if not Path(file_path).exists():
            logger.error(f"Input file not found: {file_path}")
            return 1

    try:
        # Load DIALS data
        logger.info(f"Loading DIALS data...")
        image_set, experiments, reflections = load_dials_data(
            args.raw_image, args.expt, args.refl
        )
        if image_set is None or experiments is None or reflections is None:
            logger.error("Failed to load DIALS data")
            return 1

        # Generate global pixel mask
        logger.info("Generating global pixel mask...")
        pixel_mask = generate_pixel_mask(experiments[0].detector, [image_set])
        if pixel_mask is None:
            logger.error("Failed to generate pixel mask")
            return 1

        # Parse Bragg mask configuration
        bragg_config = parse_bragg_config(args.bragg_config)
        logger.info(f"Bragg mask config: {bragg_config}")

        # Create visualizations
        logger.info(f"Creating visualizations in: {args.output_dir}")
        success = create_total_mask_visualizations(
            image_set,
            experiments,
            reflections,
            pixel_mask,
            bragg_config,
            args.output_dir,
            args.use_option_b,
        )

        if success:
            logger.info(f"✅ Total mask visual check completed successfully!")
            logger.info(f"📁 Check output files in: {args.output_dir}")

            if not args.use_option_b:
                logger.info(
                    "\n💡 To try shoebox-based masking, add --use-option-b flag"
                )

            return 0
        else:
            logger.error("❌ Total mask visual check failed")
            return 1

    except Exception as e:
        logger.error(f"Unexpected error: {e}")
        return 1

    finally:
        close_all_figures()


if __name__ == "__main__":
    sys.exit(main())
</file>

<file path="scripts/visual_diagnostics/demo_visual_checks.py">
#!/usr/bin/env python3
"""
Demonstration script showing how to use the visual diagnostic tools.

This script provides example commands for running visual checks on existing
test data and explains how to interpret the results.
"""

import sys
import subprocess
from pathlib import Path


def run_command(cmd, description):
    """Run a command and capture output."""
    print(f"\n{'='*60}")
    print(f"DEMO: {description}")
    print(f"{'='*60}")
    print(f"Command: {' '.join(cmd)}")
    print("-" * 40)

    try:
        result = subprocess.run(cmd, capture_output=True, text=True, cwd=Path.cwd())

        if result.returncode == 0:
            print("✅ Command completed successfully")
            if result.stdout:
                print("Output:")
                print(result.stdout)
        else:
            print("❌ Command failed")
            if result.stderr:
                print("Error:")
                print(result.stderr)
    except Exception as e:
        print(f"❌ Command execution failed: {e}")


def main():
    """Main demonstration function."""
    print("DiffusePipe Visual Diagnostics - Demo Script")
    print("=" * 50)

    # Check if test data exists
    test_cbf = Path("747/lys_nitr_10_6_0491.cbf")
    test_expt = Path(
        "lys_nitr_10_6_0491_dials_processing/indexed_refined_detector.expt"
    )
    test_refl = Path(
        "lys_nitr_10_6_0491_dials_processing/indexed_refined_detector.refl"
    )

    if not all(f.exists() for f in [test_cbf, test_expt, test_refl]):
        print("\n⚠️  Test data not found!")
        print("Expected files:")
        print(f"  - {test_cbf}")
        print(f"  - {test_expt}")
        print(f"  - {test_refl}")
        print("\nTo run this demo, ensure test data is available.")
        print("This demo shows the commands you would run with real data.")

    # Prepare environment
    env = {**dict(sys.environ), "PYTHONPATH": "src"}

    # Demo 1: DIALS processing check
    run_command(
        [
            "python",
            "scripts/visual_diagnostics/check_dials_processing.py",
            "--raw-image",
            str(test_cbf),
            "--expt",
            str(test_expt),
            "--refl",
            str(test_refl),
            "--output-dir",
            "demo_dials_check",
            "--verbose",
        ],
        "DIALS Processing Visual Check",
    )

    # Demo 2: Pixel mask check
    run_command(
        [
            "python",
            "scripts/visual_diagnostics/check_pixel_masks.py",
            "--expt",
            str(test_expt),
            "--images",
            str(test_cbf),
            "--static-config",
            '{"beamstop": {"type": "circle", "center_x": 1250, "center_y": 1250, "radius": 40}}',
            "--output-dir",
            "demo_pixel_mask_check",
            "--verbose",
        ],
        "Pixel Mask Visual Check",
    )

    # Demo 3: Total mask check (Option A)
    run_command(
        [
            "python",
            "scripts/visual_diagnostics/check_total_mask.py",
            "--raw-image",
            str(test_cbf),
            "--expt",
            str(test_expt),
            "--refl",
            str(test_refl),
            "--output-dir",
            "demo_total_mask_check_a",
            "--verbose",
        ],
        "Total Mask Visual Check (Option A - dials.generate_mask)",
    )

    # Demo 4: Total mask check (Option B)
    run_command(
        [
            "python",
            "scripts/visual_diagnostics/check_total_mask.py",
            "--raw-image",
            str(test_cbf),
            "--expt",
            str(test_expt),
            "--refl",
            str(test_refl),
            "--use-option-b",
            "--output-dir",
            "demo_total_mask_check_b",
            "--verbose",
        ],
        "Total Mask Visual Check (Option B - shoebox-based)",
    )

    print(f"\n{'='*60}")
    print("DEMO COMPLETE")
    print(f"{'='*60}")
    print("\nIf test data was available, check the following output directories:")
    print("  - demo_dials_check/")
    print("  - demo_pixel_mask_check/")
    print("  - demo_total_mask_check_a/")
    print("  - demo_total_mask_check_b/")
    print("\nEach directory contains:")
    print("  - PNG plot files for visual inspection")
    print("  - TXT info files with statistics and configuration")
    print("\nTo run on your own data, modify the file paths in the commands above.")


if __name__ == "__main__":
    main()
</file>

<file path="scripts/visual_diagnostics/README.md">
# Visual Diagnostics for DiffusePipe Phase 0 & 1

This directory contains scripts for visual verification of Phase 0 and Phase 1 processing outputs. These tools help validate that DIALS processing, mask generation, and data transformations are working correctly through manual inspection of generated plots.

**📖 For comprehensive documentation on the latest visual diagnostic tools (including Phase 2), see [docs/VISUAL_DIAGNOSTICS_GUIDE.md](../../docs/VISUAL_DIAGNOSTICS_GUIDE.md)**

## Overview

The visual diagnostic scripts are designed to:
- Load real crystallographic data and processing outputs
- Generate informative plots and visualizations  
- Save results for manual inspection
- Provide summary statistics and configuration details

## Scripts

### 1. `check_dials_processing.py` - Module 1.S.1 Visual Checks

**Purpose**: Verify DIALS stills processing results by visualizing spot finding, indexing, and refinement outputs.

**Key Visualizations**:
- Raw still image with logarithmic intensity scaling
- Observed spot positions overlaid on raw image
- Predicted spot positions from crystal model
- Combined view showing observed vs. predicted spots

**Usage Examples**:
```bash
# Basic DIALS processing check
python check_dials_processing.py \
  --raw-image ../../747/lys_nitr_10_6_0491.cbf \
  --expt ../../lys_nitr_10_6_0491_dials_processing/indexed_refined_detector.expt \
  --refl ../../lys_nitr_10_6_0491_dials_processing/indexed_refined_detector.refl \
  --output-dir dials_visual_check

# With verbose output
python check_dials_processing.py --raw-image image.cbf --expt processed.expt --refl processed.refl --verbose
```

**Output Files**:
- `raw_image.png` - Raw detector image
- `image_with_observed_spots.png` - Raw image with observed spots (red)
- `image_with_predicted_spots.png` - Raw image with predicted spots (blue) 
- `image_with_both_spots.png` - Combined observed and predicted spots
- `dials_processing_info.txt` - Summary statistics and crystal model details

**What to Look For**:
- Spots should be visible and well-distributed across the detector
- Observed and predicted spots should overlap well (good indexing)
- Crystal model parameters should be reasonable
- Partiality values should be in expected range (0-1)

---

### 2. `check_pixel_masks.py` - Module 1.S.2 Visual Checks

**Purpose**: Verify static and dynamic pixel mask generation by visualizing masked regions and mask combination logic.

**Key Visualizations**:
- Static mask showing beamstop, untrusted regions, panel exclusions
- Dynamic mask showing hot/negative pixels from image analysis
- Combined mask showing final pixel selection
- Panel-by-panel comparison views

**Usage Examples**:
```bash
# Basic pixel mask check with defaults
python check_pixel_masks.py \
  --expt ../../lys_nitr_10_6_0491_dials_processing/indexed_refined_detector.expt \
  --images ../../747/lys_nitr_10_6_0491.cbf \
  --output-dir pixel_mask_check

# With custom beamstop configuration
python check_pixel_masks.py \
  --expt detector.expt \
  --images image1.cbf image2.cbf \
  --static-config '{"beamstop": {"type": "circle", "center_x": 1250, "center_y": 1250, "radius": 50}}' \
  --output-dir pixel_mask_check

# Multiple representative images for dynamic analysis
python check_pixel_masks.py \
  --expt detector.expt \
  --images image1.cbf image2.cbf image3.cbf \
  --dynamic-config '{"hot_pixel_thresh": 500000, "negative_pixel_tolerance": 5.0}' \
  --output-dir pixel_mask_check
```

**Configuration Options**:

Static mask config (JSON):
```json
{
  "beamstop": {
    "type": "circle",
    "center_x": 1250,
    "center_y": 1250, 
    "radius": 50
  },
  "untrusted_rects": [
    {"min_x": 0, "max_x": 10, "min_y": 0, "max_y": 10}
  ],
  "untrusted_panels": [1, 3]
}
```

Dynamic mask config (JSON):
```json
{
  "hot_pixel_thresh": 1000000,
  "negative_pixel_tolerance": 0.0,
  "max_fraction_bad_pixels": 0.1
}
```

**Output Files**:
- `panel_X_static_mask.png` - Static mask for panel X
- `panel_X_dynamic_mask.png` - Dynamic mask for panel X  
- `panel_X_combined_mask.png` - Combined mask for panel X
- `panel_X_mask_comparison.png` - Side-by-side comparison
- `pixel_mask_info.txt` - Mask statistics and configuration summary

**What to Look For**:
- Static masks should correctly exclude beamstop and untrusted regions
- Dynamic masks should flag obvious hot/negative pixels without being overly aggressive
- Combined masks should preserve most detector area for diffuse analysis
- Mask fraction should be reasonable (typically <10-20% rejected)

---

### 3. `check_total_mask.py` - Module 1.S.3 Visual Checks

**Purpose**: Verify Bragg mask generation and combination with pixel masks to create final diffuse analysis masks.

**Key Visualizations**:
- Raw image with Bragg peak mask overlay
- Raw image with total mask showing diffuse regions
- Masked image showing only diffuse pixels
- Side-by-side mask comparison
- Individual mask components

**Usage Examples**:
```bash
# Basic total mask check (uses dials.generate_mask)
python check_total_mask.py \
  --raw-image ../../747/lys_nitr_10_6_0491.cbf \
  --expt ../../lys_nitr_10_6_0491_dials_processing/indexed_refined_detector.expt \
  --refl ../../lys_nitr_10_6_0491_dials_processing/indexed_refined_detector.refl \
  --output-dir total_mask_check

# Using shoebox-based Bragg masking (Option B)
python check_total_mask.py \
  --raw-image image.cbf --expt processed.expt --refl processed.refl \
  --use-option-b --output-dir total_mask_check

# With custom Bragg mask configuration
python check_total_mask.py \
  --raw-image image.cbf --expt processed.expt --refl processed.refl \
  --bragg-config '{"border": 3, "algorithm": "simple"}' \
  --output-dir total_mask_check
```

**Output Files**:
- `raw_image.png` - Raw detector image
- `image_with_bragg_mask.png` - Raw image with Bragg regions highlighted (red)
- `image_with_total_diffuse_mask.png` - Raw image with diffuse regions highlighted (blue)
- `image_diffuse_pixels_only.png` - Masked image showing only diffuse pixels
- `mask_comparison.png` - Side-by-side comparison of all masks
- `global_pixel_mask.png`, `bragg_mask.png`, `total_diffuse_mask.png` - Individual masks
- `total_mask_info.txt` - Mask statistics and processing summary

**What to Look For**:
- Bragg masks should cover indexed reflection positions
- Diffuse regions should exclude both Bragg peaks and bad pixels
- Final diffuse mask should preserve most detector area
- No obvious Bragg peaks should remain in diffuse regions
- Reflection statistics should match expectations

---

### 4. `check_diffuse_extraction.py` - Diffuse Extraction Verification

**Purpose**: Verify diffuse scattering extraction and correction processes by visualizing the outputs from Phase 1 (DIALS processing) and Phase 2 (DataExtractor).

**Key Visualizations**:
- Raw image with extracted diffuse pixels overlay
- Q-space coverage projections (qx vs qy, qx vs qz, qy vs qz)
- Radial Q-space distribution (intensity vs |Q|)
- Intensity distribution histograms
- Intensity heatmap on detector (if pixel coordinates available)
- Sigma vs intensity scatter plot with Poisson noise reference
- I/σ distribution histogram with statistics

**Usage Examples**:
```bash
# Basic diffuse extraction check
python check_diffuse_extraction.py \
  --raw-image ../../747/lys_nitr_10_6_0491.cbf \
  --expt ../../lys_nitr_10_6_0491_dials_processing/indexed_refined_detector.expt \
  --total-mask ../../lys_nitr_10_6_0491_dials_processing/total_diffuse_mask.pickle \
  --npz-file extraction_output.npz

# With optional masks and background map
python check_diffuse_extraction.py \
  --raw-image image.cbf --expt experiment.expt \
  --total-mask total_mask.pickle --npz-file data.npz \
  --bragg-mask bragg_mask.pickle \
  --pixel-mask pixel_mask.pickle \
  --bg-map background_map.npy \
  --output-dir custom_output \
  --verbose
```

**Output Files**:
- `diffuse_pixel_overlay.png` - Raw image with diffuse pixels highlighted (green)
- `q_projection_qx_qy.png`, `q_projection_qx_qz.png`, `q_projection_qy_qz.png` - Q-space projections
- `radial_q_distribution.png` - Intensity vs radial Q scatter plot
- `intensity_histogram.png` - Intensity distribution (linear and log scale)
- `intensity_heatmap_panel_0.png` - Intensity mapped back to detector coordinates
- `sigma_vs_intensity.png` - Error analysis with Poisson noise reference
- `isigi_histogram.png` - I/σ distribution with mean and median markers
- `intensity_correction_summary.txt` - Sample of corrected intensity values
- `extraction_diagnostics_summary.txt` - Overall summary and file listing

**What to Look For**:
- Diffuse pixels should avoid Bragg peak regions and bad pixel areas
- Q-space coverage should be reasonable and evenly distributed
- Intensity distributions should be physically reasonable (positive, not bimodal)
- I/σ values should be reasonable (typically > 1 for good data)
- Intensity heatmap should show smooth spatial distribution
- Sigma values should follow roughly Poisson statistics (σ ≈ √I)

**Important Notes**:
- Some plots (pixel overlay, intensity heatmap) require original pixel coordinates to be saved in the NPZ file
- If pixel coordinates are not available, these plots will be skipped with a warning
- The intensity correction plot is simplified due to lack of intermediate processing data
- For multi-panel detectors, only panel 0 is visualized by default

---

## General Usage Tips

### Prerequisites
- DIALS must be installed and importable
- Raw crystallographic data (CBF files) and processed outputs (EXPT/REFL files)
- Python packages: matplotlib, numpy, pathlib

### Common Workflows

1. **Full Pipeline Check**: Run all three scripts sequentially on the same dataset
2. **Troubleshooting**: Use verbose mode (`--verbose`) for detailed logging
3. **Parameter Tuning**: Modify configuration JSON to test different masking parameters
4. **Batch Analysis**: Create shell scripts to run checks on multiple datasets

### Interpreting Results

**Good Results Indicate**:
- Clear, well-distributed spot patterns
- Good agreement between observed and predicted spots
- Reasonable mask coverage (not too aggressive)
- Clean separation of Bragg and diffuse regions

**Warning Signs**:
- Very few or no indexed spots
- Large discrepancies between observed/predicted positions
- Excessive masking (>50% of detector excluded)
- Obvious Bragg peaks remaining in diffuse regions

### Output Management

All scripts create timestamped output directories and preserve input configurations in summary files. This allows for:
- Tracking parameter changes over time
- Reproducing specific analysis conditions
- Comparing results across different datasets

### Integration with CI/CD

These scripts can be integrated into automated testing pipelines by:
- Running on reference datasets with known good outputs
- Checking that output files are generated successfully
- Validating summary statistics fall within expected ranges

## Troubleshooting

### Common Issues

1. **Import Errors**: Ensure DIALS is properly installed and PYTHONPATH includes project src/
2. **File Not Found**: Check that input file paths are correct and files exist
3. **Memory Issues**: Large detector images may require sufficient RAM for visualization
4. **Display Issues**: Scripts use non-interactive matplotlib backend (Agg) for automation

### Getting Help

- Use `--help` flag with any script for detailed usage information
- Check log output for specific error messages
- Ensure input files are valid DIALS formats
- Verify detector geometry is reasonable for your experimental setup

## Future Enhancements

Potential improvements for these diagnostic tools:
- Interactive plotting modes for detailed inspection
- Automated quality metrics and pass/fail criteria  
- Integration with DIALS viewer commands
- Support for multi-panel detector visualization
- Statistical comparison against reference datasets
</file>

<file path="scripts/visual_diagnostics/test_imports.py">
#!/usr/bin/env python3
"""
Test script to verify all imports work correctly without DIALS.
"""

import sys
from pathlib import Path

# Add project src to path
sys.path.insert(0, str(Path(__file__).parent.parent.parent / "src"))


def test_imports():
    """Test that all necessary imports work."""
    print("Testing imports...")

    # Test plot utils
    try:
        from plot_utils import (
            plot_detector_image,
            plot_mask_overlay,
            plot_spot_overlay,
            ensure_output_dir,
        )

        print("✅ plot_utils imports successful")
    except Exception as e:
        print(f"❌ plot_utils import failed: {e}")

    # Test diffusepipe components
    try:
        from diffusepipe.masking.pixel_mask_generator import PixelMaskGenerator
        from diffusepipe.masking.bragg_mask_generator import BraggMaskGenerator
        from diffusepipe.crystallography.still_processor import StillProcessorComponent

        print("✅ diffusepipe components imports successful")
    except Exception as e:
        print(f"❌ diffusepipe components import failed: {e}")

    # Test matplotlib backend
    try:
        import matplotlib

        print(f"✅ matplotlib backend: {matplotlib.get_backend()}")
    except Exception as e:
        print(f"❌ matplotlib import failed: {e}")

    # Test component instantiation
    try:
        pixel_gen = PixelMaskGenerator()
        bragg_gen = BraggMaskGenerator()
        print("✅ Component instantiation successful")
    except Exception as e:
        print(f"❌ Component instantiation failed: {e}")

    print("\nAll import tests completed!")


if __name__ == "__main__":
    test_imports()
</file>

<file path="scripts/demo_phase1_processing.py">
#!/usr/bin/env python3
"""
Demo script for Phase 1 stills processing and validation.

This script demonstrates how to run and check the output of the stills processing,
orientation checking, and indexing validation components.
"""

import sys
import argparse
import logging
from pathlib import Path

# Add project src to path
sys.path.insert(0, str(Path(__file__).parent.parent / "src"))

from diffusepipe.crystallography.still_processing_and_validation import (
    StillProcessorAndValidatorComponent,
    create_default_config,
    create_default_extraction_config,
)
from diffusepipe.masking.pixel_mask_generator import (
    PixelMaskGenerator,
    create_default_static_params,
    create_default_dynamic_params,
)
from diffusepipe.masking.bragg_mask_generator import (
    BraggMaskGenerator,
    create_default_bragg_mask_config,
)

# Set up logging
logging.basicConfig(
    level=logging.INFO, format="%(asctime)s - %(name)s - %(levelname)s - %(message)s"
)
logger = logging.getLogger(__name__)


def demo_still_processing_with_validation(
    image_path: str,
    external_pdb_path: str = None,
    output_dir: str = "phase1_demo_output",
    verbose: bool = False,
):
    """
    Demonstrate complete stills processing with geometric validation.

    Args:
        image_path: Path to CBF image file
        external_pdb_path: Optional path to reference PDB file
        output_dir: Directory for output files and plots
        verbose: Enable verbose logging
    """
    print("=" * 60)
    print("PHASE 1 STILLS PROCESSING AND VALIDATION DEMO")
    print("=" * 60)

    if verbose:
        logging.getLogger().setLevel(logging.DEBUG)

    # Create output directory
    output_path = Path(output_dir)
    output_path.mkdir(parents=True, exist_ok=True)

    try:
        # Step 1: Set up configurations
        print("\n1. Setting up configurations...")

        # Use PDB crystallographic parameters to help indexing
        # From 6o2h.pdb: P 1 space group, triclinic unit cell
        dials_config = create_default_config(
            enable_partiality=True,
            enable_shoeboxes=True,  # Enable for Option B Bragg masking
            known_unit_cell="27.424,32.134,34.513,88.66,108.46,111.88",
            known_space_group="P 1",  # Use full space group symbol
        )
        extraction_config = create_default_extraction_config()

        print(f"   ✓ DIALS config: partiality={dials_config.calculate_partiality}")
        print(f"   ✓ Validation tolerances:")
        print(f"     - Cell length: {extraction_config.cell_length_tol * 100:.1f}%")
        print(f"     - Cell angle: {extraction_config.cell_angle_tol}°")
        print(f"     - Orientation: {extraction_config.orient_tolerance_deg}°")
        print(
            f"     - Q-consistency: {extraction_config.q_consistency_tolerance_angstrom_inv} Å⁻¹"
        )

        # Step 2: Initialize processor
        print("\n2. Initializing still processor and validator...")
        processor = StillProcessorAndValidatorComponent()
        print(f"   ✓ Processor: {type(processor).__name__}")
        print(f"   ✓ Adapter: {type(processor.adapter).__name__}")
        print(f"   ✓ Validator: {type(processor.validator).__name__}")

        # Step 3: Process and validate the still
        print(f"\n3. Processing still image: {image_path}")
        if not Path(image_path).exists():
            print(f"   ⚠️  Image file not found: {image_path}")
            print(
                "   📝 This demo will show you the interface without actual processing"
            )
            print("   🔧 To run with real data, provide a valid CBF file path")
            return demo_interface_without_dials(output_dir)

        print("   🔄 Running DIALS stills_process...")
        print("   🔍 Performing geometric validation...")

        outcome = processor.process_and_validate_still(
            image_path=image_path,
            config=dials_config,
            extraction_config=extraction_config,
            external_pdb_path=external_pdb_path,
            output_dir=str(output_path),
        )

        # Step 4: Analyze results
        print(f"\n4. Processing Results:")
        print(f"   Status: {outcome.status}")
        print(f"   Message: {outcome.message}")

        if outcome.status == "SUCCESS":
            print("   ✅ DIALS processing and validation successful!")
            analyze_successful_outcome(outcome, output_path)

        elif outcome.status == "FAILURE_GEOMETRY_VALIDATION":
            print("   ⚠️  DIALS processing succeeded but validation failed!")
            analyze_validation_failure(outcome, output_path)

        else:
            print("   ❌ DIALS processing failed!")
            analyze_processing_failure(outcome, output_path)

        # Step 5: Demonstrate mask generation
        if outcome.status in ["SUCCESS", "FAILURE_GEOMETRY_VALIDATION"]:
            print(f"\n5. Demonstrating mask generation...")
            demo_mask_generation(outcome, output_path)

        print(f"\n📁 Check output files in: {output_path}")
        return outcome

    except ImportError as e:
        print(f"\n❌ Missing DIALS installation: {e}")
        print("   🔧 Install DIALS to run with real crystallographic data")
        return demo_interface_without_dials(output_dir)
    except Exception as e:
        print(f"\n❌ Error during processing: {e}")
        logger.exception("Processing failed")
        return None


def demo_interface_without_dials(output_dir: str):
    """
    Demonstrate the interface and show what the output would look like,
    without requiring DIALS installation.
    """
    print("\n🔄 Running interface demonstration (no DIALS required)...")

    # Create mock outcome to show the interface
    from diffusepipe.types.types_IDL import OperationOutcome
    from diffusepipe.crystallography.still_processing_and_validation import (
        ValidationMetrics,
    )

    # Mock successful outcome
    metrics = ValidationMetrics()
    metrics.q_consistency_passed = True
    metrics.mean_delta_q_mag = 0.005
    metrics.max_delta_q_mag = 0.012
    metrics.num_reflections_tested = 1247
    metrics.pdb_cell_passed = True
    metrics.pdb_orientation_passed = True
    metrics.misorientation_angle_vs_pdb = 1.2

    mock_outcome = OperationOutcome(
        status="SUCCESS",
        message="Successfully processed and validated still image (DEMO)",
        error_code=None,
        output_artifacts={
            "experiment": "Mock_Experiment_Object",
            "reflections": "Mock_Reflections_Object",
            "validation_passed": True,
            "validation_metrics": metrics.to_dict(),
            "log_messages": "Mock DIALS processing completed successfully",
        },
    )

    print("   ✅ Mock processing successful!")
    analyze_successful_outcome(mock_outcome, Path(output_dir))

    # Create a mock validation failure example
    print("\n🔄 Example of validation failure...")
    metrics_fail = ValidationMetrics()
    metrics_fail.q_consistency_passed = False
    metrics_fail.mean_delta_q_mag = 0.025  # Above tolerance
    metrics_fail.max_delta_q_mag = 0.08
    metrics_fail.num_reflections_tested = 856

    mock_fail_outcome = OperationOutcome(
        status="FAILURE_GEOMETRY_VALIDATION",
        message="Geometric validation failed (DEMO)",
        error_code="GEOMETRY_VALIDATION_FAILED",
        output_artifacts={
            "experiment": "Mock_Experiment_Object",
            "reflections": "Mock_Reflections_Object",
            "validation_passed": False,
            "validation_metrics": metrics_fail.to_dict(),
            "log_messages": "DIALS processing successful, validation failed",
        },
    )

    print("   ⚠️  Mock validation failure!")
    analyze_validation_failure(mock_fail_outcome, Path(output_dir))

    return mock_outcome


def analyze_successful_outcome(outcome, output_path: Path):
    """Analyze and display results from successful processing."""
    artifacts = outcome.output_artifacts
    validation_metrics = artifacts.get("validation_metrics", {})

    print("   📊 Validation Results:")
    print(
        f"     • Q-consistency: {'✅ PASSED' if validation_metrics.get('q_consistency_passed') else '❌ FAILED'}"
    )

    mean_delta = validation_metrics.get("mean_delta_q_mag")
    max_delta = validation_metrics.get("max_delta_q_mag")
    if mean_delta is not None or max_delta is not None:
        mean_str = f"{mean_delta:.4f}" if mean_delta is not None else "N/A"
        max_str = f"{max_delta:.4f}" if max_delta is not None else "N/A"
        print(f"     • Mean |Δq|: {mean_str} Å⁻¹")
        print(f"     • Max |Δq|: {max_str} Å⁻¹")
        print(
            f"     • Reflections tested: {validation_metrics.get('num_reflections_tested', 'N/A')}"
        )

    if validation_metrics.get("pdb_cell_passed") is not None:
        print(
            f"     • PDB cell check: {'✅ PASSED' if validation_metrics.get('pdb_cell_passed') else '❌ FAILED'}"
        )
        print(
            f"     • PDB orientation: {'✅ PASSED' if validation_metrics.get('pdb_orientation_passed') else '❌ FAILED'}"
        )
        if validation_metrics.get("misorientation_angle_vs_pdb"):
            print(
                f"     • Misorientation angle: {validation_metrics['misorientation_angle_vs_pdb']:.2f}°"
            )

    # Ensure output directory exists
    output_path.mkdir(parents=True, exist_ok=True)

    # Save detailed report
    report_path = output_path / "validation_report.txt"
    with open(report_path, "w") as f:
        f.write("STILLS PROCESSING AND VALIDATION REPORT\n")
        f.write("=" * 40 + "\n\n")
        f.write(f"Status: {outcome.status}\n")
        f.write(f"Message: {outcome.message}\n\n")
        f.write("Validation Metrics:\n")
        for key, value in validation_metrics.items():
            f.write(f"  {key}: {value}\n")

    print(f"   📄 Detailed report saved: {report_path}")


def analyze_validation_failure(outcome, output_path: Path):
    """Analyze and display results from validation failure."""
    artifacts = outcome.output_artifacts
    validation_metrics = artifacts.get("validation_metrics", {})

    print("   📊 Validation Failure Analysis:")

    # Check which validations failed
    failures = []
    if validation_metrics.get("q_consistency_passed") is False:
        failures.append("Q-vector consistency")
        print(f"     ❌ Q-consistency failed:")
        mean_delta = validation_metrics.get("mean_delta_q_mag")
        max_delta = validation_metrics.get("max_delta_q_mag")
        mean_str = f"{mean_delta:.4f}" if mean_delta is not None else "N/A"
        max_str = f"{max_delta:.4f}" if max_delta is not None else "N/A"
        print(f"        - Mean |Δq|: {mean_str} Å⁻¹")
        print(f"        - Max |Δq|: {max_str} Å⁻¹")

    if validation_metrics.get("pdb_cell_passed") is False:
        failures.append("PDB cell parameters")
        print(f"     ❌ PDB cell parameters failed")

    if validation_metrics.get("pdb_orientation_passed") is False:
        failures.append("PDB orientation")
        print(f"     ❌ PDB orientation failed")

    print(f"   🔧 Failed checks: {', '.join(failures) if failures else 'Unknown'}")
    print(f"   💡 This still should be excluded from subsequent processing")

    # Save failure report
    failure_path = output_path / "validation_failure_report.txt"
    with open(failure_path, "w") as f:
        f.write("VALIDATION FAILURE REPORT\n")
        f.write("=" * 25 + "\n\n")
        f.write(f"Status: {outcome.status}\n")
        f.write(f"Error Code: {outcome.error_code}\n")
        f.write(f"Message: {outcome.message}\n\n")
        f.write("Failed Validations:\n")
        for failure in failures:
            f.write(f"  - {failure}\n")
        f.write("\nValidation Metrics:\n")
        for key, value in validation_metrics.items():
            f.write(f"  {key}: {value}\n")

    print(f"   📄 Failure report saved: {failure_path}")


def analyze_processing_failure(outcome, output_path: Path):
    """Analyze and display results from DIALS processing failure."""
    print("   📊 DIALS Processing Failure:")
    print(f"     Error Code: {outcome.error_code}")
    print(f"     Message: {outcome.message}")

    if outcome.output_artifacts and "log_messages" in outcome.output_artifacts:
        print(f"     DIALS Log: {outcome.output_artifacts['log_messages']}")

    print(f"   💡 Check DIALS configuration and input image quality")

    # Save processing failure report
    failure_path = output_path / "processing_failure_report.txt"
    with open(failure_path, "w") as f:
        f.write("DIALS PROCESSING FAILURE REPORT\n")
        f.write("=" * 32 + "\n\n")
        f.write(f"Status: {outcome.status}\n")
        f.write(f"Error Code: {outcome.error_code}\n")
        f.write(f"Message: {outcome.message}\n\n")
        if outcome.output_artifacts:
            f.write("Output Artifacts:\n")
            for key, value in outcome.output_artifacts.items():
                f.write(f"  {key}: {value}\n")

    print(f"   📄 Failure report saved: {failure_path}")


def demo_mask_generation(outcome, output_path: Path):
    """Demonstrate mask generation pipeline."""
    print("   🎭 Generating masks for this still...")

    try:
        # Initialize mask generators
        pixel_generator = PixelMaskGenerator()
        bragg_generator = BraggMaskGenerator()

        # Get experiment and reflections from outcome
        artifacts = outcome.output_artifacts
        experiment = artifacts.get("experiment")
        reflections = artifacts.get("reflections")

        if isinstance(experiment, str) or isinstance(reflections, str):
            print("   📝 Using mock objects for mask generation demo")
            print("   🔧 With real DIALS data, masks would be generated here")
            return

        # Generate masks (this would work with real DIALS objects)
        print("   🔄 Would generate pixel masks...")
        print("   🔄 Would generate Bragg masks...")
        print("   🔄 Would combine into total masks...")

        print("   ✅ Mask generation pipeline ready!")

    except Exception as e:
        print(f"   ⚠️  Mask generation demo skipped: {e}")


def main():
    """Main function with command line interface."""
    parser = argparse.ArgumentParser(
        description="Demo Phase 1 stills processing and validation",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  # Run with test data
  python demo_phase1_processing.py --image tests/data/test_still.cbf
  
  # Run with PDB validation
  python demo_phase1_processing.py --image data/still.cbf --pdb data/reference.pdb
  
  # Run interface demo (no DIALS required)
  python demo_phase1_processing.py --demo-mode
  
  # Run with existing processed data
  python demo_phase1_processing.py --image lys_nitr_10_6_0491_dials_processing/imported.cbf
        """,
    )

    parser.add_argument("--image", help="Path to CBF still image file")
    parser.add_argument("--pdb", help="Path to reference PDB file for validation")
    parser.add_argument(
        "--output-dir",
        default="phase1_demo_output",
        help="Output directory for results and plots",
    )
    parser.add_argument(
        "--demo-mode",
        action="store_true",
        help="Run interface demonstration without requiring DIALS",
    )
    parser.add_argument("--verbose", action="store_true", help="Enable verbose logging")

    args = parser.parse_args()

    if args.demo_mode:
        print("🎮 Running in demo mode (no DIALS required)")
        demo_interface_without_dials(args.output_dir)
    elif args.image:
        demo_still_processing_with_validation(
            image_path=args.image,
            external_pdb_path=args.pdb,
            output_dir=args.output_dir,
            verbose=args.verbose,
        )
    else:
        # Default: run with any available test data or demo mode
        test_paths = [
            "lys_nitr_10_6_0491_dials_processing/imported.cbf",
            "lys_nitr_8_2_0110_dials_processing/imported.cbf",
            "tests/data/test_still.cbf",
            "747/test.cbf",
        ]

        found_data = False
        for test_path in test_paths:
            if Path(test_path).exists():
                print(f"🔍 Found test data: {test_path}")
                demo_still_processing_with_validation(
                    image_path=test_path,
                    external_pdb_path=args.pdb,
                    output_dir=args.output_dir,
                    verbose=args.verbose,
                )
                found_data = True
                break

        if not found_data:
            print("📝 No test data found, running interface demo...")
            demo_interface_without_dials(args.output_dir)


if __name__ == "__main__":
    main()
</file>

<file path="src/diffusepipe/adapters/__init__.py">
"""Adapter layer for external DIALS/CCTBX/DXTBX Python API calls."""
</file>

<file path="src/diffusepipe/adapters/dials_sequence_process_adapter_IDL.md">
// == BEGIN IDL ==
module src.diffusepipe.adapters {

    # @depends_on_resource(type="ExternalTool:DIALS", purpose="Using DIALS CLI tools (dials.import, dials.find_spots, dials.index, dials.integrate) for sequence data processing")
    # @depends_on_resource(type="FileSystem", purpose="Creating temporary directories, executing CLI commands, loading result files")
    # @depends_on_type(src.diffusepipe.types.DIALSStillsProcessConfig)
    # @depends_on_type(dxtbx.model.Experiment)
    # @depends_on_type(dials.array_family.flex.reflection_table)

    interface DIALSSequenceProcessAdapter {

        // --- Method: process_still ---
        // Preconditions:
        // - `image_path` must be a valid path to a readable CBF file containing sequence/oscillation data (typically Angle_increment > 0.0°).
        // - `config` must be a valid `DIALSStillsProcessConfig` object with parameters compatible with sequence processing.
        // - The DIALS command-line tools must be available in the system PATH.
        // - The CBF file should contain valid crystallographic data with oscillation.
        // Postconditions:
        // - If successful (third tuple element is True), returns valid DIALS Experiment and reflection_table objects.
        // - The Experiment object contains an indexed and refined crystal model.
        // - The reflection_table contains integrated reflections with a 'partiality' column.
        // - The fourth tuple element contains human-readable log messages detailing the processing steps.
        // - If failed (third tuple element is False), the first two elements are None.
        // - All temporary files are cleaned up after processing (temporary directory is removed).
        // Behavior:
        // - Executes a sequence of DIALS CLI tools optimized for oscillation/sequence data processing.
        // - Creates a temporary directory and changes to it for isolated processing.
        // - Workflow steps:
        //   1. `dials.import`: Imports the CBF file to create imported.expt.
        //   2. `dials.find_spots`: Finds spots with critical parameters:
        //      - spotfinder.filter.min_spot_size=3 (not default 2)
        //      - spotfinder.threshold.algorithm=dispersion (not default)
        //   3. `dials.index`: Indexes with critical parameters:
        //      - indexing.method=fft3d (not fft1d)
        //      - geometry.convert_sequences_to_stills=false (preserve oscillation)
        //      - Applies known unit cell and space group if provided in config.
        //   4. `dials.integrate`: Integrates the indexed reflections.
        // - Loads the final integrated.expt and integrated.refl files using DIALS Python API.
        // - Validates that the reflection table contains the required 'partiality' column.
        // - Returns to the original working directory after processing.
        // - Note: The `base_expt_path` parameter is ignored as this adapter always imports from scratch.
        // @raises_error(condition="DIALSError", description="When any DIALS CLI command fails or result files are missing")
        // @raises_error(condition="ConfigurationError", description="When configuration is invalid (missing image file)")
        // @raises_error(condition="DataValidationError", description="When required partiality data is missing from results")
        tuple<object, object, boolean, string> process_still(
            string image_path,
            src.diffusepipe.types.DIALSStillsProcessConfig config,
            optional string base_expt_path,
            optional string output_dir_final
        );
    }
}
// == END IDL ==
</file>

<file path="src/diffusepipe/adapters/dials_stills_process_adapter_IDL.md">
// == BEGIN IDL ==
module src.diffusepipe.adapters {

    # @depends_on_resource(type="ExternalTool:DIALS", purpose="Using dials.stills_process.Processor Python API for true still image processing")
    # @depends_on_resource(type="FileSystem", purpose="Loading PHIL configuration files and managing temporary data")
    # @depends_on_type(src.diffusepipe.types.DIALSStillsProcessConfig)
    # @depends_on_type(dxtbx.model.Experiment)
    # @depends_on_type(dials.array_family.flex.reflection_table)

    interface DIALSStillsProcessAdapter {

        // --- Method: process_still ---
        // Preconditions:
        // - `image_path` must be a valid path to a readable CBF file containing a true still image (Angle_increment = 0.0°).
        // - `config` must be a valid `DIALSStillsProcessConfig` object with appropriate parameters for stills processing.
        // - If provided, `base_expt_path` must be a valid path to a DIALS experiment JSON file.
        // - The DIALS Python environment must be properly configured and accessible.
        // - The CBF file should contain valid crystallographic data suitable for stills processing.
        // Postconditions:
        // - If successful (third tuple element is True), returns valid DIALS Experiment and reflection_table objects.
        // - The Experiment object contains a refined crystal model (Crystal_i) with orientation and unit cell.
        // - The reflection_table contains indexed and integrated Bragg reflections with a 'partiality' column.
        // - The fourth tuple element contains human-readable log messages detailing the processing steps.
        // - If failed (third tuple element is False), the first two elements are None.
        // Behavior:
        // - Wraps the `dials.command_line.stills_process.Processor` Python API for processing true still images.
        // - Generates appropriate PHIL parameters from the provided configuration:
        //   1. Starts with the default dials.stills_process PHIL scope.
        //   2. Merges any user-provided PHIL file specified in config.stills_process_phil_path.
        //   3. Applies configuration overrides (unit cell, space group, spotfinder settings, etc.).
        // - Imports the CBF image (or uses provided base experiment) to create a DIALS ExperimentList.
        // - Instantiates a Processor with the extracted PHIL parameters.
        // - Runs the Processor which internally performs: spot finding, indexing, refinement, and integration.
        // - Extracts the first experiment and its associated reflections from the Processor results.
        // - Validates that the reflection table contains the required 'partiality' column.
        // - Returns the processed data with success status and detailed log messages.
        // @raises_error(condition="DIALSError", description="When DIALS operations fail (import, processing, or result extraction)")
        // @raises_error(condition="ConfigurationError", description="When configuration is invalid (missing files, invalid parameters)")
        // @raises_error(condition="DataValidationError", description="When required partiality data is missing from results")
        tuple<object, object, boolean, string> process_still(
            string image_path,
            src.diffusepipe.types.DIALSStillsProcessConfig config,
            optional string base_expt_path,
            optional string output_dir_final
        );
    }
}
// == END IDL ==
</file>

<file path="src/diffusepipe/adapters/dxtbx_io_adapter.py">
"""Adapter for DXTBX/DIALS file I/O operations."""

import logging
from pathlib import Path
from typing import Union

from diffusepipe.exceptions import FileSystemError, DIALSError

# Imports needed for patching in tests
try:
    from dxtbx.model.experiment_list import ExperimentListFactory, ExperimentListDumper
    from dials.array_family import flex
except ImportError:
    # These imports might fail in testing environments without DIALS
    ExperimentListFactory = None
    ExperimentListDumper = None
    flex = None

logger = logging.getLogger(__name__)


class DXTBXIOAdapter:
    """
    Adapter for standardizing DXTBX/DIALS file I/O operations.

    This adapter provides a consistent interface for loading and saving
    DIALS/DXTBX objects while handling errors appropriately.
    """

    def __init__(self):
        """Initialize the DXTBX I/O adapter."""
        pass

    def load_experiment_list(self, expt_path: Union[str, Path]) -> object:
        """
        Load an ExperimentList from a JSON file.

        Args:
            expt_path: Path to the .expt file

        Returns:
            DIALS ExperimentList object

        Raises:
            FileSystemError: When file operations fail
            DIALSError: When DIALS loading fails
        """
        expt_path = Path(expt_path)

        try:
            if not expt_path.exists():
                raise FileSystemError(f"Experiment file does not exist: {expt_path}")

            if not expt_path.is_file():
                raise FileSystemError(f"Path is not a file: {expt_path}")

            logger.info(f"Loading experiment list from {expt_path}")

            # Use top-level import (supports patching in tests)
            if ExperimentListFactory is None:
                raise DIALSError("Failed to import DXTBX components: ExperimentListFactory not available")

            # Load the experiment list
            experiments = ExperimentListFactory.from_json_file(str(expt_path))

            if len(experiments) == 0:
                logger.warning(f"Loaded empty experiment list from {expt_path}")
            else:
                logger.info(f"Loaded {len(experiments)} experiments from {expt_path}")

            return experiments

        except (FileSystemError, DIALSError):
            raise
        except Exception as e:
            raise FileSystemError(
                f"Failed to load experiment list from {expt_path}: {e}"
            ) from e

    def load_reflection_table(self, refl_path: Union[str, Path]) -> object:
        """
        Load a reflection table from a file.

        Args:
            refl_path: Path to the .refl file

        Returns:
            DIALS reflection_table object

        Raises:
            FileSystemError: When file operations fail
            DIALSError: When DIALS loading fails
        """
        refl_path = Path(refl_path)

        try:
            if not refl_path.exists():
                raise FileSystemError(f"Reflection file does not exist: {refl_path}")

            if not refl_path.is_file():
                raise FileSystemError(f"Path is not a file: {refl_path}")

            logger.info(f"Loading reflection table from {refl_path}")

            # Use top-level import (supports patching in tests)
            if flex is None:
                raise DIALSError("Failed to import DIALS components: flex not available")

            # Load the reflection table
            reflections = flex.reflection_table.from_file(str(refl_path))

            if len(reflections) == 0:
                logger.warning(f"Loaded empty reflection table from {refl_path}")
            else:
                logger.info(f"Loaded {len(reflections)} reflections from {refl_path}")

            return reflections

        except (FileSystemError, DIALSError):
            raise
        except Exception as e:
            raise FileSystemError(
                f"Failed to load reflection table from {refl_path}: {e}"
            ) from e

    def save_experiment_list(
        self, experiments: object, expt_path: Union[str, Path]
    ) -> None:
        """
        Save an ExperimentList to a JSON file.

        Args:
            experiments: DIALS ExperimentList object to save
            expt_path: Path where to save the .expt file

        Raises:
            FileSystemError: When file operations fail
            DIALSError: When DIALS saving fails
        """
        expt_path = Path(expt_path)

        try:
            if experiments is None:
                raise DIALSError("Cannot save None experiment list")

            # Create parent directory if it doesn't exist
            expt_path.parent.mkdir(parents=True, exist_ok=True)

            logger.info(f"Saving experiment list to {expt_path}")

            # Use top-level import (supports patching in tests)
            if ExperimentListDumper is None:
                raise DIALSError("Failed to import DXTBX components: ExperimentListDumper not available")

            # Save the experiment list
            dumper = ExperimentListDumper(experiments)
            dumper.as_json(str(expt_path))

            if not expt_path.exists():
                raise FileSystemError(f"Failed to create experiment file: {expt_path}")

            logger.info(
                f"Successfully saved {len(experiments)} experiments to {expt_path}"
            )

        except (FileSystemError, DIALSError):
            raise
        except Exception as e:
            raise FileSystemError(
                f"Failed to save experiment list to {expt_path}: {e}"
            ) from e

    def save_reflection_table(
        self, reflections: object, refl_path: Union[str, Path]
    ) -> None:
        """
        Save a reflection table to a file.

        Args:
            reflections: DIALS reflection_table object to save
            refl_path: Path where to save the .refl file

        Raises:
            FileSystemError: When file operations fail
            DIALSError: When DIALS saving fails
        """
        refl_path = Path(refl_path)

        try:
            if reflections is None:
                raise DIALSError("Cannot save None reflection table")

            # Create parent directory if it doesn't exist
            refl_path.parent.mkdir(parents=True, exist_ok=True)

            logger.info(f"Saving reflection table to {refl_path}")

            # Save the reflection table
            # This assumes reflections has a as_file method
            if hasattr(reflections, "as_file"):
                reflections.as_file(str(refl_path))
            else:
                raise DIALSError("Reflection table does not support file saving")

            if not refl_path.exists():
                raise FileSystemError(f"Failed to create reflection file: {refl_path}")

            logger.info(
                f"Successfully saved {len(reflections)} reflections to {refl_path}"
            )

        except (FileSystemError, DIALSError):
            raise
        except Exception as e:
            raise FileSystemError(
                f"Failed to save reflection table to {refl_path}: {e}"
            ) from e
</file>

<file path="src/diffusepipe/crystallography/still_processor_and_validator_IDL.md">
// == BEGIN IDL ==
module src.diffusepipe.crystallography {

    # @depends_on([src.diffusepipe.adapters.DIALSStillsProcessAdapter])
    # @depends_on([src.diffusepipe.adapters.DIALSSequenceProcessAdapter])
    # @depends_on([src.diffusepipe.crystallography.ModelValidator])
    # @depends_on([src.diffusepipe.utils.cbf_utils]) // For CBF header parsing
    # @depends_on_resource(type="FileSystem", purpose="Reading CBF files and base experiment files")
    # @depends_on_type(src.diffusepipe.types.DIALSStillsProcessConfig)
    # @depends_on_type(src.diffusepipe.types.ExtractionConfig)
    # @depends_on_type(src.diffusepipe.types.OperationOutcome)
    # @depends_on_type(src.diffusepipe.exceptions.DIALSProcessingError) // For use in @raises_error

    interface StillProcessorAndValidatorComponent {

        // --- Method: _determine_processing_route ---
        // Preconditions:
        // - `image_path` must point to a readable CBF file.
        // - `config` must be a valid DIALSStillsProcessConfig object.
        // Postconditions:
        // - Returns tuple: (processing_route: string, selected_adapter: object).
        // - `processing_route` is either "stills" or "sequence".
        // - `selected_adapter` is the appropriate adapter instance for the determined route.
        // Behavior:
        // - **Implements Module 1.S.0: CBF Data Type Detection and Processing Route Selection.**
        // - **Step 1:** If `config.force_processing_mode` is set ("stills" or "sequence"), uses that override.
        // - **Step 2:** If no override, calls `get_angle_increment_from_cbf(image_path)` to parse CBF header.
        // - **Step 3:** Route selection logic:
        //   - If `Angle_increment == 0.0`: Returns ("stills", DIALSStillsProcessAdapter).
        //   - If `Angle_increment > 0.0`: Returns ("sequence", DIALSSequenceProcessAdapter).
        //   - If `Angle_increment` is negative, null, or parsing fails: Defaults to ("sequence", DIALSSequenceProcessAdapter) as safer option.
        // - **Logging:** Records detection results and routing decisions for debugging.
        tuple<string, object> _determine_processing_route(
            string image_path,                           // Path to CBF image file
            src.diffusepipe.types.DIALSStillsProcessConfig config // Configuration with optional processing mode override
        );

        // --- Method: process_and_validate_still ---
        // Preconditions:
        // - `image_path` must point to a readable CBF file.
        // - `config` and `extraction_config` must be valid configuration objects.
        // - `base_experiment_path` (if provided) must point to a readable DIALS experiment file.
        // - `external_pdb_path` (if provided) must point to a readable PDB file.
        // - `output_dir` (if provided) must be a writable directory path.
        // Postconditions:
        // - Returns an OperationOutcome with status indicating success or specific failure mode.
        // - On success, output_artifacts contains DIALS objects, validation results, and routing information.
        // - On failure, includes appropriate error codes and diagnostic information.
        // Behavior:
        // - **Step 1:** Calls `_determine_processing_route` to select appropriate DIALS adapter (Module 1.S.0).
        // - **Step 2:** Calls selected adapter's processing method to perform DIALS processing (either `process_still` for stills or `process_sequence` for sequences).
        // - **Step 3:** If DIALS processing succeeds, calls `ModelValidator.validate_geometry` with all validation checks.
        // - **Step 4:** Packages results into OperationOutcome with comprehensive output_artifacts:
        //   - `experiment`: DIALS Experiment object
        //   - `reflections`: DIALS reflection_table object  
        //   - `validation_passed`: Boolean result of geometric validation
        //   - `validation_metrics`: Detailed validation metrics from ModelValidator
        //   - `processing_route_used`: String indicating which processing pathway was used
        //   - `log_messages`: Processing logs for debugging
        // - **Status Codes:**
        //   - "SUCCESS": All processing and validation passed
        //   - "FAILURE_DIALS_PROCESSING": DIALS adapter failed
        //   - "FAILURE_GEOMETRY_VALIDATION": DIALS succeeded but validation failed
        // @raises_error(condition="DIALSProcessingError", description="If the selected DIALS adapter raises an unexpected exception during processing.")
        src.diffusepipe.types.OperationOutcome process_and_validate_still(
            string image_path,                                      // Path to CBF image file
            src.diffusepipe.types.DIALSStillsProcessConfig config,  // DIALS processing configuration
            src.diffusepipe.types.ExtractionConfig extraction_config, // Validation tolerance parameters
            optional string base_experiment_path,                   // Optional base experiment geometry
            optional string external_pdb_path,                     // Optional PDB file for validation
            optional string output_dir                              // Optional directory for diagnostic outputs
        );

        // --- Method: process_still (Legacy API) ---
        // Preconditions:
        // - `image_path` must point to a readable CBF file.
        // - `config` must be a valid DIALSStillsProcessConfig object.
        // - `base_experiment_path` (if provided) must point to a readable DIALS experiment file.
        // Postconditions:
        // - Returns an OperationOutcome from DIALS processing only (no validation).
        // - Uses the same routing logic as `process_and_validate_still` but skips validation.
        // Behavior:
        // - **Legacy API:** Provides backward compatibility for callers that only need DIALS processing.
        // - Calls `_determine_processing_route` to select appropriate adapter.
        // - Calls selected adapter's processing method (either `process_still` for stills or `process_sequence` for sequences).
        // - Returns OperationOutcome with DIALS results but without validation metrics.
        // - Maintains same routing logic and error handling as the full method.
        src.diffusepipe.types.OperationOutcome process_still(
            string image_path,                                      // Path to CBF image file
            src.diffusepipe.types.DIALSStillsProcessConfig config,  // DIALS processing configuration
            optional string base_experiment_path                    // Optional base experiment geometry
        );
    }
}
// == END IDL ==
</file>

<file path="src/diffusepipe/diagnostics/__init__.py">
# This file makes src/diffusepipe/diagnostics a Python package.

from .q_calculator import QValueCalculator

__all__ = ["QValueCalculator"]
</file>

<file path="src/diffusepipe/diagnostics/q_calculator_IDL.md">
// == BEGIN IDL ==
module src.diffusepipe.diagnostics {

    # @depends_on_resource(type="FileSystem", purpose="Reading DIALS .expt file; Writing output q-map NumPy files")
    # @depends_on_resource(type="DIALS/dxtbx", purpose="Parsing .expt file for Experiment model")
    # @depends_on_resource(type="cctbx", purpose="Underlying geometric calculations for q-vectors")
    # @depends_on_type(src.diffusepipe.types.ComponentInputFiles) // Specifically for dials_expt_path
    # @depends_on_type(src.diffusepipe.types.OperationOutcome)
    interface QValueCalculator {
        // Preconditions:
        // - `inputs.dials_expt_path` must point to an existing, readable DIALS experiment list JSON file.
        // - The directory implied by `output_prefix_for_q_maps` must be writable.
        // Postconditions:
        // - Three NumPy array files (e.g., `[prefix]_qx.npy`, `[prefix]_qy.npy`, `[prefix]_qz.npy`) are created. Each array has dimensions matching the detector, containing the respective q-component for each pixel.
        // - The returned `OperationOutcome` object details success or failure. `output_artifacts` will map "qx_map_path", "qy_map_path", "qz_map_path" to their file paths.
        // Behavior:
        // 1. Loads the `ExperimentList` from `inputs.dials_expt_path` (typically expecting one Experiment).
        // 2. Gets the `Beam` model and `Detector` model from the `Experiment`.
        // 3. For each `Panel` in the `Detector` model (though typically one for non-multi-panel detectors):
        //    a. Initializes NumPy arrays for $q_x, q_y, q_z$ with dimensions `(panel.get_image_size()[1], panel.get_image_size()[0])`.
        //    b. Iterates through all pixel indices `(slow_scan_idx, fast_scan_idx)` of the panel.
        //    c. For each pixel:
        //       i. Gets the laboratory coordinate of the pixel center using `Panel.get_pixel_lab_coord((fast_scan_idx, slow_scan_idx))`.
        //       ii. Gets the incident beam vector $\mathbf{k}_{in}$ from `Beam.get_s0()` (scaled by $2\pi/\lambda$, where $\lambda$ is `Beam.get_wavelength()`).
        //       iii. Calculates the scattered beam vector $\mathbf{k}_{out}$ from the sample origin (assumed [0,0,0] in lab frame) to the pixel's lab coordinate. This vector is normalized and then scaled by $2\pi/\lambda$.
        //       iv. Computes the scattering vector $\mathbf{q} = \mathbf{k}_{out} - \mathbf{k}_{in}$.
        //       v. Stores the components $q_x, q_y, q_z$ into the respective NumPy arrays at `[slow_scan_idx, fast_scan_idx]`.
        // 4. Saves the $q_x, q_y, q_z$ NumPy arrays to files using `output_prefix_for_q_maps` (e.g., `[prefix]_qx.npy`).
        // 5. Returns `OperationOutcome` with status "SUCCESS" and paths to output files in `output_artifacts`.
        // @raises_error(condition="InputFileError", description="The DIALS .expt file was not found or is unreadable.")
        // @raises_error(condition="DIALSModelError", description="Error parsing the DIALS experiment file, or critical beam/detector information is missing.")
        // @raises_error(condition="CalculationError", description="An unexpected error occurred during q-vector calculations for pixels.")
        // @raises_error(condition="OutputWriteError", description="Failed to write one or more of the output q-map NumPy files.")
        src.diffusepipe.types.OperationOutcome calculate_q_map(
            src.diffusepipe.types.ComponentInputFiles inputs,
            string output_prefix_for_q_maps
        );
    }
}
// == END IDL ==
</file>

<file path="src/diffusepipe/diagnostics/q_calculator.py">
"""Q-vector calculator for diffuse scattering analysis."""

import logging
import os
from typing import Dict, Any
import numpy as np

from diffusepipe.types.types_IDL import ComponentInputFiles, OperationOutcome

logger = logging.getLogger(__name__)


class QValueCalculator:
    """
    Calculator for generating per-pixel q-vector maps from DIALS experiment geometry.
    
    This class implements the q-vector calculation as specified in q_calculator_IDL.md,
    computing q-vectors for each detector pixel based on DIALS experiment geometry.
    """

    def __init__(self):
        """Initialize the QValueCalculator."""
        pass

    def calculate_q_map(
        self, inputs: ComponentInputFiles, output_prefix_for_q_maps: str
    ) -> OperationOutcome:
        """
        Generate q-vector maps for each detector pixel.

        Args:
            inputs: Input files containing DIALS experiment path
            output_prefix_for_q_maps: Prefix for output q-map files

        Returns:
            OperationOutcome with success/failure status and output artifact paths
        """
        try:
            # Validate inputs
            if not inputs.dials_expt_path:
                return OperationOutcome(
                    status="FAILURE",
                    error_code="InputFileError",
                    message="DIALS experiment path not provided in inputs"
                )

            if not os.path.exists(inputs.dials_expt_path):
                return OperationOutcome(
                    status="FAILURE",
                    error_code="InputFileError",
                    message=f"DIALS experiment file not found: {inputs.dials_expt_path}"
                )

            logger.info(f"Loading DIALS experiment from: {inputs.dials_expt_path}")

            # Load DIALS experiment
            experiment_list = self._load_dials_experiment(inputs.dials_expt_path)
            if len(experiment_list) == 0:
                return OperationOutcome(
                    status="FAILURE",
                    error_code="DIALSModelError",
                    message="No experiments found in DIALS experiment file"
                )

            # Use the first experiment
            experiment = experiment_list[0]
            beam_model = experiment.beam
            detector_model = experiment.detector

            logger.info(f"Calculating q-maps for {len(detector_model)} detector panels")

            # Calculate q-vectors for all panels
            output_paths = {}
            
            for panel_idx, panel in enumerate(detector_model):
                logger.info(f"Processing panel {panel_idx}")
                
                qx_map, qy_map, qz_map = self._calculate_panel_q_vectors(
                    beam_model, panel
                )
                
                # Generate output file paths
                if len(detector_model) > 1:
                    # Multi-panel detector - include panel index
                    qx_path = f"{output_prefix_for_q_maps}_panel{panel_idx}_qx.npy"
                    qy_path = f"{output_prefix_for_q_maps}_panel{panel_idx}_qy.npy"
                    qz_path = f"{output_prefix_for_q_maps}_panel{panel_idx}_qz.npy"
                else:
                    # Single panel detector
                    qx_path = f"{output_prefix_for_q_maps}_qx.npy"
                    qy_path = f"{output_prefix_for_q_maps}_qy.npy"
                    qz_path = f"{output_prefix_for_q_maps}_qz.npy"
                
                # Save q-vector maps
                np.save(qx_path, qx_map)
                np.save(qy_path, qy_map)
                np.save(qz_path, qz_map)
                
                # Store paths in output artifacts
                if len(detector_model) > 1:
                    output_paths[f"panel{panel_idx}_qx_map_path"] = qx_path
                    output_paths[f"panel{panel_idx}_qy_map_path"] = qy_path
                    output_paths[f"panel{panel_idx}_qz_map_path"] = qz_path
                else:
                    output_paths["qx_map_path"] = qx_path
                    output_paths["qy_map_path"] = qy_path
                    output_paths["qz_map_path"] = qz_path
                
                logger.info(f"Saved q-maps for panel {panel_idx}: {qx_path}, {qy_path}, {qz_path}")

            return OperationOutcome(
                status="SUCCESS",
                message=f"Successfully generated q-maps for {len(detector_model)} panels",
                output_artifacts=output_paths
            )

        except ImportError as e:
            logger.error(f"Failed to import DIALS/DXTBX modules: {e}")
            return OperationOutcome(
                status="FAILURE",
                error_code="DIALSModelError",
                message=f"Failed to import DIALS/DXTBX modules: {e}"
            )
        except Exception as e:
            logger.error(f"Q-vector calculation failed: {e}")
            if "Input" in str(e) or "not found" in str(e).lower():
                error_code = "InputFileError"
            elif "write" in str(e).lower() or "permission" in str(e).lower():
                error_code = "OutputWriteError"
            elif "calculation" in str(e).lower():
                error_code = "CalculationError"
            else:
                error_code = "DIALSModelError"
            
            return OperationOutcome(
                status="FAILURE",
                error_code=error_code,
                message=f"Q-vector calculation failed: {e}"
            )

    def _load_dials_experiment(self, expt_path: str):
        """
        Load DIALS experiment from file.
        
        Args:
            expt_path: Path to DIALS experiment file
            
        Returns:
            DIALS ExperimentList object
            
        Raises:
            ImportError: If DIALS/DXTBX modules cannot be imported
            Exception: If experiment file cannot be loaded
        """
        try:
            from dxtbx.model import ExperimentList
        except ImportError:
            raise ImportError("Failed to import dxtbx.model.ExperimentList")
        
        try:
            experiment_list = ExperimentList.from_file(expt_path)
            return experiment_list
        except Exception as e:
            raise Exception(f"Failed to load DIALS experiment from {expt_path}: {e}")

    def _calculate_panel_q_vectors(self, beam_model, panel):
        """
        Calculate q-vectors for all pixels in a detector panel.
        
        Args:
            beam_model: DIALS Beam object
            panel: DIALS Panel object
            
        Returns:
            Tuple of (qx_map, qy_map, qz_map) as NumPy arrays
        """
        # Get panel dimensions
        image_size = panel.get_image_size()  # (fast_scan_size, slow_scan_size)
        
        # Initialize q-vector arrays
        # Array dimensions are (slow_scan, fast_scan) to match NumPy convention
        qx_map = np.zeros((image_size[1], image_size[0]), dtype=np.float64)
        qy_map = np.zeros((image_size[1], image_size[0]), dtype=np.float64)
        qz_map = np.zeros((image_size[1], image_size[0]), dtype=np.float64)
        
        # Get beam parameters
        wavelength = beam_model.get_wavelength()  # Angstroms
        k_magnitude = 2 * np.pi / wavelength  # |k| = 2π/λ
        
        # Get incident beam vector (k_in)
        s0 = beam_model.get_s0()  # This is k_in / |k_in|
        k_in = np.array([s0[0], s0[1], s0[2]]) * k_magnitude
        
        logger.debug(f"Panel image size: {image_size}")
        logger.debug(f"Wavelength: {wavelength} Å")
        logger.debug(f"k magnitude: {k_magnitude} Å⁻¹")
        
        # Iterate through all pixels
        for slow_idx in range(image_size[1]):  # slow scan (rows)
            for fast_idx in range(image_size[0]):  # fast scan (columns)
                # Get lab coordinate of pixel center
                lab_coord = panel.get_pixel_lab_coord((fast_idx, slow_idx))
                
                # Calculate scattered beam vector (k_out)
                # Direction from sample origin (0,0,0) to pixel
                scatter_direction = np.array([lab_coord[0], lab_coord[1], lab_coord[2]])
                scatter_direction_norm = scatter_direction / np.linalg.norm(scatter_direction)
                k_out = scatter_direction_norm * k_magnitude
                
                # Calculate scattering vector q = k_out - k_in
                q_vector = k_out - k_in
                
                # Store components
                qx_map[slow_idx, fast_idx] = q_vector[0]
                qy_map[slow_idx, fast_idx] = q_vector[1]
                qz_map[slow_idx, fast_idx] = q_vector[2]
        
        logger.info(f"Calculated q-vectors for {image_size[0]} x {image_size[1]} pixels")
        logger.debug(f"Q-vector ranges: qx=[{qx_map.min():.4f}, {qx_map.max():.4f}], "
                    f"qy=[{qy_map.min():.4f}, {qy_map.max():.4f}], "
                    f"qz=[{qz_map.min():.4f}, {qz_map.max():.4f}] Å⁻¹")
        
        return qx_map, qy_map, qz_map
</file>

<file path="src/diffusepipe/masking/bragg_mask_generator.py">
"""
Bragg mask generation for per-still Bragg peak masking.

This module implements Module 1.S.3 from the plan, providing functions to generate
Bragg peak masks for individual stills and combine them with global pixel masks.
"""

import logging
from typing import Dict, Any, Tuple, Optional, List

from diffusepipe.adapters.dials_generate_mask_adapter import DIALSGenerateMaskAdapter
from diffusepipe.exceptions import BraggMaskError

# Imports needed for patching in tests
try:
    from dials.array_family import flex
    from dials.algorithms.shoebox import MaskCode
except ImportError:
    # These imports might fail in testing environments without DIALS
    flex = None
    MaskCode = None

logger = logging.getLogger(__name__)


class BraggMaskGenerator:
    """
    Generator for Bragg peak masks and combined total masks.

    This class implements the Bragg masking logic described in Module 1.S.3,
    providing both dials.generate_mask and shoebox-based mask generation options.
    """

    def __init__(self):
        """Initialize the Bragg mask generator."""
        self.dials_adapter = DIALSGenerateMaskAdapter()

    def generate_bragg_mask_from_spots(
        self,
        experiment: object,
        reflections: object,
        config: Optional[Dict[str, Any]] = None,
    ) -> Tuple[object, ...]:
        """
        Generate Bragg mask using dials.generate_mask (Option A).

        Args:
            experiment: DIALS Experiment object containing geometry and crystal model
            reflections: DIALS reflection_table containing indexed spots
            config: Optional configuration parameters for mask generation

        Returns:
            Tuple of flex.bool arrays, one per detector panel

        Raises:
            BraggMaskError: When mask generation fails
        """
        try:
            logger.debug("Generating Bragg mask using dials.generate_mask")

            # Use the DIALS adapter to generate the mask
            bragg_mask, success, log_messages = self.dials_adapter.generate_bragg_mask(
                experiment=experiment,
                reflections=reflections,
                mask_generation_params=config,
            )

            if not success:
                raise BraggMaskError(f"DIALS mask generation failed: {log_messages}")

            logger.debug(f"Successfully generated Bragg mask: {log_messages}")
            return bragg_mask

        except Exception as e:
            if isinstance(e, BraggMaskError):
                raise
            else:
                raise BraggMaskError(f"Failed to generate Bragg mask from spots: {e}")

    def generate_bragg_mask_from_shoeboxes(
        self, reflections: object, detector: object
    ) -> Tuple[object, ...]:
        """
        Generate Bragg mask using shoebox data (Option B).

        Args:
            reflections: DIALS reflection_table containing shoeboxes
            detector: DIALS Detector object for panel information

        Returns:
            Tuple of flex.bool arrays, one per detector panel

        Raises:
            BraggMaskError: When mask generation fails
        """
        try:
            logger.debug("Generating Bragg mask using shoebox data")

            # Import DIALS components (delayed import)
            from dials.array_family import flex
            from dials.algorithms.shoebox import MaskCode

            # Initialize per-panel masks to False (no Bragg regions initially)
            panel_masks = []
            for panel in detector:
                panel_size = panel.get_image_size()
                height, width = (
                    panel_size[1],
                    panel_size[0],
                )  # (fast, slow) -> (slow, fast)
                panel_mask = flex.bool(flex.grid(height, width), False)
                panel_masks.append(panel_mask)

            # Check if reflections have shoeboxes
            if not reflections or len(reflections) == 0:
                logger.warning("No reflections provided for shoebox-based masking")
                return tuple(panel_masks)

            if not reflections.has_key("shoebox"):
                raise BraggMaskError(
                    "Reflection table missing required 'shoebox' column"
                )

            logger.debug(f"Processing {len(reflections)} reflections with shoeboxes")

            # Iterate through reflections and extract shoebox masks
            total_masked_pixels = 0
            for ref_idx, reflection in enumerate(reflections):
                try:
                    self._process_reflection_shoebox(
                        reflection, ref_idx, panel_masks, total_masked_pixels
                    )
                except Exception as e:
                    logger.warning(f"Failed to process reflection {ref_idx}: {e}")
                    continue

            logger.info(
                f"Generated Bragg mask from shoeboxes, "
                f"masked {total_masked_pixels} total pixels"
            )

            return tuple(panel_masks)

        except Exception as e:
            if isinstance(e, BraggMaskError):
                raise
            else:
                raise BraggMaskError(
                    f"Failed to generate Bragg mask from shoeboxes: {e}"
                )

    def get_total_mask_for_still(
        self, bragg_mask: Tuple[object, ...], global_pixel_mask: Tuple[object, ...]
    ) -> Tuple[object, ...]:
        """
        Combine Bragg mask with global pixel mask to create total mask.

        Args:
            bragg_mask: Tuple of Bragg masks (one per panel)
            global_pixel_mask: Tuple of global pixel masks (one per panel)

        Returns:
            Tuple of combined total masks (one per panel)

        Behavior:
            Mask_total_2D_i(px,py) = Mask_pixel(px,py) AND (NOT BraggMask_2D_raw_i(px,py))

        Raises:
            BraggMaskError: When mask combination fails
        """
        try:
            logger.debug("Combining Bragg mask with global pixel mask")

            if len(bragg_mask) != len(global_pixel_mask):
                raise BraggMaskError(
                    f"Bragg mask and global pixel mask have different panel counts: "
                    f"{len(bragg_mask)} vs {len(global_pixel_mask)}"
                )

            total_masks = []
            for panel_idx, (bragg_panel, pixel_panel) in enumerate(
                zip(bragg_mask, global_pixel_mask)
            ):
                # Apply the combination logic: pixel_mask AND (NOT bragg_mask)
                inverted_bragg = ~bragg_panel
                total_panel_mask = pixel_panel & inverted_bragg
                total_masks.append(total_panel_mask)

                # Log statistics
                pixel_good = pixel_panel.count(True)
                bragg_masked = bragg_panel.count(True)
                total_good = total_panel_mask.count(True)
                total_pixels = len(total_panel_mask)

                logger.debug(
                    f"Panel {panel_idx}: Pixel={pixel_good}/{total_pixels}, "
                    f"Bragg_masked={bragg_masked}/{total_pixels}, "
                    f"Total_good={total_good}/{total_pixels}"
                )

            logger.info(f"Generated total masks for {len(total_masks)} panels")
            return tuple(total_masks)

        except Exception as e:
            if isinstance(e, BraggMaskError):
                raise
            else:
                raise BraggMaskError(f"Failed to combine masks: {e}")

    def _process_reflection_shoebox(
        self,
        reflection: object,
        ref_idx: int,
        panel_masks: List[object],
        total_masked_pixels: int,
    ) -> int:
        """
        Process a single reflection's shoebox and update panel masks.

        Args:
            reflection: Individual reflection from the reflection table
            ref_idx: Index of the reflection for logging
            panel_masks: List of panel masks to update
            total_masked_pixels: Running count of masked pixels

        Returns:
            Updated count of total masked pixels

        Raises:
            Exception: When shoebox processing fails
        """
        from dials.algorithms.shoebox import MaskCode

        shoebox = reflection["shoebox"]
        panel_id = reflection.get("panel", 0)  # Default to panel 0 if not specified

        if panel_id >= len(panel_masks):
            raise Exception(f"Panel ID {panel_id} exceeds available panels")

        # Get shoebox mask and data
        shoebox_mask = shoebox.mask
        shoebox_bbox = shoebox.bbox

        # Extract 3D coordinates (panels, slow, fast)
        z1, y1, x1, z2, y2, x2 = shoebox_bbox

        # Iterate through shoebox voxels
        masked_pixels_this_ref = 0
        for z in range(z1, z2):
            for y in range(y1, y2):
                for x in range(x1, x2):
                    # Calculate index in the flattened shoebox mask
                    mask_idx = (
                        (z - z1) * (y2 - y1) * (x2 - x1)
                        + (y - y1) * (x2 - x1)
                        + (x - x1)
                    )

                    if mask_idx >= len(shoebox_mask):
                        continue

                    # Check if this voxel is marked as foreground or strong
                    mask_code = shoebox_mask[mask_idx]
                    if mask_code & MaskCode.Foreground or mask_code & MaskCode.Strong:

                        # Project to 2D panel coordinates
                        # For stills, z should correspond to panel_id
                        if (
                            z == panel_id
                            and 0 <= y < len(panel_masks[panel_id])
                            and 0 <= x < len(panel_masks[panel_id][0])
                        ):
                            # Set this pixel as masked (True in Bragg mask means masked)
                            panel_masks[panel_id][y, x] = True
                            masked_pixels_this_ref += 1

        total_masked_pixels += masked_pixels_this_ref
        logger.debug(f"Reflection {ref_idx}: masked {masked_pixels_this_ref} pixels")

        return total_masked_pixels


# Convenience functions for mask configuration


def create_default_bragg_mask_config() -> Dict[str, Any]:
    """
    Create default configuration for dials.generate_mask.

    Returns:
        Dictionary with default mask generation parameters
    """
    return {
        "border": 2,  # Border around each reflection
        "algorithm": "simple",  # Simple mask algorithm
    }


def create_expanded_bragg_mask_config(border: int = 3) -> Dict[str, Any]:
    """
    Create expanded configuration for dials.generate_mask with larger borders.

    Args:
        border: Size of border around each reflection

    Returns:
        Dictionary with expanded mask generation parameters
    """
    return {"border": border, "algorithm": "simple"}


def validate_mask_compatibility(
    bragg_mask: Tuple[object, ...], pixel_mask: Tuple[object, ...]
) -> bool:
    """
    Validate that Bragg and pixel masks are compatible for combination.

    Args:
        bragg_mask: Tuple of Bragg masks
        pixel_mask: Tuple of pixel masks

    Returns:
        True if masks are compatible, False otherwise
    """
    try:
        # Check panel count
        if len(bragg_mask) != len(pixel_mask):
            logger.error(
                f"Panel count mismatch: {len(bragg_mask)} vs {len(pixel_mask)}"
            )
            return False

        # Check dimensions for each panel
        for panel_idx, (bragg_panel, pixel_panel) in enumerate(
            zip(bragg_mask, pixel_mask)
        ):
            if len(bragg_panel) != len(pixel_panel):
                logger.error(
                    f"Panel {panel_idx} size mismatch: "
                    f"{len(bragg_panel)} vs {len(pixel_panel)}"
                )
                return False

        return True

    except Exception as e:
        logger.error(f"Error validating mask compatibility: {e}")
        return False
</file>

<file path="src/diffusepipe/merging/merger.py">
"""
DiffuseDataMerger implementation for Phase 3 merging.

Applies refined scaling and merges observations into final voxel data.
"""

import numpy as np
import logging
from typing import Dict, List, Tuple, Any
from dataclasses import dataclass

from diffusepipe.voxelization.global_voxel_grid import GlobalVoxelGrid
from diffusepipe.scaling.diffuse_scaling_model import DiffuseScalingModel

logger = logging.getLogger(__name__)


@dataclass
class VoxelDataRelative:
    """Relatively-scaled voxel data structure."""
    voxel_indices: np.ndarray
    H_center: np.ndarray
    K_center: np.ndarray
    L_center: np.ndarray
    q_center_x: np.ndarray
    q_center_y: np.ndarray
    q_center_z: np.ndarray
    q_magnitude_center: np.ndarray
    I_merged_relative: np.ndarray
    Sigma_merged_relative: np.ndarray
    num_observations: np.ndarray


class DiffuseDataMerger:
    """
    Applies refined scaling and merges diffuse observations into voxels.
    
    Handles scaling application, error propagation, and weighted merging
    for the final relatively-scaled 3D diffuse scattering map.
    """
    
    def __init__(self, global_voxel_grid: GlobalVoxelGrid):
        """
        Initialize merger with voxel grid.
        
        Args:
            global_voxel_grid: Grid definition for coordinate calculations
        """
        self.global_voxel_grid = global_voxel_grid
        logger.info("DiffuseDataMerger initialized")
    
    def merge_scaled_data(self, 
                         binned_pixel_data: Dict,
                         scaling_model: DiffuseScalingModel,
                         merge_config: Dict) -> VoxelDataRelative:
        """
        Apply scaling and merge observations into final voxel data.
        
        Args:
            binned_pixel_data: Voxel-organized observations
            scaling_model: Refined scaling model from Module 3.S.3
            merge_config: Merging configuration
            
        Returns:
            VoxelDataRelative with merged, relatively-scaled data
        """
        logger.info(f"Merging data for {len(binned_pixel_data)} voxels")
        
        # Configuration
        outlier_rejection = merge_config.get('outlier_rejection', {'enabled': False})
        min_observations = merge_config.get('minimum_observations', 1)
        weight_method = merge_config.get('weight_method', 'inverse_variance')
        
        # Storage for merged results
        merged_voxel_indices = []
        merged_intensities = []
        merged_sigmas = []
        merged_obs_counts = []
        
        processed_voxels = 0
        skipped_voxels = 0
        
        for voxel_idx, voxel_data in binned_pixel_data.items():
            observations = voxel_data['observations']
            
            if len(observations) < min_observations:
                skipped_voxels += 1
                continue
            
            # Apply scaling to all observations in this voxel
            scaled_observations = []
            
            for obs in observations:
                try:
                    scaled_intensity, scaled_sigma = self.apply_scaling_to_observation(
                        obs, scaling_model
                    )
                    scaled_observations.append((scaled_intensity, scaled_sigma))
                except Exception as e:
                    logger.warning(f"Error scaling observation in voxel {voxel_idx}: {e}")
                    continue
            
            if len(scaled_observations) < min_observations:
                skipped_voxels += 1
                continue
            
            # Apply outlier rejection if enabled
            if outlier_rejection.get('enabled', False):
                scaled_observations = self._reject_outliers(
                    scaled_observations, 
                    outlier_rejection.get('sigma_threshold', 3.0)
                )
                
                if len(scaled_observations) < min_observations:
                    skipped_voxels += 1
                    continue
            
            # Perform weighted merge for this voxel
            merged_intensity, merged_sigma, n_obs = self.weighted_merge_voxel(
                scaled_observations, weight_method
            )
            
            # Store results
            merged_voxel_indices.append(voxel_idx)
            merged_intensities.append(merged_intensity)
            merged_sigmas.append(merged_sigma)
            merged_obs_counts.append(n_obs)
            
            processed_voxels += 1
        
        logger.info(f"Merged {processed_voxels} voxels, skipped {skipped_voxels}")
        
        if processed_voxels == 0:
            logger.warning("No voxels passed merging criteria")
            return self._create_empty_voxel_data()
        
        # Calculate voxel coordinates
        coordinates = self.calculate_voxel_coordinates(merged_voxel_indices)
        
        # Create final data structure
        voxel_data = VoxelDataRelative(
            voxel_indices=np.array(merged_voxel_indices),
            H_center=coordinates['H_center'],
            K_center=coordinates['K_center'], 
            L_center=coordinates['L_center'],
            q_center_x=coordinates['q_center_x'],
            q_center_y=coordinates['q_center_y'],
            q_center_z=coordinates['q_center_z'],
            q_magnitude_center=coordinates['q_magnitude_center'],
            I_merged_relative=np.array(merged_intensities),
            Sigma_merged_relative=np.array(merged_sigmas),
            num_observations=np.array(merged_obs_counts)
        )
        
        logger.info(f"Final merged dataset: {len(voxel_data.voxel_indices)} voxels")
        return voxel_data
    
    def apply_scaling_to_observation(self,
                                   observation: Dict,
                                   scaling_model: DiffuseScalingModel) -> Tuple[float, float]:
        """
        Apply scaling to a single observation.
        
        Args:
            observation: Observation data
            scaling_model: Refined scaling model
            
        Returns:
            Tuple of (scaled_intensity, scaled_sigma)
        """
        # Extract observation properties
        intensity = observation['intensity']
        sigma = observation['sigma']
        still_id = observation['still_id']
        q_vector_lab = observation['q_vector_lab']
        
        # Calculate q-magnitude
        q_magnitude = np.linalg.norm(q_vector_lab)
        
        # Get scaling parameters from model
        multiplicative_scale, additive_offset = scaling_model.get_scales_for_observation(
            still_id, q_magnitude
        )
        
        # Verify v1 model constraints (additive offset should be ~0)
        if abs(additive_offset) > 1e-9:
            logger.warning(f"v1 model violation: additive_offset={additive_offset:.2e} "
                         f"(should be ~0)")
        
        # Apply v1 scaling: I_final = (I_obs - C_i) / M_i
        # For v1: C_i ≈ 0, so I_final = I_obs / M_i
        scaled_intensity = (intensity - additive_offset) / multiplicative_scale
        
        # Propagate uncertainty: Sigma_final = Sigma_obs / |M_i|
        # Note: This is simplified for v1 where C_i has no uncertainty
        scaled_sigma = sigma / abs(multiplicative_scale)
        
        return scaled_intensity, scaled_sigma
    
    def weighted_merge_voxel(self,
                           scaled_observations: List[Tuple[float, float]],
                           weight_method: str = "inverse_variance") -> Tuple[float, float, int]:
        """
        Perform weighted merge of observations within a voxel.
        
        Args:
            scaled_observations: List of (intensity, sigma) tuples
            weight_method: Weighting method for merging
            
        Returns:
            Tuple of (merged_intensity, merged_sigma, n_observations)
        """
        if not scaled_observations:
            raise ValueError("No observations to merge")
        
        # Validate weight method first
        if weight_method not in ["inverse_variance", "uniform"]:
            raise ValueError(f"Unknown weight method: {weight_method}")
        
        n_obs = len(scaled_observations)
        
        if n_obs == 1:
            # Single observation - no merging needed
            intensity, sigma = scaled_observations[0]
            return intensity, sigma, 1
        
        intensities = np.array([obs[0] for obs in scaled_observations])
        sigmas = np.array([obs[1] for obs in scaled_observations])
        
        # Calculate weights
        if weight_method == "inverse_variance":
            # Weights = 1 / sigma^2
            variances = sigmas**2
            weights = 1.0 / (variances + 1e-10)  # Add small value to avoid division by zero
        elif weight_method == "uniform":
            # Equal weights
            weights = np.ones(n_obs)
        
        # Handle edge cases
        if np.sum(weights) <= 0:
            logger.warning("All weights are zero or negative, using uniform weights")
            weights = np.ones(n_obs)
        
        # Weighted average
        total_weight = np.sum(weights)
        weighted_intensities = intensities * weights
        merged_intensity = np.sum(weighted_intensities) / total_weight
        
        # Error propagation for weighted average
        if weight_method == "inverse_variance":
            # For inverse variance weighting: sigma_merged = 1 / sqrt(sum(weights))
            merged_sigma = 1.0 / np.sqrt(total_weight)
        else:
            # For uniform weighting: sigma_merged = sqrt(sum(sigma_i^2)) / N
            merged_sigma = np.sqrt(np.sum(sigmas**2)) / n_obs
        
        return merged_intensity, merged_sigma, n_obs
    
    def _reject_outliers(self, 
                        scaled_observations: List[Tuple[float, float]], 
                        sigma_threshold: float) -> List[Tuple[float, float]]:
        """Reject outlier observations based on sigma threshold."""
        if len(scaled_observations) <= 2:
            return scaled_observations  # Can't reject from very small samples
        
        intensities = np.array([obs[0] for obs in scaled_observations])
        
        # Calculate robust statistics
        median_intensity = np.median(intensities)
        mad = np.median(np.abs(intensities - median_intensity))  # Median absolute deviation
        robust_std = 1.4826 * mad  # Scale factor for normal distribution
        
        # Filter observations
        filtered_observations = []
        for obs in scaled_observations:
            intensity = obs[0]
            deviation = abs(intensity - median_intensity)
            
            if deviation <= sigma_threshold * robust_std:
                filtered_observations.append(obs)
        
        if len(filtered_observations) == 0:
            logger.warning("All observations rejected as outliers, keeping original")
            return scaled_observations
        
        n_rejected = len(scaled_observations) - len(filtered_observations)
        if n_rejected > 0:
            logger.debug(f"Rejected {n_rejected} outlier observations")
        
        return filtered_observations
    
    def calculate_voxel_coordinates(self, voxel_indices: List[int]) -> Dict[str, np.ndarray]:
        """
        Calculate coordinate arrays for voxel centers.
        
        Args:
            voxel_indices: List of voxel indices
            
        Returns:
            Dictionary of coordinate arrays
        """
        n_voxels = len(voxel_indices)
        
        # Initialize coordinate arrays
        H_centers = np.zeros(n_voxels)
        K_centers = np.zeros(n_voxels)
        L_centers = np.zeros(n_voxels)
        q_x_centers = np.zeros(n_voxels)
        q_y_centers = np.zeros(n_voxels)
        q_z_centers = np.zeros(n_voxels)
        q_magnitudes = np.zeros(n_voxels)
        
        for i, voxel_idx in enumerate(voxel_indices):
            # Get HKL center coordinates
            h, k, l = self.global_voxel_grid.voxel_idx_to_hkl_center(voxel_idx)
            H_centers[i] = h
            K_centers[i] = k
            L_centers[i] = l
            
            # Get lab-frame q-vector for center
            q_center = self.global_voxel_grid.get_q_vector_for_voxel_center(voxel_idx)
            q_x_centers[i] = q_center.elems[0]
            q_y_centers[i] = q_center.elems[1]
            q_z_centers[i] = q_center.elems[2]
            q_magnitudes[i] = q_center.length()
        
        return {
            'H_center': H_centers,
            'K_center': K_centers,
            'L_center': L_centers,
            'q_center_x': q_x_centers,
            'q_center_y': q_y_centers,
            'q_center_z': q_z_centers,
            'q_magnitude_center': q_magnitudes
        }
    
    def get_merge_statistics(self, voxel_data_relative: VoxelDataRelative) -> Dict:
        """
        Calculate comprehensive statistics about merged data.
        
        Args:
            voxel_data_relative: Merged voxel data
            
        Returns:
            Dictionary of statistics
        """
        intensities = voxel_data_relative.I_merged_relative
        sigmas = voxel_data_relative.Sigma_merged_relative
        obs_counts = voxel_data_relative.num_observations
        q_magnitudes = voxel_data_relative.q_magnitude_center
        
        # Basic statistics
        total_voxels = len(intensities)
        total_observations = np.sum(obs_counts)
        
        # Intensity statistics
        intensity_stats = {
            'mean': float(np.mean(intensities)),
            'std': float(np.std(intensities)),
            'min': float(np.min(intensities)),
            'max': float(np.max(intensities)),
            'median': float(np.median(intensities))
        }
        
        # Observation statistics
        obs_stats = {
            'mean_per_voxel': float(np.mean(obs_counts)),
            'total_observations': int(total_observations),
            'voxels_with_single_obs': int(np.sum(obs_counts == 1)),
            'max_observations_per_voxel': int(np.max(obs_counts))
        }
        
        # Resolution coverage
        resolution_stats = {
            'q_min': float(np.min(q_magnitudes)),
            'q_max': float(np.max(q_magnitudes)),
            'mean_q': float(np.mean(q_magnitudes))
        }
        
        # Data quality metrics
        sigma_over_intensity = sigmas / (intensities + 1e-10)  # Relative uncertainty
        high_intensity_threshold = intensity_stats['mean'] + 2 * intensity_stats['std']
        good_precision_threshold = 0.1  # 10% relative uncertainty
        
        quality_stats = {
            'mean_sigma_over_intensity': float(np.mean(sigma_over_intensity)),
            'high_intensity_voxels': int(np.sum(intensities > high_intensity_threshold)),
            'low_sigma_voxels': int(np.sum(sigma_over_intensity < good_precision_threshold))
        }
        
        return {
            'total_voxels': total_voxels,
            'intensity_statistics': intensity_stats,
            'observation_statistics': obs_stats,
            'resolution_coverage': resolution_stats,
            'data_quality': quality_stats
        }
    
    def _create_empty_voxel_data(self) -> VoxelDataRelative:
        """Create empty VoxelDataRelative for edge cases."""
        return VoxelDataRelative(
            voxel_indices=np.array([]),
            H_center=np.array([]),
            K_center=np.array([]),
            L_center=np.array([]),
            q_center_x=np.array([]),
            q_center_y=np.array([]),
            q_center_z=np.array([]),
            q_magnitude_center=np.array([]),
            I_merged_relative=np.array([]),
            Sigma_merged_relative=np.array([]),
            num_observations=np.array([])
        )
    
    def save_voxel_data(self, 
                       voxel_data: VoxelDataRelative, 
                       output_path: str,
                       format: str = "npz"):
        """
        Save merged voxel data to file.
        
        Args:
            voxel_data: Merged data to save
            output_path: Output file path
            format: Output format ("npz" or "hdf5")
        """
        if format == "npz":
            np.savez_compressed(
                output_path,
                voxel_indices=voxel_data.voxel_indices,
                H_center=voxel_data.H_center,
                K_center=voxel_data.K_center,
                L_center=voxel_data.L_center,
                q_center_x=voxel_data.q_center_x,
                q_center_y=voxel_data.q_center_y,
                q_center_z=voxel_data.q_center_z,
                q_magnitude_center=voxel_data.q_magnitude_center,
                I_merged_relative=voxel_data.I_merged_relative,
                Sigma_merged_relative=voxel_data.Sigma_merged_relative,
                num_observations=voxel_data.num_observations
            )
            logger.info(f"Saved voxel data to {output_path}")
        else:
            raise ValueError(f"Unsupported format: {format}")
</file>

<file path="src/diffusepipe/orchestration/__init__.py">
"""
Orchestration module for DiffusePipe.

Provides high-level orchestration of the diffuse scattering processing pipeline.
"""

from .stills_pipeline_orchestrator import StillsPipelineOrchestrator

__all__ = ['StillsPipelineOrchestrator']
</file>

<file path="src/diffusepipe/scaling/components/resolution_smoother.py">
"""
ResolutionSmootherComponent for diffuse scaling model.

Implements 1D Gaussian smoother for resolution-dependent multiplicative scaling.
"""

import logging
import numpy as np
from typing import Tuple

from dials.algorithms.scaling.model.components.scale_components import ScaleComponentBase
from dials.algorithms.scaling.model.components.smooth_scale_components import GaussianSmoother1D
from dials.array_family import flex

logger = logging.getLogger(__name__)


class ResolutionSmootherComponent(ScaleComponentBase):
    """
    1D Gaussian smoother for resolution-dependent multiplicative scaling.
    
    Provides smooth multiplicative corrections as a function of |q| (resolution).
    Limited to ≤5 control points in v1 implementation.
    """
    
    def __init__(self,
                 active_parameter_manager,
                 n_control_points: int,
                 resolution_range: Tuple[float, float]):
        """
        Initialize resolution smoother component.
        
        Args:
            active_parameter_manager: DIALS parameter manager for refinement
            n_control_points: Number of control points (≤5 for v1)
            resolution_range: (q_min, q_max) for smoother domain
        """
        # Validate v1 constraints
        if n_control_points > 5:
            raise ValueError(f"v1 model allows ≤5 control points, got {n_control_points}")
        
        q_min, q_max = resolution_range
        if q_min >= q_max:
            raise ValueError("Invalid resolution range: q_min must be < q_max")
        
        self.n_control_points = n_control_points
        self.q_min = q_min
        self.q_max = q_max
        self.q_range = (q_min, q_max)
        
        # Create 1D Gaussian smoother
        self.smoother = GaussianSmoother1D(
            x_range=self.q_range,
            num_intervals=n_control_points - 1  # num_intervals is one less than control points
        )
        
        # Initialize control points to unity (no correction)
        initial_values = flex.double([1.0] * n_control_points)
        
        # Initialize base class with parameter values
        super().__init__(initial_values)
        
        # Store parameter manager reference
        self.active_parameter_manager = active_parameter_manager
        
        logger.info(f"ResolutionSmootherComponent initialized: {n_control_points} points, "
                   f"q_range=({q_min:.4f}, {q_max:.4f})")
    
    @property
    def n_params(self) -> int:
        """Number of parameters in this component."""
        return self.n_control_points
    
    @property
    def parameters(self) -> flex.double:
        """Current parameter values from the parameter manager."""
        # Use the parent class parameters directly
        return self._parameters
    
    def calculate_scales_and_derivatives(self, 
                                       reflection_table,
                                       block_id=None):
        """
        Calculate resolution-dependent scale factors and derivatives.
        
        Args:
            reflection_table: Data structure containing q-vectors or d-spacing
            block_id: Unused for this component
            
        Returns:
            tuple: (scales, derivatives) as flex arrays
        """
        # Extract q-magnitudes from different data formats
        q_magnitudes = self._extract_q_magnitudes(reflection_table)
        n_reflections = len(q_magnitudes)
        
        if n_reflections == 0:
            return flex.double(), flex.double()
        
        # Update smoother with current parameters
        current_params = self.parameters
# Parameter setting handled by parent class
        
        # Evaluate smoother at q-values
        q_locations = flex.double(q_magnitudes)
        
        # Clamp q-values to smoother range to avoid extrapolation
        q_clamped = flex.double()
        for q in q_locations:
            q_clamp = max(self.q_min, min(self.q_max, q))
            q_clamped.append(q_clamp)
        
        # Evaluate smoother - need to provide current parameter values
        current_params = self.parameters
        result = self.smoother.multi_value_weight(q_clamped, current_params)
        scales = result[0]  # The interpolated values
        derivatives_matrix = result[1]  # Derivatives matrix (not used here)
        # Note: derivatives matrix would be used for refinement
        
        # For multiplicative scaling, we want positive values
        # Apply softplus-like transformation to ensure positivity
        scales_positive = flex.double()
        for scale in scales:
            # Use exp to ensure positivity while allowing the smoother 
            # to work in log space around 1.0
            positive_scale = max(0.01, float(scale))  # Minimum scale
            scales_positive.append(positive_scale)
        
        # Return scales and derivatives (derivatives matrix not implemented for now)
        # For DIALS compatibility, we return a flex.double for derivatives
        derivatives_dummy = flex.double(len(scales_positive))
        
        return scales_positive, derivatives_dummy
    
    def _extract_q_magnitudes(self, reflection_table):
        """
        Extract q-magnitudes from various data formats.
        
        Args:
            reflection_table: Data structure with q-vectors or d-spacing
            
        Returns:
            List of q-magnitudes
        """
        q_magnitudes = []
        
        if isinstance(reflection_table, dict) and ('q_vectors_lab' in reflection_table or 'q_magnitudes' in reflection_table):
            # Diffuse data dictionary format
            if 'q_vectors_lab' in reflection_table:
                q_vectors = reflection_table['q_vectors_lab']
                if len(q_vectors.shape) == 2 and q_vectors.shape[1] == 3:
                    q_magnitudes = np.linalg.norm(q_vectors, axis=1)
                else:
                    raise ValueError("Invalid q_vectors shape")
            elif 'q_magnitudes' in reflection_table:
                q_magnitudes = reflection_table['q_magnitudes']
                
        elif hasattr(reflection_table, 'get'):
            # DIALS flex.reflection_table
            if 'q_vector' in reflection_table:
                q_vectors = reflection_table.get('q_vector')
                q_magnitudes = [q.length() for q in q_vectors]
            elif 'd' in reflection_table:
                d_spacings = reflection_table.get('d')
                q_magnitudes = [1.0/d if d > 0 else 0.0 for d in d_spacings]
            else:
                raise ValueError("No q-vector data in diffuse data dictionary")
        else:
            raise ValueError("Unsupported reflection_table format")
        
        return q_magnitudes
    
    def get_scale_for_q(self, q_magnitude: float) -> float:
        """
        Get resolution-dependent scale factor for a specific q-value.
        
        Args:
            q_magnitude: Magnitude of scattering vector
            
        Returns:
            Multiplicative scale factor
        """
        if q_magnitude <= 0:
            return 1.0
        
        # Clamp to smoother range
        q_clamp = max(self.q_min, min(self.q_max, q_magnitude))
        
        # Evaluate at single point using value_weight
        current_params = self.parameters
        result = self.smoother.value_weight(q_clamp, current_params)
        scale_value = result[0]  # Single interpolated value
        
        # Ensure positive scale
        scale = max(0.01, float(scale_value))
        return scale
    
    def get_component_info(self) -> dict:
        """
        Get information about this component.
        
        Returns:
            Dictionary with component details
        """
        current_params = self.parameters
        
        # Sample smoother across range for statistics
        n_samples = 20
        q_samples = np.linspace(self.q_min, self.q_max, n_samples)
        scale_samples = [self.get_scale_for_q(q) for q in q_samples]
        
        return {
            "component_type": "ResolutionSmoother",
            "n_parameters": self.n_params,
            "n_control_points": self.n_control_points,
            "q_range": self.q_range,
            "control_point_values": [float(p) for p in current_params],
            "scale_statistics": {
                "mean": np.mean(scale_samples),
                "std": np.std(scale_samples),
                "min": np.min(scale_samples),
                "max": np.max(scale_samples)
            }
        }
    
    def update_reflection_data(self, reflection_table):
        """
        Update component state based on reflection data.
        
        This component doesn't need to update state based on reflections.
        """
        pass
    
    def __str__(self) -> str:
        """String representation of component."""
        info = self.get_component_info()
        stats = info["scale_statistics"]
        return (f"ResolutionSmootherComponent({self.n_control_points} points, "
                f"scales: {stats['mean']:.3f}±{stats['std']:.3f})")
    
    def __repr__(self) -> str:
        """Detailed string representation."""
        return (f"ResolutionSmootherComponent(n_control_points={self.n_control_points}, "
                f"q_range=({self.q_min:.4f}, {self.q_max:.4f}))")
</file>

<file path="src/diffusepipe/utils/__init__.py">
"""Utility functions and classes for the diffusepipe package."""
</file>

<file path="src/diffusepipe/constants.py">
"""Constants used throughout the diffusepipe package."""

# Version information
__version__ = "0.1.0"

# Default values
DEFAULT_PIXEL_STEP = 1
DEFAULT_GAIN = 1.0
DEFAULT_MIN_PARTIALITY_THRESHOLD = 0.1

# Status codes
STATUS_SUCCESS = "SUCCESS"
STATUS_FAILURE = "FAILURE"
STATUS_WARNING = "WARNING"

# Pipeline status codes
STATUS_SUCCESS_ALL = "SUCCESS_ALL"
STATUS_SUCCESS_DIALS_ONLY = "SUCCESS_DIALS_ONLY"
STATUS_SUCCESS_EXTRACTION_ONLY = "SUCCESS_EXTRACTION_ONLY"
STATUS_FAILURE_DIALS = "FAILURE_DIALS"
STATUS_FAILURE_EXTRACTION = "FAILURE_EXTRACTION"
STATUS_FAILURE_DIAGNOSTICS = "FAILURE_DIAGNOSTICS"
</file>

<file path="src/diffusepipe/corrections.py">
"""
Corrections helper module for diffuse scattering data processing.

This module provides helper functions for applying and testing pixel correction factors
as specified in Module 2.S.2 of the plan. It centralizes correction logic and provides
regression test capabilities.
"""

import numpy as np
import logging
from typing import Tuple

logger = logging.getLogger(__name__)


def apply_corrections(
    raw_I: float, lp_mult: float, qe_mult: float, sa_mult: float, air_mult: float
) -> float:
    """
    Apply all pixel correction factors to raw intensity.

    This helper function centralizes the correction logic and ensures
    all corrections are applied as multipliers as per Module 0.7.

    Args:
        raw_I: Raw intensity value
        lp_mult: Lorentz-Polarization correction multiplier (1/LP_divisor)
        qe_mult: Quantum Efficiency correction multiplier
        sa_mult: Solid Angle correction multiplier (1/solid_angle)
        air_mult: Air Attenuation correction multiplier (1/attenuation)

    Returns:
        Corrected intensity value
    """
    total_correction_mult = lp_mult * qe_mult * sa_mult * air_mult
    corrected_I = raw_I * total_correction_mult

    logger.debug(
        f"Corrections applied: LP={lp_mult:.4f}, QE={qe_mult:.4f}, "
        f"SA={sa_mult:.4f}, Air={air_mult:.4f}, Total={total_correction_mult:.4f}"
    )

    return corrected_I


def calculate_analytic_pixel_corrections_45deg(
    raw_intensity: float,
    wavelength_angstrom: float,
    detector_distance_mm: float,
    pixel_size_mm: float,
    air_path_length_mm: float = None,
) -> Tuple[float, dict]:
    """
    Calculate analytic correction factors for a pixel at 45° scattering angle.

    This function provides known analytic values for regression testing
    of the correction pipeline as required by Module 2.S.2.

    Args:
        raw_intensity: Raw pixel intensity
        wavelength_angstrom: X-ray wavelength in Angstroms
        detector_distance_mm: Sample-to-detector distance in mm
        pixel_size_mm: Pixel size in mm (assumed square)
        air_path_length_mm: Air path length in mm (optional)

    Returns:
        Tuple of (corrected_intensity, correction_factors_dict)
    """

    # For 45° scattering angle
    theta = np.pi / 4  # 45 degrees
    sin_theta = np.sin(theta)
    cos_theta = np.cos(theta)

    # 1. Lorentz-Polarization correction for 45° scattering
    # LP = 1 / (sin²θ * (1 + cos²(2θ)) for unpolarized X-rays
    sin2_theta = sin_theta**2
    cos_2theta = np.cos(2 * theta)
    lp_divisor = sin2_theta * (1 + cos_2theta**2)
    lp_mult = 1.0 / lp_divisor

    # 2. Quantum Efficiency correction (assume ideal detector)
    qe_mult = 1.0

    # 3. Solid Angle correction
    # For 45° scattering, pixel is at distance d/cos(45°) from sample
    actual_distance = detector_distance_mm / cos_theta
    pixel_area = pixel_size_mm**2
    solid_angle = pixel_area / (actual_distance**2)
    sa_mult = 1.0 / solid_angle

    # 4. Air Attenuation correction (if specified)
    air_mult = 1.0
    if air_path_length_mm is not None:
        # Simple approximation for air attenuation
        # Very rough approximation: μ ≈ 0.01 cm⁻¹ for ~10 keV X-rays in air
        mu_air_per_cm = 0.01
        path_length_cm = air_path_length_mm / 10.0
        attenuation = np.exp(-mu_air_per_cm * path_length_cm)
        air_mult = 1.0 / attenuation

    # Apply all corrections
    corrected_intensity = apply_corrections(
        raw_intensity, lp_mult, qe_mult, sa_mult, air_mult
    )

    correction_factors = {
        "lp_mult": lp_mult,
        "qe_mult": qe_mult,
        "sa_mult": sa_mult,
        "air_mult": air_mult,
        "total_mult": lp_mult * qe_mult * sa_mult * air_mult,
    }

    return corrected_intensity, correction_factors


def validate_correction_factors(
    correction_factors: dict, tolerance: float = 0.01
) -> Tuple[bool, str]:
    """
    Validate that correction factors are reasonable.

    Args:
        correction_factors: Dictionary of correction factors
        tolerance: Tolerance for validation checks

    Returns:
        Tuple of (is_valid, error_message)
    """

    # Check that all factors are positive
    for name, value in correction_factors.items():
        if value <= 0:
            return False, f"Correction factor {name} is not positive: {value}"

    # Check that LP correction is reasonable (should be between 0.1 and 10)
    lp_mult = correction_factors.get("lp_mult", 1.0)
    if lp_mult < 0.1 or lp_mult > 10.0:
        return False, f"LP correction factor unreasonable: {lp_mult}"

    # Check that QE correction is reasonable (should be between 0.1 and 2.0)
    qe_mult = correction_factors.get("qe_mult", 1.0)
    if qe_mult < 0.1 or qe_mult > 2.0:
        return False, f"QE correction factor unreasonable: {qe_mult}"

    # Check that SA correction is reasonable (depends on geometry but shouldn't be extreme)
    sa_mult = correction_factors.get("sa_mult", 1.0)
    if sa_mult < 1e-6 or sa_mult > 1e6:
        return False, f"Solid angle correction factor unreasonable: {sa_mult}"

    # Check that Air correction is close to 1 (small correction for typical distances)
    air_mult = correction_factors.get("air_mult", 1.0)
    if air_mult < 0.9 or air_mult > 1.2:
        return False, f"Air attenuation correction factor unreasonable: {air_mult}"

    return True, "All correction factors are reasonable"


def create_synthetic_experiment_for_testing():
    """
    Create a synthetic DIALS-like experiment object for testing.

    Returns:
        Mock experiment object with proper geometric parameters
    """
    from unittest.mock import Mock

    # Create mock experiment
    experiment = Mock()

    # Mock beam at 45° geometry
    beam = Mock()
    beam.get_wavelength.return_value = 1.0  # 1 Angstrom
    beam.get_s0.return_value = [0, 0, -1]  # Beam along -z
    experiment.beam = beam

    # Mock detector panel positioned for 45° scattering
    panel = Mock()
    panel.get_pixel_size.return_value = (0.172, 0.172)  # mm
    # Position pixel at 45° scattering angle
    # For 200mm detector distance, 45° pixel is at (141.4, 0, 200)
    panel.get_pixel_lab_coord.return_value = [141.4, 0.0, 200.0]  # mm
    panel.get_fast_axis.return_value = [1, 0, 0]
    panel.get_slow_axis.return_value = [0, 1, 0]

    # Mock detector with proper magic method support
    detector = Mock()
    detector.__getitem__ = Mock(return_value=panel)
    detector.__len__ = Mock(return_value=1)
    experiment.detector = detector

    # Mock crystal
    crystal = Mock()
    experiment.crystal = crystal

    # Mock goniometer (None for stills)
    experiment.goniometer = None

    return experiment
</file>

<file path="src/diffusepipe/exceptions.py">
"""Custom exceptions for the diffusepipe package."""


class PipelineError(Exception):
    """Base exception for all pipeline-related errors."""

    pass


class ConfigurationError(PipelineError):
    """Raised when there are configuration-related issues."""

    pass


class DIALSError(PipelineError):
    """Raised when DIALS operations fail."""

    pass


class FileSystemError(PipelineError):
    """Raised when file system operations fail."""

    pass


class DataValidationError(PipelineError):
    """Raised when data validation fails."""

    pass


class NotImplementedYetError(PipelineError):
    """Raised when attempting to use functionality not yet implemented."""

    pass


class MaskGenerationError(PipelineError):
    """Raised when mask generation operations fail."""

    pass


class BraggMaskError(PipelineError):
    """Raised when Bragg mask generation operations fail."""

    pass
</file>

<file path="src/diffusepipe/logging_config.py">
"""Logging configuration for the diffusepipe package."""

import logging
import sys
from typing import Optional


def setup_logging(
    level: int = logging.INFO,
    log_file: Optional[str] = None,
    format_string: Optional[str] = None,
) -> None:
    """
    Setup basic logging configuration for the diffusepipe package.

    Args:
        level: Logging level (e.g., logging.DEBUG, logging.INFO)
        log_file: Optional path to log file. If None, logs to console only.
        format_string: Optional custom format string for log messages.
    """
    if format_string is None:
        format_string = "%(asctime)s - %(name)s - %(levelname)s - %(message)s"

    handlers = [logging.StreamHandler(sys.stdout)]

    if log_file:
        handlers.append(logging.FileHandler(log_file))

    logging.basicConfig(
        level=level,
        format=format_string,
        handlers=handlers,
        force=True,  # Override any existing configuration
    )

    # Set specific loggers to appropriate levels
    logging.getLogger("dials").setLevel(logging.WARNING)
    logging.getLogger("dxtbx").setLevel(logging.WARNING)
    logging.getLogger("cctbx").setLevel(logging.WARNING)
</file>

<file path="tests/adapters/test_dxtbx_io_adapter.py">
"""Tests for DXTBXIOAdapter."""

import pytest
from unittest.mock import Mock, patch, MagicMock
from pathlib import Path

from diffusepipe.adapters.dxtbx_io_adapter import DXTBXIOAdapter
from diffusepipe.exceptions import FileSystemError, DIALSError


@pytest.fixture
def adapter():
    """Create a DXTBXIOAdapter instance for testing."""
    return DXTBXIOAdapter()


@pytest.fixture
def mock_experiments():
    """Create a mock ExperimentList."""
    experiments = Mock()
    experiments.__len__ = Mock(return_value=2)
    return experiments


@pytest.fixture
def mock_reflections():
    """Create a mock reflection table."""
    reflections = Mock()
    reflections.__len__ = Mock(return_value=100)
    reflections.as_file = Mock()
    return reflections


class TestDXTBXIOAdapter:
    """Test cases for DXTBXIOAdapter."""

    def test_init(self, adapter):
        """Test adapter initialization."""
        assert adapter is not None

    def test_load_experiment_list_nonexistent_file(self, adapter):
        """Test loading non-existent experiment file."""
        with pytest.raises(FileSystemError, match="Experiment file does not exist"):
            adapter.load_experiment_list("/nonexistent/file.expt")

    @patch("diffusepipe.adapters.dxtbx_io_adapter.Path.exists")
    @patch("diffusepipe.adapters.dxtbx_io_adapter.Path.is_file")
    def test_load_experiment_list_not_file(self, mock_is_file, mock_exists, adapter):
        """Test loading experiment from non-file path."""
        mock_exists.return_value = True
        mock_is_file.return_value = False

        with pytest.raises(FileSystemError, match="Path is not a file"):
            adapter.load_experiment_list("/path/to/directory")

    @patch("diffusepipe.adapters.dxtbx_io_adapter.Path.exists")
    @patch("diffusepipe.adapters.dxtbx_io_adapter.Path.is_file")
    @patch("diffusepipe.adapters.dxtbx_io_adapter.ExperimentListFactory")
    def test_load_experiment_list_success(
        self, mock_factory, mock_is_file, mock_exists, adapter, mock_experiments
    ):
        """Test successful experiment list loading."""
        mock_exists.return_value = True
        mock_is_file.return_value = True
        mock_factory.from_json_file.return_value = mock_experiments

        result = adapter.load_experiment_list("/path/to/file.expt")

        assert result == mock_experiments
        mock_factory.from_json_file.assert_called_once_with("/path/to/file.expt")

    @patch("diffusepipe.adapters.dxtbx_io_adapter.Path.exists")
    @patch("diffusepipe.adapters.dxtbx_io_adapter.Path.is_file")
    @patch("diffusepipe.adapters.dxtbx_io_adapter.ExperimentListFactory")
    def test_load_experiment_list_empty(
        self, mock_factory, mock_is_file, mock_exists, adapter
    ):
        """Test loading empty experiment list."""
        mock_exists.return_value = True
        mock_is_file.return_value = True

        empty_experiments = Mock()
        empty_experiments.__len__ = Mock(return_value=0)
        mock_factory.from_json_file.return_value = empty_experiments

        result = adapter.load_experiment_list("/path/to/file.expt")

        assert result == empty_experiments

    @patch("diffusepipe.adapters.dxtbx_io_adapter.Path.exists")
    @patch("diffusepipe.adapters.dxtbx_io_adapter.Path.is_file")
    def test_load_experiment_list_import_error(
        self, mock_is_file, mock_exists, adapter
    ):
        """Test experiment loading with import error."""
        mock_exists.return_value = True
        mock_is_file.return_value = True

        with patch(
            "diffusepipe.adapters.dxtbx_io_adapter.ExperimentListFactory",
            None,
        ):
            with pytest.raises(DIALSError, match="ExperimentListFactory not available"):
                adapter.load_experiment_list("/path/to/file.expt")

    def test_load_reflection_table_nonexistent_file(self, adapter):
        """Test loading non-existent reflection file."""
        with pytest.raises(FileSystemError, match="Reflection file does not exist"):
            adapter.load_reflection_table("/nonexistent/file.refl")

    @patch("diffusepipe.adapters.dxtbx_io_adapter.Path.exists")
    @patch("diffusepipe.adapters.dxtbx_io_adapter.Path.is_file")
    @patch("diffusepipe.adapters.dxtbx_io_adapter.flex")
    def test_load_reflection_table_success(
        self, mock_flex, mock_is_file, mock_exists, adapter, mock_reflections
    ):
        """Test successful reflection table loading."""
        mock_exists.return_value = True
        mock_is_file.return_value = True
        mock_flex.reflection_table.from_file.return_value = mock_reflections

        result = adapter.load_reflection_table("/path/to/file.refl")

        assert result == mock_reflections
        mock_flex.reflection_table.from_file.assert_called_once_with(
            "/path/to/file.refl"
        )

    @patch("diffusepipe.adapters.dxtbx_io_adapter.Path.exists")
    @patch("diffusepipe.adapters.dxtbx_io_adapter.Path.is_file")
    def test_load_reflection_table_import_error(
        self, mock_is_file, mock_exists, adapter
    ):
        """Test reflection loading with import error."""
        mock_exists.return_value = True
        mock_is_file.return_value = True

        with patch(
            "diffusepipe.adapters.dxtbx_io_adapter.flex",
            None,
        ):
            with pytest.raises(DIALSError, match="flex not available"):
                adapter.load_reflection_table("/path/to/file.refl")

    def test_save_experiment_list_none(self, adapter):
        """Test saving None experiment list."""
        with pytest.raises(DIALSError, match="Cannot save None experiment list"):
            adapter.save_experiment_list(None, "/path/to/file.expt")

    @patch("diffusepipe.adapters.dxtbx_io_adapter.Path.exists")
    @patch("diffusepipe.adapters.dxtbx_io_adapter.Path.mkdir")
    @patch("diffusepipe.adapters.dxtbx_io_adapter.ExperimentListDumper")
    def test_save_experiment_list_success(
        self, mock_dumper_class, mock_mkdir, mock_exists, adapter, mock_experiments
    ):
        """Test successful experiment list saving."""
        mock_exists.return_value = True
        mock_dumper = Mock()
        mock_dumper_class.return_value = mock_dumper

        adapter.save_experiment_list(mock_experiments, "/path/to/file.expt")

        mock_dumper_class.assert_called_once_with(mock_experiments)
        mock_dumper.as_json.assert_called_once_with("/path/to/file.expt")

    @patch("diffusepipe.adapters.dxtbx_io_adapter.Path.exists")
    @patch("diffusepipe.adapters.dxtbx_io_adapter.Path.mkdir")
    @patch("diffusepipe.adapters.dxtbx_io_adapter.ExperimentListDumper")
    def test_save_experiment_list_file_not_created(
        self, mock_dumper_class, mock_mkdir, mock_exists, adapter, mock_experiments
    ):
        """Test experiment saving when file is not created."""
        mock_exists.return_value = False  # File doesn't exist after saving
        mock_dumper = Mock()
        mock_dumper_class.return_value = mock_dumper

        with pytest.raises(FileSystemError, match="Failed to create experiment file"):
            adapter.save_experiment_list(mock_experiments, "/path/to/file.expt")

    @patch("diffusepipe.adapters.dxtbx_io_adapter.Path.mkdir")
    def test_save_experiment_list_import_error(self, mock_mkdir, adapter, mock_experiments):
        """Test experiment saving with import error."""
        with patch(
            "diffusepipe.adapters.dxtbx_io_adapter.ExperimentListDumper",
            None,
        ):
            with pytest.raises(DIALSError, match="ExperimentListDumper not available"):
                adapter.save_experiment_list(mock_experiments, "/path/to/file.expt")

    def test_save_reflection_table_none(self, adapter):
        """Test saving None reflection table."""
        with pytest.raises(DIALSError, match="Cannot save None reflection table"):
            adapter.save_reflection_table(None, "/path/to/file.refl")

    @patch("diffusepipe.adapters.dxtbx_io_adapter.Path.exists")
    @patch("diffusepipe.adapters.dxtbx_io_adapter.Path.mkdir")
    def test_save_reflection_table_success(
        self, mock_mkdir, mock_exists, adapter, mock_reflections
    ):
        """Test successful reflection table saving."""
        mock_exists.return_value = True

        adapter.save_reflection_table(mock_reflections, "/path/to/file.refl")

        mock_reflections.as_file.assert_called_once_with("/path/to/file.refl")

    @patch("diffusepipe.adapters.dxtbx_io_adapter.Path.exists")
    @patch("diffusepipe.adapters.dxtbx_io_adapter.Path.mkdir")
    def test_save_reflection_table_no_as_file_method(
        self, mock_mkdir, mock_exists, adapter
    ):
        """Test reflection saving when object lacks as_file method."""
        mock_exists.return_value = True
        reflections_no_as_file = Mock()
        del reflections_no_as_file.as_file  # Remove the as_file attribute

        with pytest.raises(
            DIALSError, match="Reflection table does not support file saving"
        ):
            adapter.save_reflection_table(reflections_no_as_file, "/path/to/file.refl")

    @patch("diffusepipe.adapters.dxtbx_io_adapter.Path.exists")
    @patch("diffusepipe.adapters.dxtbx_io_adapter.Path.mkdir")
    def test_save_reflection_table_file_not_created(
        self, mock_mkdir, mock_exists, adapter, mock_reflections
    ):
        """Test reflection saving when file is not created."""
        mock_exists.return_value = False  # File doesn't exist after saving

        with pytest.raises(FileSystemError, match="Failed to create reflection file"):
            adapter.save_reflection_table(mock_reflections, "/path/to/file.refl")

    @patch("diffusepipe.adapters.dxtbx_io_adapter.Path")
    def test_pathlib_path_handling(self, mock_path_class, adapter, mock_experiments):
        """Test that adapter handles Path objects correctly."""
        # Create a mock Path instance
        mock_path_instance = Mock()
        mock_path_instance.exists.return_value = True
        mock_path_instance.is_file.return_value = True
        mock_path_instance.parent.mkdir = Mock()
        mock_path_class.return_value = mock_path_instance

        # Test with pathlib.Path object
        path_obj = Path("/test/path.expt")

        with patch(
            "diffusepipe.adapters.dxtbx_io_adapter.ExperimentListFactory"
        ) as mock_factory:
            mock_factory.from_json_file.return_value = mock_experiments
            result = adapter.load_experiment_list(path_obj)
            assert result == mock_experiments
</file>

<file path="tests/crystallography/__init__.py">
# Tests for crystallographic processing components
</file>

<file path="tests/integration/__init__.py">
"""Integration tests for the diffuse scattering pipeline."""
</file>

<file path="tests/integration/test_sequence_adapter_integration.py">
#!/usr/bin/env python3
"""Test the new sequence-based DIALS adapter."""

import sys
from pathlib import Path

# Add project src to path
sys.path.insert(0, str(Path(__file__).parent / "src"))

from diffusepipe.adapters.dials_sequence_process_adapter import (
    DIALSSequenceProcessAdapter,
)
from diffusepipe.crystallography.still_processing_and_validation import (
    create_default_config,
)


def test_sequence_adapter():
    """Test the sequence adapter with the problematic CBF file."""

    cbf_path = "747/lys_nitr_10_6_0491.cbf"

    if not Path(cbf_path).exists():
        print(f"CBF file not found: {cbf_path}")
        assert False, f"CBF file not found: {cbf_path}"

    print(f"Testing sequence adapter with: {cbf_path}")

    try:
        # Set up configuration
        config = create_default_config(
            enable_partiality=True,
            enable_shoeboxes=True,
            known_unit_cell="27.424,32.134,34.513,88.66,108.46,111.88",
            known_space_group="P 1",
        )

        # Initialize adapter
        adapter = DIALSSequenceProcessAdapter()

        # Process the image
        print("Processing with sequence adapter...")
        experiment, reflections, success, log_messages = adapter.process_still(
            image_path=cbf_path, config=config
        )

        if success:
            print("✅ SEQUENCE ADAPTER SUCCESS!")
            print(f"Experiment: {experiment}")
            print(f"Reflections: {len(reflections) if reflections else 0}")
            print(f"Log messages: {log_messages}")
            assert success is True
        else:
            print("❌ Sequence adapter failed")
            print(f"Log: {log_messages}")
            assert success is True  # This will fail and raise an AssertionError

    except Exception as e:
        print(f"❌ Sequence adapter error: {e}")
        import traceback

        traceback.print_exc()
        assert False, f"Exception occurred: {e}"


if __name__ == "__main__":
    success = test_sequence_adapter()

    if success:
        print("\n🎉 SOLUTION FOUND!")
        print(
            "The sequence-based adapter successfully processes these 0.1° oscillation images."
        )
        print(
            "The issue was that stills_process doesn't work well with oscillation data."
        )
        print(
            "The manual workflow succeeds because it treats them as sequences, not stills."
        )
    else:
        print("\n❌ The sequence adapter also failed.")
        print("Further investigation needed.")
</file>

<file path="tests/masking/__init__.py">
# Tests for masking components
</file>

<file path="tests/masking/test_bragg_mask_generator.py">
"""
Unit tests for BraggMaskGenerator.

These tests focus on the Bragg mask generation functions, including both
dials.generate_mask (Option A) and shoebox-based (Option B) approaches.
"""

import pytest
from unittest.mock import Mock, patch, MagicMock

from diffusepipe.masking.bragg_mask_generator import (
    BraggMaskGenerator,
    create_default_bragg_mask_config,
    create_expanded_bragg_mask_config,
    validate_mask_compatibility,
)
from diffusepipe.exceptions import BraggMaskError


class TestBraggMaskGenerator:
    """Test suite for BraggMaskGenerator class."""

    def setup_method(self):
        """Set up test fixtures."""
        self.generator = BraggMaskGenerator()

    def test_initialization(self):
        """Test BraggMaskGenerator initialization."""
        generator = BraggMaskGenerator()
        assert generator.dials_adapter is not None

    def test_generate_bragg_mask_from_spots_success(self):
        """Test successful Bragg mask generation using dials.generate_mask."""
        # Arrange
        mock_experiment = Mock()
        mock_reflections = Mock()
        mock_config = {"border": 2}

        # Mock successful adapter response
        mock_bragg_mask = (Mock(), Mock())  # Two panels
        with patch.object(
            self.generator.dials_adapter, "generate_bragg_mask"
        ) as mock_generate:
            mock_generate.return_value = (mock_bragg_mask, True, "Success")

            # Act
            result = self.generator.generate_bragg_mask_from_spots(
                experiment=mock_experiment,
                reflections=mock_reflections,
                config=mock_config,
            )

            # Assert
            assert result is mock_bragg_mask
            mock_generate.assert_called_once_with(
                experiment=mock_experiment,
                reflections=mock_reflections,
                mask_generation_params=mock_config,
            )

    def test_generate_bragg_mask_from_spots_failure(self):
        """Test handling of dials.generate_mask failure."""
        # Arrange
        mock_experiment = Mock()
        mock_reflections = Mock()

        # Mock failed adapter response
        with patch.object(
            self.generator.dials_adapter, "generate_bragg_mask"
        ) as mock_generate:
            mock_generate.return_value = (None, False, "DIALS error")

            # Act & Assert
            with pytest.raises(BraggMaskError) as exc_info:
                self.generator.generate_bragg_mask_from_spots(
                    experiment=mock_experiment, reflections=mock_reflections
                )

            assert "DIALS mask generation failed" in str(exc_info.value)

    def test_generate_bragg_mask_from_spots_exception(self):
        """Test handling of unexpected exceptions in spots-based generation."""
        # Arrange
        mock_experiment = Mock()
        mock_reflections = Mock()

        # Mock adapter exception
        with patch.object(
            self.generator.dials_adapter, "generate_bragg_mask"
        ) as mock_generate:
            mock_generate.side_effect = RuntimeError("Unexpected error")

            # Act & Assert
            with pytest.raises(BraggMaskError) as exc_info:
                self.generator.generate_bragg_mask_from_spots(
                    experiment=mock_experiment, reflections=mock_reflections
                )

            assert "Failed to generate Bragg mask from spots" in str(exc_info.value)

    def test_generate_bragg_mask_from_shoeboxes_success(self):
        """Test successful Bragg mask generation using shoebox data (integration test with real DIALS objects)."""
        # Arrange
        mock_detector = Mock()
        mock_panel = Mock()
        mock_panel.get_image_size.return_value = (4, 4)  # (fast, slow)
        mock_detector.__iter__ = Mock(return_value=iter([mock_panel]))

        # Mock reflections with shoeboxes
        mock_reflections = Mock()
        mock_reflections.__len__ = Mock(return_value=1)
        mock_reflections.__iter__ = Mock(
            return_value=iter([{"shoebox": Mock(), "panel": 0}])
        )
        mock_reflections.has_key.return_value = True

        # Act
        result = self.generator.generate_bragg_mask_from_shoeboxes(
            reflections=mock_reflections, detector=mock_detector
        )

        # Assert - Check that we got a valid flex.bool object
        from dials.array_family import flex
        assert result is not None
        assert len(result) == 1
        assert isinstance(result[0], flex.bool)
        assert result[0].size() == 16  # 4x4 panel
        mock_reflections.has_key.assert_called_with("shoebox")

    def test_generate_bragg_mask_from_shoeboxes_no_reflections(self):
        """Test shoebox-based generation with no reflections (integration test with real DIALS objects)."""
        # Arrange
        mock_detector = Mock()
        mock_panel = Mock()
        mock_panel.get_image_size.return_value = (4, 4)
        mock_detector.__iter__ = Mock(return_value=iter([mock_panel]))

        mock_reflections = Mock()
        mock_reflections.__len__ = Mock(return_value=0)

        # Act
        result = self.generator.generate_bragg_mask_from_shoeboxes(
            reflections=mock_reflections, detector=mock_detector
        )

        # Assert - Check that we got a valid flex.bool object
        from dials.array_family import flex
        assert result is not None
        assert len(result) == 1
        assert isinstance(result[0], flex.bool)
        assert result[0].size() == 16  # 4x4 panel

    @patch("diffusepipe.masking.bragg_mask_generator.flex")
    def test_generate_bragg_mask_from_shoeboxes_missing_shoebox_column(self, mock_flex):
        """Test error handling when shoebox column is missing."""
        # Arrange
        mock_detector = Mock()
        mock_panel = Mock()
        mock_panel.get_image_size.return_value = (4, 4)
        mock_detector.__iter__ = Mock(return_value=iter([mock_panel]))

        mock_reflections = Mock()
        mock_reflections.__len__ = Mock(return_value=1)
        mock_reflections.has_key.return_value = False  # No shoebox column

        mock_mask = Mock()
        mock_flex.bool.return_value = mock_mask
        mock_flex.grid.return_value = Mock()

        # Act & Assert
        with pytest.raises(BraggMaskError) as exc_info:
            self.generator.generate_bragg_mask_from_shoeboxes(
                reflections=mock_reflections, detector=mock_detector
            )

        assert "missing required 'shoebox' column" in str(exc_info.value)

    def test_get_total_mask_for_still_success(self):
        """Test successful combination of Bragg and pixel masks."""
        # Arrange
        mock_bragg_mask = Mock()
        mock_pixel_mask = Mock()
        mock_inverted_bragg = Mock()
        mock_total_mask = Mock()

        # Mock logical operations
        mock_bragg_mask.__invert__ = Mock(return_value=mock_inverted_bragg)
        mock_pixel_mask.__and__ = Mock(return_value=mock_total_mask)

        # Mock count operations for logging
        mock_pixel_mask.count.return_value = 90
        mock_bragg_mask.count.return_value = 10
        mock_total_mask.count.return_value = 85
        mock_total_mask.__len__ = Mock(return_value=100)

        bragg_mask = (mock_bragg_mask,)
        global_pixel_mask = (mock_pixel_mask,)

        # Act
        result = self.generator.get_total_mask_for_still(bragg_mask, global_pixel_mask)

        # Assert
        assert result is not None
        assert len(result) == 1
        assert result[0] is mock_total_mask

        # Verify logical operations were performed correctly
        mock_bragg_mask.__invert__.assert_called_once()
        mock_pixel_mask.__and__.assert_called_once_with(mock_inverted_bragg)

    def test_get_total_mask_for_still_panel_count_mismatch(self):
        """Test error handling when panel counts don't match."""
        # Arrange
        bragg_mask = (Mock(), Mock())  # 2 panels
        global_pixel_mask = (Mock(),)  # 1 panel

        # Act & Assert
        with pytest.raises(BraggMaskError) as exc_info:
            self.generator.get_total_mask_for_still(bragg_mask, global_pixel_mask)

        assert "different panel counts" in str(exc_info.value)

    def test_get_total_mask_for_still_exception_handling(self):
        """Test handling of unexpected exceptions in mask combination."""
        # Arrange
        mock_bragg_mask = Mock()
        mock_pixel_mask = Mock()

        # Mock operation to raise exception
        mock_bragg_mask.__invert__ = Mock(
            side_effect=RuntimeError("Logical operation failed")
        )

        bragg_mask = (mock_bragg_mask,)
        global_pixel_mask = (mock_pixel_mask,)

        # Act & Assert
        with pytest.raises(BraggMaskError) as exc_info:
            self.generator.get_total_mask_for_still(bragg_mask, global_pixel_mask)

        assert "Failed to combine masks" in str(exc_info.value)

    @patch("diffusepipe.masking.bragg_mask_generator.MaskCode")
    def test_process_reflection_shoebox_basic(self, mock_maskcode):
        """Test processing of a single reflection's shoebox."""
        # Arrange
        mock_maskcode.Foreground = 1
        mock_maskcode.Strong = 2

        # Create mock reflection with shoebox
        mock_shoebox = Mock()
        mock_shoebox.bbox = (0, 0, 0, 1, 2, 2)  # z1, y1, x1, z2, y2, x2
        mock_shoebox.mask = [1, 0, 1, 0]  # Some foreground pixels

        reflection = {"shoebox": mock_shoebox, "panel": 0}

        # Create mock panel masks
        mock_panel_mask = Mock()
        mock_panel_mask.__len__ = Mock(return_value=4)  # 2x2 panel
        mock_panel_mask.__getitem__ = Mock()
        mock_panel_mask.__setitem__ = Mock()
        panel_masks = [mock_panel_mask]

        # Act
        result = self.generator._process_reflection_shoebox(
            reflection, 0, panel_masks, 0
        )

        # Assert
        assert isinstance(result, int)
        assert result >= 0

    def test_process_reflection_shoebox_invalid_panel(self):
        """Test error handling for invalid panel ID in reflection."""
        # Arrange
        mock_shoebox = Mock()
        mock_shoebox.bbox = (0, 0, 0, 1, 1, 1)
        reflection = {"shoebox": mock_shoebox, "panel": 5}  # Invalid panel ID
        panel_masks = [Mock()]  # Only one panel available

        # Act & Assert
        with pytest.raises(Exception) as exc_info:
            self.generator._process_reflection_shoebox(reflection, 0, panel_masks, 0)

        assert "exceeds available panels" in str(exc_info.value)


class TestUtilityFunctions:
    """Test suite for utility functions."""

    def test_create_default_bragg_mask_config(self):
        """Test creation of default Bragg mask configuration."""
        config = create_default_bragg_mask_config()

        assert isinstance(config, dict)
        assert "border" in config
        assert "algorithm" in config
        assert config["border"] == 2
        assert config["algorithm"] == "simple"

    def test_create_expanded_bragg_mask_config(self):
        """Test creation of expanded Bragg mask configuration."""
        config = create_expanded_bragg_mask_config(border=5)

        assert isinstance(config, dict)
        assert config["border"] == 5
        assert config["algorithm"] == "simple"

    def test_create_expanded_bragg_mask_config_default(self):
        """Test creation of expanded configuration with default border."""
        config = create_expanded_bragg_mask_config()

        assert config["border"] == 3

    def test_validate_mask_compatibility_success(self):
        """Test successful mask compatibility validation."""
        # Arrange
        mock_bragg_panel = Mock()
        mock_pixel_panel = Mock()
        mock_bragg_panel.__len__ = Mock(return_value=100)
        mock_pixel_panel.__len__ = Mock(return_value=100)

        bragg_mask = (mock_bragg_panel,)
        pixel_mask = (mock_pixel_panel,)

        # Act
        result = validate_mask_compatibility(bragg_mask, pixel_mask)

        # Assert
        assert result is True

    def test_validate_mask_compatibility_panel_count_mismatch(self):
        """Test mask compatibility validation with panel count mismatch."""
        # Arrange
        bragg_mask = (Mock(), Mock())  # 2 panels
        pixel_mask = (Mock(),)  # 1 panel

        # Act
        result = validate_mask_compatibility(bragg_mask, pixel_mask)

        # Assert
        assert result is False

    def test_validate_mask_compatibility_size_mismatch(self):
        """Test mask compatibility validation with panel size mismatch."""
        # Arrange
        mock_bragg_panel = Mock()
        mock_pixel_panel = Mock()
        mock_bragg_panel.__len__ = Mock(return_value=100)
        mock_pixel_panel.__len__ = Mock(return_value=200)  # Different size

        bragg_mask = (mock_bragg_panel,)
        pixel_mask = (mock_pixel_panel,)

        # Act
        result = validate_mask_compatibility(bragg_mask, pixel_mask)

        # Assert
        assert result is False

    def test_validate_mask_compatibility_exception_handling(self):
        """Test mask compatibility validation with exception."""
        # Arrange
        mock_bragg_panel = Mock()
        mock_bragg_panel.__len__ = Mock(side_effect=RuntimeError("Access error"))

        bragg_mask = (mock_bragg_panel,)
        pixel_mask = (Mock(),)

        # Act
        result = validate_mask_compatibility(bragg_mask, pixel_mask)

        # Assert
        assert result is False


class TestIntegrationScenarios:
    """Integration-style tests for complete Bragg masking workflows."""

    def setup_method(self):
        """Set up test fixtures."""
        self.generator = BraggMaskGenerator()

    def test_complete_workflow_option_a(self):
        """Test complete workflow using dials.generate_mask (Option A)."""
        # Arrange
        mock_experiment = Mock()
        mock_reflections = Mock()
        mock_global_pixel_mask = Mock()

        # Mock successful Bragg mask generation
        mock_bragg_mask = Mock()
        mock_bragg_mask.count.return_value = 10
        mock_bragg_mask.__len__ = Mock(return_value=100)

        # Mock mask combination
        mock_inverted_bragg = Mock()
        mock_total_mask = Mock()
        mock_bragg_mask.__invert__ = Mock(return_value=mock_inverted_bragg)
        mock_global_pixel_mask.__and__ = Mock(return_value=mock_total_mask)
        mock_global_pixel_mask.count.return_value = 95
        mock_total_mask.count.return_value = 88
        mock_total_mask.__len__ = Mock(return_value=100)

        with patch.object(
            self.generator.dials_adapter, "generate_bragg_mask"
        ) as mock_generate:
            mock_generate.return_value = ((mock_bragg_mask,), True, "Success")

            # Act
            # Step 1: Generate Bragg mask
            bragg_mask = self.generator.generate_bragg_mask_from_spots(
                experiment=mock_experiment, reflections=mock_reflections
            )

            # Step 2: Combine with pixel mask
            total_mask = self.generator.get_total_mask_for_still(
                bragg_mask=bragg_mask, global_pixel_mask=(mock_global_pixel_mask,)
            )

            # Assert
            assert bragg_mask is not None
            assert len(bragg_mask) == 1
            assert total_mask is not None
            assert len(total_mask) == 1
            assert total_mask[0] is mock_total_mask

    @patch("diffusepipe.masking.bragg_mask_generator.flex")
    @patch("diffusepipe.masking.bragg_mask_generator.MaskCode")
    def test_complete_workflow_option_b(self, mock_maskcode, mock_flex):
        """Test complete workflow using shoeboxes (Option B)."""
        # Arrange
        mock_detector = Mock()
        mock_panel = Mock()
        mock_panel.get_image_size.return_value = (4, 4)
        mock_detector.__iter__ = Mock(return_value=iter([mock_panel]))

        mock_reflections = Mock()
        mock_reflections.__len__ = Mock(return_value=0)  # No reflections for simplicity
        mock_reflections.has_key.return_value = True

        mock_global_pixel_mask = Mock()

        # Mock flex operations
        mock_bragg_mask = Mock()
        mock_bragg_mask.count.return_value = 5
        mock_bragg_mask.__len__ = Mock(return_value=16)
        mock_flex.bool.return_value = mock_bragg_mask
        mock_flex.grid.return_value = Mock()

        # Mock mask combination
        mock_inverted_bragg = Mock()
        mock_total_mask = Mock()
        mock_bragg_mask.__invert__ = Mock(return_value=mock_inverted_bragg)
        mock_global_pixel_mask.__and__ = Mock(return_value=mock_total_mask)
        mock_global_pixel_mask.count.return_value = 15
        mock_total_mask.count.return_value = 12
        mock_total_mask.__len__ = Mock(return_value=16)

        # Act
        # Step 1: Generate Bragg mask from shoeboxes
        bragg_mask = self.generator.generate_bragg_mask_from_shoeboxes(
            reflections=mock_reflections, detector=mock_detector
        )

        # Step 2: Combine with pixel mask
        total_mask = self.generator.get_total_mask_for_still(
            bragg_mask=bragg_mask, global_pixel_mask=(mock_global_pixel_mask,)
        )

        # Assert
        assert bragg_mask is not None
        assert len(bragg_mask) == 1
        assert total_mask is not None
        assert len(total_mask) == 1
        assert total_mask[0] is mock_total_mask
</file>

<file path="tests/utils/test_cbf_utils.py">
"""
Test suite for CBF utilities, specifically CBF data type detection.
"""

import pytest
import tempfile
import os
from pathlib import Path
from unittest.mock import patch, MagicMock

from diffusepipe.utils.cbf_utils import (
    get_angle_increment_from_cbf,
    _parse_cbf_header_text,
)


class TestGetAngleIncrementFromCBF:
    """Test CBF data type detection for various file types and edge cases."""

    def test_file_not_found(self):
        """Test handling of non-existent files."""
        with pytest.raises(Exception):  # Should raise an exception
            get_angle_increment_from_cbf("/nonexistent/file.cbf")

    def test_empty_file(self):
        """Test handling of empty CBF files."""
        with tempfile.NamedTemporaryFile(mode="w", suffix=".cbf", delete=False) as f:
            temp_path = f.name

        try:
            # Should handle gracefully and return None
            result = get_angle_increment_from_cbf(temp_path)
            assert result is None
        finally:
            os.unlink(temp_path)

    @patch("dxtbx.load")
    def test_dxtbx_stills_data_success(self, mock_dxtbx_load):
        """Test successful dxtbx parsing for stills data (Angle_increment = 0.0)."""
        # Mock successful dxtbx loading for stills
        mock_image = MagicMock()
        mock_scan = MagicMock()
        mock_scan.get_oscillation.return_value = (
            0.0,
            0.0,
        )  # (start_angle, oscillation_width)
        mock_image.get_scan.return_value = mock_scan
        mock_dxtbx_load.return_value = mock_image

        result = get_angle_increment_from_cbf("test_stills.cbf")
        assert result == 0.0
        mock_dxtbx_load.assert_called_once_with("test_stills.cbf")

    @patch("dxtbx.load")
    def test_dxtbx_sequence_data_success(self, mock_dxtbx_load):
        """Test successful dxtbx parsing for sequence data (Angle_increment > 0.0)."""
        # Mock successful dxtbx loading for sequence data
        mock_image = MagicMock()
        mock_scan = MagicMock()
        mock_scan.get_oscillation.return_value = (0.0, 0.1)  # 0.1° oscillation
        mock_image.get_scan.return_value = mock_scan
        mock_dxtbx_load.return_value = mock_image

        result = get_angle_increment_from_cbf("test_sequence.cbf")
        assert result == 0.1
        mock_dxtbx_load.assert_called_once_with("test_sequence.cbf")

    @patch("dxtbx.load")
    def test_dxtbx_no_scan_object(self, mock_dxtbx_load):
        """Test dxtbx parsing when no scan object is available (stills case)."""
        # Mock image with no scan object
        mock_image = MagicMock()
        mock_image.get_scan.return_value = None
        mock_dxtbx_load.return_value = mock_image

        result = get_angle_increment_from_cbf("test_no_scan.cbf")
        assert result == 0.0  # Should default to stills

    @patch("dxtbx.load")
    def test_dxtbx_failure_fallback_to_text_parsing(self, mock_dxtbx_load):
        """Test fallback to text parsing when dxtbx fails."""
        # Mock dxtbx failure
        mock_dxtbx_load.side_effect = Exception("dxtbx failed")

        # Create a temporary CBF file with header
        cbf_content = """###CBF: VERSION 1.5
# Detector: PILATUS3 6M
# Angle_increment 0.1000 deg.
# Exposure_time 0.0990000 s
_array_data.data
"""
        with tempfile.NamedTemporaryFile(mode="w", suffix=".cbf", delete=False) as f:
            f.write(cbf_content)
            temp_path = f.name

        try:
            result = get_angle_increment_from_cbf(temp_path)
            assert result == 0.1  # Should parse from header text
        finally:
            os.unlink(temp_path)

    @patch("dxtbx.load")
    def test_dxtbx_scan_get_oscillation_fails(self, mock_dxtbx_load):
        """Test handling when scan.get_oscillation() raises an exception."""
        # Mock scan object that fails on get_oscillation
        mock_image = MagicMock()
        mock_scan = MagicMock()
        mock_scan.get_oscillation.side_effect = RuntimeError(
            "Oscillation data unavailable"
        )
        mock_image.get_scan.return_value = mock_scan
        mock_dxtbx_load.return_value = mock_image

        # Should fallback to text parsing
        cbf_content = """###CBF: VERSION 1.5
# Angle_increment 0.2000 deg.
"""
        with tempfile.NamedTemporaryFile(mode="w", suffix=".cbf", delete=False) as f:
            f.write(cbf_content)
            temp_path = f.name

        try:
            result = get_angle_increment_from_cbf(temp_path)
            assert result == 0.2  # Should parse from header text
        finally:
            os.unlink(temp_path)


class TestParseCBFHeaderText:
    """Test direct CBF header text parsing functionality."""

    def test_parse_stills_header(self):
        """Test parsing stills data from header text."""
        cbf_content = """###CBF: VERSION 1.5
# Detector: PILATUS3 6M, S/N 60-0127
# Angle_increment 0.0000 deg.
# Exposure_time 0.0990000 s
_array_data.data
"""
        with tempfile.NamedTemporaryFile(mode="w", suffix=".cbf", delete=False) as f:
            f.write(cbf_content)
            temp_path = f.name

        try:
            result = _parse_cbf_header_text(temp_path)
            assert result == 0.0
        finally:
            os.unlink(temp_path)

    def test_parse_sequence_header(self):
        """Test parsing sequence data from header text."""
        cbf_content = """###CBF: VERSION 1.5
# Detector: PILATUS3 6M, S/N 60-0127
# Angle_increment 0.1000 deg.
# Exposure_time 0.0990000 s
_array_data.data
"""
        with tempfile.NamedTemporaryFile(mode="w", suffix=".cbf", delete=False) as f:
            f.write(cbf_content)
            temp_path = f.name

        try:
            result = _parse_cbf_header_text(temp_path)
            assert result == 0.1
        finally:
            os.unlink(temp_path)

    def test_parse_larger_angle_increment(self):
        """Test parsing larger angle increments."""
        cbf_content = """###CBF: VERSION 1.5
# Angle_increment 0.5000 deg.
"""
        with tempfile.NamedTemporaryFile(mode="w", suffix=".cbf", delete=False) as f:
            f.write(cbf_content)
            temp_path = f.name

        try:
            result = _parse_cbf_header_text(temp_path)
            assert result == 0.5
        finally:
            os.unlink(temp_path)

    def test_parse_no_angle_increment(self):
        """Test parsing when Angle_increment is not found."""
        cbf_content = """###CBF: VERSION 1.5
# Detector: PILATUS3 6M, S/N 60-0127
# Exposure_time 0.0990000 s
_array_data.data
"""
        with tempfile.NamedTemporaryFile(mode="w", suffix=".cbf", delete=False) as f:
            f.write(cbf_content)
            temp_path = f.name

        try:
            result = _parse_cbf_header_text(temp_path)
            assert result is None
        finally:
            os.unlink(temp_path)

    def test_parse_malformed_angle_increment_line(self):
        """Test parsing when Angle_increment line is malformed."""
        cbf_content = """###CBF: VERSION 1.5
# Angle_increment invalid_value deg.
"""
        with tempfile.NamedTemporaryFile(mode="w", suffix=".cbf", delete=False) as f:
            f.write(cbf_content)
            temp_path = f.name

        try:
            result = _parse_cbf_header_text(temp_path)
            assert result is None  # Should handle parsing error gracefully
        finally:
            os.unlink(temp_path)

    def test_parse_case_insensitive_angle_increment(self):
        """Test parsing when case varies in the header."""
        cbf_content = """###CBF: VERSION 1.5
# angle_increment 0.1000 deg.
"""
        with tempfile.NamedTemporaryFile(mode="w", suffix=".cbf", delete=False) as f:
            f.write(cbf_content)
            temp_path = f.name

        try:
            # Implementation uses re.IGNORECASE, so lowercase should work
            result = _parse_cbf_header_text(temp_path)
            assert result == 0.1
        finally:
            os.unlink(temp_path)

    def test_parse_different_spacing(self):
        """Test parsing with different spacing in header line."""
        cbf_content = """###CBF: VERSION 1.5
#   Angle_increment    0.2500   deg.
"""
        with tempfile.NamedTemporaryFile(mode="w", suffix=".cbf", delete=False) as f:
            f.write(cbf_content)
            temp_path = f.name

        try:
            result = _parse_cbf_header_text(temp_path)
            assert result == 0.25  # Should handle extra spacing
        finally:
            os.unlink(temp_path)

    def test_parse_negative_angle_increment(self):
        """Test parsing negative angle increment (edge case)."""
        cbf_content = """###CBF: VERSION 1.5
# Angle_increment -0.1000 deg.
"""
        with tempfile.NamedTemporaryFile(mode="w", suffix=".cbf", delete=False) as f:
            f.write(cbf_content)
            temp_path = f.name

        try:
            result = _parse_cbf_header_text(temp_path)
            assert result == -0.1  # Should parse negative values
        finally:
            os.unlink(temp_path)

    def test_parse_file_read_error(self):
        """Test handling of file read errors."""
        # Test with a directory instead of a file
        with tempfile.TemporaryDirectory() as temp_dir:
            with pytest.raises(Exception):  # Should raise an exception
                _parse_cbf_header_text(temp_dir)


# Integration tests with both methods
class TestIntegrationCBFDetection:
    """Integration tests for the complete CBF detection workflow."""

    @patch("dxtbx.load")
    def test_full_workflow_dxtbx_success(self, mock_dxtbx_load):
        """Test full workflow when dxtbx succeeds."""
        mock_image = MagicMock()
        mock_scan = MagicMock()
        mock_scan.get_oscillation.return_value = (0.0, 0.1)
        mock_image.get_scan.return_value = mock_scan
        mock_dxtbx_load.return_value = mock_image

        result = get_angle_increment_from_cbf("test.cbf")
        assert result == 0.1

    @patch("dxtbx.load")
    def test_full_workflow_fallback_success(self, mock_dxtbx_load):
        """Test full workflow when dxtbx fails but text parsing succeeds."""
        mock_dxtbx_load.side_effect = Exception("Import error")

        cbf_content = """###CBF: VERSION 1.5
# Angle_increment 0.3000 deg.
"""
        with tempfile.NamedTemporaryFile(mode="w", suffix=".cbf", delete=False) as f:
            f.write(cbf_content)
            temp_path = f.name

        try:
            result = get_angle_increment_from_cbf(temp_path)
            assert result == 0.3
        finally:
            os.unlink(temp_path)

    @patch("dxtbx.load")
    def test_full_workflow_all_methods_fail(self, mock_dxtbx_load):
        """Test full workflow when both dxtbx and text parsing fail."""
        mock_dxtbx_load.side_effect = Exception("Import error")

        # Create file with no Angle_increment
        cbf_content = """###CBF: VERSION 1.5
# Some other header info
"""
        with tempfile.NamedTemporaryFile(mode="w", suffix=".cbf", delete=False) as f:
            f.write(cbf_content)
            temp_path = f.name

        try:
            # When all methods fail gracefully, function returns None
            result = get_angle_increment_from_cbf(temp_path)
            assert result is None
        finally:
            os.unlink(temp_path)
</file>

<file path="tests/__init__.py">
"""Test suite for the diffusepipe package."""
</file>

<file path="tests/conftest.py">
"""Shared pytest fixtures and configuration for the test suite."""

import pytest
from pathlib import Path
import tempfile
import shutil




@pytest.fixture
def test_data_dir():
    """Path to the test data directory."""
    return Path(__file__).parent / "data"


@pytest.fixture
def mock_cbf_file(tmp_path):
    """Create a minimal mock CBF file for testing."""
    cbf_path = tmp_path / "test_image.cbf"
    # Create a minimal CBF file (this would need to be a valid CBF in real tests)
    cbf_path.write_text("# Mock CBF file for testing")
    return str(cbf_path)


@pytest.fixture
def mock_pdb_file(tmp_path):
    """Create a minimal mock PDB file for testing."""
    pdb_path = tmp_path / "test.pdb"
    # Create a minimal PDB file content
    pdb_content = """HEADER    TEST STRUCTURE
CRYST1   50.000   50.000   50.000  90.00  90.00  90.00 P 1
ATOM      1  CA  ALA A   1      25.000  25.000  25.000  1.00 20.00           C
END
"""
    pdb_path.write_text(pdb_content)
    return str(pdb_path)
</file>

<file path="plan_adaptation.md">
# Plan Adaptation: Supporting Both Stills and Sequence Data

## Overview

This document outlines the necessary adaptations to `plan.md` based on the critical discovery that CBF files with oscillation data (Angle_increment > 0°) cannot be processed using `dials.stills_process` and require a sequential DIALS workflow instead.

## Root Cause of Required Changes

**Original Assumption (Incorrect):**
- All CBF data can be processed using `dials.stills_process`
- Single processing pathway for all input data

**Reality:**
- CBF files with `Angle_increment > 0°` are **sequence data**, not stills
- `dials.stills_process` **fails** on oscillation data due to fundamental algorithmic assumptions
- Two distinct processing pathways required based on data type

## Impact Assessment

### Scope of Changes: **Limited and Surgical**

**What Remains Unchanged (85% of plan):**
- Core 4-phase pipeline structure
- All of Phase 2 (diffuse extraction and correction)
- All of Phase 3 (voxelization, scaling, merging)
- All of Phase 4 (absolute scaling)
- Output data structures and interfaces
- Testing framework and principles
- Most of Module 1.S.1 validation logic

**What Requires Adaptation (15% of plan):**
- Module 1.S.1 processing logic
- Addition of data type detection
- Adapter layer implementations
- Some testing scenarios

## Detailed Adaptation Requirements

### 1. New Module: Data Type Detection

**Insert before Module 1.S.1:**

```markdown
**Module 1.S.0: CBF Data Type Detection and Processing Route Selection**
*   **Action:** Analyze CBF file headers to determine if data is true stills (Angle_increment = 0°) or sequence data (Angle_increment > 0°), then route to appropriate processing pathway.
*   **Input (per still `i`):**
    *   File path to raw CBF image
*   **Process:**
    1.  Parse CBF header to extract `Angle_increment` value
    2.  Determine data type:
        *   IF `Angle_increment = 0.0`: Route to stills processing pathway
        *   IF `Angle_increment > 0.0`: Route to sequence processing pathway
    3.  Log data type determination for debugging
*   **Output:** 
    *   `data_type`: String ("stills" or "sequence")
    *   `processing_route`: Enum or string indicating which adapter to use
*   **Testing:**
    *   **Input:** Sample CBF files with known `Angle_increment` values
    *   **Verification:** Assert correct data type classification and routing decisions
```

### 2. Module 1.S.1 Process Section Adaptations

**Replace lines 95-107 in current plan:**

```markdown
*   **Process (Orchestrated per still `i` by the `StillsPipelineOrchestrator`, which determines processing route based on Module 1.S.0 output):**
    
    **Route A: True Stills Processing (Angle_increment = 0°):**
    1.  Initialize `dials.command_line.stills_process.Processor` via `DIALSStillsProcessAdapter`
    2.  Call `do_import()` using image file path and base geometry
    3.  Invoke `processor.process_experiments()` for spot finding, indexing, refinement, integration
    4.  Collect output `integrated_experiments` and `integrated_reflections`
    
    **Route B: Sequence Processing (Angle_increment > 0°):**
    1.  Use `DIALSSequenceProcessAdapter` with CLI-based sequential workflow
    2.  Execute `dials.import` with sequence-appropriate parameters
    3.  Execute `dials.find_spots` with critical PHIL parameters:
        *   `spotfinder.filter.min_spot_size=3`
        *   `spotfinder.threshold.algorithm=dispersion`
    4.  Execute `dials.index` with parameters:
        *   `indexing.method=fft3d`
        *   `geometry.convert_sequences_to_stills=false`
    5.  Execute `dials.integrate` with sequence integration
    6.  Load output experiment and reflection objects from generated files
    
    **Common Continuation:**
    5.  If processing reports success, proceed to Sub-Module 1.S.1.Validation
    6.  Retrieve `Experiment_dials_i` and `Reflections_dials_i` objects (identical structure regardless of processing route)
```

### 3. Adapter Layer Enhancements

**Update Section 0.6 (Adapter Layer):**

```markdown
**0.6 Adapter Layer Enhancement for Dual Processing Modes:**
External DIALS processing **shall be wrapped** in two complementary adapter implementations:

*   **`DIALSStillsProcessAdapter`:** Wraps `dials.stills_process` Python API for true still images (Angle_increment = 0°)
*   **`DIALSSequenceProcessAdapter`:** Implements CLI-based sequential workflow for oscillation data (Angle_increment > 0°)

Both adapters **must** produce identical output interfaces (`Experiment` and `reflection_table` objects) to ensure downstream compatibility. The choice between adapters is determined by Module 1.S.0 data type detection.

**Critical PHIL Parameters for Sequence Processing:**
The `DIALSSequenceProcessAdapter` must apply the following non-default parameters:
*   `spotfinder.filter.min_spot_size=3` (not default 2)
*   `spotfinder.threshold.algorithm=dispersion` (not default)
*   `indexing.method=fft3d` (not fft1d)
*   `geometry.convert_sequences_to_stills=false` (preserve oscillation)

These parameters were determined through systematic comparison with working manual DIALS processing.
```

### 4. Configuration Updates

**Enhance `DIALSStillsProcessConfig` in types_IDL.md:**

```markdown
**Additional Fields for Dual Processing Support:**
*   `force_processing_mode`: Optional[str] = None  # "stills", "sequence", or None for auto-detection
*   `sequence_processing_phil_overrides`: Optional[List[str]] = None  # PHIL parameters specific to sequence processing
*   `data_type_detection_enabled`: bool = True  # Enable/disable automatic data type detection
```

### 5. Testing Adaptations

**Add to Module 1.S.1 Testing section:**

```markdown
**Testing for Dual Processing Mode Support:**
*   **Data Type Detection Testing:**
    *   **Input:** CBF files with known `Angle_increment` values (0.0°, 0.1°, 0.5°)
    *   **Verification:** Assert correct routing to stills vs sequence processing
    
*   **Sequence Processing Adapter Testing:**
    *   **Input:** CBF file with oscillation data, sequence processing configuration
    *   **Execution:** Call `DIALSSequenceProcessAdapter.process_still()`
    *   **Verification:** Assert successful processing and correct output object types
    
*   **Processing Route Integration Testing:**
    *   **Input:** Mixed dataset with both stills and sequence CBF files
    *   **Verification:** Assert each file is processed with correct adapter and produces valid results
    
*   **PHIL Parameter Validation Testing:**
    *   **Input:** Sequence data with incorrect PHIL parameters (default values)
    *   **Verification:** Assert processing failure, then success with correct parameters
```

### 6. Validation Logic Adaptations

**Module 1.S.1.Validation remains largely unchanged**, but add diagnostic information:

```markdown
**Enhanced Validation Reporting:**
*   Include `processing_route_used` in validation output
*   Log critical PHIL parameters used for sequence processing
*   Add specific checks for sequence processing quality metrics (e.g., minimum reflections expected)
```

### 7. Error Handling and Logging

**Enhance error handling for dual processing modes:**

```markdown
**Processing Mode Error Handling:**
*   If auto-detected processing mode fails, log the failure with specific error details
*   For sequence processing, capture and report DIALS CLI error messages
*   Add fallback mechanisms (e.g., try alternative PHIL parameters if initial sequence processing fails)
*   Enhanced logging to distinguish between stills vs sequence processing failures
```

## Implementation Strategy

### Phase 1: Core Infrastructure
1. Implement `DIALSSequenceProcessAdapter` based on working CLI approach
2. Add data type detection logic (CBF header parsing)
3. Update `StillsPipelineOrchestrator` to route based on data type

### Phase 2: Integration and Testing
1. Integrate both adapters into Module 1.S.1 processing logic
2. Implement comprehensive test suite for both processing modes
3. Validate that output objects are identical regardless of processing route

### Phase 3: Configuration and Documentation
1. Update configuration classes and IDL specifications
2. Enhance error handling and logging for dual processing modes
3. Update documentation to reflect dual processing capability

## Backward Compatibility

**Maintaining Existing Functionality:**
- All existing stills processing functionality remains unchanged
- Original `DIALSStillsProcessAdapter` preserved for true stills data
- Default behavior maintains auto-detection with fallback to stills processing
- Configuration option to force specific processing mode if needed

## Success Criteria

**The adaptation is successful when:**
1. Both stills (Angle_increment = 0°) and sequence (Angle_increment > 0°) data process correctly
2. Output objects (`Experiment_dials_i`, `Reflections_dials_i`) are identical in structure regardless of processing route
3. All downstream modules (Phases 2-4) work unchanged with both data types
4. Processing failures are correctly diagnosed and logged with appropriate error messages
5. Test suite validates both processing pathways comprehensively

## Risk Mitigation

**Potential Risks and Mitigations:**
1. **API Stability:** Sequence processing uses CLI calls, reducing dependency on unstable Python APIs
2. **Performance Impact:** CLI-based processing may be slower but is more reliable
3. **Configuration Complexity:** Auto-detection minimizes user configuration burden
4. **Output Compatibility:** Extensive testing ensures identical output object structures

## Future Considerations

**Potential Future Enhancements:**
1. Optimize sequence processing performance through batch CLI calls
2. Add support for micro-rotation data (small but non-zero oscillations)
3. Implement hybrid processing modes for edge cases
4. Add advanced data type classification beyond simple Angle_increment checking

## Conclusion

The required adaptations to `plan.md` are **surgical rather than fundamental**. The core insight is that both processing pathways produce identical output objects, meaning 85% of the pipeline remains unchanged. The primary adaptation is adding intelligent routing logic and a robust sequence processing adapter while preserving all existing stills processing functionality.

This adaptation transforms the pipeline from a single-mode processor into a **data-type-aware processor** that automatically selects the appropriate DIALS workflow based on the experimental data characteristics.
</file>

<file path="checklists/phase0.md">
**Agent Task: Implement Phase 0 of `plan.md` for Stills Diffuse Scattering Pipeline**

**Overall Goal for Agent (Phase 0):** To set up the project structure, define core utilities/adapters that will be used by Phase 1 modules, and establish testing infrastructure.

**Checklist Usage Instructions for Agent:**

1.  **Copy this entire checklist into your working memory or a dedicated scratchpad area.**
2.  **Context Priming:** Before starting a new phase or major module, carefully read all "Context Priming" points for that section.
3.  **Sequential Execution:** Address checklist items in the order presented, unless an item explicitly states it can be done in parallel or depends on a later item being drafted first (e.g., an IDL).
4.  **Update State:** As you work on an item, change its state field:
    *   `[ ] Open` -> `[P] In Progress` when you start.
    *   `[P] In Progress` -> `[D] Done` when completed successfully.
    *   `[P] In Progress` -> `[B] Blocked` if you encounter a blocker. Add a note explaining the blocker.
5.  **Record Details:**
    *   If a step requires creating a new file, add the **full path** to the created file in the "Details/Notes/Path" column.
    *   If a decision is made, note it briefly.
    *   If a sub-task needs to be broken down, add sub-bullets with their own states.
6.  **Iterative Review:** Periodically re-read completed sections and notes.
7.  **Save State:** Ensure this checklist with current states and notes is saved if pausing.

---

**Phase 0: Foundational Setup, Principles, and Adapters**

| Item ID | Task Description                                                                | State | Details/Notes/Path                                                                                                                                                              |
| :------ | :------------------------------------------------------------------------------ | :---- | :------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |
| **0.A** | **Context Priming (Phase 0)**                                                   | `[ ]` |                                                                                                                                                                               |
| 0.A.1   | Re-read `plan.md` Section 0.0: Testing Principles and Conventions             | `[ ]` |                                                                                                                                                                               |
| 0.A.2   | Re-read `plan.md` Section 0.6: Adapter Layer strategy                         | `[ ]` |                                                                                                                                                                               |
| 0.A.3   | Re-read `plan.md` Section 0.X: Intermediate QC Checkpoints                      | `[ ]` | Understand *types* of QC planned.                                                                                                                                               |
| 0.A.4   | Review `docs/01_IDL_GUIDELINES.md`                                              | `[ ]` |                                                                                                                                                                               |
| 0.A.5   | Review `docs/02_IMPLEMENTATION_RULES.md`                                        | `[ ]` |                                                                                                                                                                               |
| 0.A.6   | Review `docs/03_PROJECT_RULES.md`                                               | `[ ]` |                                                                                                                                                                               |
| 0.A.7   | Review `docs/ARCHITECTURE/types.md`                                             | `[ ]` |                                                                                                                                                                               |
| 0.A.8   | Review `src/diffusepipe/types/types_IDL.md`                                     | `[ ]` |                                                                                                                                                                               |
| 0.A.9   | Understand Goal: Setup project, core utilities/adapters, testing infra.       | `[ ]` |                                                                                                                                                                               |
|         |                                                                                 |       |                                                                                                                                                                               |
| **0.B** | **Project Structure & Core Utilities**                                          | `[ ]` |                                                                                                                                                                               |
| 0.B.1   | **Establish Directory Structure (as per `docs/03_PROJECT_RULES.md` & `plan.md`)** | `[ ]` |                                                                                                                                                                               |
| 0.B.1.a | Create `src/diffusepipe/` and `src/diffusepipe/__init__.py`                     | `[D]` | Path: `src/diffusepipe/` - Exists with __init__.py                                                                                                                              |
| 0.B.1.b | Create `src/diffusepipe/adapters/` & `__init__.py`                              | `[D]` | Path: `src/diffusepipe/adapters/` - Exists with multiple DIALS adapters                                                                                                        |
| 0.B.1.c | Create `src/diffusepipe/constants.py` (if needed)                               | `[D]` | Path: `src/diffusepipe/constants.py` - Exists                                                                                                                                   |
| 0.B.1.d | Create `src/diffusepipe/exceptions.py`                                          | `[D]` | Path: `src/diffusepipe/exceptions.py` - Exists                                                                                                                                  |
| 0.B.1.e | Create `src/diffusepipe/logging_config.py`                                      | `[D]` | Path: `src/diffusepipe/logging_config.py` - Exists                                                                                                                              |
| 0.B.1.f | Create `src/diffusepipe/utils/` & `__init__.py`                                 | `[D]` | Path: `src/diffusepipe/utils/` - Exists with cbf_utils.py                                                                                                                       |
| 0.B.1.g | Verify `src/diffusepipe/types/` exists                                          | `[D]` | Path: `src/diffusepipe/types/` - Exists with types_IDL.py                                                                                                                       |
| 0.B.1.h | Verify `src/diffusepipe/orchestration/` exists                                  | `[D]` | Path: `src/diffusepipe/orchestration/` - Exists with IDL files                                                                                                                  |
| 0.B.1.i | Verify `src/diffusepipe/extraction/` exists                                     | `[D]` | Path: `src/diffusepipe/extraction/` - Exists with data_extractor.py                                                                                                            |
| 0.B.1.j | Verify `src/diffusepipe/diagnostics/` exists                                    | `[D]` | Path: `src/diffusepipe/diagnostics/` - Exists with q_calculator.py                                                                                                             |
| 0.B.1.k | Create `src/diffusepipe/masking/` & `__init__.py`                               | `[D]` | Path: `src/diffusepipe/masking/` - Exists with mask generators                                                                                                                  |
| 0.B.1.l | Create `src/diffusepipe/crystallography/` & `__init__.py`                       | `[D]` | Path: `src/diffusepipe/crystallography/` - Exists with still processing modules                                                                                                 |
| 0.B.1.m | Create `src/diffusepipe/scaling/` & `__init__.py`                               | `[D]` | Path: `src/diffusepipe/scaling/` - Exists                                                                                                                                       |
| 0.B.1.n | Create `src/diffusepipe/merging/` & `__init__.py`                               | `[D]` | Path: `src/diffusepipe/merging/` - Exists                                                                                                                                       |
| 0.B.1.o | Create `tests/` & `__init__.py`                                                 | `[D]` | Path: `tests/` - Exists with comprehensive test structure                                                                                                                       |
| 0.B.1.p | Create `tests/conftest.py`                                                      | `[D]` | Path: `tests/conftest.py` - Exists                                                                                                                                              |
| 0.B.1.q | Create `tests/data/`                                                            | `[D]` | Path: `tests/data/` - Exists                                                                                                                                                    |
| 0.B.1.r | Create mirrored test subdirectories (e.g., `tests/adapters/`, etc.)           | `[D]` | Exists: adapters/, crystallography/, diagnostics/, extraction/, masking/, etc.                                                                                                  |
|         |                                                                                 |       |                                                                                                                                                                               |
| 0.B.2   | **Define Core Project Exceptions (`src/diffusepipe/exceptions.py`)**            | `[ ]` | Path: `src/diffusepipe/exceptions.py`                                                                                                                                           |
| 0.B.2.a | Define `PipelineError(Exception)`                                               | `[ ]` |                                                                                                                                                                               |
| 0.B.2.b | Define `ConfigurationError(PipelineError)`                                      | `[ ]` |                                                                                                                                                                               |
| 0.B.2.c | Define `DIALSError(PipelineError)`                                              | `[ ]` |                                                                                                                                                                               |
| 0.B.2.d | Define `FileSystemError(PipelineError)`                                         | `[ ]` |                                                                                                                                                                               |
| 0.B.2.e | Define `DataValidationError(PipelineError)`                                     | `[ ]` |                                                                                                                                                                               |
| 0.B.2.f | Define `NotImplementedYetError(PipelineError)`                                  | `[ ]` |                                                                                                                                                                               |
|         |                                                                                 |       |                                                                                                                                                                               |
| 0.B.3   | **Setup Basic Logging Configuration (`src/diffusepipe/logging_config.py`)**     | `[ ]` | Path: `src/diffusepipe/logging_config.py`                                                                                                                                       |
| 0.B.3.a | Implement function for basic console/file logging setup                       | `[ ]` | Decision: Use `logging.basicConfig` initially for simplicity.                                                                                                                   |
|         |                                                                                 |       |                                                                                                                                                                               |
| 0.B.4   | **Implement Testing Framework Setup (`tests/conftest.py`)**                     | `[ ]` | Path: `tests/conftest.py`                                                                                                                                                       |
| 0.B.4.a | Add `tmp_path` fixture (standard pytest) or equivalent for temp dirs.         | `[ ]` | Note: `tmp_path` is a built-in pytest fixture.                                                                                                                                  |
|         |                                                                                 |       |                                                                                                                                                                               |
| **0.C** | **Adapter Layer Implementation (Core Adapters for Phase 1)**                    | `[ ]` |                                                                                                                                                                               |
| 0.C.A   | **Context Priming (Adapters)**                                                  | `[ ]` |                                                                                                                                                                               |
| 0.C.A.1 | Review `plan.md` Section 0.6 (Adapter Layer strategy)                         | `[ ]` |                                                                                                                                                                               |
| 0.C.A.2 | Confirm understanding: Adapters encapsulate DIALS/CCTBX calls.                  | `[ ]` |                                                                                                                                                                               |
| 0.C.A.3 | Confirm understanding: Adapters handle config translation (e.g., to PHIL).      | `[ ]` |                                                                                                                                                                               |
| 0.C.A.4 | Confirm understanding: Adapters raise project-specific exceptions.              | `[ ]` |                                                                                                                                                                               |
|         |                                                                                 |       |                                                                                                                                                                               |
| 0.C.1   | **Adapter for `dials.stills_process.Processor` (Module 1.S.1)**               | `[ ]` |                                                                                                                                                                               |
| **0.C.1.idl** | **Define/Review Conceptual IDL for `DIALSStillsProcessAdapter`**            | `[ ]` | **Purpose:** Clarify inputs, outputs, core behavior, and error conditions *before Python coding*. Not a formal project IDL file unless decided, but a clear interface spec. <br>Input: `image_path: str`, `config: DIALSStillsProcessConfig`, `base_expt_path?: str`. <br>Output: `(Experiment?, reflection_table?, success_bool, logs_str)`. <br>Behavior: Wraps `dials.stills_process.Processor`. Handles PHIL, import, process, results. <br>Errors: `DIALSImportError`, `DIALSProcessError`, `PartialityMissingError`. |
| 0.C.1.a | Implement `DIALSStillsProcessAdapter` class structure                         | `[ ]` | Path: `src/diffusepipe/adapters/dials_stills_process_adapter.py` (Based on 0.C.1.idl)                                                                                           |
| 0.C.1.b | Implement `__init__` method                                                     | `[ ]` |                                                                                                                                                                               |
| 0.C.1.c | Implement `process_still` method core logic                                     | `[ ]` | Incl. PHIL generation, `Processor` instantiation, `do_import`, `process_experiments`.                                                                                           |
| 0.C.1.d | Implement result extraction & partiality check in `process_still`             | `[ ]` |                                                                                                                                                                               |
| 0.C.1.e | Implement error handling (DIALS exceptions -> `DIALSError`) in `process_still`  | `[ ]` |                                                                                                                                                                               |
| 0.C.1.f | Write Unit Tests for `DIALSStillsProcessAdapter`                                | `[ ]` | Path: `tests/adapters/test_dials_stills_process_adapter.py`                                                                                                                     |
| 0.C.1.g |   - Test: successful processing (mocked `Processor`)                              | `[ ]` |                                                                                                                                                                               |
| 0.C.1.h |   - Test: PHIL parameter generation                                                 | `[ ]` |                                                                                                                                                                               |
| 0.C.1.i |   - Test: `do_import` failure handling                                              | `[ ]` |                                                                                                                                                                               |
| 0.C.1.j |   - Test: `process_experiments` failure handling                                    | `[ ]` |                                                                                                                                                                               |
| 0.C.1.k |   - Test: result extraction and partiality check                                    | `[ ]` |                                                                                                                                                                               |
|         |                                                                                 |       |                                                                                                                                                                               |
| 0.C.2   | **Adapter for `dials.generate_mask` (Module 1.S.3, Option A)**                | `[ ]` |                                                                                                                                                                               |
| **0.C.2.idl** | **Define/Review Conceptual IDL for `DIALSGenerateMaskAdapter`**             | `[ ]` | **Purpose:** Clarify interface before Python coding. <br>Input: `experiment: Experiment`, `reflections: reflection_table`, `mask_generation_params: dict`. <br>Output: `tuple[flex.bool]` (mask per panel), `success_bool`, `logs_str`. <br>Behavior: Wraps `dials.util.masking.generate_mask` or `Script` execution. <br>Errors: `DIALSMaskGenError`. |
| 0.C.2.a | Implement `DIALSGenerateMaskAdapter` class structure                          | `[ ]` | Path: `src/diffusepipe/adapters/dials_generate_mask_adapter.py` (Based on 0.C.2.idl)                                                                                          |
| 0.C.2.b | Implement `generate_bragg_mask` method                                          | `[ ]` | Calls `dials.util.masking.generate_mask` or `Script` prog.                                                                                                                      |
| 0.C.2.c | Write Unit Tests for `DIALSGenerateMaskAdapter`                                 | `[ ]` | Path: `tests/adapters/test_dials_generate_mask_adapter.py`                                                                                                                      |
| 0.C.2.d |   - Test: successful mask generation (mocked `generate_mask`)                       | `[ ]` |                                                                                                                                                                               |
| 0.C.2.e |   - Test: parameter translation                                                     | `[ ]` |                                                                                                                                                                               |
|         |                                                                                 |       |                                                                                                                                                                               |
| 0.C.3   | **Adapter for DXTBX/flex I/O (General Utility)**                              | `[ ]` | Path: `src/diffusepipe/adapters/dxtbx_i_o_adapter.py` (Renamed for clarity)                                                                                                     |
| **0.C.3.idl** | **Define/Review Conceptual IDL for `DXTBXIOAdapter`**                       | `[ ]` | **Purpose:** Standardize file I/O for DIALS objects. <br>Methods: `load_experiment_list(path) -> ExperimentList`, `load_reflection_table(path) -> reflection_table`, `save_experiment_list(expts, path)`, `save_reflection_table(refls, path)`. <br>Errors: `FileReadError`, `FileWriteError`. |
| 0.C.3.a | Implement `load_experiment_list(expt_path)`                                     | `[ ]` | Wraps `ExperimentListFactory.from_json_file`. (Based on 0.C.3.idl)                                                                                                              |
| 0.C.3.b | Implement `load_reflection_table(refl_path)`                                    | `[ ]` | Wraps `flex.reflection_table.from_file`.                                                                                                                                        |
| 0.C.3.c | Implement `save_reflection_table(table, path)`                                  | `[ ]` |                                                                                                                                                                               |
| 0.C.3.d | Implement `save_experiment_list(experiments, path)`                             | `[ ]` |                                                                                                                                                                               |
| 0.C.3.e | Write Unit Tests for DXTBX/flex I/O adapter                                     | `[ ]` | Path: `tests/adapters/test_dxtbx_i_o_adapter.py`. Test loading/saving valid/invalid files.                                                                                      |
|         |                                                                                 |       |                                                                                                                                                                               |
| **0.D** | **Phase 0 Review & Next Steps**                                                 | `[ ]` |                                                                                                                                                                               |
| 0.D.1   | Self-Review: All Phase 0 items addressed? IDLs defined/reviewed?                | `[ ]` |                                                                                                                                                                               |
| 0.D.2   | Context Refresh: Re-read `plan.md` sections for Phase 1.                      | `[ ]` |                                                                                                                                                                               |
| 0.D.3   | Decision: Proceed to Phase 1 Checklist.                                         | `[ ]` |                                                                                                                                                                               |
</file>

<file path="docs/03_PROJECT_RULES.md">
# Project Rules and Conventions

**1. Purpose**

This document outlines general project-level rules, conventions, and processes that complement the coding and IDL guidelines. These rules help maintain a consistent and organized development environment.

**2. Directory Structure (Recommended)**

A consistent directory structure is crucial for navigability and maintainability. We recommend the following structure as a baseline, which can be adapted as needed:

```
DiffusePipe/
├── .git/                     # Version control
├── .gitignore
├── pyproject.toml            # Or requirements.txt, package.json, etc. (Project dependencies & config)
├── Makefile                  # Optional: for common development tasks (test, lint, format, build)
├── README.md                 # Top-level project overview, setup, and usage
├── src/                      # Main source code for the application/library
│   ├── diffusepipe/          # Primary package
│   │   ├── __init__.py
│   │   ├── main.py             # Main application entry point (if applicable)
│   │   ├── components/         # Logical grouping of components/modules
│   │   │   ├── component_a/
│   │   │   │   ├── __init__.py
│   │   │   │   ├── module_a1.py
│   │   │   │   └── module_a1_IDL.md # IDL specification for module_a1
│   │   │   └── component_b/
│   │   │       └── ...
│   │   ├── core/               # Core functionalities, shared utilities, base classes
│   │   │   ├── __init__.py
│   │   │   ├── utils.py
│   │   │   └── errors.py
│   │   └── models/             # Data models (e.g., Pydantic models, data classes)
│   │       ├── __init__.py
│   │       └── general_models.py
│   └── scripts/                # Utility or operational scripts (not part of main app)
│       └── some_script.py
├── tests/                    # All tests
│   ├── __init__.py
│   ├── conftest.py             # Shared pytest fixtures (if using pytest)
│   ├── components/             # Tests mirroring src/components structure
│   │   └── component_a/
│   │       ├── __init__.py
│   │       └── test_module_a1.py
│   └── core/
│       └── test_utils.py
└── docs/                     # All project documentation (as outlined in this template)
    ├── 00_START_HERE.md
    ├── 01_IDL_GUIDELINES.md
    ├── 02_IMPLEMENTATION_RULES.md
    ├── 03_PROJECT_RULES.md     # This file
    ├── ...
    └── ARCHITECTURE/
        ├── overview.md
        ├── types.md
        └── adr/
            ├── README.md
            └── ADR_TEMPLATE.md
```

*   **IDL Files:** Place IDL files (e.g., `*_IDL.md`) alongside the code module they define (e.g., `src/components/component_a/module_a1.py` and `src/components/component_a/module_a1_IDL.md`).
*   **Source Root:** The `src/` directory is typically the root for application code imports.
*   **Test Structure:** The `tests/` directory should mirror the structure of `src/` for easy navigation.

**3. Module/File Length Guideline**

*   **Principle:** Strive to keep modules and files concise and focused on a single responsibility (Single Responsibility Principle).
*   **Guideline:** As a general rule of thumb, aim to keep individual source code files (e.g., Python `.py` files) under **300-500 lines of code (LoC)**, excluding comments and blank lines.
*   **Rationale:** Shorter files are generally easier to understand, test, and maintain.
*   **Action:** If a file significantly exceeds this guideline, consider it a signal to refactor. Look for opportunities to extract classes, functions, or sub-modules. Refer to `04_REFACTORING_GUIDE.md`.
*   **Exception:** This is a guideline, not an absolute rule. Some files, by their nature (e.g., extensive data definitions, large auto-generated files), might be longer. Justify exceptions if they occur.

**4. Version Control (e.g., Git) Workflow**

*   **Branching Strategy:**
    *   `main` (or `master`): Stable, production-ready code. Protected branch. Merges typically happen via Pull Requests.
    *   `develop`: Integration branch for ongoing development. Features are merged here before going to `main`.
    *   `feature/[feature-name]` or `feat/[issue-id]-[short-desc]`: For new features. Branched from `develop`.
    *   `bugfix/[bug-name]` or `fix/[issue-id]-[short-desc]`: For bug fixes. Branched from `develop` (or `main` for hotfixes).
    *   `hotfix/[issue-id]`: For critical production fixes. Branched from `main`, merged back to `main` and `develop`.
*   **Commit Messages:**
    *   Follow conventional commit message format (e.g., `<type>(<scope>): <subject>`).
        *   Types: `feat`, `fix`, `docs`, `style`, `refactor`, `test`, `chore`, `perf`.
        *   Scope: Optional, e.g., `(component_a)`, `(auth)`.
    *   Subject line: Concise (e.g., <50 chars), imperative mood (e.g., "Add user login" not "Added user login").
    *   Body: Explain *what* and *why* vs. *how*. Reference issue numbers.
*   **Pull Requests (PRs) / Merge Requests (MRs):**
    *   All changes to `develop` and `main` should go through PRs/MRs.
    *   PRs should be focused and address a single feature or bug fix.
    *   Include a clear description of changes and link to relevant issues.
    *   Require at least one approval (or as per team policy).
    *   Ensure all automated checks (CI tests, linting) pass before merging.
    *   Prefer squash merging or rebase merging for a cleaner `develop`/`main` history, if agreed by the team.
*   **Rebasing:** Prefer rebasing feature branches onto `develop` before creating a PR to maintain a linear history and simplify merges. `git pull --rebase` for updating local branches.

**5. Architecture Decision Records (ADRs)**

*   **Purpose:** To document significant architectural decisions, their context, consequences, and alternatives considered.
*   **When to Create an ADR:**
    *   Introducing a new technology, library, or framework.
    *   Making a significant change to system structure or component responsibilities.
    *   Choosing a specific pattern or approach over others for a critical part of the system.
    *   Deprecating a major feature or component.
*   **Format:** Use the template provided in `docs/ARCHITECTURE/adr/ADR_TEMPLATE.md`.
*   **Storage:** Store ADRs in `docs/ARCHITECTURE/adr/`.
*   **Review:** ADRs should be reviewed by the team before being marked as "Accepted."

**5.1. Configuration Files and Resources**

*   **Configuration Files:** Store configuration files in appropriate directories based on their purpose:
    *   **Application Configs:** Store in `src/diffusepipe/config/`.
    *   **Build/Environment Configs:** Store in the project root or dedicated `config/` directory.
    *   **Task-specific Configs:** (e.g., PHIL files) Store in the related component's directory under `config/`.
        *   For the sequential DIALS workflow (used for sequence data), consider creating base PHIL files in `src/diffusepipe/config/` for each step (e.g., `import_sequence.phil`, `find_spots_sequence.phil`, `index_sequence.phil`, `integrate_sequence.phil`). The `DIALSSequenceProcessAdapter` can then use these as defaults and apply overrides.
*   **Referencing Configs:** Scripts should use absolute paths from the project root to reference configuration files rather than assuming relative locations.
*   **Documentation:** Document the purpose and format of configuration files in a README or in comments within the files themselves.

**6. Documentation Conventions**

*   **Living Document:** Documentation should be treated as a living part of the project and kept up-to-date with code changes.
*   **Audience:** Write for other developers (current and future) who will work on the project.
*   **Location:** All primary project documentation resides in the `docs/` directory.
*   **Updates:** Follow the process outlined in `05_DOCUMENTATION_GUIDE.md` for reviewing and updating documentation after significant changes.
*   **Clarity and Conciseness:** Be clear, concise, and unambiguous. Use diagrams where helpful.

**7. Dependency Management**

*   **File:** Use a standard dependency management file for your language/ecosystem (e.g., `pyproject.toml` with Poetry or PDM, `requirements.txt` for Python; `package.json` for Node.js).
*   **Pinning:** Pin versions of direct dependencies to ensure reproducible builds and avoid unexpected breaking changes from transitive dependencies. Use version ranges (e.g., `^1.2.3`, `~1.2.3`) thoughtfully.
*   **Review:** Regularly review and update dependencies to address security vulnerabilities and leverage new features/fixes.
*   **Minimize Dependencies:** Only add dependencies that provide significant value and are well-maintained.

**8. Code Reviews**

*   **Purpose:** Improve code quality, share knowledge, ensure adherence to standards, and catch bugs early.
*   **Focus Areas:**
    *   Correctness: Does the code do what it's supposed to do?
    *   Clarity & Readability.
    *   Test Coverage & Quality.
    *   Adherence to IDL contract and project rules.
    *   Design: Simplicity, maintainability, potential issues.
    *   Security considerations.
*   **Constructive Feedback:** Provide specific, actionable, and respectful feedback. Explain the "why" behind suggestions.
*   **Timeliness:** Aim to review PRs in a timely manner.

**9. Issue Tracking**

*   Use an issue tracker (e.g., GitHub Issues, Jira) for managing tasks, bugs, and feature requests.
*   Clearly describe issues, including steps to reproduce for bugs.
*   Link commits and PRs to relevant issues.

**10. Logging DIALS CLI Commands**

*   All command lines used to invoke DIALS tools (e.g., `dials.find_spots`, `dials.refine`, etc.) must be logged.
*   This logging should capture the exact command and its arguments as executed.
*   Logging can be to standard output, a dedicated log file, or an integrated logging system.
*   The purpose is to ensure reproducibility and aid in debugging pipeline issues.

**11. Continuous Integration/Continuous Deployment (CI/CD) (Recommended)**

*   Set up CI pipelines to automatically:
    *   Run linters and formatters.
    *   Run all tests.
    *   Build the application/library.
    *   (Optional) Deploy to staging/production environments.
*   Ensure CI checks must pass before merging PRs.
</file>

<file path="docs/06_DIALS_DEBUGGING_GUIDE.md">
# DIALS Integration Debugging Guide

## Overview

This guide provides comprehensive troubleshooting strategies for DIALS crystallography software integration issues based on real debugging experiences. It covers the most common failure modes and their solutions.

## Critical Issue: Stills vs Sequences Processing

### Problem Identification

**Symptoms:**
- DIALS processing fails with "no spots found" or indexing errors
- CBF files that work with manual DIALS commands fail in Python integration
- Inconsistent results between CLI DIALS and Python API usage

**Root Cause:**
CBF files with oscillation data (Angle_increment > 0) require sequence processing, not stills processing.

### Solution

**1. Check CBF Header:**
```bash
# Look for this key indicator in CBF headers:
# Angle_increment 0.1000 deg.  # Sequence data - use sequential workflow
# Angle_increment 0.0000 deg.  # Still data - use stills_process
```

**2. Use Correct Processing Mode:**
- **Stills data (0° oscillation):** Use `dials.stills_process`
- **Sequence data (>0° oscillation):** Use sequential CLI workflow

**3. Sequential Workflow Implementation:**
```python
# Correct approach for oscillation data:
subprocess.run(["dials.import", cbf_file])
subprocess.run(["dials.find_spots", "imported.expt"])  
subprocess.run(["dials.index", "imported.expt", "strong.refl"])
subprocess.run(["dials.integrate", "indexed.expt", "indexed.refl"])
```

## PHIL Parameter Issues

### Critical Parameters for Oscillation Data

```python
# Essential PHIL parameters that differ from defaults:
phil_overrides = [
    "spotfinder.filter.min_spot_size=3",          # Not default 2
    "spotfinder.threshold.algorithm=dispersion",   # Not default
    "indexing.method=fft3d",                       # Not fft1d  
    "geometry.convert_sequences_to_stills=false"   # Preserve oscillation
]
```

### Debugging PHIL Parameters

**1. Compare with Working Logs:**
- Check existing working DIALS processing directories
- Extract PHIL parameters from successful runs
- Compare against current parameters

**2. Test Parameter Changes Incrementally:**
- Change one parameter at a time
- Monitor spot finding results (should find ~100+ spots for good data)
- Validate indexing success

## Configuration Object Issues

### Common Configuration Errors

**Error Pattern:**
```
AttributeError: 'dict' object has no attribute 'spotfinder_threshold_algorithm'
```

**Root Cause:** Configuration object structure mismatch - code expects a structured object but receives a dictionary.

**Debugging Steps:**
1. **Verify Configuration Object Creation:**
   ```python
   # Check type of config object
   print(f"Config type: {type(config)}")
   print(f"Config contents: {config}")
   
   # Expected: <class 'diffusepipe.types.types_IDL.DIALSSequenceProcessConfig'>
   # Problem: <class 'dict'>
   ```

2. **Fix Configuration Construction:**
   ```python
   # WRONG - passing raw dict
   config = {"spotfinder_threshold_algorithm": "dispersion"}
   
   # CORRECT - using proper configuration class
   from diffusepipe.types.types_IDL import DIALSSequenceProcessConfig
   config = DIALSSequenceProcessConfig(
       spotfinder_threshold_algorithm="dispersion"
   )
   ```

3. **Validate Configuration Fields:**
   - Check that all required fields are present
   - Verify field names match the configuration class definition
   - Ensure proper type conversion (strings, floats, booleans)

### Data Type Detection Failures

**Log Pattern:**
```
num stills: 0
sweep: 1
```

**Interpretation:** DIALS import detects sequence data (oscillation), not still data.

**Common Issues:**
1. **Incorrect Adapter Selection:**
   - Code attempts to use `DIALSStillsProcessAdapter` on sequence data
   - Should use `DIALSSequenceProcessAdapter` for oscillation data

2. **CBF Header Detection Failure:**
   - Module 1.S.0 data type detection may be bypassed or failing
   - Manual check: `grep "Angle_increment" your_file.cbf`

3. **Configuration Override Issues:**
   - `force_processing_mode` setting may be incorrect
   - Routing logic may have bugs in condition checking

**Debugging Approach:**
```python
# Add explicit logging in data type detection
logger.info(f"CBF Angle_increment detected: {angle_increment}")
logger.info(f"Processing route selected: {processing_route}")
logger.info(f"Adapter type being used: {type(adapter)}")
```

## DIALS Python API Issues

### Import Problems

**Common Import Failures:**
```python
# This frequently breaks:
from dials.algorithms.indexing.indexer import Indexer  # WRONG

# Correct import:
from dials.algorithms.indexing import indexer
indexer_obj = indexer.Indexer.from_parameters(reflections, experiments, params)
```

**Solution Strategy:**
1. Check DIALS documentation: `libdocs/dials/DIALS_Python_API_Reference.md`
2. Test imports independently before integration
3. Use CLI-based adapters as fallback for unstable APIs

### API Stability Issues

**Problem:** DIALS Python API changes frequently between versions.

**Solutions:**
- Prefer CLI-based subprocess calls for production code
- Use Python API only for data access, not processing
- Implement fallback mechanisms

### PHIL Scope Import Changes

**Common Failure:**
```python
# This breaks in newer DIALS versions:
from dials.command_line.stills_process import master_phil_scope

# Correct approach:
from dials.command_line.stills_process import phil_scope
```

**Fix Strategy:**
1. Check DIALS version and documentation
2. Update import statements systematically
3. Test imports before main logic execution
4. Use try-catch blocks for version compatibility

## Validation Issues

### Primary Validation Method: Q-Vector Consistency

**Official Project Strategy:** The project uses **Q-vector consistency checking as the primary geometric validation method** (Module 1.S.1.Validation). This compares `q_model` (from DIALS-refined crystal models) with `q_observed` (recalculated from pixel coordinates).

**Key Implementation:**
- Calculate `q_model = s1 - s0` from DIALS reflection table
- Calculate `q_observed` by converting pixel coordinates to lab frame  
- Compare `|Δq| = |q_model - q_observed|` against `q_consistency_tolerance_angstrom_inv`
- Goal: `|Δq|` typically < 0.01 Å⁻¹ for good data

### Q-Vector Debugging Strategies

**Common Q-Vector Issues:**
- Physically impossible Q-vector magnitudes (>1.0 Å⁻¹)
- Large `|Δq|` discrepancies (>0.05 Å⁻¹)
- Coordinate system transformation errors

**Debugging Approach:**
1. **Verify coordinate transformations** in q_observed calculations
2. **Check reflection table columns** (`s1`, `xyzobs.px.value`, panel assignments)
3. **Validate detector geometry** and beam parameters

### Alternative Diagnostic: Pixel-Based Validation

**Use Case:** Pixel-based validation serves as a **diagnostic tool** or **simpler fallback** when Q-vector validation proves persistently problematic. It is **not** the primary validation method.

**Implementation:**
```python
def simple_position_validation(reflections, tolerance=2.0):
    """Use pixel position differences. Useful for debugging or as a simpler check."""
    obs_pos = reflections['xyzobs.px.value']
    calc_pos = reflections['xyzcal.px']
    
    # Calculate pixel distance differences
    dx = obs_pos[:, 0] - calc_pos[:, 0]
    dy = obs_pos[:, 1] - calc_pos[:, 1]
    distances = np.sqrt(dx*dx + dy*dy)
    
    # Reasonable tolerance: 1-2 pixels
    passed = np.mean(distances) <= tolerance
    return passed, distances
```
This pixel-based check avoids complex coordinate transformations and can help isolate whether the geometric model itself is poor or if the Q-vector calculations are problematic. However, strive to make the Q-vector validation the primary pass/fail criterion.

## Configuration Object vs Dictionary Issues

### Common Error Pattern

**Symptom:**
```
AttributeError: 'dict' object has no attribute 'spotfinder_threshold_algorithm'
```

**Root Cause:** Code expects structured configuration objects but receives raw dictionaries.

### Debugging Strategy

**1. Configuration Object Validation:**
```python
def debug_configuration_object(config, expected_type):
    """Comprehensive configuration debugging"""
    logger.info(f"Config actual type: {type(config)}")
    logger.info(f"Config expected type: {expected_type}")
    logger.info(f"Config contents: {config}")
    
    if isinstance(config, dict):
        logger.error("Configuration is raw dict, should be structured object")
        logger.info(f"Available dict keys: {list(config.keys())}")
        return False
    elif hasattr(config, '__dict__'):
        logger.info(f"Object attributes: {list(config.__dict__.keys())}")
        
    # Check for common field name mismatches
    if hasattr(config, 'spotfinder_threshold_algorithm'):
        logger.info("✓ spotfinder_threshold_algorithm field found")
        return True
    else:
        logger.error("✗ spotfinder_threshold_algorithm field missing")
        return False
```

**2. Common Resolution Patterns:**
```python
# WRONG - passing raw dict
config = {"spotfinder_threshold_algorithm": "dispersion"}

# CORRECT - using proper configuration class
from diffusepipe.types.types_IDL import DIALSSequenceProcessConfig
config = DIALSSequenceProcessConfig(
    spotfinder_threshold_algorithm="dispersion"
)
```

### Systematic Resolution Process

**1. Data Type Detection Issues:**
- Verify Module 1.S.0 correctly identifies stills vs sequences
- Check `Angle_increment` parsing logic
- Validate routing logic for adapter selection

**2. Configuration Class Verification:**
- Ensure proper configuration object instantiation
- Check field names match configuration class definitions
- Verify type conversion for all fields

**3. Adapter Selection Validation:**
- Confirm correct adapter chosen based on data type
- Validate adapter receives expected configuration type
- Check adapter internal configuration handling

## Debugging Workflow

### Step-by-Step Approach

**1. Verify Data Type:**
```bash
# Check CBF header for oscillation
grep "Angle_increment" your_file.cbf
```

**2. Compare with Working Approach:**
```bash
# Find existing working logs
ls -la *_dials_processing/
# Compare PHIL parameters and workflow
```

**3. Test Incremental Changes:**
- Start with working PHIL parameters
- Test spot finding first (should find 100+ spots)
- Validate indexing success
- Check integration results

**4. Use Diagnostic Outputs:**
```python
# Log key metrics at each step
logger.info(f"Found {len(reflections)} spots")
logger.info(f"Indexed {len(indexed_reflections)} reflections")
logger.info(f"Integrated {len(integrated_reflections)} reflections")
```

### Common Debugging Commands

```bash
# Test manual DIALS workflow
dials.import your_file.cbf
dials.find_spots imported.expt
dials.index imported.expt strong.refl
dials.integrate indexed.expt indexed.refl

# Check logs for errors
cat dials.find_spots.log
cat dials.index.log
cat dials.integrate.log

# Verify output files
ls -la *.expt *.refl
```

## Performance Optimization

### CLI vs Python API

**When to Use CLI:**
- Production processing pipelines
- When API stability is a concern
- For complex DIALS workflows

**When to Use Python API:**
- Data access and analysis
- Simple operations on DIALS objects
- When tight integration is needed

### Error Recovery

**Graceful Fallbacks:**
```python
try:
    # Try Python API first
    result = dials_python_api_call()
except ImportError:
    # Fall back to CLI
    result = dials_cli_subprocess_call()
```

## Key Files and Locations

**Working Examples:**
- `lys_nitr_10_6_0491_dials_processing/` - Successful processing logs
- `lys_nitr_8_2_0110_dials_processing/` - Another working example

**Implementation Files:**
- `src/diffusepipe/adapters/dials_stills_process_adapter.py` - Python API adapter
- `src/diffusepipe/adapters/dials_sequence_process_adapter.py` - CLI adapter
- `src/diffusepipe/crystallography/still_processing_and_validation.py` - Validation logic

**Configuration:**
- `src/diffusepipe/config/find_spots.phil` - Spot finding parameters
- `src/diffusepipe/config/refine_detector.phil` - Refinement parameters

## Quick Reference

### Decision Tree

1. **Check CBF Angle_increment**
   - = 0°: Use stills_process
   - > 0°: Use sequential workflow

2. **If Sequential Workflow Fails:**
   - Check PHIL parameters against working logs
   - Verify spot finding (>100 spots expected)
   - Test DIALS API imports

3. **If Q-Vector Validation Fails:**
   - Debug coordinate transformations in q_observed calculation
   - Check reflection table data quality (s1 vectors, pixel positions)
   - Use pixel position validation as diagnostic tool
   - Verify detector geometry and beam parameters

### Success Criteria

- **Spot Finding:** 100+ spots found
- **Indexing:** Crystal model successfully determined
- **Integration:** 500+ reflections integrated
- **Q-Vector Validation:** Mean |Δq| < 0.01 Å⁻¹ (primary criterion)
- **Pixel Validation:** Pixel differences < 2 pixels (diagnostic/fallback)

This guide should be consulted whenever DIALS integration issues arise, as it captures hard-learned lessons from real debugging sessions.
</file>

<file path="scripts/visual_diagnostics/check_diffuse_extraction.py">
#!/usr/bin/env python3
"""
Visual diagnostics for diffuse scattering extraction verification.

This script takes outputs from Phase 1 and Phase 2 (DataExtractor NPZ file) and generates
a series of diagnostic plots to visually verify the correctness of the diffuse scattering
extraction and correction process.

Key diagnostic plots include:
1. Raw image with extracted diffuse pixels overlay
2. Intensity correction effects (simplified version)
3. Q-space coverage projections
4. Radial Q-space distribution
5. Intensity distribution histogram
6. Intensity heatmap (conditional on pixel coordinates)
7. Sigma vs intensity scatter plot
8. I/sigma histogram

Author: DiffusePipe
"""

import argparse
import logging
import pickle
import sys
from pathlib import Path
from typing import Dict, List, Optional, Tuple, Union, Any

import matplotlib

matplotlib.use("Agg")  # Set early for non-interactive backend
import matplotlib.colors as mcolors
import matplotlib.pyplot as plt
import numpy as np

# Default maximum number of points to display in scatter plots for performance
DEFAULT_MAX_SCATTER_POINTS = 100000

# Import DIALS/DXTBX libraries
try:
    from dxtbx.imageset import ImageSetFactory
    from dxtbx.model.experiment_list import ExperimentListFactory
except ImportError as e:
    print(f"Error importing DIALS/DXTBX libraries: {e}")
    print("Please ensure DIALS is properly installed and accessible.")
    sys.exit(1)

# Import project utilities
try:
    from plot_utils import (
        plot_detector_image,
        plot_spot_overlay,
        setup_logging_for_plots,
        ensure_output_dir,
        close_all_figures,
    )
except ImportError as e:
    print(f"Error importing plot utilities: {e}")
    print("Please ensure you're running from the scripts/visual_diagnostics directory.")
    sys.exit(1)

logger = logging.getLogger(__name__)


def parse_arguments() -> argparse.Namespace:
    """Parse command-line arguments."""
    parser = argparse.ArgumentParser(
        description="Visual diagnostics for diffuse scattering extraction verification",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  # Basic usage
  python check_diffuse_extraction.py \\
    --raw-image ../../747/lys_nitr_10_6_0491.cbf \\
    --expt ../../lys_nitr_10_6_0491_dials_processing/indexed_refined_detector.expt \\
    --total-mask ../../lys_nitr_10_6_0491_dials_processing/total_diffuse_mask.pickle \\
    --npz-file extraction_output.npz

  # With optional masks and background map
  python check_diffuse_extraction.py \\
    --raw-image image.cbf --expt experiment.expt \\
    --total-mask total_mask.pickle --npz-file data.npz \\
    --bragg-mask bragg_mask.pickle \\
    --pixel-mask pixel_mask.pickle \\
    --bg-map background_map.npy \\
    --output-dir custom_output \\
    --verbose
        """,
    )

    # Required arguments
    parser.add_argument(
        "--raw-image", type=str, required=True, help="Path to raw CBF image file"
    )
    parser.add_argument(
        "--expt", type=str, required=True, help="Path to DIALS experiment .expt file"
    )
    parser.add_argument(
        "--total-mask",
        type=str,
        required=True,
        help="Path to total diffuse mask .pickle file",
    )
    parser.add_argument(
        "--npz-file",
        type=str,
        required=True,
        help="Path to DataExtractor output .npz file",
    )

    # Optional arguments
    parser.add_argument(
        "--bragg-mask", type=str, help="Path to Bragg mask .pickle file (optional)"
    )
    parser.add_argument(
        "--pixel-mask",
        type=str,
        help="Path to global pixel mask .pickle file (optional)",
    )
    parser.add_argument(
        "--bg-map",
        type=str,
        help="Path to measured background map .npy/.pickle file (optional)",
    )
    parser.add_argument(
        "--output-dir",
        type=str,
        default="extraction_visual_check",
        help="Output directory for diagnostic plots (default: extraction_visual_check)",
    )
    parser.add_argument("--verbose", action="store_true", help="Enable verbose logging")
    parser.add_argument(
        "--max-plot-points",
        type=int,
        default=DEFAULT_MAX_SCATTER_POINTS,
        help="Maximum number of points to display in scatter plots for performance.",
    )

    return parser.parse_args()


def load_npz_data(npz_path: str) -> Dict[str, np.ndarray]:
    """
    Load NPZ data from DataExtractor output.

    Args:
        npz_path: Path to NPZ file

    Returns:
        Dictionary containing loaded arrays
    """
    try:
        data = np.load(npz_path)
        result = {}

        # Required arrays
        required_keys = ["q_vectors", "intensities", "sigmas"]
        for key in required_keys:
            if key not in data:
                raise KeyError(f"Required key '{key}' not found in NPZ file")
            result[key] = data[key]

        # Optional coordinate arrays
        optional_keys = [
            "original_panel_ids",
            "original_fast_coords",
            "original_slow_coords",
        ]
        missing_coords = []
        for key in optional_keys:
            if key in data:
                result[key] = data[key]
            else:
                missing_coords.append(key)

        if missing_coords:
            logger.warning(
                f"Missing coordinate arrays in NPZ file: {missing_coords}. "
                "Some plots (pixel overlay, intensity heatmap) will be limited or skipped."
            )

        logger.info(f"Loaded NPZ data with {len(result['q_vectors'])} diffuse points")
        return result

    except Exception as e:
        logger.error(f"Failed to load NPZ data from {npz_path}: {e}")
        raise


def load_mask_pickle(mask_path: str) -> Optional[Tuple]:
    """
    Load a mask pickle file.

    Args:
        mask_path: Path to pickle file

    Returns:
        Mask tuple or None on error
    """
    try:
        with open(mask_path, "rb") as f:
            mask_data = pickle.load(f)
        logger.info(f"Loaded mask from {mask_path}")
        return mask_data
    except Exception as e:
        logger.error(f"Failed to load mask from {mask_path}: {e}")
        return None


def load_cbf_image(image_path: str):
    """
    Load CBF image using DXTBX.

    Args:
        image_path: Path to CBF file

    Returns:
        ImageSet object
    """
    try:
        imageset = ImageSetFactory.new([image_path])
        logger.info(f"Loaded CBF image from {image_path}")
        return imageset
    except Exception as e:
        logger.error(f"Failed to load CBF image from {image_path}: {e}")
        raise


def load_experiment(expt_path: str):
    """
    Load DIALS experiment.

    Args:
        expt_path: Path to .expt file

    Returns:
        First Experiment object from the list
    """
    try:
        experiments = ExperimentListFactory.from_json_file(expt_path)
        if len(experiments) == 0:
            raise ValueError("No experiments found in file")

        experiment = experiments[0]
        logger.info(f"Loaded experiment from {expt_path}")
        return experiment
    except Exception as e:
        logger.error(f"Failed to load experiment from {expt_path}: {e}")
        raise


def load_background_map(bg_map_path: str) -> Optional[np.ndarray]:
    """
    Load background map from .npy or .pickle file.

    Args:
        bg_map_path: Path to background map file

    Returns:
        Background map array or None on error
    """
    try:
        if bg_map_path.endswith(".npy"):
            bg_map = np.load(bg_map_path)
        elif bg_map_path.endswith(".pickle"):
            with open(bg_map_path, "rb") as f:
                bg_map = pickle.load(f)
        else:
            raise ValueError("Background map must be .npy or .pickle file")

        logger.info(f"Loaded background map from {bg_map_path}")
        return bg_map
    except Exception as e:
        logger.error(f"Failed to load background map from {bg_map_path}: {e}")
        return None


def validate_input_files(args: argparse.Namespace) -> None:
    """Validate that all required input files exist."""
    required_files = [args.raw_image, args.expt, args.total_mask, args.npz_file]

    optional_files = [args.bragg_mask, args.pixel_mask, args.bg_map]

    # Check required files
    for file_path in required_files:
        if not Path(file_path).exists():
            raise FileNotFoundError(f"Required file not found: {file_path}")

    # Check optional files
    for file_path in optional_files:
        if file_path and not Path(file_path).exists():
            logger.warning(f"Optional file not found: {file_path}")


def plot_diffuse_pixel_overlay(
    raw_image_data: Union[Any, np.ndarray],
    panel_id: int,
    pixel_coords_for_panel: List[Tuple[float, float]],
    title: str,
    output_path: str,
    bragg_coords: Optional[List[Tuple[float, float]]] = None,
    pixel_mask_coords: Optional[List[Tuple[float, float]]] = None,
) -> plt.Figure:
    """
    Plot raw image with extracted diffuse pixels overlaid.

    Args:
        raw_image_data: Raw detector image data
        panel_id: Panel ID being plotted
        pixel_coords_for_panel: List of (fast, slow) coordinates for diffuse pixels
        title: Plot title
        output_path: Output file path
        bragg_coords: Optional Bragg pixel coordinates
        pixel_mask_coords: Optional masked pixel coordinates

    Returns:
        matplotlib Figure object
    """
    fig = plot_spot_overlay(
        raw_image_data,
        pixel_coords_for_panel,
        title=title,
        output_path=output_path,
        spot_color="green",
        spot_size=1,
        log_scale=True,
        max_points=None,  # Subsampling handled before calling this function
    )

    # Add additional overlays if provided
    if bragg_coords or pixel_mask_coords:
        ax = fig.axes[0]

        if bragg_coords:
            bragg_x, bragg_y = zip(*bragg_coords) if bragg_coords else ([], [])
            ax.scatter(bragg_x, bragg_y, c="red", s=1, alpha=0.5, label="Bragg regions")

        if pixel_mask_coords:
            mask_x, mask_y = zip(*pixel_mask_coords) if pixel_mask_coords else ([], [])
            ax.scatter(mask_x, mask_y, c="black", s=1, alpha=0.3, label="Masked pixels")

        ax.legend()

        # Re-save with updated legend
        fig.savefig(output_path, dpi=150, bbox_inches="tight")

    return fig


def plot_q_projections(
    q_vectors: np.ndarray,
    intensities: np.ndarray,
    output_dir: Path,
    title_suffix: str = "",
    norm: Optional[mcolors.LogNorm] = None,
) -> None:
    """
    Generate Q-space projection plots.

    Args:
        q_vectors: Array of shape (N, 3) with qx, qy, qz
        intensities: Array of shape (N,) with intensity values
        output_dir: Output directory for plots
        title_suffix: Additional text to append to plot titles
        norm: Color normalization for scatter plots (e.g., LogNorm for log scale)
    """
    qx, qy, qz = q_vectors[:, 0], q_vectors[:, 1], q_vectors[:, 2]

    # Create three projection plots
    projections = [
        (qx, qy, "qx (Å⁻¹)", "qy (Å⁻¹)", "qx_qy"),
        (qx, qz, "qx (Å⁻¹)", "qz (Å⁻¹)", "qx_qz"),
        (qy, qz, "qy (Å⁻¹)", "qz (Å⁻¹)", "qy_qz"),
    ]

    for x_data, y_data, xlabel, ylabel, filename in projections:
        fig, ax = plt.subplots(figsize=(8, 6))

        scatter = ax.scatter(
            x_data, y_data, c=intensities, cmap="viridis", alpha=0.6, s=1, norm=norm
        )

        ax.set_xlabel(xlabel)
        ax.set_ylabel(ylabel)
        ax.set_title(f"Q-space projection: {xlabel} vs {ylabel}{title_suffix}")

        colorbar_label = (
            "Intensity (log scale)"
            if isinstance(norm, mcolors.LogNorm)
            else "Intensity"
        )
        plt.colorbar(scatter, ax=ax, label=colorbar_label)

        output_path = output_dir / f"q_projection_{filename}.png"
        fig.savefig(output_path, dpi=150, bbox_inches="tight")
        plt.close(fig)

        logger.info(f"Saved Q-space projection to {output_path}")


def plot_radial_q(
    q_vectors: np.ndarray,
    intensities: np.ndarray,
    output_path: Path,
    title_suffix: str = "",
    norm: Optional[mcolors.LogNorm] = None,
) -> None:
    """
    Plot intensity vs radial Q.

    Args:
        q_vectors: Array of shape (N, 3) with qx, qy, qz
        intensities: Array of shape (N,) with intensity values
        output_path: Output file path
        title_suffix: Additional text to append to plot title
        norm: Color normalization for scatter plot (e.g., LogNorm for log scale)
    """
    q_radial = np.sqrt(np.sum(q_vectors**2, axis=1))

    fig, ax = plt.subplots(figsize=(10, 6))

    scatter = ax.scatter(
        q_radial, intensities, c=intensities, cmap="viridis", alpha=0.6, s=1, norm=norm
    )

    ax.set_xlabel("Q (Å⁻¹)")
    ax.set_ylabel("Intensity")
    ax.set_title(f"Intensity vs Radial Q{title_suffix}")

    colorbar_label = (
        "Intensity (log scale)" if isinstance(norm, mcolors.LogNorm) else "Intensity"
    )
    plt.colorbar(scatter, ax=ax, label=colorbar_label)

    fig.savefig(output_path, dpi=150, bbox_inches="tight")
    plt.close(fig)

    logger.info(f"Saved radial Q plot to {output_path}")


def plot_intensity_histogram(intensities: np.ndarray, output_path: Path) -> None:
    """
    Plot intensity distribution histogram.

    Args:
        intensities: Array of intensity values
        output_path: Output file path
    """
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))

    # Linear scale histogram
    ax1.hist(intensities, bins=50, alpha=0.7, color="blue", edgecolor="black")
    ax1.set_xlabel("Intensity")
    ax1.set_ylabel("Count")
    ax1.set_title("Intensity Distribution (Linear)")

    # Log scale histogram (for better visualization of tails)
    ax2.hist(intensities, bins=50, alpha=0.7, color="green", edgecolor="black")
    ax2.set_xlabel("Intensity")
    ax2.set_ylabel("Count")
    ax2.set_yscale("log")
    ax2.set_title("Intensity Distribution (Log Scale)")

    plt.tight_layout()
    fig.savefig(output_path, dpi=150, bbox_inches="tight")
    plt.close(fig)

    logger.info(f"Saved intensity histogram to {output_path}")


def plot_intensity_heatmap(
    panel_id: int,
    pixel_coords_for_panel: List[Tuple[float, float]],
    intensities_for_panel: np.ndarray,
    panel_shape: Tuple[int, int],
    title: str,
    output_path: Path,
    max_points: Optional[int] = 500000,
) -> None:
    """
    Plot intensity heatmap for a detector panel.

    Args:
        panel_id: Panel ID
        pixel_coords_for_panel: List of (fast, slow) coordinates
        intensities_for_panel: Intensity values for each coordinate
        panel_shape: (height, width) of detector panel
        title: Plot title
        output_path: Output file path
        max_points: Maximum number of points to use for heatmap
    """
    # Apply subsampling if necessary
    if max_points and len(pixel_coords_for_panel) > max_points:
        indices = np.random.choice(
            len(pixel_coords_for_panel), max_points, replace=False
        )
        coords_subset = [pixel_coords_for_panel[i] for i in indices]
        intensities_subset = intensities_for_panel[indices]
        sampled_note = (
            f" (sampled {max_points} of {len(pixel_coords_for_panel)} points)"
        )
        title = title + sampled_note
    else:
        coords_subset = pixel_coords_for_panel
        intensities_subset = intensities_for_panel

    # Create 2D array for heatmap
    height, width = panel_shape
    heatmap = np.full((height, width), np.nan)

    # Populate heatmap with intensity values
    for (fast, slow), intensity in zip(coords_subset, intensities_subset):
        if 0 <= slow < height and 0 <= fast < width:
            heatmap[int(slow), int(fast)] = intensity

    fig = plot_detector_image(
        heatmap,
        title=title,
        output_path=str(output_path),
        log_scale=False,
        cmap="viridis",
    )

    plt.close(fig)
    logger.info(f"Saved intensity heatmap to {output_path}")


def plot_sigma_vs_intensity(
    intensities: np.ndarray,
    sigmas: np.ndarray,
    output_path: Path,
    title_suffix: str = "",
) -> None:
    """
    Plot sigma vs intensity scatter plot.

    Args:
        intensities: Array of intensity values
        sigmas: Array of sigma (error) values
        output_path: Output file path
        title_suffix: Additional text to append to plot title
    """
    fig, ax = plt.subplots(figsize=(10, 6))

    ax.scatter(intensities, sigmas, alpha=0.6, s=1, c="blue")

    ax.set_xlabel("Intensity")
    ax.set_ylabel("Sigma")
    ax.set_title(f"Sigma vs Intensity{title_suffix}")

    # Add ideal sigma relationship line (if reasonable)
    if len(intensities) > 0:
        max_intensity = np.max(intensities)
        x_ideal = np.linspace(0, max_intensity, 100)
        y_ideal = np.sqrt(x_ideal)  # Poisson noise relationship
        ax.plot(x_ideal, y_ideal, "r--", alpha=0.5, label="Poisson √I")
        ax.legend()

    fig.savefig(output_path, dpi=150, bbox_inches="tight")
    plt.close(fig)

    logger.info(f"Saved sigma vs intensity plot to {output_path}")


def plot_isigi_histogram(
    intensities: np.ndarray, sigmas: np.ndarray, output_path: Path
) -> None:
    """
    Plot I/sigma histogram.

    Args:
        intensities: Array of intensity values
        sigmas: Array of sigma (error) values
        output_path: Output file path
    """
    # Calculate I/sigma, handling sigma=0 carefully
    with np.errstate(divide="ignore", invalid="ignore"):
        isigi = intensities / sigmas
        isigi = isigi[np.isfinite(isigi)]  # Remove inf and nan values

    if len(isigi) == 0:
        logger.warning("No valid I/sigma values found")
        return

    fig, ax = plt.subplots(figsize=(10, 6))

    ax.hist(isigi, bins=50, alpha=0.7, color="orange", edgecolor="black")
    ax.set_xlabel("I/σ")
    ax.set_ylabel("Count")
    ax.set_title("I/σ Distribution")

    # Add statistics text
    mean_isigi = np.mean(isigi)
    median_isigi = np.median(isigi)
    ax.axvline(mean_isigi, color="red", linestyle="--", label=f"Mean: {mean_isigi:.2f}")
    ax.axvline(
        median_isigi, color="green", linestyle="--", label=f"Median: {median_isigi:.2f}"
    )
    ax.legend()

    fig.savefig(output_path, dpi=150, bbox_inches="tight")
    plt.close(fig)

    logger.info(f"Saved I/sigma histogram to {output_path}")


def generate_intensity_correction_summary(
    npz_data: Dict[str, np.ndarray], output_path: Path
) -> None:
    """
    Generate simplified intensity correction effects summary.

    Since full step-by-step intensity transformation data is not typically
    available in the NPZ file, this creates a simplified summary.

    Args:
        npz_data: Loaded NPZ data dictionary
        output_path: Output file path for summary
    """
    intensities = npz_data["intensities"]
    sigmas = npz_data["sigmas"]
    q_vectors = npz_data["q_vectors"]

    # Select a random sample of points for detailed inspection
    n_sample = min(100, len(intensities))
    sample_indices = np.random.choice(len(intensities), n_sample, replace=False)

    with open(output_path, "w") as f:
        f.write("Intensity Correction Effects Summary\n")
        f.write("=" * 40 + "\n\n")
        f.write(f"Total diffuse points: {len(intensities)}\n")
        f.write(f"Sample size for detailed inspection: {n_sample}\n\n")

        f.write("Sample of corrected intensity values:\n")
        f.write("Index\tqx\tqy\tqz\tIntensity\tSigma\tI/σ\n")
        f.write("-" * 60 + "\n")

        for i, idx in enumerate(sample_indices[:20]):  # Show first 20
            qx, qy, qz = q_vectors[idx]
            intensity = intensities[idx]
            sigma = sigmas[idx]
            isigi = intensity / sigma if sigma > 0 else np.inf

            f.write(
                f"{idx}\t{qx:.4f}\t{qy:.4f}\t{qz:.4f}\t{intensity:.2f}\t{sigma:.2f}\t{isigi:.2f}\n"
            )

        if n_sample > 20:
            f.write(f"... and {n_sample - 20} more points\n")

        f.write("\n" + "=" * 40 + "\n")
        f.write("NOTE: Full step-by-step intensity transformation plotting\n")
        f.write("requires enhanced logging/output from DataExtractor.\n")

    logger.info(f"Saved intensity correction summary to {output_path}")


def main():
    """Main function."""
    # Parse arguments
    args = parse_arguments()

    # Set up logging
    if args.verbose:
        logging.basicConfig(level=logging.DEBUG)
    else:
        setup_logging_for_plots()

    logger.info("Starting diffuse extraction visual diagnostics")

    try:
        # Validate input files
        validate_input_files(args)

        # Create output directory
        output_dir = ensure_output_dir(args.output_dir)
        logger.info(f"Output directory: {output_dir}")

        # Load data
        logger.info("Loading input data...")
        npz_data = load_npz_data(args.npz_file)
        imageset = load_cbf_image(args.raw_image)
        experiment = load_experiment(args.expt)
        # Load mask files (currently only validating they exist)
        load_mask_pickle(args.total_mask)

        # Load optional data (for validation)
        if args.bragg_mask:
            load_mask_pickle(args.bragg_mask)

        if args.pixel_mask:
            load_mask_pickle(args.pixel_mask)

        if args.bg_map:
            load_background_map(args.bg_map)

        # Generate diagnostic plots
        logger.info("Generating diagnostic plots...")

        # Extract key data
        q_vectors = npz_data["q_vectors"]
        intensities = npz_data["intensities"]
        sigmas = npz_data["sigmas"]

        # Centralized data subsampling for scatter plots
        max_points_for_plot = args.max_plot_points
        plot_title_suffix = ""
        if len(intensities) > max_points_for_plot:
            logger.info(
                f"Subsampling data from {len(intensities)} to {max_points_for_plot} points for relevant scatter plots."
            )
            indices = np.random.choice(
                len(intensities), max_points_for_plot, replace=False
            )
            q_vectors_sampled = q_vectors[indices]
            intensities_sampled = intensities[indices]
            sigmas_sampled = sigmas[indices]
            plot_title_suffix = (
                f" (sampled {max_points_for_plot} of {len(intensities)} points)"
            )
        else:
            q_vectors_sampled = q_vectors
            intensities_sampled = intensities
            sigmas_sampled = sigmas

        # Calculate common LogNorm for consistent intensity visualization
        common_log_norm = None
        if len(intensities_sampled) > 0:
            positive_intensities_sampled = intensities_sampled[intensities_sampled > 0]
            if len(positive_intensities_sampled) > 0:
                plot_vmin_log = np.percentile(
                    positive_intensities_sampled, 1
                )  # 1st percentile
                plot_vmin_log = max(plot_vmin_log, 1e-6)  # Ensure positive
                plot_vmax_log = np.percentile(
                    intensities_sampled, 99.9
                )  # 99.9th percentile
                if (
                    plot_vmax_log <= plot_vmin_log
                ):  # Handle edge case if percentiles are too close or inverted
                    plot_vmax_log = plot_vmin_log * 10 if plot_vmin_log > 0 else 1.0
                    # Fallback if all positive intensities are the same, use actual max of sampled positive data
                    if len(np.unique(positive_intensities_sampled)) == 1:
                        plot_vmax_log = positive_intensities_sampled[0] * 1.1
                    elif np.any(intensities_sampled > 0):
                        plot_vmax_log = max(
                            plot_vmax_log,
                            np.max(intensities_sampled[intensities_sampled > 0]),
                        )
                common_log_norm = mcolors.LogNorm(
                    vmin=plot_vmin_log, vmax=plot_vmax_log
                )
                logger.info(
                    f"Using logarithmic color scale with vmin={plot_vmin_log:.2e}, vmax={plot_vmax_log:.2e}"
                )
            else:  # No positive intensities in sample
                common_log_norm = mcolors.LogNorm(
                    vmin=1e-6, vmax=1.0
                )  # Default fallback norm
                logger.warning(
                    "No positive intensities found in sample, using fallback logarithmic norm"
                )
        else:  # No points in sample
            common_log_norm = mcolors.LogNorm(
                vmin=1e-6, vmax=1.0
            )  # Default fallback norm
            logger.warning("No points in sample, using fallback logarithmic norm")

        # Check if pixel coordinates are available
        has_pixel_coords = all(
            key in npz_data
            for key in [
                "original_panel_ids",
                "original_fast_coords",
                "original_slow_coords",
            ]
        )

        if has_pixel_coords:
            panel_ids = npz_data["original_panel_ids"]
            fast_coords = npz_data["original_fast_coords"]
            slow_coords = npz_data["original_slow_coords"]
            logger.info(
                "Pixel coordinates available - will generate full diagnostic suite"
            )
        else:
            logger.warning(
                "Pixel coordinates not available - skipping pixel overlay and heatmap plots"
            )

        # Load raw image data (use first panel)
        # Corrected logic:
        # ImageSetFactory.new() returns a list, so get the first ImageSet
        if isinstance(imageset, list):
            imageset_obj = imageset[0]
        else:
            imageset_obj = imageset

        raw_data_tuple_or_array = imageset_obj.get_raw_data(
            0
        )  # Get raw data for the 0-th image in the set
        if isinstance(raw_data_tuple_or_array, tuple):
            # Multi-panel detector, using data from the first panel for this plot
            raw_image_data = raw_data_tuple_or_array[0]
            logger.info(
                "Multi-panel detector: using data from panel 0 for diagnostic plot."
            )
        else:
            # Single-panel detector
            raw_image_data = raw_data_tuple_or_array

        # Get detector information for panel shapes
        detector = experiment.detector
        panel_0 = detector[0]  # First panel
        panel_shape = panel_0.get_image_size()[::-1]  # (height, width)

        # Plot 1: Diffuse pixel overlay (conditional on pixel coordinates)
        if has_pixel_coords:
            logger.info("Generating diffuse pixel overlay plot...")

            # Filter coordinates for panel 0
            panel_0_mask = panel_ids == 0
            if np.any(panel_0_mask):
                panel_0_coords = list(
                    zip(fast_coords[panel_0_mask], slow_coords[panel_0_mask])
                )

                # Apply subsampling for pixel overlay if necessary
                if len(panel_0_coords) > args.max_plot_points:
                    sample_indices = np.random.choice(
                        len(panel_0_coords), args.max_plot_points, replace=False
                    )
                    panel_0_coords_sampled = [panel_0_coords[i] for i in sample_indices]
                    overlay_title = f"Raw Image with Extracted Diffuse Pixels (Panel 0) - sampled {args.max_plot_points} of {len(panel_0_coords)} points"
                else:
                    panel_0_coords_sampled = panel_0_coords
                    overlay_title = "Raw Image with Extracted Diffuse Pixels (Panel 0)"

                plot_diffuse_pixel_overlay(
                    raw_image_data,
                    panel_id=0,
                    pixel_coords_for_panel=panel_0_coords_sampled,
                    title=overlay_title,
                    output_path=str(output_dir / "diffuse_pixel_overlay.png"),
                )
        else:
            logger.info("Skipping diffuse pixel overlay plot (no pixel coordinates)")

        # Plot 2: Intensity correction effects (simplified)
        logger.info("Generating intensity correction summary...")
        generate_intensity_correction_summary(
            npz_data, output_dir / "intensity_correction_summary.txt"
        )

        # Plot 3 & 4: Q-space coverage
        logger.info("Generating Q-space coverage plots...")
        plot_q_projections(
            q_vectors_sampled,
            intensities_sampled,
            output_dir,
            title_suffix=plot_title_suffix,
            norm=common_log_norm,
        )
        plot_radial_q(
            q_vectors_sampled,
            intensities_sampled,
            output_dir / "radial_q_distribution.png",
            title_suffix=plot_title_suffix,
            norm=common_log_norm,
        )

        # Plot 5: Intensity distribution
        logger.info("Generating intensity distribution histogram...")
        plot_intensity_histogram(intensities, output_dir / "intensity_histogram.png")

        # Plot 6: Intensity heatmap (conditional on pixel coordinates)
        if has_pixel_coords:
            logger.info("Generating intensity heatmap...")

            # Generate heatmap for panel 0
            panel_0_mask = panel_ids == 0
            if np.any(panel_0_mask):
                panel_0_coords = list(
                    zip(fast_coords[panel_0_mask], slow_coords[panel_0_mask])
                )
                panel_0_intensities = intensities[panel_0_mask]

                plot_intensity_heatmap(
                    panel_id=0,
                    pixel_coords_for_panel=panel_0_coords,
                    intensities_for_panel=panel_0_intensities,
                    panel_shape=panel_shape,
                    title="Intensity Heatmap (Panel 0)",
                    output_path=output_dir / "intensity_heatmap_panel_0.png",
                    max_points=args.max_plot_points,
                )
        else:
            logger.info("Skipping intensity heatmap plot (no pixel coordinates)")

        # Plot 7 & 8: Sigma analysis
        logger.info("Generating sigma analysis plots...")
        plot_sigma_vs_intensity(
            intensities_sampled,
            sigmas_sampled,
            output_dir / "sigma_vs_intensity.png",
            title_suffix=plot_title_suffix,
        )
        plot_isigi_histogram(intensities, sigmas, output_dir / "isigi_histogram.png")

        # Generate summary report
        logger.info("Generating summary report...")
        summary_path = output_dir / "extraction_diagnostics_summary.txt"
        with open(summary_path, "w") as f:
            f.write("Diffuse Extraction Visual Diagnostics Summary\n")
            f.write("=" * 50 + "\n\n")
            f.write("Input files:\n")
            f.write(f"  Raw image: {args.raw_image}\n")
            f.write(f"  Experiment: {args.expt}\n")
            f.write(f"  Total mask: {args.total_mask}\n")
            f.write(f"  NPZ file: {args.npz_file}\n\n")

            f.write("Data summary:\n")
            f.write(f"  Total diffuse points: {len(intensities)}\n")
            f.write(
                f"  Q-vector range: [{q_vectors.min():.4f}, {q_vectors.max():.4f}] Å⁻¹\n"
            )
            f.write(
                f"  Intensity range: [{intensities.min():.2f}, {intensities.max():.2f}]\n"
            )
            f.write(f"  Sigma range: [{sigmas.min():.2f}, {sigmas.max():.2f}]\n")
            f.write(
                f"  Mean I/σ: {np.mean(intensities/np.where(sigmas > 0, sigmas, np.inf)):.2f}\n"
            )
            f.write(f"  Pixel coordinates available: {has_pixel_coords}\n\n")

            f.write("Generated plots:\n")
            if has_pixel_coords:
                f.write("  ✓ diffuse_pixel_overlay.png\n")
                f.write("  ✓ intensity_heatmap_panel_0.png\n")
            else:
                f.write("  ✗ diffuse_pixel_overlay.png (no pixel coordinates)\n")
                f.write("  ✗ intensity_heatmap_panel_0.png (no pixel coordinates)\n")

            f.write("  ✓ intensity_correction_summary.txt\n")
            f.write("  ✓ q_projection_qx_qy.png\n")
            f.write("  ✓ q_projection_qx_qz.png\n")
            f.write("  ✓ q_projection_qy_qz.png\n")
            f.write("  ✓ radial_q_distribution.png\n")
            f.write("  ✓ intensity_histogram.png\n")
            f.write("  ✓ sigma_vs_intensity.png\n")
            f.write("  ✓ isigi_histogram.png\n")

        logger.info("Diffuse extraction visual diagnostics completed successfully")
        logger.info(f"Results saved to: {output_dir}")

    except Exception as e:
        logger.error(f"Error during processing: {e}")
        raise
    finally:
        # Clean up
        close_all_figures()


if __name__ == "__main__":
    main()
</file>

<file path="src/diffusepipe/adapters/dials_generate_mask_adapter.py">
"""Adapter for dials.generate_mask functionality."""

import logging
from typing import Dict, Any, Tuple, Optional

from diffusepipe.exceptions import DIALSError
from dials.command_line.generate_mask import phil_scope as generate_mask_phil_scope
from libtbx.phil import parse as phil_parse

logger = logging.getLogger(__name__)


class DIALSGenerateMaskAdapter:
    """
    Adapter for wrapping dials.generate_mask operations.

    This adapter encapsulates DIALS mask generation operations and provides
    error handling with project-specific exceptions.
    """

    def __init__(self):
        """Initialize the DIALS generate mask adapter."""
        pass

    def generate_bragg_mask(
        self,
        experiment: object,
        reflections: object,
        mask_generation_params: Optional[Dict[str, Any]] = None,
    ) -> Tuple[object, bool, str]:
        """
        Generate Bragg peak mask using dials.generate_mask.

        Args:
            experiment: DIALS Experiment object containing geometry and crystal model
            reflections: DIALS reflection_table containing indexed spots
            mask_generation_params: Optional parameters for mask generation

        Returns:
            Tuple containing:
            - Bragg mask as tuple of flex.bool arrays (one per panel)
            - Success boolean
            - Log messages string

        Raises:
            DIALSError: When mask generation fails
        """
        log_messages = []

        try:
            # Validate inputs
            if experiment is None:
                raise DIALSError("Experiment object cannot be None")
            if reflections is None:
                raise DIALSError("Reflections object cannot be None")

            log_messages.append("Starting Bragg mask generation")

            # Set default parameters if not provided
            if mask_generation_params is None:
                mask_generation_params = {
                    "border": 2,  # Border around each reflection
                }

            log_messages.append("Starting Bragg mask generation")

            # Create ExperimentList if we have a single experiment
            try:
                from dxtbx.model import ExperimentList

                if not isinstance(experiment, ExperimentList):
                    experiment_list = ExperimentList([experiment])
                else:
                    experiment_list = experiment
            except ImportError as e:
                raise DIALSError(f"Failed to import ExperimentList: {e}")

            # Convert mask_generation_params dict to PHIL object
            phil_params_object = self._create_phil_params(mask_generation_params)
            log_messages.append("Created PHIL parameters object")

            # Generate the mask
            mask_result = self._call_generate_mask(
                experiment_list, reflections, phil_params_object
            )

            log_messages.append("Generated Bragg mask successfully")

            # Validate the result
            self._validate_mask_result(mask_result)
            log_messages.append("Validated mask result")

            return mask_result, True, "\n".join(log_messages)

        except Exception as e:
            error_msg = f"Bragg mask generation failed: {e}"
            log_messages.append(error_msg)
            logger.error(error_msg)

            if isinstance(e, DIALSError):
                raise
            else:
                raise DIALSError(error_msg) from e

    def _call_generate_mask(
        self, experiment_list: object, reflections: object, phil_params_object: object
    ) -> object:
        """
        Generate Bragg masks from reflections positions.

        Note: The dials.util.masking.generate_mask function is for general detector
        masking and expects an ImageSet, not ExperimentList. For Bragg-specific
        masking from reflections, we create masks directly from reflection positions.

        Args:
            experiment_list: DIALS ExperimentList object
            reflections: DIALS reflection_table containing indexed spots
            phil_params_object: PHIL parameters object for mask generation

        Returns:
            Mask result as tuple of flex.bool arrays

        Raises:
            DIALSError: When the mask generation fails
        """
        try:
            from dials.array_family import flex

            # Get the experiment (assuming single experiment for now)
            if len(experiment_list) == 0:
                raise DIALSError("ExperimentList is empty")

            experiment = experiment_list[0]
            detector = experiment.detector

            # Get border parameter (default to 2 if not specified)
            border = getattr(phil_params_object, "border", 2)

            # Initialize mask for each panel (True = unmasked, False = masked)
            panel_masks = []
            for panel_idx, panel in enumerate(detector):
                panel_size = panel.get_image_size()
                # Create a mask of all True (unmasked) initially
                panel_mask = flex.bool(flex.grid(panel_size[1], panel_size[0]), True)
                panel_masks.append(panel_mask)

            # Mask regions around reflection centroids
            if reflections is not None and len(reflections) > 0:
                logger.info(
                    f"Masking {len(reflections)} reflection regions with border={border}"
                )

                # Get reflection centroids
                if "xyzobs.px.value" in reflections:
                    centroids = reflections["xyzobs.px.value"]
                elif "xyzcal.px" in reflections:
                    centroids = reflections["xyzcal.px"]
                else:
                    logger.warning(
                        "No centroid data found in reflections, using all-unmasked mask"
                    )
                    return tuple(panel_masks)

                # Get panel assignments if available
                if "panel" in reflections:
                    panels = reflections["panel"]
                else:
                    # Assume all reflections are on panel 0
                    panels = flex.int(len(reflections), 0)

                # Mask around each reflection
                for i in range(len(reflections)):
                    centroid = centroids[i]
                    panel_id = panels[i]

                    if panel_id >= len(panel_masks):
                        continue

                    # Get integer pixel coordinates
                    x_center = int(round(centroid[0]))
                    y_center = int(round(centroid[1]))

                    # Define masking region around the centroid
                    y_min = max(0, y_center - border)
                    y_max = min(panel_masks[panel_id].all()[0], y_center + border + 1)
                    x_min = max(0, x_center - border)
                    x_max = min(panel_masks[panel_id].all()[1], x_center + border + 1)

                    # Mask the region (set to False)
                    for y in range(y_min, y_max):
                        for x in range(x_min, x_max):
                            panel_masks[panel_id][y, x] = False

                # Count masked pixels for logging
                total_masked = sum((~mask).count(True) for mask in panel_masks)
                logger.info(f"Masked {total_masked} pixels around Bragg reflections")
            else:
                logger.info("No reflections provided, using all-unmasked Bragg mask")

            return tuple(panel_masks)

        except Exception as e:
            raise DIALSError(f"Bragg mask generation failed: {e}")

    def _create_phil_params(self, mask_generation_params: Dict[str, Any]) -> object:
        """
        Convert mask generation parameters dict to PHIL object.

        Args:
            mask_generation_params: Dictionary of mask generation parameters

        Returns:
            PHIL parameters object suitable for dials.util.masking.generate_mask

        Raises:
            DIALSError: When PHIL object creation fails
        """
        try:
            # Create PHIL string from parameters dict
            phil_string_parts = []

            # Handle border parameter
            if "border" in mask_generation_params:
                phil_string_parts.append(f"border = {mask_generation_params['border']}")

            # Handle other supported parameters (can be extended as needed)
            if "d_min" in mask_generation_params:
                phil_string_parts.append(f"d_min = {mask_generation_params['d_min']}")

            if "d_max" in mask_generation_params:
                phil_string_parts.append(f"d_max = {mask_generation_params['d_max']}")

            # Create the full PHIL string
            phil_string = "\n".join(phil_string_parts)

            # Parse the PHIL string and extract parameters
            current_phil = generate_mask_phil_scope.fetch(phil_parse(phil_string))
            phil_params_object = current_phil.extract()

            return phil_params_object

        except Exception as e:
            raise DIALSError(f"Failed to create PHIL parameters object: {e}")

    def _validate_mask_result(self, mask_result: object) -> None:
        """
        Validate the mask generation result.

        Args:
            mask_result: Result from DIALS mask generation

        Raises:
            DIALSError: When validation fails
        """
        if mask_result is None:
            raise DIALSError("Mask generation returned None")

        if not isinstance(mask_result, (tuple, list)):
            raise DIALSError("Mask result should be a tuple or list of panel masks")

        if len(mask_result) == 0:
            raise DIALSError("Mask result contains no panel masks")

        # Check that each panel mask is a valid flex.bool array
        for i, panel_mask in enumerate(mask_result):
            try:
                # In a real implementation, this would check for flex.bool type
                if panel_mask is None:
                    raise DIALSError(f"Panel {i} mask is None")

                # Additional validation could check mask dimensions, etc.
                logger.debug(f"Panel {i} mask validated successfully")

            except Exception as e:
                raise DIALSError(f"Panel {i} mask validation failed: {e}")

        logger.info(f"Validated mask with {len(mask_result)} panels")
</file>

<file path="src/diffusepipe/crystallography/q_consistency_checker.py">
"""Utility class that performs the q‑vector consistency test.

This was lost during the last refactor – the validator depends on it.  The
implementation below is **self‑contained**, uses only dxtbx/DIALS public APIs,
and does *zero* numpy/scitbx matrix gymnastics beyond basic linear algebra.

Algorithm (lab frame):
----------------------
1.  Pick up to *N* random reflections (default 500).
2.  For each reflection i:
    • `q_model` = s1_i – s0  (already in reflection table)
    • Obtain **observed** centroid `(x_px, y_px)` (prefer `xyzobs.px.value`; fall
      back to `xyzcal.px` if needed).
    • Convert that pixel to lab XYZ with `panel.get_pixel_lab_coord`.
    • Build `s1_obs` as the unit vector in that direction scaled by |s0|.
    • `q_obs` = s1_obs – s0.
    • Δq_i = |q_model – q_obs|.
3.  Aggregate mean / median / max and decide **pass** if:
      mean ≤ tolerance  **and**  max ≤ 5× tolerance.
"""

from __future__ import annotations

import logging
import random
from typing import Dict, Tuple

import numpy as np

logger = logging.getLogger(__name__)

__all__ = ["QConsistencyChecker"]


class QConsistencyChecker:
    """Stateless helper to keep the main validator lean."""

    # ------------------------------------------------------------------
    def check_q_consistency(
        self,
        experiment: object,
        reflections: object,
        tolerance: float = 0.01,
        max_reflections: int = 500,
    ) -> Tuple[bool, Dict[str, float]]:
        """Return (pass?, statistics) based on Δq magnitudes."""

        # Ensure required columns exist --------------------------------
        required_cols = {"miller_index", "panel", "s1"}
        if not required_cols.issubset(reflections):
            logger.warning(
                "Reflection table missing required columns %s", required_cols
            )
            return False, {"count": 0, "mean": None, "median": None, "max": None}

        # Prefer observed pixel centroid columns ------------------------
        pos_cols_priority = ["xyzobs.px.value", "xyzcal.px"]
        pos_col = next((c for c in pos_cols_priority if c in reflections), None)
        if pos_col is None:
            logger.warning("No pixel‑centroid column found in reflection table")
            return False, {"count": 0, "mean": None, "median": None, "max": None}

        n_total = len(reflections)
        if n_total == 0:
            logger.warning("No reflections available for q‑consistency test")
            return False, {"count": 0, "mean": None, "median": None, "max": None}

        indices = random.sample(range(n_total), k=min(max_reflections, n_total))

        beam = experiment.beam
        detector = experiment.detector
        s0 = np.array(beam.get_s0())
        k_mod = np.linalg.norm(s0)

        delta_q = []
        for idx in indices:
            try:
                panel_id = int(reflections["panel"][idx])
                panel = detector[panel_id]

                # model q from reflection table (s1 column)
                q_model = np.array(reflections["s1"][idx]) - s0

                px, py, _ = reflections[pos_col][idx]
                lab_xyz = np.array(panel.get_pixel_lab_coord((px, py)))
                direction = lab_xyz / np.linalg.norm(lab_xyz)
                s1_obs = direction * k_mod
                q_obs = s1_obs - s0

                delta_q.append(np.linalg.norm(q_model - q_obs))
            except Exception as exc:
                logger.debug("Δq calc failed for refl %d: %s", idx, exc)
                continue

        if not delta_q:
            return False, {"count": 0, "mean": None, "median": None, "max": None}

        arr = np.array(delta_q)
        stats = {
            "count": int(arr.size),
            "mean": float(arr.mean()),
            "median": float(np.median(arr)),
            "max": float(arr.max()),
        }
        passed = stats["mean"] <= tolerance and stats["max"] <= 5 * tolerance
        logger.info(
            "Q‑consistency: mean %.4g Å⁻¹, max %.4g Å⁻¹, n=%d, pass=%s",
            stats["mean"],
            stats["max"],
            stats["count"],
            passed,
        )
        return passed, stats
</file>

<file path="src/diffusepipe/extraction/data_extractor_IDL.md">
// == BEGIN IDL ==
module src.diffusepipe.extraction {

    # @depends_on_resource(type="FileSystem", purpose="Reading CBF, DIALS .expt, DIALS .refl (for consistency check), PDB, Bragg Mask files; Writing NPZ and diagnostic plot files")
    # @depends_on_resource(type="DIALS/dxtbx", purpose="Parsing .expt file for geometric model (beam, detector, crystal)")
    # @depends_on_resource(type="cctbx", purpose="Optional: PDB parsing, advanced geometric calculations, crystallographic math primitives")
    # @depends_on_type(src.diffusepipe.types.ExtractionConfig)
    # @depends_on_type(src.diffusepipe.types.ComponentInputFiles)
    # @depends_on_type(src.diffusepipe.types.OperationOutcome)
    interface DataExtractor {
        // Preconditions:
        // - `inputs.cbf_image_path` must point to an existing, readable CBF image file.
        // - `inputs.dials_expt_path` must point to an existing, readable DIALS experiment list JSON file.
        // - If `mask_total_2d` is not provided, `inputs.bragg_mask_path` must point to an existing, readable pickle file containing the Bragg mask.
        // - If `mask_total_2d` is provided, it must be a tuple of boolean arrays (one per detector panel) representing the combined mask (Mask_pixel AND NOT BraggMask_2D_raw_i).
        // - If `inputs.external_pdb_path` is provided, it must be an existing, readable PDB file.
        // - `config` must be a valid `ExtractionConfig` object.
        // - The directory for `output_npz_path` must be writable.
        // Postconditions:
        // - If `status` in the returned `OperationOutcome` is "SUCCESS":
        //   - An NPZ file is created at `output_npz_path`. This file minimally contains 'q_vectors' (Nx3 array), 'intensities' (N array), and 'sigmas' (N array).
        //   - If `config.plot_diagnostics` is true, diagnostic plot image files may be created (paths reported in `output_artifacts`).
        // - The returned `OperationOutcome` object details the success or failure of the operation.
        // Behavior:
        // 1. Logs verbose messages if `config.verbose` is true.
        // 2. **Load Data:**
        //    a. Parses `inputs.dials_expt_path` using dxtbx to obtain the DIALS `Experiment` object (containing beam, detector, crystal models).
        //    b. Reads the raw pixel data from `inputs.cbf_image_path`.
        //    c. Loads the Bragg mask (boolean NumPy array) from `inputs.bragg_mask_path`.
        //    d. If `inputs.external_pdb_path` is provided, parses it to extract reference unit cell parameters and potentially orientation information.
        // 3. **Consistency Checks (if `inputs.external_pdb_path` provided):**
        //    a. Compares unit cell parameters (lengths and angles) from the DIALS crystal model against the reference PDB, using `config.cell_length_tol` and `config.cell_angle_tol`.
        //    b. Compares crystal orientation from the DIALS crystal model against the reference PDB, using `config.orient_tolerance_deg`.
        //    c. If any consistency check fails, returns an `OperationOutcome` with status "FAILURE" and appropriate error message/code.
        // 4. **Pixel Processing Loop (or equivalent vectorized operations):**
        //    a. Iterates through each pixel of the image, respecting `config.pixel_step`.
        //    b. Skips pixels that are set to true in the Bragg mask.
        //    c. For each valid pixel:
        //       i. **Q-Vector Calculation:** Calculates the scattering vector `q` (components qx, qy, qz) for the pixel center using the DIALS `Experiment` geometry (detector panel, beam vector).
        //       ii. **Intensity & Initial Error:** Reads raw intensity $I_{raw}$. Applies `config.gain`. Initial sigma $\sigma_{raw} = \sqrt{I_{raw} \cdot \text{gain}}$ (assuming Poisson statistics).
        //       iii. **Corrections (and error propagation) using DIALS API and Custom Calculations:**
        //            - **Lorentz-Polarization (LP) correction:** Obtained via DIALS `dials.algorithms.integration.Corrections` class. LP correction can be enabled/disabled via `config.lp_correction_enabled`. Returns divisors which are converted to multipliers (LP_mult = 1/LP_divisor).
        //            - **Detector Quantum Efficiency (QE) correction:** Obtained via DIALS `dials.algorithms.integration.Corrections` class. Returns multipliers directly.
        //            - **Solid angle correction:** Custom calculation: SA_mult = 1/((pixel_area × cos_θ) / r²) where θ is angle between panel normal and scattered beam direction.
        //            - **Air attenuation correction:** Custom calculation using Beer-Lambert law: Air_mult = 1/exp(-μ_air × path_length). Uses configurable air temperature and pressure via `config.air_temperature_k` and `config.air_pressure_atm`.
        //            All correction factors are combined as multipliers: `TotalCorrection_mult = LP_mult × QE_mult × SA_mult × Air_mult`. Error propagation assumes correction factors have negligible uncertainty: σ_corrected = σ_initial × TotalCorrection_mult.
        //       iv. **Background Subtraction (and error propagation):**
        //            - If `config.subtract_measured_background_path` is provided: Load background map, subtract value for current pixel. Add variance of background to current $\sigma_I^2$.
        //            - Else if `config.subtract_constant_background_value` is provided: Subtract constant. (Error propagation for constant subtraction is typically zero unless constant has uncertainty).
        //       v. **Filtering:**
        //            - Resolution Filter: Exclude if d-spacing (derived from $|q|$) is outside [`config.max_res`, `config.min_res`].
        //            - Intensity Filter: Exclude if current corrected intensity is outside [`config.min_intensity`, `config.max_intensity`].
        //       vi. If pixel passes all filters, store its $(q_x, q_y, q_z)$, final corrected $I_{corr}$, and final propagated $\sigma_{I_{corr}}$.
        // 5. **Output Generation:**
        //    a. If no pixels pass filters, return `OperationOutcome` with status "FAILURE" or "WARNING".
        //    b. Saves the collected q-vectors, intensities, and sigmas as arrays in an NPZ file at `output_npz_path`.
        //    c. If `config.plot_diagnostics` is true, generates diagnostic plots (e.g., q-space coverage, intensity distributions) and saves them. Adds paths to `OperationOutcome.output_artifacts`.
        // 6. Returns `OperationOutcome` with status "SUCCESS".
        // @raises_error(condition="InputFileError", description="Failure to read or parse one of the essential input files (CBF, EXPT, Bragg Mask, PDB).")
        // @raises_error(condition="DIALSModelIncomplete", description="The DIALS .expt file is missing critical geometry information (e.g., beam, detector, or crystal model).")
        // @raises_error(condition="ConsistencyCheckFailed", description="Geometric parameters derived from DIALS data failed consistency checks against the external PDB within the specified tolerances.")
        // @raises_error(condition="BackgroundFileError", description="Failed to read or apply the `subtract_measured_background_path` file.")
        // @raises_error(condition="ProcessingError", description="An unexpected error occurred during pixel processing, q-vector calculation, or application of corrections.")
        // @raises_error(condition="OutputWriteError", description="Failed to write the output NPZ file or any diagnostic plot files.")
        src.diffusepipe.types.OperationOutcome extract_from_still(
            src.diffusepipe.types.ComponentInputFiles inputs,
            src.diffusepipe.types.ExtractionConfig config,
            string output_npz_path,
            optional tuple mask_total_2d
        );
    }
}
// == END IDL ==
</file>

<file path="src/diffusepipe/extraction/data_extractor.py">
"""Data extractor for diffuse scattering analysis."""

import logging
import os
import pickle
import numpy as np
import matplotlib.pyplot as plt
from typing import Dict, Tuple, Optional

from diffusepipe.types.types_IDL import (
    ComponentInputFiles,
    ExtractionConfig,
    OperationOutcome,
)

logger = logging.getLogger(__name__)


class DataExtractor:
    """
    Data extractor for processing diffuse scattering data from detector pixels.

    This class implements the extraction pipeline as specified in data_extractor_IDL.md,
    processing raw detector data through geometric calculations, corrections, and filtering.
    """

    def __init__(self):
        """Initialize the DataExtractor."""
        pass

    def extract_from_still(
        self,
        inputs: ComponentInputFiles,
        config: ExtractionConfig,
        output_npz_path: str,
        mask_total_2d: Optional[tuple] = None,
    ) -> OperationOutcome:
        """
        Extract diffuse scattering data from a still image.

        Args:
            inputs: Input file paths including CBF, experiment, and PDB
            config: Extraction configuration parameters
            output_npz_path: Path for output NPZ file
            mask_total_2d: Optional tuple of combined masks (Mask_pixel AND NOT BraggMask_2D_raw_i)
                           for each detector panel. If None, loads bragg_mask from inputs.

        Returns:
            OperationOutcome with success/failure status and output artifacts
        """
        try:
            if config.verbose:
                logger.setLevel(logging.DEBUG)

            logger.info("Starting diffuse data extraction")

            # 1. Validate inputs
            validation_result = self._validate_inputs(
                inputs, config, output_npz_path, mask_total_2d
            )
            if validation_result.status != "SUCCESS":
                return validation_result

            # 2. Load data
            logger.info("Loading input data")
            experiment, image_data, total_mask, pdb_data = self._load_data(
                inputs, mask_total_2d
            )

            # 3. Consistency checks (if PDB provided)
            if inputs.external_pdb_path:
                logger.info("Performing consistency checks against reference PDB")
                consistency_result = self._check_pdb_consistency(
                    experiment, pdb_data, config
                )
                if consistency_result.status != "SUCCESS":
                    return consistency_result

            # 4. Process pixels
            logger.info("Processing detector pixels")
            q_vectors, intensities, sigmas, panel_ids, fast_coords, slow_coords = (
                self._process_pixels(experiment, image_data, total_mask, config)
            )

            if len(q_vectors) == 0:
                return OperationOutcome(
                    status="FAILURE",
                    error_code="ProcessingError",
                    message="No pixels passed filtering criteria",
                )

            # 5. Save output
            logger.info(f"Saving {len(q_vectors)} data points to {output_npz_path}")
            self._save_output(
                q_vectors,
                intensities,
                sigmas,
                output_npz_path,
                panel_ids,
                fast_coords,
                slow_coords,
            )

            # 6. Generate diagnostics if requested
            output_artifacts = {"npz_file": output_npz_path}
            if config.plot_diagnostics:
                logger.info("Generating diagnostic plots")
                plot_paths = self._generate_diagnostic_plots(
                    q_vectors, intensities, sigmas, output_npz_path
                )
                output_artifacts.update(plot_paths)

            return OperationOutcome(
                status="SUCCESS",
                message=f"Successfully extracted {len(q_vectors)} data points",
                output_artifacts=output_artifacts,
            )

        except Exception as e:
            logger.error(f"Data extraction failed: {e}")

            # Determine error code based on error type
            if "Input" in str(e) or "not found" in str(e).lower():
                error_code = "InputFileError"
            elif "consistency" in str(e).lower():
                error_code = "ConsistencyCheckFailed"
            elif "background" in str(e).lower():
                error_code = "BackgroundFileError"
            elif "write" in str(e).lower() or "save" in str(e).lower():
                error_code = "OutputWriteError"
            elif "DIALS" in str(e) or "model" in str(e).lower():
                error_code = "DIALSModelIncomplete"
            else:
                error_code = "ProcessingError"

            return OperationOutcome(
                status="FAILURE",
                error_code=error_code,
                message=f"Data extraction failed: {e}",
            )

    def _validate_inputs(
        self,
        inputs: ComponentInputFiles,
        config: ExtractionConfig,
        output_npz_path: str,
        mask_total_2d: Optional[tuple] = None,
    ) -> OperationOutcome:
        """Validate all input files and parameters."""
        try:
            # Check required input files
            if not inputs.cbf_image_path:
                return OperationOutcome(
                    status="FAILURE",
                    error_code="InputFileError",
                    message="CBF image path not provided",
                )

            if not inputs.dials_expt_path:
                return OperationOutcome(
                    status="FAILURE",
                    error_code="InputFileError",
                    message="DIALS experiment path not provided",
                )

            # Bragg mask is now optional since mask_total_2d can be passed directly
            if mask_total_2d is None and not inputs.bragg_mask_path:
                return OperationOutcome(
                    status="FAILURE",
                    error_code="InputFileError",
                    message="Bragg mask path not provided and mask_total_2d not passed",
                )

            # Check file existence
            required_files = [
                (inputs.cbf_image_path, "CBF image"),
                (inputs.dials_expt_path, "DIALS experiment"),
            ]

            # Only require bragg_mask_path if mask_total_2d not provided
            if mask_total_2d is None and inputs.bragg_mask_path:
                required_files.append((inputs.bragg_mask_path, "Bragg mask"))

            if inputs.external_pdb_path:
                required_files.append((inputs.external_pdb_path, "External PDB"))

            for file_path, file_type in required_files:
                if not os.path.exists(file_path):
                    return OperationOutcome(
                        status="FAILURE",
                        error_code="InputFileError",
                        message=f"{file_type} file not found: {file_path}",
                    )

            # Check output directory is writable
            output_dir = os.path.dirname(output_npz_path)
            if output_dir and not os.path.exists(output_dir):
                os.makedirs(output_dir, exist_ok=True)

            return OperationOutcome(status="SUCCESS", message="Input validation passed")

        except Exception as e:
            return OperationOutcome(
                status="FAILURE",
                error_code="InputFileError",
                message=f"Input validation failed: {e}",
            )

    def _load_data(
        self, inputs: ComponentInputFiles, mask_total_2d: Optional[tuple] = None
    ) -> Tuple[object, np.ndarray, np.ndarray, Optional[object]]:
        """Load all required data files."""
        try:
            # Load DIALS experiment
            from dxtbx.model import ExperimentList

            experiment_list = ExperimentList.from_file(inputs.dials_expt_path)
            if len(experiment_list) == 0:
                raise ValueError("No experiments found in DIALS experiment file")
            experiment = experiment_list[0]

            # Validate experiment has required models
            if experiment.beam is None:
                raise ValueError("DIALS experiment missing beam model")
            if experiment.detector is None:
                raise ValueError("DIALS experiment missing detector model")
            if experiment.crystal is None:
                raise ValueError("DIALS experiment missing crystal model")

            # Load image data
            from dxtbx.imageset import ImageSetFactory

            imagesets = ImageSetFactory.new([inputs.cbf_image_path])
            if not imagesets:
                raise ValueError(f"Failed to load image from {inputs.cbf_image_path}")
            imageset = imagesets[0]
            image_data = imageset.get_raw_data(0)  # Get first (and only) image

            # Convert to numpy array if needed
            if isinstance(image_data, tuple):
                # Multi-panel detector - use first panel for now
                image_data = image_data[0].as_numpy_array()
            else:
                # Single panel detector
                image_data = image_data.as_numpy_array()

            # Load total mask (either passed in-memory or from file)
            if mask_total_2d is not None:
                # Use passed in-memory mask
                total_mask = mask_total_2d
                if isinstance(total_mask, (tuple, list)):
                    # Multi-panel mask - use first panel for now
                    total_mask = total_mask[0]
                    if hasattr(total_mask, "as_numpy_array"):
                        total_mask = total_mask.as_numpy_array()
            else:
                # Load Bragg mask from file (legacy path)
                with open(inputs.bragg_mask_path, "rb") as f:
                    bragg_mask_data = pickle.load(f)

                # Convert to numpy array if it's a DIALS flex array
                if hasattr(bragg_mask_data, "as_numpy_array"):
                    total_mask = bragg_mask_data.as_numpy_array()
                elif isinstance(bragg_mask_data, (tuple, list)):
                    # Multi-panel mask - use first panel for now
                    total_mask = bragg_mask_data[0]
                    if hasattr(total_mask, "as_numpy_array"):
                        total_mask = total_mask.as_numpy_array()
                else:
                    total_mask = bragg_mask_data

            # Load PDB data if provided
            pdb_data = None
            if inputs.external_pdb_path:
                from iotbx.pdb import input as pdb_input

                pdb_data = pdb_input(file_name=inputs.external_pdb_path)

            logger.debug(f"Loaded experiment with {len(experiment.detector)} panels")
            logger.debug(f"Image shape: {image_data.shape}")
            logger.debug(f"Total mask shape: {total_mask.shape}")

            return experiment, image_data, total_mask, pdb_data

        except Exception as e:
            raise Exception(f"Failed to load input data: {e}")

    def _check_pdb_consistency(
        self, experiment: object, pdb_data: object, config: ExtractionConfig
    ) -> OperationOutcome:
        """Check consistency between DIALS crystal model and reference PDB."""
        try:
            # Extract unit cell from DIALS crystal model
            dials_cell = experiment.crystal.get_unit_cell()
            dials_params = dials_cell.parameters()  # (a, b, c, alpha, beta, gamma)

            # Extract unit cell from PDB
            pdb_crystal_symmetry = pdb_data.crystal_symmetry()
            if pdb_crystal_symmetry is None:
                logger.warning("PDB file has no crystal symmetry information")
                return OperationOutcome(
                    status="SUCCESS", message="No PDB crystal symmetry to check"
                )

            pdb_cell = pdb_crystal_symmetry.unit_cell()
            pdb_params = pdb_cell.parameters()

            # Check cell length tolerances
            for i in range(3):  # a, b, c
                dials_length = dials_params[i]
                pdb_length = pdb_params[i]
                rel_diff = abs(dials_length - pdb_length) / pdb_length

                if rel_diff > config.cell_length_tol:
                    return OperationOutcome(
                        status="FAILURE",
                        error_code="ConsistencyCheckFailed",
                        message=f"Cell length {['a', 'b', 'c'][i]} differs by {rel_diff:.3f} "
                        f"(tolerance: {config.cell_length_tol}): "
                        f"DIALS={dials_length:.3f}, PDB={pdb_length:.3f}",
                    )

            # Check cell angle tolerances
            for i in range(3, 6):  # alpha, beta, gamma
                dials_angle = dials_params[i]
                pdb_angle = pdb_params[i]
                abs_diff = abs(dials_angle - pdb_angle)

                if abs_diff > config.cell_angle_tol:
                    return OperationOutcome(
                        status="FAILURE",
                        error_code="ConsistencyCheckFailed",
                        message=f"Cell angle {['alpha', 'beta', 'gamma'][i-3]} differs by {abs_diff:.3f}° "
                        f"(tolerance: {config.cell_angle_tol}°): "
                        f"DIALS={dials_angle:.3f}°, PDB={pdb_angle:.3f}°",
                    )

            logger.info("PDB consistency checks passed")
            return OperationOutcome(
                status="SUCCESS", message="PDB consistency checks passed"
            )

        except Exception as e:
            return OperationOutcome(
                status="FAILURE",
                error_code="ConsistencyCheckFailed",
                message=f"PDB consistency check failed: {e}",
            )

    def _process_pixels(
        self,
        experiment: object,
        image_data: np.ndarray,
        total_mask: np.ndarray,
        config: ExtractionConfig,
    ) -> Tuple[
        np.ndarray,
        np.ndarray,
        np.ndarray,
        Optional[np.ndarray],
        Optional[np.ndarray],
        Optional[np.ndarray],
    ]:
        """Process detector pixels to extract q-vectors, intensities, and errors.

        Uses vectorized implementation for improved performance.

        Returns:
            tuple: (q_vectors, intensities, sigmas, panel_ids, fast_coords, slow_coords)
                   The last three are None if save_original_pixel_coordinates is False
        """
        # Use vectorized implementation for better performance
        return self._process_pixels_vectorized(
            experiment, image_data, total_mask, config
        )

    def _process_pixels_iterative(
        self,
        experiment: object,
        image_data: np.ndarray,
        total_mask: np.ndarray,
        config: ExtractionConfig,
    ) -> Tuple[
        np.ndarray,
        np.ndarray,
        np.ndarray,
        Optional[np.ndarray],
        Optional[np.ndarray],
        Optional[np.ndarray],
    ]:
        """Original iterative implementation - kept for comparison and fallback.

        Returns:
            tuple: (q_vectors, intensities, sigmas, panel_ids, fast_coords, slow_coords)
                   The last three are None if save_original_pixel_coordinates is False
        """
        q_vectors_list = []
        intensities_list = []
        sigmas_list = []

        # Track pixel coordinates if requested
        panel_ids_list = [] if config.save_original_pixel_coordinates else None
        fast_coords_list = [] if config.save_original_pixel_coordinates else None
        slow_coords_list = [] if config.save_original_pixel_coordinates else None

        # Get detector panel (assume single panel for now)
        panel = experiment.detector[0]
        beam = experiment.beam

        # Get wavelength and beam vector for q-vector calculations
        wavelength = beam.get_wavelength()
        k_magnitude = 2 * np.pi / wavelength
        s0 = beam.get_s0()
        k_in = np.array([s0[0], s0[1], s0[2]]) * k_magnitude

        # Load background data if specified
        background_data = None
        if config.subtract_measured_background_path:
            try:
                background_data = np.load(config.subtract_measured_background_path)
                logger.info(
                    f"Loaded background data from {config.subtract_measured_background_path}"
                )
            except Exception as e:
                raise Exception(f"Failed to load background data: {e}")

        # Process pixels with step size
        height, width = image_data.shape
        total_pixels = (height // config.pixel_step) * (width // config.pixel_step)
        processed_pixels = 0

        logger.info(
            f"Processing {total_pixels} pixels with step size {config.pixel_step}"
        )

        for slow_idx in range(0, height, config.pixel_step):
            for fast_idx in range(0, width, config.pixel_step):
                processed_pixels += 1

                # Skip if in total mask (Bragg regions or bad pixels)
                if total_mask[slow_idx, fast_idx]:
                    continue

                # Calculate q-vector for this pixel
                try:
                    lab_coord = panel.get_pixel_lab_coord(
                        (float(fast_idx), float(slow_idx))
                    )
                    scatter_direction = np.array(
                        [lab_coord[0], lab_coord[1], lab_coord[2]]
                    )
                    scatter_direction_norm = scatter_direction / np.linalg.norm(
                        scatter_direction
                    )
                    k_out = scatter_direction_norm * k_magnitude
                    q_vector = k_out - k_in
                except Exception as e:
                    logger.debug(
                        f"Failed to calculate q-vector for pixel ({fast_idx}, {slow_idx}): {e}"
                    )
                    continue

                # Get raw intensity
                raw_intensity = float(image_data[slow_idx, fast_idx])

                # Apply background subtraction first (Module 2.S.2 order)
                bg_value = 0.0
                bg_variance = 0.0

                if background_data is not None:
                    bg_value = background_data[slow_idx, fast_idx]
                    # Assume Poisson statistics for background
                    bg_variance = bg_value if bg_value > 0 else 0.0
                elif config.subtract_constant_background_value is not None:
                    bg_value = config.subtract_constant_background_value
                    bg_variance = 0.0  # Constant background has no variance

                # Background-subtracted intensity
                intensity_bg_sub = raw_intensity - bg_value

                # Apply gain and exposure time normalization
                # Note: exposure time normalization will be added when config includes it
                intensity_processed = intensity_bg_sub * config.gain

                # Calculate error with proper error propagation (Module 2.S.2)
                # Var_photon_initial = I_raw / gain (variance of original raw count)
                var_photon_initial = (
                    raw_intensity / config.gain if config.gain > 0 else raw_intensity
                )
                # Var_processed = (Var_photon_initial + Var_bkg) * gain^2
                var_processed = (var_photon_initial + bg_variance) * (config.gain**2)
                sigma_processed = np.sqrt(var_processed)

                # Apply pixel corrections (Module 2.S.2)
                corrected_intensity, corrected_sigma = self._apply_pixel_corrections(
                    intensity_processed,
                    sigma_processed,
                    q_vector,
                    lab_coord,
                    panel,
                    beam,
                    experiment,
                    slow_idx,
                    fast_idx,
                    config,
                )

                # Apply resolution filter
                q_magnitude = np.linalg.norm(q_vector)
                d_spacing = 2 * np.pi / q_magnitude if q_magnitude > 0 else float("inf")

                if config.min_res is not None and d_spacing > config.min_res:
                    continue
                if config.max_res is not None and d_spacing < config.max_res:
                    continue

                # Apply intensity filter
                if (
                    config.min_intensity is not None
                    and corrected_intensity < config.min_intensity
                ):
                    continue
                if (
                    config.max_intensity is not None
                    and corrected_intensity > config.max_intensity
                ):
                    continue

                # Store valid pixel data
                q_vectors_list.append(q_vector)
                intensities_list.append(corrected_intensity)
                sigmas_list.append(corrected_sigma)

                # Store pixel coordinates if requested
                if config.save_original_pixel_coordinates:
                    panel_ids_list.append(0)  # Single panel for now
                    fast_coords_list.append(fast_idx)
                    slow_coords_list.append(slow_idx)

                # Progress logging
                if processed_pixels % 10000 == 0:
                    logger.debug(
                        f"Processed {processed_pixels}/{total_pixels} pixels, "
                        f"kept {len(q_vectors_list)} valid points"
                    )

        logger.info(
            f"Kept {len(q_vectors_list)} pixels out of {processed_pixels} processed"
        )

        # Convert to numpy arrays
        if len(q_vectors_list) > 0:
            q_vectors = np.array(q_vectors_list)
            intensities = np.array(intensities_list)
            sigmas = np.array(sigmas_list)

            # Convert coordinate arrays if tracking enabled
            if config.save_original_pixel_coordinates:
                panel_ids = np.array(panel_ids_list)
                fast_coords = np.array(fast_coords_list)
                slow_coords = np.array(slow_coords_list)
            else:
                panel_ids = fast_coords = slow_coords = None
        else:
            q_vectors = np.empty((0, 3))
            intensities = np.empty(0)
            sigmas = np.empty(0)
            panel_ids = fast_coords = slow_coords = None

        return q_vectors, intensities, sigmas, panel_ids, fast_coords, slow_coords

    def _process_pixels_vectorized(
        self,
        experiment: object,
        image_data: np.ndarray,
        total_mask: np.ndarray,
        config: ExtractionConfig,
    ) -> Tuple[
        np.ndarray,
        np.ndarray,
        np.ndarray,
        Optional[np.ndarray],
        Optional[np.ndarray],
        Optional[np.ndarray],
    ]:
        """Vectorized implementation of pixel processing for improved performance.

        Returns:
            tuple: (q_vectors, intensities, sigmas, panel_ids, fast_coords, slow_coords)
        """
        logger.info("Using vectorized pixel processing implementation")

        # Get detector panel (assume single panel for now)
        panel = experiment.detector[0]
        beam = experiment.beam

        # Get wavelength and beam vector for q-vector calculations
        wavelength = beam.get_wavelength()
        k_magnitude = 2 * np.pi / wavelength
        s0 = beam.get_s0()
        k_in = np.array([s0[0], s0[1], s0[2]]) * k_magnitude

        # Load background data if specified
        background_data = None
        if config.subtract_measured_background_path:
            try:
                background_data = np.load(config.subtract_measured_background_path)
                logger.info(
                    f"Loaded background data from {config.subtract_measured_background_path}"
                )
            except Exception as e:
                raise Exception(f"Failed to load background data: {e}")

        # Step 1: Generate coordinate arrays for pixels to process
        height, width = image_data.shape

        # Create coordinate grids with step size
        slow_indices = np.arange(0, height, config.pixel_step)
        fast_indices = np.arange(0, width, config.pixel_step)
        slow_grid, fast_grid = np.meshgrid(slow_indices, fast_indices, indexing="ij")

        # Flatten coordinate grids
        slow_coords_all = slow_grid.flatten()
        fast_coords_all = fast_grid.flatten()

        # Apply mask filtering
        mask_values = total_mask[slow_coords_all, fast_coords_all]
        unmasked_indices = ~mask_values

        slow_coords = slow_coords_all[unmasked_indices]
        fast_coords = fast_coords_all[unmasked_indices]
        n_pixels = len(slow_coords)

        logger.info(f"Processing {n_pixels} unmasked pixels (vectorized)")

        if n_pixels == 0:
            # No pixels to process
            if config.save_original_pixel_coordinates:
                return (
                    np.empty((0, 3)),
                    np.empty(0),
                    np.empty(0),
                    np.empty(0, dtype=int),
                    np.empty(0, dtype=int),
                    np.empty(0, dtype=int),
                )
            else:
                return np.empty((0, 3)), np.empty(0), np.empty(0), None, None, None

        # Step 2: Batch calculate lab coordinates and q-vectors
        lab_coords = np.zeros((n_pixels, 3))
        for i in range(n_pixels):
            try:
                lab_coord = panel.get_pixel_lab_coord(
                    (float(fast_coords[i]), float(slow_coords[i]))
                )
                lab_coords[i] = [lab_coord[0], lab_coord[1], lab_coord[2]]
            except Exception as e:
                logger.debug(
                    f"Failed to get lab coord for pixel ({fast_coords[i]}, {slow_coords[i]}): {e}"
                )
                # Mark as invalid - will be filtered later
                lab_coords[i] = [np.nan, np.nan, np.nan]

        # Filter out invalid coordinates
        valid_coords_mask = ~np.isnan(lab_coords).any(axis=1)
        lab_coords = lab_coords[valid_coords_mask]
        slow_coords = slow_coords[valid_coords_mask]
        fast_coords = fast_coords[valid_coords_mask]
        n_pixels = len(slow_coords)

        if n_pixels == 0:
            if config.save_original_pixel_coordinates:
                return (
                    np.empty((0, 3)),
                    np.empty(0),
                    np.empty(0),
                    np.empty(0, dtype=int),
                    np.empty(0, dtype=int),
                    np.empty(0, dtype=int),
                )
            else:
                return np.empty((0, 3)), np.empty(0), np.empty(0), None, None, None

        # Calculate q-vectors vectorized
        scatter_directions = (
            lab_coords / np.linalg.norm(lab_coords, axis=1)[:, np.newaxis]
        )
        k_out = scatter_directions * k_magnitude
        q_vectors = k_out - k_in[np.newaxis, :]

        # Step 3: Extract raw intensities and apply background subtraction
        raw_intensities = image_data[slow_coords, fast_coords].astype(float)

        # Background subtraction
        if background_data is not None:
            bg_values = background_data[slow_coords, fast_coords]
            bg_variances = np.maximum(bg_values, 0.0)  # Poisson statistics
        elif config.subtract_constant_background_value is not None:
            bg_values = np.full(n_pixels, config.subtract_constant_background_value)
            bg_variances = np.zeros(n_pixels)
        else:
            bg_values = np.zeros(n_pixels)
            bg_variances = np.zeros(n_pixels)

        # Apply background subtraction and gain
        intensities_bg_sub = raw_intensities - bg_values
        intensities_processed = intensities_bg_sub * config.gain

        # Error propagation
        var_photon_initial = (
            raw_intensities / config.gain if config.gain > 0 else raw_intensities
        )
        var_processed = (var_photon_initial + bg_variances) * (config.gain**2)
        sigmas_processed = np.sqrt(var_processed)

        # Step 4: Apply vectorized pixel corrections
        corrected_intensities, corrected_sigmas = (
            self._apply_pixel_corrections_vectorized(
                intensities_processed,
                sigmas_processed,
                q_vectors,
                lab_coords,
                panel,
                beam,
                experiment,
                slow_coords,
                fast_coords,
                config,
            )
        )

        # Step 5: Apply filters
        q_magnitudes = np.linalg.norm(q_vectors, axis=1)
        d_spacings = 2 * np.pi / q_magnitudes
        d_spacings[q_magnitudes == 0] = np.inf

        # Resolution filter
        resolution_mask = np.ones(n_pixels, dtype=bool)
        if config.min_res is not None:
            resolution_mask &= d_spacings <= config.min_res
        if config.max_res is not None:
            resolution_mask &= d_spacings >= config.max_res

        # Intensity filter
        intensity_mask = np.ones(n_pixels, dtype=bool)
        if config.min_intensity is not None:
            intensity_mask &= corrected_intensities >= config.min_intensity
        if config.max_intensity is not None:
            intensity_mask &= corrected_intensities <= config.max_intensity

        # Combine filters
        final_mask = resolution_mask & intensity_mask

        # Apply final filtering
        final_q_vectors = q_vectors[final_mask]
        final_intensities = corrected_intensities[final_mask]
        final_sigmas = corrected_sigmas[final_mask]

        logger.info(f"Kept {len(final_intensities)} pixels after filtering")

        # Handle coordinate tracking
        if config.save_original_pixel_coordinates:
            final_panel_ids = np.zeros(
                len(final_intensities), dtype=int
            )  # Single panel
            final_fast_coords = fast_coords[final_mask]
            final_slow_coords = slow_coords[final_mask]
            return (
                final_q_vectors,
                final_intensities,
                final_sigmas,
                final_panel_ids,
                final_fast_coords,
                final_slow_coords,
            )
        else:
            return final_q_vectors, final_intensities, final_sigmas, None, None, None

    def _apply_pixel_corrections_vectorized(
        self,
        intensities: np.ndarray,
        sigmas: np.ndarray,
        q_vectors: np.ndarray,
        lab_coords: np.ndarray,
        panel: object,
        beam: object,
        experiment: object,
        slow_coords: np.ndarray,
        fast_coords: np.ndarray,
        config: ExtractionConfig,
    ) -> Tuple[np.ndarray, np.ndarray]:
        """Apply pixel corrections in vectorized fashion."""

        n_pixels = len(intensities)

        # Calculate s1 vectors for DIALS corrections
        s1_vectors = q_vectors + np.array(beam.get_s0())[np.newaxis, :]

        # Get LP and QE corrections from DIALS (batch)
        if config.lp_correction_enabled:
            try:
                from dials.algorithms.integration import Corrections
                from dials.array_family import flex

                # Create or use cached corrections object
                if not hasattr(self, "_corrections_obj"):
                    self._corrections_obj = Corrections(
                        beam, experiment.goniometer, experiment.detector
                    )

                # Convert to DIALS flex arrays
                s1_flex = flex.vec3_double()
                panel_indices_flex = flex.size_t()

                for i in range(n_pixels):
                    s1_flex.append(tuple(s1_vectors[i]))
                    panel_indices_flex.append(0)  # Single panel for now

                # Get LP corrections (returns divisors)
                lp_divisors = self._corrections_obj.lp(s1_flex)
                lp_multipliers = np.array(
                    [1.0 / lp_divisors[i] for i in range(len(lp_divisors))]
                )

                # Get QE corrections (returns multipliers)
                qe_multipliers_flex = self._corrections_obj.qe(
                    s1_flex, panel_indices_flex
                )
                qe_multipliers = np.array(
                    [qe_multipliers_flex[i] for i in range(len(qe_multipliers_flex))]
                )

            except Exception as e:
                logger.debug(f"DIALS corrections failed, using defaults: {e}")
                lp_multipliers = np.ones(n_pixels)
                qe_multipliers = np.ones(n_pixels)
        else:
            lp_multipliers = np.ones(n_pixels)
            qe_multipliers = np.ones(n_pixels)

        # Calculate custom corrections vectorized
        sa_multipliers = self._calculate_solid_angle_correction_vectorized(
            lab_coords, panel, slow_coords, fast_coords
        )
        air_multipliers = self._calculate_air_attenuation_correction_vectorized(
            lab_coords, beam, config
        )

        # Combine all corrections
        total_corrections = (
            lp_multipliers * qe_multipliers * sa_multipliers * air_multipliers
        )

        # Apply corrections
        corrected_intensities = intensities * total_corrections
        corrected_sigmas = sigmas * total_corrections

        return corrected_intensities, corrected_sigmas

    def _calculate_solid_angle_correction_vectorized(
        self,
        lab_coords: np.ndarray,
        panel: object,
        slow_coords: np.ndarray,
        fast_coords: np.ndarray,
    ) -> np.ndarray:
        """Calculate solid angle corrections for multiple pixels."""
        try:
            pixel_sizes = panel.get_pixel_size()
            pixel_area = pixel_sizes[0] * pixel_sizes[1]  # mm²

            # Get panel normal
            fast_axis = np.array(panel.get_fast_axis())
            slow_axis = np.array(panel.get_slow_axis())
            normal = np.cross(fast_axis, slow_axis)
            normal = normal / np.linalg.norm(normal)

            # Calculate distances and directions
            distances = np.linalg.norm(lab_coords, axis=1)
            scatter_directions = lab_coords / distances[:, np.newaxis]

            # Calculate solid angles
            cos_theta = np.abs(np.dot(scatter_directions, normal))
            solid_angles = (pixel_area * cos_theta) / (distances**2)

            # Convert to correction multipliers
            sa_multipliers = 1.0 / solid_angles

            return sa_multipliers

        except Exception as e:
            logger.debug(f"Vectorized solid angle calculation failed: {e}")
            return np.ones(len(lab_coords))

    def _calculate_air_attenuation_correction_vectorized(
        self, lab_coords: np.ndarray, beam: object, config: ExtractionConfig
    ) -> np.ndarray:
        """Calculate air attenuation corrections for multiple pixels."""
        try:
            # Get X-ray energy
            wavelength = beam.get_wavelength()
            energy_ev = 12398.4 / wavelength

            # Calculate path lengths
            path_lengths = (
                np.linalg.norm(lab_coords, axis=1) / 1000.0
            )  # Convert mm to m

            # Get air attenuation coefficient (same for all pixels)
            temperature_k = getattr(config, "air_temperature_k", 293.15)
            pressure_atm = getattr(config, "air_pressure_atm", 1.0)
            mu_air = self._calculate_air_attenuation_coefficient(
                energy_ev, temperature_k, pressure_atm
            )

            # Calculate attenuation corrections
            attenuations = np.exp(-mu_air * path_lengths)
            air_multipliers = 1.0 / attenuations

            return air_multipliers

        except Exception as e:
            logger.debug(f"Vectorized air attenuation calculation failed: {e}")
            return np.ones(len(lab_coords))

    def _save_output(
        self,
        q_vectors: np.ndarray,
        intensities: np.ndarray,
        sigmas: np.ndarray,
        output_path: str,
        panel_ids: Optional[np.ndarray] = None,
        fast_coords: Optional[np.ndarray] = None,
        slow_coords: Optional[np.ndarray] = None,
    ):
        """Save extracted data to NPZ file."""
        try:
            # Base data to save
            save_data = {
                "q_vectors": q_vectors,
                "intensities": intensities,
                "sigmas": sigmas,
            }

            # Add pixel coordinates if available
            if (
                panel_ids is not None
                and fast_coords is not None
                and slow_coords is not None
            ):
                save_data["original_panel_ids"] = panel_ids
                save_data["original_fast_coords"] = fast_coords
                save_data["original_slow_coords"] = slow_coords
                logger.info("Including original pixel coordinates in NPZ output")

            np.savez_compressed(output_path, **save_data)
            logger.info(f"Saved data to {output_path}")
        except Exception as e:
            raise Exception(f"Failed to save output file: {e}")

    def _generate_diagnostic_plots(
        self,
        q_vectors: np.ndarray,
        intensities: np.ndarray,
        sigmas: np.ndarray,
        output_prefix: str,
    ) -> Dict[str, str]:
        """Generate diagnostic plots."""
        try:
            base_path = output_prefix.replace(".npz", "")
            plot_paths = {}

            # Q-space coverage plot
            fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))

            # 2D q-space projection
            ax1.scatter(q_vectors[:, 0], q_vectors[:, 1], alpha=0.5, s=1)
            ax1.set_xlabel("qx (Å⁻¹)")
            ax1.set_ylabel("qy (Å⁻¹)")
            ax1.set_title("Q-space Coverage (qx vs qy)")
            ax1.set_aspect("equal")

            # Intensity distribution
            ax2.hist(np.log10(intensities + 1), bins=50, alpha=0.7)
            ax2.set_xlabel("log10(Intensity + 1)")
            ax2.set_ylabel("Count")
            ax2.set_title("Intensity Distribution")

            plt.tight_layout()
            qspace_plot_path = f"{base_path}_qspace_coverage.png"
            plt.savefig(qspace_plot_path, dpi=150, bbox_inches="tight")
            plt.close()

            plot_paths["qspace_coverage_plot"] = qspace_plot_path

            # Resolution shell plot
            q_magnitudes = np.linalg.norm(q_vectors, axis=1)
            d_spacings = 2 * np.pi / q_magnitudes

            fig, ax = plt.subplots(figsize=(8, 6))
            ax.scatter(d_spacings, intensities, alpha=0.5, s=1)
            ax.set_xlabel("d-spacing (Å)")
            ax.set_ylabel("Intensity")
            ax.set_title("Intensity vs Resolution")
            ax.set_xlim(0, min(20, np.max(d_spacings)))

            resolution_plot_path = f"{base_path}_resolution_analysis.png"
            plt.savefig(resolution_plot_path, dpi=150, bbox_inches="tight")
            plt.close()

            plot_paths["resolution_analysis_plot"] = resolution_plot_path

            logger.info(f"Generated diagnostic plots: {', '.join(plot_paths.values())}")
            return plot_paths

        except Exception as e:
            logger.warning(f"Failed to generate diagnostic plots: {e}")
            return {}

    def _apply_pixel_corrections(
        self,
        intensity: float,
        sigma: float,
        q_vector: np.ndarray,
        lab_coord: np.ndarray,
        panel: object,
        beam: object,
        experiment: object,
        slow_idx: int,
        fast_idx: int,
        config: ExtractionConfig,
    ) -> Tuple[float, float]:
        """
        Apply pixel-level corrections as per Module 2.S.2.

        Applies Lorentz-Polarization, Quantum Efficiency, Solid Angle,
        and Air Attenuation corrections, with error propagation.

        Args:
            intensity: Raw intensity (after gain)
            sigma: Initial sigma (Poisson error)
            q_vector: Scattering vector for this pixel
            lab_coord: Lab coordinates of pixel center
            panel: Detector panel object
            beam: Beam model
            experiment: Complete experiment object
            slow_idx, fast_idx: Pixel coordinates
            config: Extraction configuration

        Returns:
            Tuple of (corrected_intensity, corrected_sigma)
        """
        try:
            # Calculate s1 vector for DIALS Corrections API
            s1_magnitude = 1.0 / beam.get_wavelength()
            scatter_direction = lab_coord / np.linalg.norm(lab_coord)
            s1_vector = scatter_direction * s1_magnitude

            # 1. Lorentz-Polarization Correction (using DIALS Corrections API)
            lp_mult = 1.0  # Default no correction
            if config.lp_correction_enabled:
                lp_mult = self._get_lp_correction(s1_vector, beam, experiment)

            # 2. Quantum Efficiency Correction (using DIALS Corrections API)
            qe_mult = self._get_qe_correction(s1_vector, beam, experiment, panel_idx=0)

            # 3. Solid Angle Correction (custom calculation)
            sa_mult = self._calculate_solid_angle_correction(
                lab_coord, panel, fast_idx, slow_idx
            )

            # 4. Air Attenuation Correction (custom calculation)
            air_mult = self._calculate_air_attenuation_correction(
                lab_coord, beam, config
            )

            # 5. Combine all corrections
            total_correction_mult = lp_mult * qe_mult * sa_mult * air_mult

            # 6. Apply correction to intensity
            corrected_intensity = intensity * total_correction_mult

            # 7. Error propagation (assuming correction factors have negligible uncertainty)
            corrected_sigma = sigma * total_correction_mult

            logger.debug(
                f"Pixel ({fast_idx}, {slow_idx}): LP={lp_mult:.3f}, QE={qe_mult:.3f}, "
                f"SA={sa_mult:.3f}, Air={air_mult:.3f}, Total={total_correction_mult:.3f}"
            )

            return corrected_intensity, corrected_sigma

        except Exception as e:
            logger.debug(f"Correction failed for pixel ({fast_idx}, {slow_idx}): {e}")
            return intensity, sigma

    def _calculate_solid_angle_correction(
        self, lab_coord: np.ndarray, panel: object, fast_idx: int, slow_idx: int
    ) -> float:
        """
        Calculate solid angle correction factor.

        Implements: Ω = (A_pixel × cos θ_normal_to_s1) / r²
        Returns: SA_mult = 1.0 / Ω (where Ω is the solid angle divisor)
        """
        try:
            # Get pixel size
            pixel_size = panel.get_pixel_size()  # (fast_size, slow_size) in mm
            pixel_area = pixel_size[0] * pixel_size[1]  # mm²

            # Get distance from sample to pixel
            r = np.linalg.norm(lab_coord)  # mm

            # Get panel normal vector
            fast_axis = np.array(panel.get_fast_axis())
            slow_axis = np.array(panel.get_slow_axis())
            normal = np.cross(fast_axis, slow_axis)
            normal = normal / np.linalg.norm(normal)

            # Calculate angle between normal and scattered beam direction
            scatter_direction = lab_coord / r
            cos_theta = np.abs(np.dot(normal, scatter_direction))

            # Calculate solid angle
            solid_angle = (pixel_area * cos_theta) / (r * r)

            # Return multiplicative correction (1/solid_angle)
            sa_mult = 1.0 / solid_angle if solid_angle > 0 else 1.0

            return sa_mult

        except Exception as e:
            logger.debug(f"Solid angle calculation failed: {e}")
            return 1.0

    def _calculate_air_attenuation_correction(
        self, lab_coord: np.ndarray, beam: object, config: ExtractionConfig
    ) -> float:
        """
        Calculate air attenuation correction factor.

        Implements Beer-Lambert law using NIST tabulated mass attenuation coefficients:
        Attenuation = exp(-μ_air * path_length)
        Returns: Air_mult = 1.0 / Attenuation

        Uses scientifically accurate calculation based on:
        - NIST X-ray mass attenuation coefficients for air components
        - Standard air composition (N: 78.084%, O: 20.946%, Ar: 0.934%, C: 0.036%)
        - Ideal gas law for density calculation with configurable T and P
        """
        try:
            # Get X-ray energy from wavelength
            wavelength = beam.get_wavelength()  # Angstroms
            energy_ev = 12398.4 / wavelength  # eV

            # Path length from sample to pixel (assuming sample at origin)
            path_length = np.linalg.norm(lab_coord) / 1000.0  # Convert mm to meters

            # Calculate linear attenuation coefficient for air at this energy
            # Using NIST tabulated data for air components
            temperature_k = getattr(config, "air_temperature_k", 293.15)
            pressure_atm = getattr(config, "air_pressure_atm", 1.0)
            mu_air = self._calculate_air_attenuation_coefficient(
                energy_ev, temperature_k, pressure_atm
            )

            # Apply Beer-Lambert law
            attenuation = np.exp(-mu_air * path_length)

            # Return multiplicative correction (1/attenuation)
            air_mult = 1.0 / attenuation if attenuation > 0 else 1.0

            return air_mult

        except Exception as e:
            logger.debug(f"Air attenuation calculation failed: {e}")
            return 1.0

    def _calculate_air_attenuation_coefficient(
        self, energy_ev: float, temperature_k: float = 293.15, pressure_atm: float = 1.0
    ) -> float:
        """
        Calculate linear attenuation coefficient for air at given X-ray energy.

        Uses tabulated NIST mass attenuation coefficients for air components
        (N: 78.084%, O: 20.946%, Ar: 0.934%, C: 0.036% by mass).

        Args:
            energy_ev: X-ray energy in eV
            temperature_k: Air temperature in Kelvin (default: 293.15 K = 20°C)
            pressure_atm: Air pressure in atmospheres (default: 1.0 atm)

        Returns:
            Linear attenuation coefficient in m⁻¹
        """
        try:
            # Standard air composition by mass (NIST dry air at sea level)
            air_composition = {
                "N": 0.78084,  # Nitrogen
                "O": 0.20946,  # Oxygen
                "Ar": 0.00934,  # Argon
                "C": 0.00036,  # Carbon (CO2)
            }

            # Molar masses (g/mol)
            molar_masses = {"N": 14.0067, "O": 15.9994, "Ar": 39.948, "C": 12.0107}

            # Calculate effective molar mass of air
            M_air = sum(
                air_composition[element] * molar_masses[element]
                for element in air_composition
            )

            # Calculate air density using ideal gas law
            # ρ = (P × M) / (R × T), where R = 8.314 J/(mol·K) = 0.08206 L·atm/(mol·K)
            R_atm = 0.08206  # L·atm/(mol·K)
            air_density_g_per_L = (pressure_atm * M_air) / (R_atm * temperature_k)
            air_density = air_density_g_per_L / 1000.0  # Convert to g/cm³

            # Get mass attenuation coefficient for each component
            mu_over_rho_total = 0.0
            for element, mass_fraction in air_composition.items():
                mu_over_rho_element = self._get_mass_attenuation_coefficient(
                    element, energy_ev
                )
                mu_over_rho_total += mass_fraction * mu_over_rho_element

            # Calculate linear attenuation coefficient
            mu_linear_cm = mu_over_rho_total * air_density  # cm⁻¹
            mu_linear_m = mu_linear_cm * 100  # Convert to m⁻¹

            return mu_linear_m

        except Exception as e:
            logger.debug(f"Air attenuation coefficient calculation failed: {e}")
            # Fallback: very small attenuation for typical X-ray energies
            return 0.001

    def _get_mass_attenuation_coefficient(
        self, element: str, energy_ev: float
    ) -> float:
        """
        Get mass attenuation coefficient (μ/ρ) for an element at given X-ray energy.

        Uses tabulated NIST data for X-ray mass attenuation coefficients.
        Data covers the range 1 keV to 1000 keV, with interpolation between points.

        Args:
            element: Element symbol ('N', 'O', 'Ar', 'C')
            energy_ev: X-ray energy in eV

        Returns:
            Mass attenuation coefficient in cm²/g
        """
        # NIST X-ray mass attenuation coefficients (μ/ρ) in cm²/g
        # Energy values in eV, coefficients interpolated from NIST tables
        # Source: NIST XCOM database (https://physics.nist.gov/PhysRefData/Xcom/html/xcom1.html)

        nist_data = {
            "N": {  # Nitrogen (Z=7)
                "energies": [
                    1000,
                    1500,
                    2000,
                    3000,
                    4000,
                    5000,
                    6000,
                    8000,
                    10000,
                    15000,
                    20000,
                    30000,
                    40000,
                    50000,
                    60000,
                    80000,
                    100000,
                ],
                "mu_rho": [
                    9.04e-1,
                    3.69e-1,
                    1.96e-1,
                    8.54e-2,
                    4.81e-2,
                    3.14e-2,
                    2.26e-2,
                    1.47e-2,
                    1.07e-2,
                    5.86e-3,
                    3.78e-3,
                    2.02e-3,
                    1.35e-3,
                    1.01e-3,
                    8.21e-4,
                    5.72e-4,
                    4.30e-4,
                ],
            },
            "O": {  # Oxygen (Z=8)
                "energies": [
                    1000,
                    1500,
                    2000,
                    3000,
                    4000,
                    5000,
                    6000,
                    8000,
                    10000,
                    15000,
                    20000,
                    30000,
                    40000,
                    50000,
                    60000,
                    80000,
                    100000,
                ],
                "mu_rho": [
                    1.18,
                    4.77e-1,
                    2.48e-1,
                    1.06e-1,
                    5.95e-2,
                    3.87e-2,
                    2.78e-2,
                    1.80e-2,
                    1.30e-2,
                    7.13e-3,
                    4.61e-3,
                    2.46e-3,
                    1.64e-3,
                    1.22e-3,
                    9.95e-4,
                    6.91e-4,
                    5.18e-4,
                ],
            },
            "Ar": {  # Argon (Z=18)
                "energies": [
                    1000,
                    1500,
                    2000,
                    3000,
                    4000,
                    5000,
                    6000,
                    8000,
                    10000,
                    15000,
                    20000,
                    30000,
                    40000,
                    50000,
                    60000,
                    80000,
                    100000,
                ],
                "mu_rho": [
                    8.21,
                    3.88,
                    2.14,
                    9.68e-1,
                    5.49e-1,
                    3.58e-1,
                    2.57e-1,
                    1.65e-1,
                    1.18e-1,
                    6.32e-2,
                    4.02e-2,
                    2.12e-2,
                    1.40e-2,
                    1.04e-2,
                    8.41e-3,
                    5.82e-3,
                    4.35e-3,
                ],
            },
            "C": {  # Carbon (Z=6)
                "energies": [
                    1000,
                    1500,
                    2000,
                    3000,
                    4000,
                    5000,
                    6000,
                    8000,
                    10000,
                    15000,
                    20000,
                    30000,
                    40000,
                    50000,
                    60000,
                    80000,
                    100000,
                ],
                "mu_rho": [
                    6.36e-1,
                    2.71e-1,
                    1.49e-1,
                    6.82e-2,
                    3.95e-2,
                    2.60e-2,
                    1.89e-2,
                    1.25e-2,
                    9.14e-3,
                    5.08e-3,
                    3.29e-3,
                    1.76e-3,
                    1.18e-3,
                    8.84e-4,
                    7.19e-4,
                    5.01e-4,
                    3.76e-4,
                ],
            },
        }

        if element not in nist_data:
            logger.warning(
                f"No mass attenuation data for element {element}, using default"
            )
            return 1e-3  # Default small value

        data = nist_data[element]
        energies = np.array(data["energies"])
        mu_rho_values = np.array(data["mu_rho"])

        # Convert energy to eV if needed and clamp to data range
        energy_ev = max(min(energy_ev, energies[-1]), energies[0])

        # Interpolate in log-log space for better accuracy across wide energy range
        log_energies = np.log(energies)
        log_mu_rho = np.log(mu_rho_values)
        log_energy_target = np.log(energy_ev)

        # Linear interpolation in log space
        log_mu_rho_interp = np.interp(log_energy_target, log_energies, log_mu_rho)
        mu_rho_result = np.exp(log_mu_rho_interp)

        return mu_rho_result

    def _get_lp_correction(
        self, s1_vector: np.ndarray, beam: object, experiment: object
    ) -> float:
        """Get Lorentz-Polarization correction for a single s1 vector."""
        try:
            from dials.algorithms.integration import Corrections
            from dials.array_family import flex

            # Cache corrections object to avoid repeated instantiation
            if not hasattr(self, "_corrections_obj"):
                self._corrections_obj = Corrections(
                    beam, experiment.goniometer, experiment.detector
                )

            # Convert to DIALS flex array format for single pixel
            s1_flex = flex.vec3_double([tuple(s1_vector)])

            # Get LP correction (returns divisors, convert to multipliers)
            lp_divisors = self._corrections_obj.lp(s1_flex)
            lp_mult = (
                1.0 / float(lp_divisors[0])
                if len(lp_divisors) > 0 and lp_divisors[0] != 0
                else 1.0
            )

            return lp_mult

        except Exception as e:
            logger.debug(f"LP correction failed: {e}")
            return 1.0

    def _get_qe_correction(
        self,
        s1_vector: np.ndarray,
        beam: object,
        experiment: object,
        panel_idx: int = 0,
    ) -> float:
        """Get Quantum Efficiency correction for a single s1 vector."""
        try:
            from dials.algorithms.integration import Corrections
            from dials.array_family import flex

            # Cache corrections object to avoid repeated instantiation
            if not hasattr(self, "_corrections_obj"):
                self._corrections_obj = Corrections(
                    beam, experiment.goniometer, experiment.detector
                )

            # Convert to DIALS flex array format
            s1_flex = flex.vec3_double([tuple(s1_vector)])
            panel_indices = flex.size_t([panel_idx])

            # Get QE correction (returns multipliers)
            qe_multipliers = self._corrections_obj.qe(s1_flex, panel_indices)
            qe_mult = float(qe_multipliers[0]) if len(qe_multipliers) > 0 else 1.0

            return qe_mult

        except Exception as e:
            logger.debug(f"QE correction failed: {e}")
            return 1.0
</file>

<file path="src/diffusepipe/masking/__init__.py">
"""Masking utilities for Bragg peaks and detector pixels."""

from .pixel_mask_generator import (
    PixelMaskGenerator,
    StaticMaskParams,
    DynamicMaskParams,
    Circle,
    Rectangle,
    create_circular_beamstop,
    create_rectangular_beamstop,
    create_default_static_params,
    create_default_dynamic_params,
)

from .bragg_mask_generator import (
    BraggMaskGenerator,
    create_default_bragg_mask_config,
    create_expanded_bragg_mask_config,
    validate_mask_compatibility,
)

__all__ = [
    # Pixel mask generation
    "PixelMaskGenerator",
    "StaticMaskParams",
    "DynamicMaskParams",
    "Circle",
    "Rectangle",
    "create_circular_beamstop",
    "create_rectangular_beamstop",
    "create_default_static_params",
    "create_default_dynamic_params",
    # Bragg mask generation
    "BraggMaskGenerator",
    "create_default_bragg_mask_config",
    "create_expanded_bragg_mask_config",
    "validate_mask_compatibility",
]
</file>

<file path="src/diffusepipe/merging/__init__.py">
"""
Merging module for Phase 3 data merging.

This module provides:
- DiffuseDataMerger: Merge scaled observations into final voxel data
- VoxelDataRelative: Data structure for relatively-scaled results
"""

from .merger import DiffuseDataMerger, VoxelDataRelative

__all__ = [
    'DiffuseDataMerger',
    'VoxelDataRelative'
]
</file>

<file path="src/diffusepipe/scaling/__init__.py">
"""
Scaling module for Phase 3 relative scaling.

This module provides:
- DiffuseScalingModel: Custom scaling model for diffuse data
- PerStillMultiplierComponent: Per-still multiplicative scaling
- ResolutionSmootherComponent: Resolution-dependent scaling  
"""

from .diffuse_scaling_model import DiffuseScalingModel
from .components.per_still_multiplier import PerStillMultiplierComponent
from .components.resolution_smoother import ResolutionSmootherComponent

__all__ = [
    'DiffuseScalingModel',
    'PerStillMultiplierComponent', 
    'ResolutionSmootherComponent'
]
</file>

<file path="src/diffusepipe/utils/cbf_utils.py">
"""
CBF header parsing utilities for determining data type (stills vs sequence).
"""

import logging
from typing import Optional

# Import needed for patching in tests
try:
    import dxtbx
except ImportError:
    # This import might fail in testing environments without DIALS
    dxtbx = None

logger = logging.getLogger(__name__)


class CBFUtils:
    """Utilities for working with CBF files."""
    
    def get_angle_increment(self, image_path: str) -> Optional[float]:
        """
        Extract the Angle_increment value from a CBF file header.
        
        Args:
            image_path: Path to the CBF file
            
        Returns:
            Angle increment in degrees, or None if not found
        """
        return get_angle_increment_from_cbf(image_path)


def get_angle_increment_from_cbf(image_path: str) -> Optional[float]:
    """
    Extract the Angle_increment value from a CBF file header to determine
    if the data is true stills (0.0°) or sequence data (> 0.0°).

    Uses a two-phase approach:
    1. Primary: dxtbx.load() to get scan information (robust for most files)
    2. Fallback: Direct header text parsing with regex (handles edge cases)

    Args:
        image_path: Path to the CBF file

    Returns:
        Angle increment in degrees, or None if not found/determinable
        - 0.0: True stills data (no oscillation)
        - > 0.0: Sequence data (oscillation per frame)
        - None: Could not determine (caller should default to sequence processing)

    Raises:
        Exception: If file cannot be read or both parsing methods fail critically
    """
    try:
        # First attempt: Use dxtbx to get scan information
        import dxtbx

        logger.debug(f"Attempting to parse CBF header for: {image_path}")

        # Load the image using dxtbx
        image = dxtbx.load(image_path)

        # Try to get scan information with enhanced error handling
        try:
            scan = image.get_scan()
        except AttributeError:
            logger.debug("Image object has no get_scan() method, treating as still")
            return 0.0

        if scan is not None:
            try:
                # Get oscillation information: (start_angle, oscillation_width)
                oscillation = scan.get_oscillation()
                if oscillation is None or len(oscillation) < 2:
                    logger.debug(
                        "Scan object returned None or incomplete oscillation data"
                    )
                    return 0.0
                angle_increment = oscillation[1]  # oscillation width per frame
                logger.info(
                    f"Angle increment from dxtbx scan object: {angle_increment}°"
                )
                return angle_increment
            except (AttributeError, IndexError, TypeError) as e:
                logger.debug(f"Failed to get oscillation from scan object: {e}")
                return 0.0
        else:
            logger.debug("No scan object found, likely a still image")
            return 0.0

    except ImportError:
        logger.warning(
            f"dxtbx not available, falling back to text parsing for {image_path}"
        )
    except Exception as e:
        logger.warning(f"dxtbx method failed for {image_path}: {e}")

        # Fallback: Parse CBF header text directly
        try:
            fallback_result = _parse_cbf_header_text(image_path)
            if fallback_result is not None:
                logger.info(
                    f"Angle increment from header text parsing: {fallback_result}°"
                )
                return fallback_result
            else:
                logger.warning(
                    f"Could not determine Angle_increment for {image_path}. Defaulting to sequence processing. This may lead to incorrect results. Consider using 'force_processing_mode'."
                )
                return None
        except Exception as fallback_error:
            logger.error(
                f"All parsing methods failed for {image_path}: {fallback_error}"
            )
            raise


def _parse_cbf_header_text(image_path: str) -> Optional[float]:
    """
    Fallback method to parse CBF header text directly for Angle_increment.

    Uses regex for more flexible parsing of the Angle_increment line with
    case-insensitive matching and variable spacing.

    Args:
        image_path: Path to the CBF file

    Returns:
        Angle increment in degrees, or None if not found
    """
    import re

    logger.debug(f"Attempting direct header parsing for: {image_path}")

    # Regex pattern for flexible Angle_increment parsing
    # Matches: # (optional spaces) Angle_increment (spaces) number (optional spaces) (optional deg.)
    angle_pattern = re.compile(
        r"^\s*#\s*Angle_increment\s+([+-]?\d*\.?\d+)\s*(?:deg\.?)?", re.IGNORECASE
    )

    try:
        with open(image_path, "r", encoding="utf-8", errors="ignore") as f:
            # Read in chunks to handle large files efficiently
            # Headers are typically in the first 16KB
            chunk_size = 16384
            content = f.read(chunk_size)

            # Split into lines for processing
            for line in content.split("\n"):
                # Stop when we hit the binary data section
                if (
                    "_array_data.data" in line
                    or "Content-Type: application/octet-stream" in line
                ):
                    break

                # Try to match the Angle_increment pattern
                match = angle_pattern.match(line.strip())
                if match:
                    try:
                        angle_increment = float(match.group(1))
                        logger.debug(
                            f"Found angle increment from header text: {angle_increment}°"
                        )
                        return angle_increment
                    except ValueError as e:
                        logger.warning(
                            f"Could not convert angle increment '{match.group(1)}' to float: {e}"
                        )
                        continue

            # If we haven't found it in the first chunk and there's more to read, read another chunk
            if len(content) == chunk_size:
                additional_content = f.read(chunk_size)
                for line in additional_content.split("\n"):
                    if (
                        "_array_data.data" in line
                        or "Content-Type: application/octet-stream" in line
                    ):
                        break
                    match = angle_pattern.match(line.strip())
                    if match:
                        try:
                            angle_increment = float(match.group(1))
                            logger.debug(
                                f"Found angle increment from header text: {angle_increment}°"
                            )
                            return angle_increment
                        except ValueError as e:
                            logger.warning(
                                f"Could not convert angle increment '{match.group(1)}' to float: {e}"
                            )
                            continue

    except IOError as e:
        logger.error(f"Failed to read CBF file {image_path}: {e}")
        raise
    except Exception as e:
        logger.error(f"Unexpected error parsing CBF header for {image_path}: {e}")
        raise

    logger.debug("Angle_increment not found in header text")
    return None
</file>

<file path="tests/adapters/test_dials_generate_mask_adapter.py">
"""Tests for DIALSGenerateMaskAdapter."""

import pytest
from unittest.mock import Mock, patch, MagicMock

from diffusepipe.adapters.dials_generate_mask_adapter import DIALSGenerateMaskAdapter
from diffusepipe.exceptions import DIALSError

try:
    from dials.array_family import flex
except ImportError:
    # For environments without DIALS, create a minimal mock
    class MockFlex:
        class bool:
            def __init__(self, grid, value):
                self.grid = grid
                self.value = value

        class grid:
            def __init__(self, x, y):
                self.x = x
                self.y = y

        class vec3_double:
            def __init__(self, values):
                self.values = values

        class int:
            def __init__(self, values, default=0):
                self.values = values

    flex = MockFlex()


@pytest.fixture
def adapter():
    """Create a DIALSGenerateMaskAdapter instance for testing."""
    return DIALSGenerateMaskAdapter()


@pytest.fixture
def mock_experiment():
    """Create a mock DIALS experiment object."""
    experiment = MagicMock()

    # Mock detector with panels
    panel1 = MagicMock()
    panel1.get_image_size.return_value = (100, 100)
    panel2 = MagicMock()
    panel2.get_image_size.return_value = (100, 100)

    detector = MagicMock()
    detector.__iter__.return_value = iter([panel1, panel2])
    detector.__len__.return_value = 2

    experiment.detector = detector
    return experiment


@pytest.fixture
def mock_reflections():
    """Create a mock DIALS reflection table."""
    reflections = MagicMock()
    reflections.__len__.return_value = 100  # Mock 100 reflections

    # Setup for _call_generate_mask tests
    reflections.__contains__.side_effect = lambda k: k in ["xyzobs.px.value", "panel"]

    # Mock centroid data
    mock_centroids = flex.vec3_double([(10.0, 10.0, 0.0)])
    mock_panels = flex.int([0])

    reflections.__getitem__.side_effect = lambda key: {
        "xyzobs.px.value": mock_centroids,
        "panel": mock_panels,
    }.get(key)

    return reflections


@pytest.fixture
def mock_mask_params():
    """Create mock mask generation parameters."""
    return {"border": 3, "algorithm": "simple"}


class TestDIALSGenerateMaskAdapter:
    """Test cases for DIALSGenerateMaskAdapter."""

    def test_init(self, adapter):
        """Test adapter initialization."""
        assert adapter is not None

    def test_generate_bragg_mask_none_experiment(self, adapter, mock_reflections):
        """Test mask generation with None experiment."""
        with pytest.raises(DIALSError, match="Experiment object cannot be None"):
            adapter.generate_bragg_mask(None, mock_reflections)

    def test_generate_bragg_mask_none_reflections(self, adapter, mock_experiment):
        """Test mask generation with None reflections."""
        with pytest.raises(DIALSError, match="Reflections object cannot be None"):
            adapter.generate_bragg_mask(mock_experiment, None)

    def test_generate_bragg_mask_success(
        self, adapter, mock_experiment, mock_reflections
    ):
        """Test successful mask generation."""

        # Create a proper ExperimentList mock class that works with isinstance
        class MockExperimentList:
            def __init__(self, experiments):
                self.experiments = experiments

            def __len__(self):
                return len(self.experiments)

            def __getitem__(self, index):
                return self.experiments[index]

        with patch.object(
            adapter, "_call_generate_mask"
        ) as mock_call_generate_mask_method:
            # Mock the _call_generate_mask method to return panel masks
            mock_panel_mask1 = Mock()
            mock_panel_mask2 = Mock()
            mock_call_generate_mask_method.return_value = (
                mock_panel_mask1,
                mock_panel_mask2,
            )

            with patch("dxtbx.model.ExperimentList", MockExperimentList):
                mask_result, success, logs = adapter.generate_bragg_mask(
                    mock_experiment, mock_reflections
                )

                assert success is True
                assert isinstance(mask_result, tuple)
                assert len(mask_result) == 2  # Two panels
                assert "Generated Bragg mask successfully" in logs

                # Verify _call_generate_mask was called with correct parameters
                # The first argument should be an instance of MockExperimentList
                mock_call_generate_mask_method.assert_called_once()
                call_args = mock_call_generate_mask_method.call_args[0]
                assert isinstance(call_args[0], MockExperimentList)
                assert call_args[1] is mock_reflections

    def test_generate_bragg_mask_with_params(
        self, adapter, mock_experiment, mock_reflections, mock_mask_params
    ):
        """Test mask generation with custom parameters."""

        # Create a proper ExperimentList mock class that works with isinstance
        class MockExperimentList:
            def __init__(self, experiments):
                self.experiments = experiments

            def __len__(self):
                return len(self.experiments)

            def __getitem__(self, index):
                return self.experiments[index]

        with patch.object(
            adapter, "_call_generate_mask"
        ) as mock_call_generate_mask_method:
            # Mock the _call_generate_mask method to return panel masks
            mock_panel_mask1 = Mock()
            mock_panel_mask2 = Mock()
            mock_call_generate_mask_method.return_value = (
                mock_panel_mask1,
                mock_panel_mask2,
            )

            with patch("dxtbx.model.ExperimentList", MockExperimentList):
                mask_result, success, logs = adapter.generate_bragg_mask(
                    mock_experiment, mock_reflections, mock_mask_params
                )

                assert success is True
                assert isinstance(mask_result, tuple)

                # Verify _call_generate_mask was called with correct parameters
                mock_call_generate_mask_method.assert_called_once()
                call_args = mock_call_generate_mask_method.call_args[0]
                assert isinstance(call_args[0], MockExperimentList)
                assert call_args[1] is mock_reflections

    def test_generate_bragg_mask_dials_failure(
        self, adapter, mock_experiment, mock_reflections
    ):
        """Test mask generation when DIALS generate_mask fails."""

        # Create a proper ExperimentList mock class that works with isinstance
        class MockExperimentList:
            def __init__(self, experiments):
                self.experiments = experiments

            def __len__(self):
                return len(self.experiments)

            def __getitem__(self, index):
                return self.experiments[index]

        with patch.object(
            adapter, "_call_generate_mask"
        ) as mock_call_generate_mask_method:
            # Make _call_generate_mask raise an exception
            mock_call_generate_mask_method.side_effect = DIALSError(
                "Mocked DIALS masking failed"
            )

            with patch("dxtbx.model.ExperimentList", MockExperimentList):
                with pytest.raises(DIALSError, match="Mocked DIALS masking failed"):
                    adapter.generate_bragg_mask(mock_experiment, mock_reflections)

    def test_generate_bragg_mask_import_error(
        self, adapter, mock_experiment, mock_reflections
    ):
        """Test mask generation with DIALS import error."""
        import sys

        # Mock the import by removing the module from sys.modules temporarily
        original_module = sys.modules.get("dxtbx.model")
        if "dxtbx.model" in sys.modules:
            del sys.modules["dxtbx.model"]

        try:
            with patch("builtins.__import__") as mock_import:

                def side_effect(name, *args, **kwargs):
                    if name == "dxtbx.model":
                        raise ImportError("Mocked ExperimentList import error")
                    return __import__(name, *args, **kwargs)

                mock_import.side_effect = side_effect

                with pytest.raises(
                    DIALSError,
                    match="Failed to import ExperimentList: Mocked ExperimentList import error",
                ):
                    adapter.generate_bragg_mask(mock_experiment, mock_reflections)
        finally:
            # Restore the original module
            if original_module is not None:
                sys.modules["dxtbx.model"] = original_module

    def test_call_generate_mask_success(self, adapter):
        """Test successful _call_generate_mask."""
        # Setup mock reflections with proper structure for the actual implementation
        mock_reflections = MagicMock()
        mock_reflections.__len__.return_value = 1
        mock_reflections.__contains__.side_effect = lambda k: k in [
            "xyzobs.px.value",
            "panel",
        ]

        # Mock centroid and panel data
        mock_centroids = [MagicMock()]
        mock_centroids[0].__getitem__.side_effect = lambda i: [10.0, 10.0, 0.0][i]
        mock_panels = [0]

        mock_reflections.__getitem__.side_effect = lambda key: {
            "xyzobs.px.value": mock_centroids,
            "panel": mock_panels,
        }.get(key)

        # Setup mock experiment list with proper structure
        mock_panel = MagicMock()
        mock_panel.get_image_size.return_value = (100, 100)

        # Create a mock flex.bool that behaves like the real one
        mock_mask = MagicMock()
        mock_mask.all.return_value = (100, 100)  # height, width
        mock_mask.__setitem__ = MagicMock()

        # Mock the entire flex module
        with patch("dials.array_family.flex") as mock_flex:
            # Setup flex.bool mock
            mock_flex.bool.return_value = mock_mask
            mock_flex.grid = MagicMock()
            mock_flex.int = MagicMock(return_value=[0])  # For panel assignments

            mock_experiment = MagicMock()
            mock_experiment.detector = MagicMock()
            mock_experiment.detector.__iter__.return_value = iter([mock_panel])
            mock_experiment.detector.__len__.return_value = 1

            mock_experiment_list = MagicMock()
            mock_experiment_list.__len__.return_value = 1
            mock_experiment_list.__getitem__.return_value = mock_experiment

            # Setup mock phil params
            mock_params = MagicMock()
            mock_params.border = 2

            result = adapter._call_generate_mask(
                mock_experiment_list, mock_reflections, mock_params
            )

            assert isinstance(result, tuple)
            assert len(result) == 1  # One panel from mock_experiment

    def test_call_generate_mask_failure(self, adapter, mock_reflections):
        """Test _call_generate_mask with failure."""
        # Setup empty experiment list to trigger failure
        mock_experiment_list = MagicMock()
        mock_experiment_list.__len__.return_value = 0

        mock_params = MagicMock()
        mock_params.border = 2

        with pytest.raises(
            DIALSError, match="Bragg mask generation failed: ExperimentList is empty"
        ):
            adapter._call_generate_mask(
                mock_experiment_list, mock_reflections, mock_params
            )

    def test_validate_mask_result_none(self, adapter):
        """Test mask validation with None result."""
        with pytest.raises(DIALSError, match="Mask generation returned None"):
            adapter._validate_mask_result(None)

    def test_validate_mask_result_not_tuple_or_list(self, adapter):
        """Test mask validation with invalid type."""
        with pytest.raises(DIALSError, match="Mask result should be a tuple or list"):
            adapter._validate_mask_result("invalid_type")

    def test_validate_mask_result_empty(self, adapter):
        """Test mask validation with empty result."""
        with pytest.raises(DIALSError, match="Mask result contains no panel masks"):
            adapter._validate_mask_result(tuple())

    def test_validate_mask_result_none_panel(self, adapter):
        """Test mask validation with None panel mask."""
        mask_result = (Mock(), None)  # Second panel is None

        with pytest.raises(DIALSError, match="Panel 1 mask is None"):
            adapter._validate_mask_result(mask_result)

    def test_validate_mask_result_success(self, adapter):
        """Test successful mask validation."""
        panel1_mask = Mock()
        panel2_mask = Mock()
        mask_result = (panel1_mask, panel2_mask)

        # Should not raise an exception
        adapter._validate_mask_result(mask_result)

    def test_generate_bragg_mask_integration(self, adapter):
        """Test integration of the full generate_bragg_mask workflow."""

        # Create a proper ExperimentList mock class that works with isinstance
        class MockExperimentList:
            def __init__(self, experiments):
                self.experiments = experiments

            def __len__(self):
                return len(self.experiments)

            def __getitem__(self, index):
                return self.experiments[index]

        with patch.object(
            adapter, "_call_generate_mask"
        ) as mock_call_generate_mask_method:
            # Mock the _call_generate_mask method to return a single panel mask
            mock_panel_mask = Mock()
            mock_call_generate_mask_method.return_value = (mock_panel_mask,)

            # Create experiment with detector
            experiment = MagicMock()
            panel = MagicMock()
            panel.get_image_size.return_value = (100, 100)
            detector = MagicMock()
            detector.__iter__.return_value = iter([panel])
            detector.__len__.return_value = 1
            experiment.detector = detector

            # Create reflections
            reflections = MagicMock()

            with patch("dxtbx.model.ExperimentList", MockExperimentList):
                # Test the full workflow
                mask_result, success, logs = adapter.generate_bragg_mask(
                    experiment, reflections
                )

                assert success is True
                assert isinstance(mask_result, tuple)
                assert len(mask_result) == 1
                assert "Starting Bragg mask generation" in logs
                assert "Generated Bragg mask successfully" in logs
                assert "Validated mask result" in logs

                # Verify _call_generate_mask was called
                mock_call_generate_mask_method.assert_called_once()
                call_args = mock_call_generate_mask_method.call_args[0]
                assert isinstance(call_args[0], MockExperimentList)
                assert call_args[1] is reflections
</file>

<file path="tests/adapters/test_dials_stills_process_adapter.py">
"""Tests for DIALSStillsProcessAdapter."""

import pytest
from unittest.mock import Mock, patch

from diffusepipe.adapters.dials_stills_process_adapter import DIALSStillsProcessAdapter
from diffusepipe.exceptions import ConfigurationError, DataValidationError


@pytest.fixture
def adapter():
    """Create a DIALSStillsProcessAdapter instance for testing."""
    return DIALSStillsProcessAdapter()


@pytest.fixture
def mock_config():
    """Create a mock configuration object."""
    config = Mock()
    config.stills_process_phil_path = None
    config.known_unit_cell = None
    config.known_space_group = None
    config.spotfinder_threshold_algorithm = None
    config.min_spot_area = None
    config.output_shoeboxes = None
    config.calculate_partiality = True
    return config


@pytest.fixture
def mock_config_with_phil(tmp_path):
    """Create a mock configuration with PHIL file."""
    phil_file = tmp_path / "test.phil"
    phil_file.write_text("spotfinder.threshold.dispersion.gain=1.0")

    config = Mock()
    config.stills_process_phil_path = str(phil_file)
    config.known_unit_cell = "10,10,10,90,90,90"
    config.known_space_group = "P1"
    config.spotfinder_threshold_algorithm = "dispersion"
    config.min_spot_area = 3
    config.output_shoeboxes = True
    config.calculate_partiality = True
    return config


class TestDIALSStillsProcessAdapter:
    """Test cases for DIALSStillsProcessAdapter."""

    def test_init(self, adapter):
        """Test adapter initialization."""
        assert adapter._processor is None

    def test_process_still_nonexistent_file(self, adapter, mock_config):
        """Test processing with non-existent image file."""
        with pytest.raises(ConfigurationError, match="Image file does not exist"):
            adapter.process_still("/nonexistent/file.cbf", mock_config)

    @patch("diffusepipe.adapters.dials_stills_process_adapter.Path.exists")
    def test_phil_parameter_generation_basic(self, mock_exists, adapter, mock_config):
        """Test basic PHIL parameter generation."""
        mock_exists.return_value = True

        with patch(
            "diffusepipe.adapters.dials_stills_process_adapter.parse"
        ) as mock_parse:
            mock_parse.return_value = Mock()
            result = adapter._generate_phil_parameters(mock_config)
            assert result is not None

    @patch("diffusepipe.adapters.dials_stills_process_adapter.Path.exists")
    def test_phil_parameter_generation_with_file(
        self, mock_exists, adapter, mock_config_with_phil
    ):
        """Test PHIL parameter generation with file."""
        mock_exists.return_value = True

        with (
            patch(
                "diffusepipe.adapters.dials_stills_process_adapter.parse"
            ) as mock_parse,
            patch("builtins.open", mock_open_read="test phil content"),
        ):
            mock_phil_scope = Mock()
            mock_phil_scope.fetch.return_value = Mock()
            mock_parse.return_value = mock_phil_scope

            result = adapter._generate_phil_parameters(mock_config_with_phil)
            assert result is not None

    def test_phil_parameter_generation_missing_file(self, adapter):
        """Test PHIL parameter generation with missing file."""
        config = Mock()
        config.stills_process_phil_path = "/nonexistent/file.phil"
        config.known_unit_cell = None
        config.known_space_group = None
        config.spotfinder_threshold_algorithm = None
        config.min_spot_area = None
        config.output_shoeboxes = None
        config.calculate_partiality = None

        with pytest.raises(ConfigurationError, match="PHIL file not found"):
            adapter._generate_phil_parameters(config)

    def test_extract_experiment_success(self, adapter):
        """Test successful experiment extraction."""
        mock_exp1 = Mock()
        mock_exp2 = Mock()
        mock_experiments = [mock_exp1, mock_exp2]

        result = adapter._extract_experiment(mock_experiments)
        assert result == mock_exp1

    def test_extract_experiment_empty(self, adapter):
        """Test experiment extraction with empty list."""
        result = adapter._extract_experiment([])
        assert result is None

    def test_extract_experiment_none(self, adapter):
        """Test experiment extraction with None input."""
        result = adapter._extract_experiment(None)
        assert result is None

    def test_extract_reflections(self, adapter):
        """Test reflection extraction."""
        mock_reflections = Mock()
        result = adapter._extract_reflections(mock_reflections)
        assert result == mock_reflections

    def test_validate_partiality_none(self, adapter):
        """Test partiality validation with None reflections."""
        # Should not raise an exception
        adapter._validate_partiality(None)

    def test_validate_partiality_missing_column(self, adapter):
        """Test partiality validation with missing column."""
        mock_reflections = Mock()
        mock_reflections.has_key.return_value = False

        with pytest.raises(
            DataValidationError, match="missing required 'partiality' column"
        ):
            adapter._validate_partiality(mock_reflections)

    def test_validate_partiality_success(self, adapter):
        """Test successful partiality validation."""
        mock_reflections = Mock()
        mock_reflections.has_key.return_value = True

        # Mock the __getitem__ method properly
        def mock_getitem(key):
            if key == "partiality":
                return [0.5, 0.8, 1.0]
            raise KeyError(key)

        mock_reflections.__getitem__ = Mock(side_effect=mock_getitem)

        # Should not raise an exception
        adapter._validate_partiality(mock_reflections)

    def test_validate_partiality_no_has_key_attribute(self, adapter):
        """Test partiality validation with object lacking has_key method."""
        mock_reflections = Mock()
        del mock_reflections.has_key  # Remove the has_key attribute

        # Should not raise an exception (warning logged instead)
        adapter._validate_partiality(mock_reflections)

    @patch("diffusepipe.adapters.dials_stills_process_adapter.Path.exists")
    def test_process_still_import_error(self, mock_exists, adapter, mock_config):
        """Test process_still with DIALS import error."""
        mock_exists.return_value = True

        # Patch the imports to raise ImportError
        with patch.dict("sys.modules", {"dials.command_line.stills_process": None}):
            with pytest.raises(
                ConfigurationError, match="Failed to generate PHIL parameters"
            ):
                adapter.process_still("/test/image.cbf", mock_config)


def mock_open_read(content):
    """Helper function to create a mock open context manager."""
    from unittest.mock import mock_open

    return mock_open(read_data=content)
</file>

<file path="tests/crystallography/test_still_processing_and_validation.py">
"""
Integration tests for StillProcessorAndValidatorComponent.

These tests focus on the integration between StillProcessorAndValidatorComponent,
DIALSStillsProcessAdapter, and ModelValidator, following the testing principles from plan.md.
"""

from unittest.mock import Mock, patch

from diffusepipe.crystallography.still_processing_and_validation import (
    StillProcessorAndValidatorComponent,
    ModelValidator,
    ValidationMetrics,
    create_default_config,
    create_default_extraction_config,
)
from diffusepipe.types.types_IDL import (
    DIALSStillsProcessConfig,
    ExtractionConfig,
)


class TestStillProcessorAndValidatorComponent:
    """Test suite for StillProcessorAndValidatorComponent integration."""

    def setup_method(self):
        """Set up test fixtures."""
        self.processor = StillProcessorAndValidatorComponent()
        self.test_image_path = "tests/data/minimal_still.cbf"
        self.test_phil_path = "tests/data/minimal_stills_process.phil"
        self.test_pdb_path = "tests/data/reference.pdb"

    def test_process_and_validate_still_successfully(self):
        """Test successful processing and validation of a single still image."""
        # Arrange
        config = create_default_config(
            phil_path=self.test_phil_path, enable_partiality=True
        )
        extraction_config = create_default_extraction_config()

        # Mock the adapter to return successful results
        mock_experiment = Mock()
        mock_reflections = Mock()
        mock_reflections.has_key = Mock(return_value=True)  # Has partiality column

        # Mock validation to pass
        mock_validation_metrics = ValidationMetrics()
        mock_validation_metrics.q_consistency_passed = True

        # Mock the processing route to return a specific adapter instance
        mock_adapter = Mock()
        mock_adapter.process_still.return_value = (
            mock_experiment,
            mock_reflections,
            True,
            "Processing completed successfully",
        )

        with patch.object(self.processor, "_determine_processing_route") as mock_route:
            with patch.object(
                self.processor.validator, "validate_geometry"
            ) as mock_validate:
                mock_route.return_value = ("stills", mock_adapter)
                mock_validate.return_value = (True, mock_validation_metrics)

                # Act
                outcome = self.processor.process_and_validate_still(
                    image_path=self.test_image_path,
                    config=config,
                    extraction_config=extraction_config,
                    external_pdb_path=self.test_pdb_path,
                )

                # Assert
                assert outcome.status == "SUCCESS"
                assert "Processed and validated" in outcome.message
                assert outcome.error_code is None
                assert outcome.output_artifacts is not None

                # Verify required artifacts are present
                assert "experiment" in outcome.output_artifacts
                assert "reflections" in outcome.output_artifacts
                assert "validation_passed" in outcome.output_artifacts
                assert "validation_metrics" in outcome.output_artifacts
                assert outcome.output_artifacts["experiment"] is mock_experiment
                assert outcome.output_artifacts["reflections"] is mock_reflections
                assert outcome.output_artifacts["validation_passed"] is True

                # Verify adapter and validator were called
                mock_adapter.process_still.assert_called_once_with(
                    image_path=self.test_image_path,
                    config=config,
                    base_expt_path=None,
                    output_dir_final=None,
                )
                mock_validate.assert_called_once_with(
                    experiment=mock_experiment,
                    reflections=mock_reflections,
                    external_pdb_path=self.test_pdb_path,
                    extraction_config=extraction_config,
                    output_dir=None,
                )

    def test_process_and_validate_still_validation_failure(self):
        """Test handling when validation fails after successful DIALS processing."""
        # Arrange
        config = create_default_config()
        extraction_config = create_default_extraction_config()

        mock_experiment = Mock()
        mock_reflections = Mock()
        mock_reflections.has_key = Mock(return_value=True)

        # Mock validation to fail
        mock_validation_metrics = ValidationMetrics()
        mock_validation_metrics.q_consistency_passed = False
        mock_validation_metrics.mean_delta_q_mag = 0.1  # Above tolerance

        # Mock the processing route to return a specific adapter instance
        mock_adapter = Mock()
        mock_adapter.process_still.return_value = (
            mock_experiment,
            mock_reflections,
            True,
            "Success",
        )

        with patch.object(self.processor, "_determine_processing_route") as mock_route:
            with patch.object(
                self.processor.validator, "validate_geometry"
            ) as mock_validate:
                mock_route.return_value = ("stills", mock_adapter)
                mock_validate.return_value = (False, mock_validation_metrics)

                # Act
                outcome = self.processor.process_and_validate_still(
                    image_path=self.test_image_path,
                    config=config,
                    extraction_config=extraction_config,
                )

                # Assert
                assert outcome.status == "FAILURE_GEOMETRY_VALIDATION"
                assert "Validation failed" in outcome.message
                assert outcome.error_code == "GEOMETRY_VALIDATION_FAILED"
                assert outcome.output_artifacts["validation_passed"] is False
                assert (
                    outcome.output_artifacts["validation_metrics"][
                        "q_consistency_passed"
                    ]
                    is False
                )

    def test_process_and_validate_still_dials_failure(self):
        """Test handling when DIALS processing fails."""
        # Arrange
        config = create_default_config()
        extraction_config = create_default_extraction_config()

        # Mock the processing route to return a specific adapter instance
        mock_adapter = Mock()
        mock_adapter.process_still.return_value = (
            None,
            None,
            False,
            "DIALS processing failed",
        )

        with patch.object(self.processor, "_determine_processing_route") as mock_route:
            mock_route.return_value = ("stills", mock_adapter)

            # Act
            outcome = self.processor.process_and_validate_still(
                image_path=self.test_image_path,
                config=config,
                extraction_config=extraction_config,
            )

            # Assert
            assert outcome.status == "FAILURE_DIALS_PROCESSING"
            assert "DIALS processing failed" in outcome.message
            assert outcome.error_code == "DIALS_PROCESSING_FAILED"

            # Validator should not be called since DIALS processing failed
            # We can't easily check if validator.validate_geometry was not called without patching it,
            # so we'll just verify the expected error state without this check


class TestModelValidator:
    """Test suite for ModelValidator component."""

    def setup_method(self):
        """Set up test fixtures."""
        self.validator = ModelValidator()

    def test_validate_geometry_q_vector_consistency_pass(self):
        """Test Q-vector consistency check that passes."""
        # Arrange
        mock_experiment = Mock()
        mock_crystal = Mock()
        mock_detector = Mock()
        mock_beam = Mock()

        mock_experiment.crystal = mock_crystal
        mock_experiment.detector = mock_detector
        mock_experiment.beam = mock_beam

        # Create mock reflections with required columns for Q-vector validation
        mock_reflections = Mock()
        mock_reflections.__len__ = Mock(return_value=100)
        mock_reflections.__contains__ = Mock(
            side_effect=lambda x: x in ["miller_index", "panel", "xyzcal.mm"]
        )

        extraction_config = create_default_extraction_config()

        # Mock the Q-vector consistency check to pass (mean = 0.005 Å⁻¹, tolerance = 0.01 Å⁻¹)
        with patch.object(self.validator, "_check_q_consistency") as mock_q_check:
            mock_q_check.return_value = (
                True,
                {"mean": 0.005, "max": 0.012, "median": 0.004, "count": 20},
            )

            # Act
            validation_passed, metrics = self.validator.validate_geometry(
                experiment=mock_experiment,
                reflections=mock_reflections,
                extraction_config=extraction_config,
            )

            # Assert
            assert validation_passed is True
            assert metrics.q_consistency_passed is True
            assert (
                metrics.mean_delta_q_mag == 0.005
            )  # Q-vector magnitude difference in Å⁻¹
            assert metrics.num_reflections_tested == 20

    def test_validate_geometry_q_vector_consistency_fail(self):
        """Test Q-vector consistency check that fails."""
        # Arrange
        mock_experiment = Mock()
        mock_crystal = Mock()
        mock_detector = Mock()
        mock_beam = Mock()

        mock_experiment.crystal = mock_crystal
        mock_experiment.detector = mock_detector
        mock_experiment.beam = mock_beam

        # Create mock reflections with required columns for Q-vector validation
        mock_reflections = Mock()
        mock_reflections.__len__ = Mock(return_value=100)
        mock_reflections.__contains__ = Mock(
            side_effect=lambda x: x in ["miller_index", "panel", "xyzcal.mm"]
        )

        extraction_config = create_default_extraction_config()

        # Mock the Q-vector consistency check to fail (mean = 0.025 Å⁻¹ > tolerance = 0.01 Å⁻¹)
        with patch.object(self.validator, "_check_q_consistency") as mock_q_check:
            mock_q_check.return_value = (
                False,
                {"mean": 0.025, "max": 0.080, "median": 0.021, "count": 20},
            )

            # Act
            validation_passed, metrics = self.validator.validate_geometry(
                experiment=mock_experiment,
                reflections=mock_reflections,
                extraction_config=extraction_config,
            )

            # Assert
            assert validation_passed is False
            assert metrics.q_consistency_passed is False
            assert (
                metrics.mean_delta_q_mag == 0.025
            )  # Q-vector magnitude difference in Å⁻¹

    def test_validate_geometry_missing_q_vector_columns(self):
        """Test Q-vector consistency check when required columns are missing."""
        # Arrange
        mock_experiment = Mock()
        mock_reflections = Mock()
        mock_reflections.__len__ = Mock(return_value=100)
        mock_reflections.__contains__ = Mock(
            return_value=False
        )  # Missing required columns

        extraction_config = create_default_extraction_config()

        # Mock the Q-vector check to fail due to missing columns
        with patch.object(self.validator, "_check_q_consistency") as mock_q_check:
            mock_q_check.return_value = (
                False,
                {"count": 0, "mean": None, "max": None, "median": None},
            )

            # Act
            validation_passed, metrics = self.validator.validate_geometry(
                experiment=mock_experiment,
                reflections=mock_reflections,
                extraction_config=extraction_config,
            )

            # Assert
            assert validation_passed is False
            assert metrics.q_consistency_passed is False
            assert metrics.num_reflections_tested == 0

    def test_validate_geometry_no_reflections(self):
        """Test Q-vector consistency check when no reflections are available."""
        # Arrange
        mock_experiment = Mock()
        mock_reflections = Mock()
        mock_reflections.__len__ = Mock(return_value=0)  # No reflections

        extraction_config = create_default_extraction_config()

        # Mock the Q-vector check to fail due to no reflections
        with patch.object(self.validator, "_check_q_consistency") as mock_q_check:
            mock_q_check.return_value = (
                False,
                {"count": 0, "mean": None, "max": None, "median": None},
            )

            # Act
            validation_passed, metrics = self.validator.validate_geometry(
                experiment=mock_experiment,
                reflections=mock_reflections,
                extraction_config=extraction_config,
            )

            # Assert
            assert validation_passed is False
            assert metrics.q_consistency_passed is False
            assert metrics.num_reflections_tested == 0

    def test_validate_geometry_with_pdb_check(self):
        """Test validation with PDB consistency check."""
        # Arrange
        mock_experiment = Mock()
        mock_crystal = Mock()
        mock_detector = Mock()
        mock_beam = Mock()

        mock_experiment.crystal = mock_crystal
        mock_experiment.detector = mock_detector
        mock_experiment.beam = mock_beam

        mock_reflections = Mock()
        mock_reflections.__len__ = Mock(return_value=100)

        extraction_config = create_default_extraction_config()
        pdb_path = "tests/data/test.pdb"

        # Mock PDB file existence
        with patch("pathlib.Path.exists", return_value=True):
            with patch.object(
                self.validator, "_check_pdb_consistency"
            ) as mock_pdb_check:
                with patch.object(
                    self.validator, "_check_q_consistency"
                ) as mock_q_check:
                    mock_pdb_check.return_value = (
                        True,
                        True,
                        1.5,
                    )  # cell_pass, orient_pass, angle
                    mock_q_check.return_value = (True, {"mean": 0.005, "count": 100})

                    # Act
                    validation_passed, metrics = self.validator.validate_geometry(
                        experiment=mock_experiment,
                        reflections=mock_reflections,
                        external_pdb_path=pdb_path,
                        extraction_config=extraction_config,
                    )

                    # Assert
                    assert validation_passed is True
                    assert metrics.pdb_cell_passed is True
                    assert metrics.pdb_orientation_passed is True
                    assert metrics.misorientation_angle_vs_pdb == 1.5

    def test_validate_geometry_pdb_mismatch_cell(self):
        """Test validation failure due to PDB cell parameter mismatch."""
        # Arrange
        mock_experiment = Mock()
        mock_crystal = Mock()
        mock_detector = Mock()
        mock_beam = Mock()

        mock_experiment.crystal = mock_crystal
        mock_experiment.detector = mock_detector
        mock_experiment.beam = mock_beam

        mock_reflections = Mock()
        mock_reflections.__len__ = Mock(return_value=100)

        extraction_config = create_default_extraction_config()
        pdb_path = "tests/data/test.pdb"

        # Mock PDB file existence
        with patch("pathlib.Path.exists", return_value=True):
            with patch.object(
                self.validator, "_check_pdb_consistency"
            ) as mock_pdb_check:
                with patch.object(
                    self.validator, "_check_q_consistency"
                ) as mock_q_check:
                    # PDB cell check fails, orientation passes, Q-consistency passes
                    mock_pdb_check.return_value = (False, True, 1.5)
                    mock_q_check.return_value = (True, {"mean": 0.005, "count": 100})

                    # Act
                    validation_passed, metrics = self.validator.validate_geometry(
                        experiment=mock_experiment,
                        reflections=mock_reflections,
                        external_pdb_path=pdb_path,
                        extraction_config=extraction_config,
                    )

                    # Assert
                    assert (
                        validation_passed is False
                    )  # Overall fails due to PDB cell mismatch
                    assert metrics.pdb_cell_passed is False
                    assert metrics.pdb_orientation_passed is True
                    assert metrics.q_consistency_passed is True

    def test_check_q_consistency_ideal_match(self):
        """Test Q-vector consistency check with ideal synthetic data."""
        # Arrange
        mock_experiment = Mock()
        mock_reflections = Mock()
        tolerance = 0.01  # Å⁻¹

        # Mock the Q-vector consistency check directly to simulate ideal match scenario
        with patch.object(self.validator, "q_checker") as mock_q_checker:
            mock_q_checker.check_q_consistency.return_value = (
                True,  # passed
                {
                    "mean": 0.005,
                    "max": 0.008,
                    "median": 0.004,
                    "count": 20,
                },  # Well within tolerance
            )

            # Act
            passed, stats = self.validator._check_q_consistency(
                experiment=mock_experiment,
                reflections=mock_reflections,
                tolerance=tolerance,
            )

            # Assert
            assert passed is True
            assert stats["count"] == 20
            assert stats["mean"] < tolerance  # Should be very small for ideal case
            assert stats["max"] < tolerance

    def test_check_q_consistency_mismatch_exceeding_tolerance(self):
        """Test Q-vector consistency check with mismatched data exceeding tolerance."""
        # Arrange
        mock_experiment = Mock()
        mock_reflections = Mock()
        tolerance = 0.01  # Å⁻¹ - strict tolerance

        # Mock the Q-vector consistency check directly to simulate mismatch scenario
        with patch.object(self.validator, "q_checker") as mock_q_checker:
            mock_q_checker.check_q_consistency.return_value = (
                False,  # failed
                {
                    "mean": 0.025,
                    "max": 0.080,
                    "median": 0.021,
                    "count": 20,
                },  # Exceeds tolerance
            )

            # Act
            passed, stats = self.validator._check_q_consistency(
                experiment=mock_experiment,
                reflections=mock_reflections,
                tolerance=tolerance,
            )

            # Assert
            assert passed is False  # Should fail due to large mismatch
            assert stats["count"] == 20
            assert stats["mean"] > tolerance  # Should exceed tolerance

    def test_check_q_consistency_missing_columns(self):
        """Test Q-vector consistency check with missing required columns."""
        # Arrange
        mock_experiment = Mock()
        mock_reflections = Mock()
        mock_reflections.__len__ = Mock(return_value=10)
        # Simulate missing required columns - return False for all column checks
        mock_reflections.__contains__ = Mock(return_value=False)
        # Also make it iterable to avoid TypeError
        mock_reflections.__iter__ = Mock(return_value=iter([]))

        tolerance = 0.01

        # Act
        passed, stats = self.validator._check_q_consistency(
            experiment=mock_experiment,
            reflections=mock_reflections,
            tolerance=tolerance,
        )

        # Assert
        assert passed is False
        assert stats["count"] == 0
        assert stats["mean"] is None
        assert stats["max"] is None


class TestBackwardCompatibility:
    """Test suite to ensure backward compatibility with original StillProcessorComponent."""

    def setup_method(self):
        """Set up test fixtures."""
        self.processor = StillProcessorAndValidatorComponent()
        self.test_image_path = "tests/data/minimal_still.cbf"

    def test_process_still_backward_compatible(self):
        """Test that the original process_still interface still works."""
        # Arrange
        config = create_default_config()

        # Mock the adapter to return successful results
        mock_experiment = Mock()
        mock_reflections = Mock()
        mock_reflections.has_key = Mock(return_value=True)

        # Mock the processing route to return a specific adapter instance
        mock_adapter = Mock()
        mock_adapter.process_still.return_value = (
            mock_experiment,
            mock_reflections,
            True,
            "Success",
        )

        with patch.object(self.processor, "_determine_processing_route") as mock_route:
            mock_route.return_value = ("stills", mock_adapter)

            # Act
            outcome = self.processor.process_still(
                image_path=self.test_image_path, config=config
            )

            # Assert
            assert outcome.status == "SUCCESS"
            assert "experiment" in outcome.output_artifacts
            assert "reflections" in outcome.output_artifacts
            # Should NOT have validation artifacts in backward compatible mode
            assert "validation_passed" not in outcome.output_artifacts
            assert "validation_metrics" not in outcome.output_artifacts


class TestCreateDefaultConfigs:
    """Test suite for configuration creation functions."""

    def test_create_default_config_basic(self):
        """Test creation of basic default configuration."""
        # Act
        config = create_default_config()

        # Assert
        assert isinstance(config, DIALSStillsProcessConfig)
        assert config.calculate_partiality is True
        assert config.output_shoeboxes is False

    def test_create_default_extraction_config(self):
        """Test creation of default extraction configuration."""
        # Act
        config = create_default_extraction_config()

        # Assert
        assert isinstance(config, ExtractionConfig)
        assert config.gain == 1.0
        assert config.cell_length_tol == 0.02
        assert config.cell_angle_tol == 2.0
        assert config.orient_tolerance_deg == 5.0
        assert config.q_consistency_tolerance_angstrom_inv == 0.01
        assert config.lp_correction_enabled is False


class TestValidationMetrics:
    """Test suite for ValidationMetrics class."""

    def test_validation_metrics_initialization(self):
        """Test ValidationMetrics initialization."""
        # Act
        metrics = ValidationMetrics()

        # Assert
        assert metrics.pdb_cell_passed is None
        assert metrics.pdb_orientation_passed is None
        assert metrics.q_consistency_passed is None
        assert metrics.num_reflections_tested == 0

    def test_validation_metrics_to_dict(self):
        """Test ValidationMetrics to_dict conversion."""
        # Arrange
        metrics = ValidationMetrics()
        metrics.q_consistency_passed = True
        metrics.mean_delta_q_mag = 0.005
        metrics.num_reflections_tested = 100

        # Act
        metrics_dict = metrics.to_dict()

        # Assert
        assert isinstance(metrics_dict, dict)
        assert metrics_dict["q_consistency_passed"] is True
        assert metrics_dict["mean_delta_q_mag"] == 0.005
        assert metrics_dict["num_reflections_tested"] == 100
        assert "pdb_cell_passed" in metrics_dict


class TestCBFDataTypeDetection:
    """Test suite for CBF data type detection and processing route selection (Module 1.S.0)."""

    def setup_method(self):
        """Set up test fixtures."""
        self.processor = StillProcessorAndValidatorComponent()
        self.config = DIALSStillsProcessConfig()

    def test_determine_processing_route_force_stills(self):
        """Test forced stills processing mode override."""
        # Arrange
        self.config.force_processing_mode = "stills"

        # Act
        route, adapter = self.processor._determine_processing_route(
            "dummy_path.cbf", self.config
        )

        # Assert
        assert route == "stills"
        assert adapter == self.processor.stills_adapter

    def test_determine_processing_route_force_sequence(self):
        """Test forced sequence processing mode override."""
        # Arrange
        self.config.force_processing_mode = "sequence"

        # Act
        route, adapter = self.processor._determine_processing_route(
            "dummy_path.cbf", self.config
        )

        # Assert
        assert route == "sequence"
        assert adapter == self.processor.sequence_adapter

    def test_determine_processing_route_auto_detect_stills(self):
        """Test auto-detection of stills data (Angle_increment = 0.0°)."""
        # Arrange
        self.config.force_processing_mode = None

        with patch(
            "diffusepipe.crystallography.still_processing_and_validation.get_angle_increment_from_cbf"
        ) as mock_get_angle:
            mock_get_angle.return_value = 0.0

            # Act
            route, adapter = self.processor._determine_processing_route(
                "test_still.cbf", self.config
            )

            # Assert
            assert route == "stills"
            assert adapter == self.processor.stills_adapter
            mock_get_angle.assert_called_once_with("test_still.cbf")

    def test_determine_processing_route_auto_detect_sequence(self):
        """Test auto-detection of sequence data (Angle_increment > 0.0°)."""
        # Arrange
        self.config.force_processing_mode = None

        with patch(
            "diffusepipe.crystallography.still_processing_and_validation.get_angle_increment_from_cbf"
        ) as mock_get_angle:
            mock_get_angle.return_value = 0.1

            # Act
            route, adapter = self.processor._determine_processing_route(
                "test_sequence.cbf", self.config
            )

            # Assert
            assert route == "sequence"
            assert adapter == self.processor.sequence_adapter
            mock_get_angle.assert_called_once_with("test_sequence.cbf")

    def test_determine_processing_route_auto_detect_fallback(self):
        """Test fallback to sequence processing when CBF parsing fails."""
        # Arrange
        self.config.force_processing_mode = None

        with patch(
            "diffusepipe.crystallography.still_processing_and_validation.get_angle_increment_from_cbf"
        ) as mock_get_angle:
            mock_get_angle.return_value = None  # Unable to determine

            # Act
            route, adapter = self.processor._determine_processing_route(
                "unknown.cbf", self.config
            )

            # Assert
            assert route == "sequence"  # Default fallback
            assert adapter == self.processor.sequence_adapter

    def test_determine_processing_route_invalid_force_mode(self):
        """Test handling of invalid force_processing_mode."""
        # Arrange
        self.config.force_processing_mode = "invalid_mode"

        with patch(
            "diffusepipe.crystallography.still_processing_and_validation.get_angle_increment_from_cbf"
        ) as mock_get_angle:
            mock_get_angle.return_value = 0.1

            # Act
            route, adapter = self.processor._determine_processing_route(
                "test.cbf", self.config
            )

            # Assert
            assert route == "sequence"  # Falls back to auto-detection
            assert adapter == self.processor.sequence_adapter


class TestPDBConsistencyCheck:
    """Test suite for PDB consistency checking functionality (Module 1.S.1.Validation)."""

    def setup_method(self):
        """Set up test fixtures."""
        self.validator = ModelValidator()

    def test_check_pdb_consistency_no_symmetry(self):
        """Test PDB consistency check when PDB file has no crystal symmetry."""
        # Arrange
        mock_experiment = Mock()
        pdb_path = "test_no_symmetry.pdb"

        with patch("iotbx.pdb.input") as mock_pdb_input:
            mock_input = Mock()
            mock_input.crystal_symmetry.return_value = None  # No symmetry
            mock_pdb_input.return_value = mock_input

            # Act
            cell_passed, orient_passed, misorientation = (
                ModelValidator._check_pdb_consistency(
                    mock_experiment, pdb_path, 0.02, 2.0, 5.0
                )
            )

            # Assert
            assert cell_passed is True  # Should pass if PDB lacks symmetry
            assert orient_passed is True
            assert misorientation is None

    def test_check_pdb_consistency_cell_match(self):
        """Test PDB consistency check with matching unit cell parameters."""
        # Arrange

        mock_experiment = Mock()
        mock_crystal = Mock()
        mock_experiment.crystal = mock_crystal

        # Mock experiment unit cell
        mock_exp_uc = Mock()
        mock_exp_uc.parameters.return_value = (50.0, 60.0, 70.0, 90.0, 90.0, 90.0)
        mock_exp_uc.is_similar_to.return_value = True  # Cells match
        mock_crystal.get_unit_cell.return_value = mock_exp_uc

        # Mock experiment space group
        mock_exp_sg = Mock()
        mock_exp_sg_type = Mock()
        mock_exp_sg_type.number.return_value = 1
        mock_exp_sg.type.return_value = mock_exp_sg_type
        mock_crystal.get_space_group.return_value = mock_exp_sg

        # Mock experiment A matrix
        mock_crystal.get_A.return_value = [1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0]

        pdb_path = "test_matching.pdb"

        with patch("iotbx.pdb.input") as mock_pdb_input:
            # Mock PDB crystal symmetry
            mock_input = Mock()
            mock_pdb_cs = Mock()

            mock_pdb_uc = Mock()
            mock_pdb_uc.parameters.return_value = (50.0, 60.0, 70.0, 90.0, 90.0, 90.0)
            mock_pdb_uc.fractionalization_matrix.return_value = [
                1.0,
                0.0,
                0.0,
                0.0,
                1.0,
                0.0,
                0.0,
                0.0,
                1.0,
            ]
            mock_pdb_cs.unit_cell.return_value = mock_pdb_uc

            mock_pdb_sg = Mock()
            mock_pdb_sg_type = Mock()
            mock_pdb_sg_type.number.return_value = 1
            mock_pdb_sg.type.return_value = mock_pdb_sg_type
            mock_pdb_cs.space_group.return_value = mock_pdb_sg

            mock_input.crystal_symmetry.return_value = mock_pdb_cs
            mock_pdb_input.return_value = mock_input

            with patch.object(
                ModelValidator, "_calculate_misorientation_static"
            ) as mock_misori:
                mock_misori.return_value = 1.0  # Small misorientation

                # Act
                cell_passed, orient_passed, misorientation = (
                    ModelValidator._check_pdb_consistency(
                        mock_experiment, pdb_path, 0.02, 2.0, 5.0
                    )
                )

                # Assert
                assert cell_passed is True
                assert orient_passed is True
                assert misorientation == 1.0

    def test_check_pdb_consistency_cell_mismatch(self):
        """Test PDB consistency check with mismatched unit cell parameters."""
        # Arrange
        mock_experiment = Mock()
        mock_crystal = Mock()
        mock_experiment.crystal = mock_crystal

        # Mock experiment unit cell
        mock_exp_uc = Mock()
        mock_exp_uc.parameters.return_value = (50.0, 60.0, 70.0, 90.0, 90.0, 90.0)
        mock_exp_uc.is_similar_to.return_value = False  # Cells don't match
        mock_crystal.get_unit_cell.return_value = mock_exp_uc

        # Mock experiment space group
        mock_exp_sg = Mock()
        mock_exp_sg_type = Mock()
        mock_exp_sg_type.number.return_value = 1
        mock_exp_sg.type.return_value = mock_exp_sg_type
        mock_crystal.get_space_group.return_value = mock_exp_sg

        # Mock experiment A matrix
        mock_crystal.get_A.return_value = [1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0]

        pdb_path = "test_mismatched.pdb"

        with patch("iotbx.pdb.input") as mock_pdb_input:
            # Mock PDB crystal symmetry with different cell
            mock_input = Mock()
            mock_pdb_cs = Mock()

            mock_pdb_uc = Mock()
            mock_pdb_uc.parameters.return_value = (
                100.0,
                120.0,
                140.0,
                90.0,
                90.0,
                90.0,
            )  # Different cell
            mock_pdb_uc.fractionalization_matrix.return_value = [
                1.0,
                0.0,
                0.0,
                0.0,
                1.0,
                0.0,
                0.0,
                0.0,
                1.0,
            ]
            mock_pdb_cs.unit_cell.return_value = mock_pdb_uc

            mock_pdb_sg = Mock()
            mock_pdb_sg_type = Mock()
            mock_pdb_sg_type.number.return_value = 1
            mock_pdb_sg.type.return_value = mock_pdb_sg_type
            mock_pdb_cs.space_group.return_value = mock_pdb_sg

            mock_input.crystal_symmetry.return_value = mock_pdb_cs
            mock_pdb_input.return_value = mock_input

            with patch.object(
                ModelValidator, "_calculate_misorientation_static"
            ) as mock_misori:
                mock_misori.return_value = 8.0  # Large misorientation

                # Act
                cell_passed, orient_passed, misorientation = (
                    ModelValidator._check_pdb_consistency(
                        mock_experiment, pdb_path, 0.02, 2.0, 5.0  # tolerance: 5°
                    )
                )

                # Assert
                assert cell_passed is False
                assert orient_passed is False  # 8° > 5° tolerance
                assert misorientation == 8.0

    def test_calculate_misorientation_static_identity(self):
        """Test misorientation calculation between identical matrices."""
        # Arrange
        from scitbx import matrix

        A1 = matrix.sqr([1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0])
        A2 = matrix.sqr([1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0])

        # Act
        misorientation = ModelValidator._calculate_misorientation_static(A1, A2)

        # Assert
        assert misorientation < 1e-6  # Should be essentially zero

    def test_calculate_misorientation_static_hand_inversion(self):
        """Test misorientation calculation handles hand inversion correctly."""
        # Arrange
        from scitbx import matrix

        A1 = matrix.sqr([1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0])
        A2 = matrix.sqr(
            [-1.0, 0.0, 0.0, 0.0, -1.0, 0.0, 0.0, 0.0, -1.0]
        )  # Inverted hand

        # Act
        misorientation = ModelValidator._calculate_misorientation_static(A1, A2)

        # Assert
        # Should return the minimum of direct and inverted comparison
        assert (
            misorientation < 1e-6
        )  # Should be essentially zero after considering inversion

    def test_pdb_consistency_check_file_error(self):
        """Test PDB consistency check handles file errors gracefully."""
        # Arrange
        mock_experiment = Mock()
        pdb_path = "nonexistent.pdb"

        with patch("iotbx.pdb.input") as mock_pdb_input:
            mock_pdb_input.side_effect = FileNotFoundError("File not found")

            # Act
            cell_passed, orient_passed, misorientation = (
                ModelValidator._check_pdb_consistency(
                    mock_experiment, pdb_path, 0.02, 2.0, 5.0
                )
            )

            # Assert
            assert cell_passed is False
            assert orient_passed is False
            assert misorientation is None
</file>

<file path="tests/crystallography/test_still_processor.py">
"""
Integration tests for StillProcessorComponent.

These tests focus on the integration between StillProcessorComponent and 
DIALSStillsProcessAdapter, following the testing principles from plan.md.
"""

import pytest
from unittest.mock import Mock, patch

from diffusepipe.crystallography.still_processing_and_validation import (
    StillProcessorComponent,
    create_default_config,
)
from diffusepipe.types.types_IDL import DIALSStillsProcessConfig
from diffusepipe.exceptions import DIALSError, ConfigurationError


class TestStillProcessorComponent:
    """Test suite for StillProcessorComponent integration."""

    def setup_method(self):
        """Set up test fixtures."""
        self.processor = StillProcessorComponent()
        self.test_image_path = "tests/data/minimal_still.cbf"
        self.test_phil_path = "tests/data/minimal_stills_process.phil"

    def test_process_single_still_successfully(self):
        """Test successful processing of a single still image."""
        # Arrange
        config = create_default_config(
            phil_path=self.test_phil_path, enable_partiality=True
        )

        # Mock the adapter to return successful results
        mock_experiment = Mock()
        mock_reflections = Mock()
        mock_reflections.has_key = Mock(return_value=True)  # Has partiality column

        # Mock the routing to return stills adapter and the process_still method
        with patch.object(self.processor, "_determine_processing_route") as mock_route:
            with patch.object(
                self.processor.stills_adapter, "process_still"
            ) as mock_process:
                mock_route.return_value = ("stills", self.processor.stills_adapter)
                mock_process.return_value = (
                    mock_experiment,
                    mock_reflections,
                    True,
                    "Processing completed successfully",
                )

                # Act
                outcome = self.processor.process_still(
                    image_path=self.test_image_path, config=config
                )

                # Assert
                assert outcome.status == "SUCCESS"
                assert "DIALS processing only (legacy path)" in outcome.message
                assert outcome.error_code is None
                assert outcome.output_artifacts is not None

                # Verify required artifacts are present
                assert "experiment" in outcome.output_artifacts
                assert "reflections" in outcome.output_artifacts
                assert outcome.output_artifacts["experiment"] is mock_experiment
                assert outcome.output_artifacts["reflections"] is mock_reflections

                # Verify adapter was called with correct parameters
                mock_process.assert_called_once_with(
                    image_path=self.test_image_path,
                    config=config,
                    base_expt_path=None,
                    output_dir_final=None,
                )

    def test_process_still_with_base_experiment(self):
        """Test processing with a base experiment file."""
        # Arrange
        config = create_default_config()
        base_expt_path = "tests/data/base_experiment.expt"

        mock_experiment = Mock()
        mock_reflections = Mock()
        mock_reflections.has_key = Mock(return_value=True)

        with patch.object(self.processor, "_determine_processing_route") as mock_route:
            with patch.object(
                self.processor.stills_adapter, "process_still"
            ) as mock_process:
                mock_route.return_value = ("stills", self.processor.stills_adapter)
                mock_process.return_value = (
                    mock_experiment,
                    mock_reflections,
                    True,
                    "Success",
                )

                # Act
                outcome = self.processor.process_still(
                    image_path=self.test_image_path,
                    config=config,
                    base_experiment_path=base_expt_path,
                )

                # Assert
                assert outcome.status == "SUCCESS"
                mock_process.assert_called_once_with(
                    image_path=self.test_image_path,
                    config=config,
                    base_expt_path=base_expt_path,
                    output_dir_final=None,
                )

    def test_process_still_indexing_failure(self):
        """Test handling of indexing failure."""
        # Arrange
        config = create_default_config()

        with patch.object(self.processor, "_determine_processing_route") as mock_route:
            with patch.object(
                self.processor.stills_adapter, "process_still"
            ) as mock_process:
                mock_route.return_value = ("stills", self.processor.stills_adapter)
                mock_process.return_value = (None, None, False, "Indexing failed")

                # Act
                outcome = self.processor.process_still(
                    image_path=self.test_image_path, config=config
                )

                # Assert
                assert outcome.status == "FAILURE"
                assert "DIALS processing failed (legacy path)" in outcome.message
                assert outcome.error_code == "DIALS_PROCESSING_FAILED"

    def test_process_still_dials_error(self):
        """Test handling of DIALSError exception."""
        # Arrange
        config = create_default_config()

        with patch.object(self.processor, "_determine_processing_route") as mock_route:
            with patch.object(
                self.processor.stills_adapter, "process_still"
            ) as mock_process:
                mock_route.return_value = ("stills", self.processor.stills_adapter)
                mock_process.side_effect = DIALSError("DIALS internal error")

                # Act & Assert - legacy method doesn't catch exceptions
                with pytest.raises(DIALSError, match="DIALS internal error"):
                    self.processor.process_still(
                        image_path=self.test_image_path, config=config
                    )

    def test_process_still_configuration_error(self):
        """Test handling of ConfigurationError exception."""
        # Arrange
        config = create_default_config()

        with patch.object(self.processor, "_determine_processing_route") as mock_route:
            with patch.object(
                self.processor.stills_adapter, "process_still"
            ) as mock_process:
                mock_route.return_value = ("stills", self.processor.stills_adapter)
                mock_process.side_effect = ConfigurationError("Invalid configuration")

                # Act & Assert - legacy method doesn't catch exceptions
                with pytest.raises(ConfigurationError, match="Invalid configuration"):
                    self.processor.process_still(
                        image_path=self.test_image_path, config=config
                    )

    def test_process_still_unexpected_error(self):
        """Test handling of unexpected exceptions."""
        # Arrange
        config = create_default_config()

        with patch.object(self.processor, "_determine_processing_route") as mock_route:
            with patch.object(
                self.processor.stills_adapter, "process_still"
            ) as mock_process:
                mock_route.return_value = ("stills", self.processor.stills_adapter)
                mock_process.side_effect = RuntimeError("Unexpected runtime error")

                # Act & Assert - legacy method doesn't catch exceptions
                with pytest.raises(RuntimeError, match="Unexpected runtime error"):
                    self.processor.process_still(
                        image_path=self.test_image_path, config=config
                    )


class TestCreateDefaultConfig:
    """Test suite for create_default_config function."""

    def test_create_default_config_basic(self):
        """Test creation of basic default configuration."""
        # Act
        config = create_default_config()

        # Assert
        assert isinstance(config, DIALSStillsProcessConfig)
        assert config.stills_process_phil_path is None
        assert config.calculate_partiality is True
        assert config.output_shoeboxes is False

    def test_create_default_config_with_phil_path(self):
        """Test creation of configuration with PHIL path."""
        # Arrange
        phil_path = "tests/data/test.phil"

        # Act
        config = create_default_config(phil_path=phil_path)

        # Assert
        assert config.stills_process_phil_path == phil_path

    def test_create_default_config_with_shoeboxes(self):
        """Test creation of configuration with shoeboxes enabled."""
        # Act
        config = create_default_config(enable_shoeboxes=True)

        # Assert
        assert config.output_shoeboxes is True

    def test_create_default_config_disable_partiality(self):
        """Test creation of configuration with partiality disabled."""
        # Act
        config = create_default_config(enable_partiality=False)

        # Assert
        assert config.calculate_partiality is False
</file>

<file path="tests/integration/test_phase1_workflow.py">
"""
Integration test for complete Phase 1 workflow.

This test demonstrates the complete Phase 1 pipeline from still processing
through mask generation, validating the integration between all components.
"""

from unittest.mock import Mock, patch, MagicMock

from diffusepipe.crystallography.still_processing_and_validation import (
    StillProcessorAndValidatorComponent,
    create_default_config,
    create_default_extraction_config,
)
from diffusepipe.masking.pixel_mask_generator import (
    PixelMaskGenerator,
    create_default_static_params,
    create_default_dynamic_params,
)
from diffusepipe.masking.bragg_mask_generator import (
    BraggMaskGenerator,
    create_default_bragg_mask_config,
)


class TestPhase1Workflow:
    """Integration test for complete Phase 1 workflow."""

    def setup_method(self):
        """Set up test fixtures."""
        self.processor = StillProcessorAndValidatorComponent()
        self.pixel_generator = PixelMaskGenerator()
        self.bragg_generator = BraggMaskGenerator()

        self.test_image_path = "tests/data/test_still.cbf"
        self.test_pdb_path = "tests/data/reference.pdb"

    def test_complete_phase1_workflow_success(self):
        """Test complete Phase 1 workflow with successful processing and validation."""
        # Arrange
        dials_config = create_default_config()
        extraction_config = create_default_extraction_config()
        static_params = create_default_static_params()
        dynamic_params = create_default_dynamic_params()
        bragg_config = create_default_bragg_mask_config()

        # Mock DIALS objects
        mock_experiment = Mock()
        mock_detector = Mock()
        mock_panel = Mock()
        mock_panel.get_image_size.return_value = (100, 50)

        mock_detector.__iter__ = MagicMock(side_effect=lambda: iter([mock_panel]))
        mock_detector.__len__ = Mock(return_value=1)
        mock_experiment.detector = mock_detector

        mock_reflections = Mock()
        mock_reflections.has_key = Mock(return_value=True)
        mock_reflections.__len__ = Mock(return_value=100)

        # Mock image sets for pixel mask generation
        mock_image_set = Mock()
        # Create a real numpy array for panel data that can be processed
        import numpy as np

        mock_panel_data_instance = (
            np.ones((50, 100), dtype=np.uint16) * 1000
        )  # Realistic detector data
        mock_image_set.get_raw_data.return_value = (
            mock_panel_data_instance,
        )  # Return tuple containing one panel
        representative_images = [mock_image_set]

        # Remove flex mocking to allow real flex operations for internal workings
        # The test will work with real flex arrays but mock the external DIALS calls

        # Mock DIALS processing and validation
        with patch.object(self.processor, "_determine_processing_route") as mock_route:
            with patch.object(
                self.processor.stills_adapter, "process_still"
            ) as mock_process:
                with patch.object(
                    self.processor.validator, "validate_geometry"
                ) as mock_validate:
                    mock_route.return_value = ("stills", self.processor.stills_adapter)
                    mock_process.return_value = (
                        mock_experiment,
                        mock_reflections,
                        True,
                        "Success",
                    )
                    mock_validate.return_value = (True, Mock())

                    # Mock DIALS generate mask
                    with patch.object(
                        self.bragg_generator.dials_adapter, "generate_bragg_mask"
                    ) as mock_gen_mask:
                        # Create a real flex array for the bragg mask
                        from dials.array_family import flex

                        mock_bragg_mask = flex.bool(
                            flex.grid(50, 100), True
                        )  # Real flex array
                        mock_gen_mask.return_value = (
                            (mock_bragg_mask,),
                            True,
                            "Mask generated",
                        )

                        # Act - Step 1: Process and validate still
                        still_outcome = self.processor.process_and_validate_still(
                            image_path=self.test_image_path,
                            config=dials_config,
                            extraction_config=extraction_config,
                            external_pdb_path=self.test_pdb_path,
                        )

                        # Assert Step 1
                        assert still_outcome.status == "SUCCESS"
                        assert (
                            still_outcome.output_artifacts["validation_passed"] is True
                        )

                        # Act - Step 2: Generate global pixel mask
                        global_pixel_mask = (
                            self.pixel_generator.generate_combined_pixel_mask(
                                detector=mock_detector,
                                static_params=static_params,
                                representative_images=representative_images,
                                dynamic_params=dynamic_params,
                            )
                        )

                        # Assert Step 2
                        assert isinstance(global_pixel_mask, tuple)
                        assert len(global_pixel_mask) == 1  # One panel

                        # Act - Step 3: Generate Bragg mask for this still
                        bragg_mask = (
                            self.bragg_generator.generate_bragg_mask_from_spots(
                                experiment=mock_experiment,
                                reflections=mock_reflections,
                                config=bragg_config,
                            )
                        )

                        # Assert Step 3
                        assert isinstance(bragg_mask, tuple)
                        assert len(bragg_mask) == 1  # One panel

                        # Act - Step 4: Generate total mask
                        total_mask = self.bragg_generator.get_total_mask_for_still(
                            bragg_mask=bragg_mask,
                            global_pixel_mask=global_pixel_mask,
                        )

                        # Assert Step 4
                        assert isinstance(total_mask, tuple)
                        assert len(total_mask) == 1  # One panel

                        # Verify all components were called
                        mock_process.assert_called_once()
                        mock_validate.assert_called_once()
                        mock_gen_mask.assert_called_once()

    def test_phase1_workflow_with_validation_failure(self):
        """Test Phase 1 workflow when geometric validation fails."""
        # Arrange
        dials_config = create_default_config()
        extraction_config = create_default_extraction_config()

        mock_experiment = Mock()
        mock_reflections = Mock()
        mock_reflections.has_key = Mock(return_value=True)

        # Mock DIALS processing to succeed but validation to fail
        with patch.object(self.processor, "_determine_processing_route") as mock_route:
            with patch.object(
                self.processor.stills_adapter, "process_still"
            ) as mock_process:
                with patch.object(
                    self.processor.validator, "validate_geometry"
                ) as mock_validate:
                    mock_route.return_value = ("stills", self.processor.stills_adapter)
                    mock_process.return_value = (
                        mock_experiment,
                        mock_reflections,
                        True,
                        "Success",
                    )
                    mock_validate.return_value = (False, Mock())  # Validation fails

                    # Act
                    outcome = self.processor.process_and_validate_still(
                        image_path=self.test_image_path,
                        config=dials_config,
                        extraction_config=extraction_config,
                    )

                    # Assert
                    assert outcome.status == "FAILURE_GEOMETRY_VALIDATION"
                    assert outcome.output_artifacts["validation_passed"] is False

                # Subsequent mask generation should be skipped in real orchestrator
                # This test demonstrates that the validation caught the issue

    def test_phase1_workflow_with_dials_failure(self):
        """Test Phase 1 workflow when DIALS processing fails."""
        # Arrange
        dials_config = create_default_config()
        extraction_config = create_default_extraction_config()

        # Mock DIALS processing to fail
        with patch.object(self.processor, "_determine_processing_route") as mock_route:
            with patch.object(
                self.processor.stills_adapter, "process_still"
            ) as mock_process:
                mock_route.return_value = ("stills", self.processor.stills_adapter)
                mock_process.return_value = (None, None, False, "DIALS failed")

                # Act
                outcome = self.processor.process_and_validate_still(
                    image_path=self.test_image_path,
                    config=dials_config,
                    extraction_config=extraction_config,
                )

                # Assert
                assert outcome.status == "FAILURE_DIALS_PROCESSING"

            # Validation and mask generation should be skipped
            # This test demonstrates proper error handling in the pipeline

    def test_phase1_mask_compatibility_validation(self):
        """Test validation of mask compatibility between components."""
        # Arrange
        mock_detector = Mock()
        mock_panel = Mock()
        mock_panel.get_image_size.return_value = (100, 50)
        mock_detector.__iter__ = Mock(return_value=iter([mock_panel]))
        mock_detector.__len__ = Mock(return_value=1)

        # Mock masks with compatible dimensions
        with patch("diffusepipe.masking.bragg_mask_generator.flex"):
            mock_pixel_mask = Mock()
            mock_pixel_mask.__len__ = Mock(return_value=5000)
            mock_bragg_mask = Mock()
            mock_bragg_mask.__len__ = Mock(return_value=5000)

            pixel_mask_tuple = (mock_pixel_mask,)
            bragg_mask_tuple = (mock_bragg_mask,)

            # Act
            from diffusepipe.masking.bragg_mask_generator import (
                validate_mask_compatibility,
            )

            is_compatible = validate_mask_compatibility(
                bragg_mask_tuple, pixel_mask_tuple
            )

            # Assert
            assert is_compatible is True

    def test_phase1_component_integration_interfaces(self):
        """Test that all Phase 1 components have compatible interfaces."""
        # This test validates that the components can be connected together
        # without runtime interface mismatches

        # Test StillProcessorAndValidatorComponent interface
        processor = StillProcessorAndValidatorComponent()
        assert hasattr(processor, "process_and_validate_still")
        assert hasattr(processor, "stills_adapter")
        assert hasattr(processor, "sequence_adapter")
        assert hasattr(processor, "validator")

        # Test PixelMaskGenerator interface
        pixel_gen = PixelMaskGenerator()
        assert hasattr(pixel_gen, "generate_combined_pixel_mask")
        assert hasattr(pixel_gen, "generate_static_mask")
        assert hasattr(pixel_gen, "generate_dynamic_mask")

        # Test BraggMaskGenerator interface
        bragg_gen = BraggMaskGenerator()
        assert hasattr(bragg_gen, "generate_bragg_mask_from_spots")
        assert hasattr(bragg_gen, "generate_bragg_mask_from_shoeboxes")
        assert hasattr(bragg_gen, "get_total_mask_for_still")

        # Test configuration creation functions
        dials_config = create_default_config()
        extraction_config = create_default_extraction_config()
        static_params = create_default_static_params()
        dynamic_params = create_default_dynamic_params()
        bragg_config = create_default_bragg_mask_config()

        # All configurations should be created without errors
        assert dials_config is not None
        assert extraction_config is not None
        assert static_params is not None
        assert dynamic_params is not None
        assert bragg_config is not None
</file>

<file path="tests/masking/test_pixel_mask_generator.py">
"""
Unit tests for PixelMaskGenerator.

These tests focus on individual functions within the PixelMaskGenerator class,
using mock objects to isolate the masking logic.
"""

import pytest
import numpy as np
from unittest.mock import Mock, patch, MagicMock

from diffusepipe.masking.pixel_mask_generator import (
    PixelMaskGenerator,
    StaticMaskParams,
    DynamicMaskParams,
    Circle,
    Rectangle,
    create_circular_beamstop,
    create_rectangular_beamstop,
    create_default_static_params,
    create_default_dynamic_params,
)
from diffusepipe.exceptions import MaskGenerationError


class TestPixelMaskGenerator:
    """Test suite for PixelMaskGenerator class."""

    def setup_method(self):
        """Set up test fixtures."""
        self.generator = PixelMaskGenerator()

    def test_generate_static_mask_basic(self):
        """Test basic static mask generation."""
        # Arrange
        mock_detector = Mock()
        mock_panel = Mock()
        mock_panel.get_image_size.return_value = (100, 50)  # (fast, slow)
        mock_panel.get_trusted_range.return_value = (-10, 65535)
        mock_detector.__iter__ = Mock(return_value=iter([mock_panel]))
        mock_detector.__len__ = Mock(return_value=1)

        static_params = StaticMaskParams()

        # Act
        result = self.generator.generate_static_mask(mock_detector, static_params)

        # Assert
        assert result is not None
        assert len(result) == 1
        # Check that we got a valid flex.bool object
        from dials.array_family import flex

        assert isinstance(result[0], flex.bool)
        # Check the mask has the expected size (50 * 100 = 5000 pixels)
        assert result[0].size() == 5000

    def test_generate_static_mask_with_beamstop(self):
        """Test static mask generation with circular beamstop using real flex arrays."""
        # Arrange
        mock_detector = Mock()
        mock_panel = Mock()
        mock_panel.get_image_size.return_value = (100, 50)

        mock_detector.__iter__ = MagicMock(side_effect=lambda: iter([mock_panel]))
        mock_detector.__len__ = Mock(return_value=1)

        beamstop = Circle(center_x=50, center_y=25, radius=10)
        static_params = StaticMaskParams(beamstop=beamstop)

        # Act
        result = self.generator.generate_static_mask(mock_detector, static_params)

        # Assert
        assert result is not None
        assert len(result) == 1
        # Check that the beamstop region is masked
        from dials.array_family import flex

        assert isinstance(result[0], flex.bool)
        # Count of False pixels should be > 0 due to beamstop
        assert result[0].count(False) > 0

    def test_generate_static_mask_with_untrusted_panels(self):
        """Test static mask generation with untrusted panels using real flex arrays."""
        # Arrange
        mock_detector = Mock()
        mock_panel1 = Mock()
        mock_panel1.get_image_size.return_value = (100, 50)
        mock_panel2 = Mock()
        mock_panel2.get_image_size.return_value = (100, 50)
        mock_detector.__iter__ = Mock(return_value=iter([mock_panel1, mock_panel2]))
        mock_detector.__len__ = Mock(return_value=2)

        static_params = StaticMaskParams(untrusted_panels=[1])  # Panel 1 is untrusted

        # Act
        result = self.generator.generate_static_mask(mock_detector, static_params)

        # Assert
        assert result is not None
        assert len(result) == 2
        from dials.array_family import flex

        # Panel 0 should have some True pixels
        assert isinstance(result[0], flex.bool)
        assert result[0].count(True) > 0
        # Panel 1 should be all False (untrusted)
        assert isinstance(result[1], flex.bool)
        assert result[1].count(True) == 0  # All pixels should be False

    def test_generate_dynamic_mask_no_images(self):
        """Test dynamic mask generation with no representative images using real flex arrays."""
        # Arrange
        from dials.array_family import flex

        mock_detector = Mock()
        mock_panel = Mock()
        mock_panel.get_image_size.return_value = (100, 50)

        mock_detector.__iter__ = MagicMock(side_effect=lambda: iter([mock_panel]))

        representative_images = []
        dynamic_params = DynamicMaskParams()

        # Act
        result = self.generator.generate_dynamic_mask(
            mock_detector, representative_images, dynamic_params
        )

        # Assert
        assert result is not None
        assert len(result) == 1
        # Should return all-True masks when no images provided
        assert isinstance(result[0], flex.bool)
        assert result[0].count(True) == 100 * 50  # All pixels should be True
        assert result[0].count(False) == 0  # No pixels should be False

    @patch("diffusepipe.masking.pixel_mask_generator.np")
    @patch("diffusepipe.masking.pixel_mask_generator.flex")
    def test_generate_dynamic_mask_with_images(self, mock_flex, mock_np):
        """Test dynamic mask generation with representative images."""
        # Arrange
        mock_detector = Mock()
        mock_panel = Mock()
        mock_panel.get_image_size.return_value = (4, 4)  # Small for testing

        mock_detector.__iter__ = MagicMock(side_effect=lambda: iter([mock_panel]))

        # Mock image data
        mock_image_set = Mock()
        mock_panel_data = Mock()
        mock_image_set.get_raw_data.return_value = [mock_panel_data]

        # Create test data with some hot and negative pixels
        test_array = np.array(
            [
                [100, 200, 300, 400],
                [150, 1000000, 250, 350],  # Hot pixel at [1,1]
                [175, 225, -50, 375],  # Negative pixel at [2,2]
                [200, 250, 300, 400],
            ],
            dtype=np.float64,
        )

        mock_np.array.return_value = test_array

        # Mock numpy zeros and full to return arrays with shape attribute
        mock_zeros = MagicMock()
        mock_zeros.shape = (4, 4)
        mock_np.zeros.return_value = mock_zeros

        mock_full = MagicMock()
        mock_full.shape = (4, 4)
        mock_np.full.return_value = mock_full

        mock_np.inf = float("inf")

        # Mock flex operations
        mock_mask = Mock()
        mock_flex.bool.return_value = mock_mask
        mock_flex.grid.return_value = Mock()

        representative_images = [mock_image_set]
        dynamic_params = DynamicMaskParams(
            hot_pixel_thresh=500000, negative_pixel_tolerance=0.0
        )

        # Act
        result = self.generator.generate_dynamic_mask(
            mock_detector, representative_images, dynamic_params
        )

        # Assert
        assert result is not None
        assert len(result) == 1
        mock_image_set.get_raw_data.assert_called_with(0)

    @patch("diffusepipe.masking.pixel_mask_generator.logger")
    def test_combine_masks_success(self, mock_logger):
        """Test successful combination of static and dynamic masks."""
        # Arrange
        mock_static_mask = Mock()
        mock_dynamic_mask = Mock()
        mock_combined_mask = Mock()

        # Mock the logical AND operation
        mock_static_mask.__and__ = Mock(return_value=mock_combined_mask)
        mock_combined_mask.count.return_value = 80  # 80 good pixels out of 100
        mock_static_mask.count.return_value = 90
        mock_dynamic_mask.count.return_value = 85
        mock_combined_mask.__len__ = Mock(return_value=100)

        static_masks = (mock_static_mask,)
        dynamic_masks = (mock_dynamic_mask,)

        # Act
        result = self.generator._combine_masks(static_masks, dynamic_masks)

        # Assert
        assert result is not None
        assert len(result) == 1
        assert result[0] is mock_combined_mask
        mock_static_mask.__and__.assert_called_once_with(mock_dynamic_mask)

    def test_combine_masks_different_panel_counts(self):
        """Test error handling when static and dynamic masks have different panel counts."""
        # Arrange
        static_masks = (Mock(), Mock())  # 2 panels
        dynamic_masks = (Mock(),)  # 1 panel

        # Act & Assert
        with pytest.raises(MaskGenerationError) as exc_info:
            self.generator._combine_masks(static_masks, dynamic_masks)

        assert "different panel counts" in str(exc_info.value)

    def test_apply_beamstop_mask_circular(self):
        """Test application of circular beamstop mask using real flex arrays."""
        # Arrange
        from dials.array_family import flex

        mock_panel = Mock()
        mock_panel.get_image_size.return_value = (6, 6)

        # Create real flex arrays
        initial_mask_array = flex.bool(
            flex.grid(6, 6), True
        )  # All pixels initially good
        beamstop = Circle(center_x=3, center_y=3, radius=1)

        # Act
        result_mask_array = self.generator._apply_beamstop_mask(
            mock_panel, initial_mask_array, beamstop
        )

        # Assert
        assert isinstance(result_mask_array, flex.bool)
        # Check that some pixels were masked (center should be False)
        assert result_mask_array.count(False) > 0
        # Check that not all pixels were masked
        assert result_mask_array.count(True) > 0
        # Specifically check center pixel is masked
        assert result_mask_array[3, 3] is False

    def test_apply_beamstop_mask_rectangular(self):
        """Test application of rectangular beamstop mask using real flex arrays."""
        # Arrange
        from dials.array_family import flex

        mock_panel = Mock()
        mock_panel.get_image_size.return_value = (10, 10)

        # Create real flex arrays
        initial_mask_array = flex.bool(
            flex.grid(10, 10), True
        )  # All pixels initially good
        beamstop = Rectangle(min_x=2, max_x=5, min_y=3, max_y=6)

        # Act
        result_mask_array = self.generator._apply_beamstop_mask(
            mock_panel, initial_mask_array, beamstop
        )

        # Assert
        assert isinstance(result_mask_array, flex.bool)
        # Check that some pixels were masked in the rectangular region
        assert result_mask_array.count(False) > 0
        # Check that not all pixels were masked
        assert result_mask_array.count(True) > 0
        # Check specific pixel in rectangle is masked
        assert result_mask_array[4, 3] is False  # Inside rectangle


class TestMaskParameterClasses:
    """Test suite for mask parameter dataclasses and utility functions."""

    def test_circle_creation(self):
        """Test Circle dataclass creation."""
        circle = Circle(center_x=10.5, center_y=20.3, radius=5.0)
        assert circle.center_x == 10.5
        assert circle.center_y == 20.3
        assert circle.radius == 5.0

    def test_rectangle_creation(self):
        """Test Rectangle dataclass creation."""
        rect = Rectangle(min_x=0, max_x=10, min_y=5, max_y=15)
        assert rect.min_x == 0
        assert rect.max_x == 10
        assert rect.min_y == 5
        assert rect.max_y == 15

    def test_static_mask_params_creation(self):
        """Test StaticMaskParams dataclass creation."""
        beamstop = Circle(center_x=50, center_y=60, radius=10)
        rects = [Rectangle(min_x=0, max_x=5, min_y=0, max_y=5)]
        panels = [1, 3]

        params = StaticMaskParams(
            beamstop=beamstop, untrusted_rects=rects, untrusted_panels=panels
        )

        assert params.beamstop is beamstop
        assert params.untrusted_rects == rects
        assert params.untrusted_panels == panels

    def test_dynamic_mask_params_creation(self):
        """Test DynamicMaskParams dataclass creation."""
        params = DynamicMaskParams(
            hot_pixel_thresh=1000000.0,
            negative_pixel_tolerance=5.0,
            max_fraction_bad_pixels=0.2,
        )

        assert params.hot_pixel_thresh == 1000000.0
        assert params.negative_pixel_tolerance == 5.0
        assert params.max_fraction_bad_pixels == 0.2


class TestUtilityFunctions:
    """Test suite for utility functions."""

    def test_create_circular_beamstop(self):
        """Test creation of circular beamstop."""
        beamstop = create_circular_beamstop(center_x=100, center_y=150, radius=25)

        assert isinstance(beamstop, Circle)
        assert beamstop.center_x == 100
        assert beamstop.center_y == 150
        assert beamstop.radius == 25

    def test_create_rectangular_beamstop(self):
        """Test creation of rectangular beamstop."""
        beamstop = create_rectangular_beamstop(min_x=10, max_x=20, min_y=30, max_y=40)

        assert isinstance(beamstop, Rectangle)
        assert beamstop.min_x == 10
        assert beamstop.max_x == 20
        assert beamstop.min_y == 30
        assert beamstop.max_y == 40

    def test_create_default_static_params(self):
        """Test creation of default static mask parameters."""
        params = create_default_static_params()

        assert isinstance(params, StaticMaskParams)
        assert params.beamstop is None
        assert params.untrusted_rects is None
        assert params.untrusted_panels is None

    def test_create_default_dynamic_params(self):
        """Test creation of default dynamic mask parameters."""
        params = create_default_dynamic_params()

        assert isinstance(params, DynamicMaskParams)
        assert params.hot_pixel_thresh == 1e6
        assert params.negative_pixel_tolerance == 0.0
        assert params.max_fraction_bad_pixels == 0.1


class TestIntegrationScenarios:
    """Integration-style tests for complete masking workflows."""

    def setup_method(self):
        """Set up test fixtures."""
        self.generator = PixelMaskGenerator()

    def test_generate_combined_pixel_mask_success(self):
        """Test successful generation of combined pixel mask using real flex arrays."""
        # Arrange
        mock_detector = Mock()
        mock_panel = Mock()
        mock_panel.get_image_size.return_value = (4, 4)

        mock_detector.__iter__ = MagicMock(side_effect=lambda: iter([mock_panel]))
        mock_detector.__len__ = Mock(return_value=1)

        static_params = StaticMaskParams()
        representative_images = []  # Empty for simplicity
        dynamic_params = DynamicMaskParams()

        # Act
        result = self.generator.generate_combined_pixel_mask(
            mock_detector, static_params, representative_images, dynamic_params
        )

        # Assert
        assert result is not None
        assert len(result) == 1
        from dials.array_family import flex

        assert isinstance(result[0], flex.bool)
        # Should have 16 pixels (4x4)
        assert result[0].size() == 16

    def test_generate_combined_pixel_mask_error_handling(self):
        """Test error handling in combined mask generation."""
        # Arrange
        mock_detector = Mock()
        mock_detector.__iter__ = Mock(side_effect=Exception("Detector error"))

        static_params = StaticMaskParams()
        representative_images = []
        dynamic_params = DynamicMaskParams()

        # Act & Assert
        with pytest.raises(MaskGenerationError) as exc_info:
            self.generator.generate_combined_pixel_mask(
                mock_detector, static_params, representative_images, dynamic_params
            )

        assert "Failed to generate combined pixel mask" in str(exc_info.value)
</file>

<file path="docs/TEMPLATES/WORKING_MEMORY_LOG.md">
# Developer Working Memory Log

## Current Task/Focus (As of: 2025-06-12)

**Goal:** `IDL SYNCHRONIZATION AND UPDATE - Interface Definition Language Specifications Alignment`

**Current Sub-task:** `COMPLETED - Systematic IDL review, updates, and creation for plan.md compliance and implementation consistency`

**Relevant Files:**
*   `src/diffusepipe/adapters/dials_sequence_process_adapter_IDL.md` - Fixed method naming and parameter inconsistencies
*   `src/diffusepipe/adapters/dials_stills_process_adapter_IDL.md` - Added missing output_dir_final parameter
*   `src/diffusepipe/corrections_IDL.md` - Created IDL for centralized correction factor logic
*   `src/diffusepipe/utils/cbf_utils_IDL.md` - Created IDL for CBF header parsing utilities

**Key Requirements/Acceptance Criteria (ALL MET):**
1.  ✅ Systematic IDL-Implementation Consistency Check - Reviewed all existing IDL files against implementations
2.  ✅ Fixed Adapter Interface Inconsistencies - Corrected method naming (process_sequence → process_still) and parameter alignment
3.  ✅ Created Missing IDL Specifications - Added corrections_IDL.md and cbf_utils_IDL.md for plan.md Module references
4.  ✅ Plan.md Compliance Verification - Analyzed Phase 1-2 coverage (95% complete) and identified Phase 3-4 gaps
5.  ✅ Dual Processing Mode Support - Verified both adapters correctly implement identical interfaces per plan_adaptation.md
6.  ✅ All Multipliers Convention - Ensured corrections IDL enforces rule 0.7 (all corrections as multipliers)
7.  ✅ Dependencies and Error Handling - Updated IDL annotations and error conditions for complete contract specification

---

## Recent Activity Log & Progress

*   **2025-06-12 (Latest):**
    *   **IDL SYNCHRONIZATION AND UPDATE COMPLETED:**
    *   **Goal Achieved:** Systematic review and update of Interface Definition Language specifications for plan.md compliance and implementation consistency
    *   **IDL Coverage Analysis:** Conducted comprehensive analysis showing Phase 1-2 at 95% compliance, identified Phase 3-4 gaps (0% coverage)
    *   **Key Technical Achievements:**
        *   ✅ **Adapter Interface Consistency:** Fixed critical naming inconsistency (process_sequence → process_still) in DIALSSequenceProcessAdapter IDL
        *   ✅ **Parameter Alignment:** Added missing output_dir_final parameter to both adapter IDLs to match implementation signatures
        *   ✅ **Created Missing IDL Specifications:** Added corrections_IDL.md for Module 2.S.2 correction factor logic and cbf_utils_IDL.md for Module 1.S.0 data type detection
        *   ✅ **Plan.md Compliance Verification:** Systematic analysis of all modules showing excellent Phase 1-2 coverage and documenting Phase 3-4 requirements
        *   ✅ **Dual Processing Mode Support:** Verified both adapters implement identical interfaces as required by plan_adaptation.md
        *   ✅ **All Multipliers Convention:** Ensured corrections IDL enforces rule 0.7 (all corrections as multipliers) from plan.md
        *   ✅ **Complete Dependency Mapping:** Updated IDL annotations with proper @depends_on and @raises_error specifications
    *   **IDL File Status Summary:**
        *   ✅ **DIALSStillsProcessAdapter/DIALSSequenceProcessAdapter:** Updated and aligned with implementations and plan requirements
        *   ✅ **DataExtractor:** Verified complete coverage of Module 2.S.1 & 2.S.2 requirements
        *   ✅ **Masking Components:** BraggMaskGenerator and PixelMaskGenerator properly specified for Module 1.S.2 & 1.S.3
        *   ✅ **StillProcessorAndValidator:** Covers Module 1.S.1 with validation logic properly specified
        *   ✅ **CorrectionsHelper:** Created comprehensive specification for centralized correction factor logic
        *   ✅ **CBFHeaderParser:** Created specification for CBF data type detection required by Module 1.S.0
        *   ⚠️ **ModelValidator Structure:** Identified inconsistency between IDL (separate interface) and implementation (internal class)
    *   **Plan.md Alignment Analysis:**
        *   ✅ **Phase 1 (Per-Still Geometry/Indexing):** Excellent coverage with all 4 modules properly specified
        *   ✅ **Phase 2 (Diffuse Extraction):** Complete coverage with DataExtractor and correction components
        *   ❌ **Phase 3 (Voxelization/Scaling):** Missing IDLs for GlobalVoxelGrid, VoxelAccumulator, DiffuseScalingModel, DiffuseDataMerger
        *   ❌ **Phase 4 (Absolute Scaling):** Missing IDL for AbsoluteScalingCalculator with Krogh-Moe method
    *   **Interface Contract Quality:**
        *   ✅ **Behavioral Specifications:** All IDLs include comprehensive Behavior, Preconditions, and Postconditions
        *   ✅ **Error Handling:** Proper @raises_error annotations with specific error conditions
        *   ✅ **Dependency Management:** Complete @depends_on and @depends_on_resource declarations
        *   ✅ **Type Safety:** Consistent parameter and return type specifications across all interfaces
        *   ✅ **Implementation Consistency:** All IDLs match actual implementation method signatures and behavior

*   **2025-06-12 (Earlier):**
    *   **DIALS GENERATE MASK ADAPTER TEST FAILURES RESOLUTION COMPLETED:**
    *   **Goal Achieved:** Fixed all 7 test failures in DIALSGenerateMaskAdapter through C++ type compatibility improvements and proper mocking strategies
    *   **Test Success Rate:** All 15 tests in test_dials_generate_mask_adapter.py now pass, with overall test suite stability maintained (no regressions)
    *   **Root Cause Analysis:**
        *   ✅ **C++ Type Conversion Error:** ExperimentList([MagicMock]) failed due to C++ backend requiring real Experiment objects
        *   ✅ **Wrong Patch Targets:** Tests patched `dials.util.masking.generate_mask` but implementation uses custom `_call_generate_mask` method
        *   ✅ **Incomplete Mock Setup:** Mock objects lacked proper structure to work with isinstance() checks and real implementation logic
    *   **Key Technical Fixes Applied:**
        *   ✅ **MockExperimentList Class:** Created proper mock class with `__len__()`, `__getitem__()`, and `__init__()` methods for isinstance compatibility
        *   ✅ **Patch Strategy Improvement:** Changed from module-level patching to `patch.object(adapter, '_call_generate_mask')` for proper method mocking
        *   ✅ **Flex Module Mocking:** Enhanced `dials.array_family.flex` mocking with proper bool, grid, and int mock setup for vectorized operations
        *   ✅ **Import Error Simulation:** Fixed ImportError test using `builtins.__import__` patching with sys.modules cleanup
        *   ✅ **Reflections Mock Enhancement:** Added proper `__contains__` and `__getitem__` setup for mock reflections to work with real implementation
        *   ✅ **Method Call Verification:** Improved assertion patterns to verify mock calls with proper argument type checking
        *   ✅ **Context Manager Usage:** Organized patches using proper context managers for clean test isolation
    *   **Mocking Strategy Evolution for C++ Backed Objects:**
        *   ✅ **Real Class Mocking:** Created actual Python classes that mimic C++ object interfaces instead of using MagicMock for constructors
        *   ✅ **Type Compatibility:** Ensured mock objects work with isinstance() checks and C++ type conversion requirements
        *   ✅ **Method vs Constructor Patching:** Distinguished between patching methods (use patch.object) and classes (use proper mock classes)
        *   ✅ **Magic Method Implementation:** Added proper `__len__`, `__getitem__`, and other magic methods to mock classes
    *   **Testing Infrastructure Improvements:**
        *   ✅ **C++ Integration Patterns:** Established patterns for testing adapters that use C++ backed DIALS objects
        *   ✅ **Mock Class Design:** Created reusable mock class patterns for ExperimentList and other DIALS container objects
        *   ✅ **Flex Module Mocking:** Comprehensive mocking strategy for dials.array_family.flex with all required components
        *   ✅ **Import Error Testing:** Robust patterns for testing ImportError scenarios with proper module cleanup
    *   **Code Quality and Maintenance:**
        *   ✅ **Black Formatting:** Applied automatic code formatting to test_dials_generate_mask_adapter.py
        *   ✅ **Ruff Linting:** Fixed all linting issues including unused imports optimization
        *   ✅ **Test Code Quality:** Improved test readability with proper mock class definitions and clear patch organization
        *   ✅ **Documentation:** Added comprehensive comments explaining mock setup rationale for C++ compatibility

*   **2025-06-12 (Earlier):**
    *   **TEST SUITE MAINTENANCE - 11 CRITICAL TEST FAILURE RESOLUTION COMPLETED:**
    *   **Goal Achieved:** Systematic resolution of 11 identified test failures through targeted fixes and improved mocking strategies
    *   **Test Success Rate:** All 11 previously failing tests now pass, with overall test suite stability maintained (no regressions introduced)
    *   **Key Technical Fixes Applied:**
        *   ✅ **Exception Matching Fix:** Corrected test_process_still_import_error to expect ConfigurationError instead of DIALSError
        *   ✅ **Mock Call Assertion Fixes:** Added missing output_dir_final=None parameter to 3 test assertions in still processor components
        *   ✅ **Iterator Mocking Enhancement:** Fixed mock_detector.__iter__ using MagicMock with side_effect for proper iterator behavior
        *   ✅ **Magic Method Support:** Converted Mock to MagicMock in 6 tests for proper __getitem__, __iter__, and __len__ method support
        *   ✅ **Real DIALS Integration:** Replaced complex mocking with real flex arrays in pixel mask generator tests for more authentic behavior
        *   ✅ **Numpy Mock Shape Attributes:** Fixed mock numpy arrays to include .shape attributes needed by actual implementation
        *   ✅ **Test Assertion Improvements:** Updated test assertions to check behavior rather than object identity where appropriate
    *   **Mocking Strategy Evolution:**
        *   ✅ **Mock → MagicMock Migration:** Systematic conversion where magic methods (__getitem__, __iter__, __and__, __invert__) were required
        *   ✅ **Real Component Integration:** Adopted real DIALS flex arrays over complex mock objects for more reliable integration testing
        *   ✅ **Iterator Pattern Fixes:** Used side_effect=lambda: iter([items]) for proper iterator mocking that returns fresh iterators per call
        *   ✅ **Shape Attribute Mocking:** Added proper .shape attributes to mock numpy arrays accessed by implementation code
    *   **Code Quality and Maintenance:**
        *   ✅ **Black Formatting:** Applied automatic code formatting to all modified test files
        *   ✅ **Ruff Linting:** Fixed all linting issues including unused imports and variable assignments
        *   ✅ **Import Optimization:** Removed unused imports and consolidated redundant from unittest.mock import statements
        *   ✅ **Test Code Quality:** Improved test readability and maintainability through cleaner mock patterns
    *   **Test Categories Fixed:**
        *   ✅ **1 Adapter Test:** DIALS stills process adapter exception handling
        *   ✅ **3 Crystallography Tests:** Still processing and validation mock call assertions
        *   ✅ **1 Integration Test:** Phase 1 workflow detector iterator mocking
        *   ✅ **4 Masking Tests:** Pixel mask generator with various mocking and assertion issues
        *   ✅ **2 Regression Tests:** Corrections pipeline MagicMock usage for magic methods
    *   **Testing Infrastructure Benefits:**
        *   ✅ **Reduced Mock Complexity:** Simplified test setup by using real DIALS components where feasible
        *   ✅ **Improved Test Authenticity:** Tests now validate actual implementation behavior rather than mock interactions
        *   ✅ **Enhanced Reliability:** Fixed iterator and magic method mocking patterns that were causing intermittent failures
        *   ✅ **Future-Proof Patterns:** Established proper mocking patterns for DIALS integration tests

*   **2025-06-11:**
    *   **PHASE 2 CRITICAL FIXES AND REFINEMENTS COMPLETED:**
    *   **Goal Achieved:** Systematic implementation of scientifically accurate corrections and performance optimization
    *   **Air Attenuation Enhancement:** Replaced rough approximation with NIST-based scientific accuracy
    *   **Key Technical Achievements:**
        *   ✅ **Section 1:** Implemented NIST X-ray mass attenuation coefficients for air components (N: 78.084%, O: 20.946%, Ar: 0.934%, C: 0.036%)
        *   ✅ **Section 1:** Created `_get_mass_attenuation_coefficient` with tabulated NIST data covering 1-100 keV energy range
        *   ✅ **Section 1:** Enhanced `_calculate_air_attenuation_coefficient` using ideal gas law with configurable temperature and pressure
        *   ✅ **Section 1:** Added comprehensive unit tests validating coefficients against NIST reference values within 1% tolerance
        *   ✅ **Section 2:** Implemented `_process_pixels_vectorized` achieving 2.4x performance improvement (4.0s → 1.7s)
        *   ✅ **Section 2:** Created vectorized versions of all correction calculations (LP, QE, SA, Air Attenuation)
        *   ✅ **Section 2:** Added equivalence tests proving vectorized and iterative implementations produce identical results
        *   ✅ **Section 3:** Updated phase0.md and phase2.md checklists to reflect completed implementation status
        *   ✅ **Section 4:** Fixed linting issues (unused variable warning) and ensured code quality compliance
    *   **Scientific Accuracy Improvements:**
        *   ✅ Air density calculation using ideal gas law: ρ = (P × M) / (R × T) with configurable T and P
        *   ✅ NIST tabulated mass attenuation coefficients with log-log interpolation for accuracy across energy range
        *   ✅ Standard atmospheric composition with proper mass fractions and molar masses
        *   ✅ Beer-Lambert law implementation: Attenuation = exp(-μ_air * path_length) with proper unit conversions
    *   **Performance Optimization Results:**
        *   ✅ Vectorized coordinate extraction and q-vector calculations for batch processing
        *   ✅ Vectorized correction factor application for all four correction types
        *   ✅ Vectorized filtering operations for resolution and intensity bounds
        *   ✅ Maintained original iterative implementation for comparison and fallback
    *   **Testing Infrastructure Enhancement:**
        *   ✅ 18 comprehensive tests covering all correction factors and their combinations
        *   ✅ NIST reference value validation tests with precise tolerance checking
        *   ✅ Vectorization equivalence tests proving algorithmic correctness
        *   ✅ Performance characterization tests documenting speedup achievements
        *   ✅ Error handling tests for graceful degradation under failure conditions

*   **2025-06-11 (Earlier):**
    *   **TEST FAILURE RESOLUTION AND STRATEGY REFINEMENT COMPLETED:**
    *   **Goal Achieved:** Systematic resolution of test failures through targeted fixes and improved testing strategy
    *   **Test Failure Reduction:** Successfully reduced test failures from 22 to 8 (64% improvement), with 186 tests now passing
    *   **Key Technical Fixes Applied:**
        *   ✅ **F.1** - Fixed DIALSStillsProcessAdapter TypeError by correcting `do_import([image_path])` to `do_import(image_path)`
        *   ✅ **F.2** - Resolved AttributeError __getitem__ issues by switching from Mock to MagicMock for flex array returns
        *   ✅ **F.3** - Fixed solid angle correction assertion limit from < 1e6 to < 3e6 for realistic detector geometries
        *   ✅ **F.4** - Corrected patch target for Corrections class to use proper module path
        *   ✅ **F.5** - Fixed MaskGenerationError by making _accumulate_image_stats robust to handle both tuple and single-panel returns
        *   ✅ **F.6** - Converted PixelMaskGenerator tests to use real flex arrays instead of complex mocking
        *   ✅ **F.7** - Fixed RuntimeWarning divide by zero in corrections regression tests with proper error handling
        *   ✅ **F.8-F.9** - Refined test strategy using real flex arrays for more authentic DIALS integration testing
        *   ✅ **F.10** - Enhanced Phase1 integration test with real numpy arrays for panel data
    *   **Testing Strategy Improvements:**
        *   ✅ Evolved from Mock → MagicMock for proper magic method support (__getitem__, __and__, etc.)
        *   ✅ Implemented real flex array usage for authentic DIALS object integration
        *   ✅ Enhanced error handling for edge cases (divide by zero, large correction factors)
        *   ✅ Improved patch target resolution for imported modules
        *   ✅ Strengthened test robustness with proper bounds checking and realistic assertions
    *   **Technical Debt Reduction:**
        *   ✅ Removed excessive mocking in favor of real component integration
        *   ✅ Fixed API compatibility issues with DIALS imports and method calls
        *   ✅ Improved error propagation test expectations to account for large solid angle corrections
        *   ✅ Enhanced mask handling to support both tuple and single-panel detector configurations

    *   **VISUAL DIAGNOSTICS IMPLEMENTATION COMPLETED:**
    *   **Goal Achieved:** Created comprehensive visual verification tools for Phase 2 diffuse scattering pipeline
    *   **Standalone Diagnostics Script (`check_diffuse_extraction.py`):**
        *   ✅ Created comprehensive visual diagnostics script with 8 diagnostic plot types
        *   ✅ Implemented diffuse pixel overlay, Q-space projections, intensity analysis
        *   ✅ Added sigma analysis, I/σ histograms, and intensity heatmap generation
        *   ✅ Support for conditional plotting based on available pixel coordinates
        *   ✅ Flexible argument parsing with required and optional inputs
        *   ✅ Comprehensive error handling and input validation
    *   **End-to-End Pipeline Script (`run_phase2_e2e_visual_check.py`):**
        *   ✅ Complete pipeline orchestration from CBF to visual diagnostics
        *   ✅ Phase 1 integration: DIALS processing, pixel masks, Bragg masks
        *   ✅ Phase 2 integration: DataExtractor with pixel coordinate tracking
        *   ✅ Phase 3 integration: Automatic visual diagnostic generation
        *   ✅ JSON-based configuration for all processing parameters
        *   ✅ Support for both spot-based and shoebox-based Bragg masking
        *   ✅ Comprehensive logging and structured output directories
    *   **DataExtractor Enhancement:**
        *   ✅ Added `save_original_pixel_coordinates` field to ExtractionConfig
        *   ✅ Modified `_process_pixels()` to track panel IDs and pixel coordinates
        *   ✅ Updated `_save_output()` to include coordinates in NPZ files
        *   ✅ Enhanced method signatures and return types for coordinate support
    *   **Documentation and Integration:**
        *   ✅ Created comprehensive `VISUAL_DIAGNOSTICS_GUIDE.md` with full usage docs
        *   ✅ Updated README files with cross-references to main documentation
        *   ✅ Added visual diagnostics section to main project documentation
        *   ✅ Included troubleshooting guide and performance optimization tips

*   **Previous Context - Test Suite Remediation (2025-06-11):**
    *   **Goal Achieved:** Fixed DIALS API compatibility issues and test failures (132→171 passing tests)
    *   **Key Fixes:** DIALS PHIL scope imports (`master_phil_scope` → `phil_scope`), mock patching strategies, routing logic
    *   **Integration Tests:** Converted flex array tests to real DIALS object integration tests
    *   **API Compatibility:** Resolved DIALS API breaking changes across multiple adapter components
    *   **Testing Infrastructure:** Enhanced with proper module-level mock patching and real flex.bool objects
    *   **Files Modified:** `dials_stills_process_adapter.py`, multiple test files, `cbf_utils.py`

*   **Earlier Context:**
    *   **Phase 2:** Complete pixel correction pipeline with error propagation implemented
    *   **Phase 1:** Spot finding and processing pipeline established  
    *   **Phase 0:** DIALS adapter integration working with real CBF files
    *   **Codebase Cleanup:** Systematic cleanup and organization completed

---

## Next Steps (Post-IDL Synchronization)

1.  **IDL Completeness & Structural Consistency:**
    *   Resolve ModelValidator structural inconsistency (separate interface vs internal class)
    *   Create missing Phase 3 IDL specifications (GlobalVoxelGrid, VoxelAccumulator, DiffuseScalingModel, DiffuseDataMerger)
    *   Create missing Phase 4 IDL specification (AbsoluteScalingCalculator with Krogh-Moe method)
    *   Consider creating IDLs for exceptions.py and constants.py if they form part of public interfaces

2.  **Phase 3: Voxelization and Relative Scaling Implementation:**
    *   Implement missing Phase 3 components based on plan.md Module 3 specifications
    *   Create GlobalVoxelGrid definition for 3D reciprocal space
    *   Implement VoxelAccumulator for binning corrected diffuse pixels with HDF5 backend
    *   Develop custom DiffuseScalingModel using DIALS/CCTBX components
    *   Add per-still scaling model parameter refinement with proper configuration

3.  **Pipeline Integration & Production Readiness:**
    *   Complete end-to-end pipeline integration from Phase 1 through Phase 4
    *   Implement batch processing capabilities for multiple CBF files
    *   Add comprehensive visual diagnostics for Phase 3 and Phase 4 outputs
    *   Performance optimization and memory management for large-scale datasets

---

## Implementation Notes & Decisions Made

### **IDL Synchronization and Update Strategy:**
*   **Systematic Review Approach:** Conducted comprehensive analysis of all existing IDL files against their implementations to identify inconsistencies
*   **Plan.md Alignment Priority:** Prioritized alignment with plan.md and plan_adaptation.md requirements over implementation convenience  
*   **Interface Contract Completeness:** Ensured all IDLs include comprehensive Behavior, Preconditions, Postconditions, and error handling specifications
*   **Dual Processing Mode Compliance:** Verified both DIALS adapters implement identical interfaces as required by plan_adaptation.md for seamless routing
*   **All Multipliers Convention Enforcement:** Created corrections IDL to formally specify rule 0.7 (all corrections as multipliers) from plan.md

### **IDL Design Philosophy:**
*   **Implementation Consistency Over Idealism:** Updated IDLs to match actual working implementations rather than forcing code changes
*   **Dependency Transparency:** Added comprehensive @depends_on and @depends_on_resource annotations for clear component relationships
*   **Error Contract Specification:** Used @raises_error annotations to formally specify all failure modes and error conditions
*   **Future-Proof Structure:** Designed IDL specifications to accommodate Phase 3-4 expansion while maintaining backward compatibility
*   **Type Safety Focus:** Ensured all parameter and return types are consistently specified across related interfaces

### **Gap Analysis and Prioritization:**
*   **Phase 1-2 Excellence:** Achieved 95% compliance coverage for implemented phases with comprehensive behavioral specifications
*   **Phase 3-4 Documentation Debt:** Identified complete absence of IDL specifications for voxelization, scaling, and absolute scaling modules
*   **Structural Inconsistency Identification:** Found ModelValidator implemented as internal class but specified as separate interface
*   **Missing Utility Specifications:** Created IDLs for corrections and CBF utilities referenced in plan.md but previously unspecified
*   **Configuration Completeness:** Verified types_IDL.md contains necessary configuration structures for dual processing mode support

### **IDL Creation Standards:**
*   **Plan.md Module Alignment:** Ensured each created IDL directly addresses specific modules and requirements from plan.md
*   **Behavioral Specification Depth:** Included detailed step-by-step behavior descriptions matching implementation logic
*   **Error Handling Completeness:** Specified all error conditions with specific descriptions and triggering circumstances
*   **Dependency Declaration Accuracy:** Mapped all external dependencies (DIALS, CCTBX, filesystem, etc.) used by implementations
*   **Interface Signature Precision:** Matched all method signatures exactly to implementation function/method signatures

### **Phase 2 Air Attenuation Scientific Enhancement:**
*   **NIST Data Integration:** Implemented tabulated X-ray mass attenuation coefficients for air components (N, O, Ar, C) from NIST XCOM database
*   **Energy Range Coverage:** Supporting 1-100 keV X-ray energies with log-log interpolation for accuracy across wide energy range
*   **Atmospheric Composition:** Standard dry air composition by mass (N: 78.084%, O: 20.946%, Ar: 0.934%, C: 0.036%) with proper molar masses
*   **Thermodynamic Accuracy:** Ideal gas law implementation with configurable temperature (default: 293.15 K) and pressure (default: 1.0 atm)
*   **Unit Conversion Precision:** Proper conversion from mm to m for path lengths, maintaining dimensional consistency throughout calculations

### **Performance Optimization Strategy:**
*   **Vectorization Approach:** Implemented complete vectorized pipeline alongside original iterative implementation for comparison and fallback
*   **Memory Efficiency:** Batch processing of coordinates, q-vectors, and corrections to minimize Python loop overhead
*   **Algorithmic Equivalence:** Rigorous testing proving vectorized and iterative implementations produce identical results within floating-point tolerance
*   **Performance Measurement:** Documented 2.4x speedup (4.0s → 1.7s) with structured performance testing and characterization
*   **Scalability Design:** Framework supports efficient processing of large detector images through vectorized NumPy operations

### **Testing Strategy Enhancement:**
*   **Scientific Validation:** Tests validate NIST coefficient values within 1% tolerance using reference data points
*   **Cross-Implementation Verification:** Equivalence tests comparing vectorized vs iterative results ensure algorithmic correctness
*   **Error Handling Robustness:** Comprehensive failure mode testing with graceful degradation and appropriate fallback values
*   **Performance Characterization:** Systematic measurement of processing speeds with realistic detector geometries and data sizes
*   **Integration Testing:** Real DIALS component integration with proper mock strategies for external dependencies

### **Test Failure Resolution Strategy (Previous):**
*   **Systematic Approach:** Applied a comprehensive 10-step fix process targeting specific failure categories
*   **Mock Evolution:** Transitioned from basic Mock to MagicMock for proper magic method support (__getitem__, __and__)
*   **Real Component Integration:** Replaced complex mocking with real DIALS flex arrays for more authentic testing
*   **Error Handling Enhancement:** Improved bounds checking and added proper handling for edge cases (divide by zero, large corrections)
*   **API Compatibility:** Fixed DIALS import issues and corrected method call signatures for current API versions
*   **Test Robustness:** Enhanced assertions with realistic bounds based on actual detector geometries and correction factors

### **Visual Diagnostics Implementation Strategy:**
*   **Standalone vs Orchestrated Approach:** Created both standalone diagnostic script for existing data and complete end-to-end orchestration script
*   **Pixel Coordinate Tracking:** Enhanced DataExtractor to conditionally save original pixel coordinates for enhanced visualizations
*   **Configuration Management:** JSON-based parameter overrides for all processing stages with comprehensive error handling
*   **Diagnostic Plot Comprehensiveness:** 8 different plot types covering all aspects of diffuse extraction verification
*   **Real DIALS Integration:** Scripts work with actual DIALS dependencies and real CBF data for authentic validation

### **DataExtractor Enhancement Design:**
*   **Backward Compatibility:** Added pixel coordinate saving as optional feature controlled by configuration flag
*   **Memory Efficiency:** Coordinate tracking only enabled when needed for visual diagnostics
*   **Multi-Panel Support:** Framework for multi-panel detectors (currently focused on single panel)
*   **NPZ Format Extension:** Extended NPZ output format while maintaining compatibility with existing consumers
*   **Method Signature Evolution:** Enhanced internal methods to return coordinate data without breaking existing interfaces

### **Script Architecture Decisions:**
*   **Error Handling Philosophy:** Comprehensive try-catch blocks with detailed logging and graceful failure modes
*   **Configuration Flexibility:** Support for both default configurations and JSON-based parameter overrides
*   **Output Organization:** Structured directory hierarchy with all intermediate files preserved for debugging
*   **Subprocess Integration:** Robust subprocess execution for visual diagnostics with proper error capture and logging
*   **Documentation Integration:** Cross-referenced documentation with clear usage examples and troubleshooting guides

### **Test Infrastructure Patterns (Enhanced Through Recent Remediation):**
*   **DIALS API Compatibility:** Systematic handling of DIALS version changes and import evolution (enhanced through F.1-F.4 fixes)
*   **Mock Strategy Evolution:** Migrated from Mock → MagicMock for proper magic method support, with reduced mocking in favor of real components
*   **Integration vs Unit Testing:** Strengthened real DIALS object integration tests for flex arrays and complex dependencies (F.6, F.8-F.9)
*   **Error Handling Robustness:** Enhanced with proper divide-by-zero handling and realistic correction factor bounds (F.3, F.7)
*   **Component Interface Testing:** Improved validation of actual implementation interfaces with real flex arrays rather than assumed mock interfaces

---

## Key Technical Benefits Achieved

### **IDL Synchronization and Documentation Quality:**
*   **Interface Contract Clarity:** All Phase 1-2 components now have precise, comprehensive IDL specifications defining exact behavioral contracts
*   **Implementation Consistency Assurance:** Fixed critical inconsistencies between IDL specifications and actual code implementations
*   **Plan.md Compliance Achievement:** Achieved 95% compliance for Phase 1-2 modules with systematic verification against plan requirements
*   **Developer Documentation Excellence:** Created missing IDL specifications for utilities referenced in plan.md but previously unspecified
*   **Dual Processing Mode Verification:** Confirmed both DIALS adapters implement identical interfaces for seamless data type routing
*   **Future Development Foundation:** Provided complete specifications for Phase 1-2 components enabling confident Phase 3-4 development

### **Interface Design and Architecture Quality:**
*   **Behavioral Specification Completeness:** All IDLs include comprehensive Behavior, Preconditions, Postconditions covering all edge cases
*   **Error Handling Formalization:** Complete @raises_error annotations providing specific error conditions and triggering circumstances
*   **Dependency Transparency:** Comprehensive @depends_on and @depends_on_resource declarations clarifying all component relationships
*   **Type Safety Enhancement:** Consistent parameter and return type specifications across all related interfaces
*   **Configuration Structure Validation:** Verified types_IDL.md contains all necessary configuration structures for current implementations

### **Scientific Accuracy and Performance:**
*   **NIST-Based Air Attenuation:** Replaced rough approximation with scientifically accurate calculation using tabulated NIST X-ray mass attenuation data
*   **Configurable Environmental Conditions:** Support for variable temperature and pressure conditions in air density calculations
*   **Performance Optimization:** Achieved 2.4x speedup through comprehensive vectorization of pixel processing pipeline
*   **Algorithmic Verification:** Rigorous testing proving equivalence between optimized and reference implementations
*   **Energy Range Coverage:** Accurate corrections across 1-100 keV X-ray energy range with proper interpolation

### **Implementation Quality and Maintainability:**
*   **Backward Compatibility:** Enhanced DataExtractor without breaking existing interfaces or consumer code
*   **Dual Implementation Strategy:** Maintained both iterative and vectorized implementations for comparison and fallback
*   **Comprehensive Testing:** 18 specialized tests covering all correction factors, NIST validation, and performance characterization
*   **Code Quality Compliance:** Black formatting and ruff linting compliance with no remaining warnings
*   **Documentation Updates:** Synchronized checklists and implementation status across project documentation

### **Test Suite Quality and Reliability (Previous):**
*   **Significant Failure Reduction:** Achieved 64% reduction in test failures (22 → 8), with 186 tests now passing reliably
*   **Enhanced Test Authenticity:** Replaced complex mocking with real DIALS flex arrays for more accurate integration testing
*   **Improved Error Handling:** Strengthened edge case handling with proper bounds checking and divide-by-zero protection
*   **API Compatibility Assurance:** Fixed DIALS import issues and method call compatibility for long-term stability
*   **Reduced Technical Debt:** Eliminated excessive mocking in favor of real component integration testing

### **Visual Diagnostics Infrastructure:**
*   **Comprehensive Verification Suite:** 8 diagnostic plot types covering all aspects of diffuse extraction verification
*   **End-to-End Pipeline Automation:** Complete CBF-to-diagnostics pipeline with automatic intermediate file generation
*   **Enhanced DataExtractor Capabilities:** Pixel coordinate tracking for spatial analysis and detector visualization
*   **Production-Ready Error Handling:** Robust error handling with detailed logging and graceful failure modes

### **Development Workflow Enhancement:**
*   **Developer Productivity Tools:** Automated pipeline for testing Phase 2 implementations with real data
*   **Documentation Excellence:** Comprehensive user guides with troubleshooting and performance optimization
*   **Configuration Management:** Flexible JSON-based parameter overrides for all processing stages
*   **Integration Testing Foundation:** Framework for validating complete diffuse scattering processing pipeline

### **Technical Implementation Quality:**
*   **Backward Compatibility:** Enhanced DataExtractor without breaking existing interfaces or consumers
*   **Performance Considerations:** Memory-efficient coordinate tracking and vectorized processing support
*   **Multi-Panel Framework:** Architecture ready for multi-panel detector support
*   **Maintainable Code Structure:** Clear separation of concerns with modular, testable components

---

## Architecture & Design Context

### **Project Directory Structure:**
```
project/
├── archive/                               # Superseded documentation
│   ├── validationfix.md.ARCHIVED
│   └── refactorfix.md.ARCHIVED
├── dev_scripts/                          # Development utilities
│   ├── debug_manual_vs_stills.py
│   ├── check_reflection_columns.py
│   ├── compare_coordinates.py
│   └── debug_q_vector_suite/             # Themed collections
│       ├── debug_q_fix.py
│       ├── debug_q_validation.py
│       └── test_q_fix.py
├── tests/integration/                    # Formal integration tests
│   └── test_sequence_adapter_integration.py
└── .claude/commands/                     # Reusable commands
    └── cleanup_codebase.md              # Cleanup methodology
```

### **Cleanup Command Structure:**
```
cleanup_codebase.md
├── Phase 1: Setup and Planning            # Todo list and file identification
├── Phase 2: Archive Superseded Files     # Historical document preservation
├── Phase 3: Organize Development Scripts # Utility organization and categorization
├── Phase 4: Remove Dead Code             # Code quality maintenance
└── Phase 5: Quality and Verification     # Testing and validation
```

### **Maintenance Coverage:**
*   Systematic cleanup methodology
*   File categorization patterns
*   Version control integration
*   Quality preservation verification
*   Reusable command documentation

---

## Resolved Critical Issues

### **Test Suite Maintenance and Reliability Enhancement:**
*   **Latest (2025-06-12):** Successfully resolved all 7 DIALSGenerateMaskAdapter test failures with 0 regressions introduced  
*   **Previous:** Resolved all 11 critical test failures across multiple components with systematic mock improvements
*   **Earlier:** Reduced failures from 22 to 8 (64% improvement), establishing stable foundation
*   **Current Status:** 196 passed, 2 skipped tests with highly stable test suite 
*   **Impact:** Comprehensive test coverage for DIALS C++ object integration with proper mocking patterns

### **Test Strategy and Mocking Problems:**
*   **Before:** Excessive mocking leading to tests that passed but didn't validate real functionality
*   **Enhanced (2025-06-12):** Systematic Mock → MagicMock migration with real DIALS component integration where feasible, plus C++ object compatibility fixes
*   **Current:** Strategic use of real flex arrays, proper magic method mocking, and C++ compatible mock classes for authentic DIALS integration testing
*   **Latest:** Created reusable mock class patterns for C++ backed DIALS objects (ExperimentList, etc.) with proper isinstance() compatibility
*   **Impact:** Tests now validate actual implementation behavior, handle C++ type conversion requirements, and provide reliable integration validation

### **DIALS API Compatibility Issues:**
*   **Before:** Multiple test failures due to incorrect method calls and import patterns
*   **After:** Corrected API usage with proper error handling and fallback mechanisms
*   **Impact:** Future-proof integration with DIALS library updates and version changes

### **Visual Diagnostics Gap:**
*   **Before:** No automated visual verification tools for Phase 2 diffuse extraction pipeline
*   **After:** Comprehensive visual diagnostic suite with 8 plot types and end-to-end orchestration
*   **Impact:** Developers can now systematically verify Phase 2 implementation correctness

### **DataExtractor Visualization Limitations:**
*   **Before:** NPZ output lacked pixel coordinates needed for spatial analysis and detector visualization
*   **After:** Enhanced DataExtractor with optional pixel coordinate tracking for visual diagnostics
*   **Impact:** Enables pixel overlay plots, intensity heatmaps, and spatial verification

### **Development Workflow Integration:**
*   **Before:** Manual, error-prone process to test complete pipeline from CBF to verification
*   **After:** Automated end-to-end script orchestrating DIALS processing, masking, extraction, and diagnostics
*   **Impact:** Streamlined development workflow with comprehensive intermediate file preservation

### **Previous: DIALS API Compatibility Failures (Remediated):**
*   **Issue:** 32 failing tests due to DIALS API breaking changes (`master_phil_scope` → `phil_scope`)
*   **Resolution:** Fixed PHIL imports, mock patching strategies, and integration test patterns
*   **Outcome:** Improved test pass rate from 132 to 171 tests, stable foundation for development

---

## Key References & Documentation

*   `src/diffusepipe/adapters/dials_sequence_process_adapter_IDL.md` - Fixed adapter IDL with method naming and parameter consistency (latest)
*   `src/diffusepipe/adapters/dials_stills_process_adapter_IDL.md` - Updated adapter IDL with missing parameter alignment (latest)
*   `src/diffusepipe/corrections_IDL.md` - Created IDL for centralized correction factor logic per Module 2.S.2 (latest)
*   `src/diffusepipe/utils/cbf_utils_IDL.md` - Created IDL for CBF header parsing required by Module 1.S.0 (latest)
*   `docs/TEMPLATES/WORKING_MEMORY_LOG.md` - Updated memory log with IDL synchronization work details (latest)
*   `tests/adapters/test_dials_generate_mask_adapter.py` - DIALSGenerateMaskAdapter test suite with C++ compatibility fixes (previous)
*   `src/diffusepipe/adapters/dials_generate_mask_adapter.py` - Target implementation file with custom _call_generate_mask method (previous)
*   `src/diffusepipe/extraction/data_extractor.py` - Core DataExtractor with NIST-based air attenuation and vectorization (lines 775-922, 1176-1441)
*   `tests/extraction/test_data_extractor_phase2.py` - Phase 2 test suite with NIST validation and performance testing
*   `checklists/phase2.md` - Updated implementation checklist with completed air attenuation and vectorization status
*   `checklists/phase0.md` - Directory structure status reflecting completed project organization
*   `src/diffusepipe/types/types_IDL.py` - Enhanced configuration parameters for air temperature and pressure
*   `scripts/visual_diagnostics/check_diffuse_extraction.py` - Standalone visual diagnostics with 8 plot types (previous implementation)
*   `scripts/dev_workflows/run_phase2_e2e_visual_check.py` - Complete end-to-end pipeline orchestration (previous implementation)
*   `docs/VISUAL_DIAGNOSTICS_GUIDE.md` - Comprehensive documentation for visual diagnostic tools (previous implementation)

---
</file>

<file path="scripts/visual_diagnostics/plot_utils.py">
"""
Plotting utilities for visual diagnostics.

This module provides reusable plotting functions for visualizing detector images,
masks, and other crystallographic data with matplotlib.
"""

import logging
import numpy as np
from pathlib import Path
from typing import List, Tuple, Optional, Union, Any

# Set matplotlib to non-interactive backend for automation
import matplotlib

matplotlib.use("Agg")
import matplotlib.pyplot as plt
from matplotlib.colors import LogNorm

logger = logging.getLogger(__name__)


def plot_detector_image(
    image_data: Union[Any, np.ndarray],
    title: str = "Detector Image",
    output_path: Optional[str] = None,
    figsize: Tuple[float, float] = (10, 8),
    log_scale: bool = False,
    vmin: Optional[float] = None,
    vmax: Optional[float] = None,
    cmap: str = "viridis",
) -> plt.Figure:
    """
    Plot a single detector panel image.

    Args:
        image_data: Image data as numpy array or DIALS flex array
        title: Title for the plot
        output_path: Path to save the plot (optional)
        figsize: Figure size (width, height) in inches
        log_scale: Whether to use logarithmic color scale
        vmin: Minimum value for color scale
        vmax: Maximum value for color scale
        cmap: Colormap name

    Returns:
        matplotlib Figure object
    """
    # Convert DIALS flex arrays to numpy if needed
    if hasattr(image_data, "as_numpy_array"):
        img_array = image_data.as_numpy_array()
    else:
        img_array = np.array(image_data)

    # Ensure 2D array
    if len(img_array.shape) == 1:
        # Try to infer shape from flex array
        if hasattr(image_data, "accessor"):
            accessor = image_data.accessor()
            height, width = accessor.all()
            img_array = img_array.reshape(height, width)
        else:
            raise ValueError("Cannot determine image dimensions")

    fig, ax = plt.subplots(figsize=figsize)

    # Set up color scaling
    norm = None
    if log_scale:
        # Avoid log(0) by setting minimum to small positive value
        img_array = np.where(img_array <= 0, 1e-6, img_array)
        norm = LogNorm(vmin=vmin or img_array.min(), vmax=vmax or img_array.max())

    # Create the image plot
    im = ax.imshow(
        img_array,
        cmap=cmap,
        norm=norm,
        vmin=vmin,
        vmax=vmax,
        origin="lower",  # Use lower origin to match DIALS convention
        aspect="equal",
    )

    # Add colorbar
    plt.colorbar(im, ax=ax, label="Intensity" if not log_scale else "Intensity (log)")

    # Set title and labels
    ax.set_title(title)
    ax.set_xlabel("Fast axis (pixels)")
    ax.set_ylabel("Slow axis (pixels)")

    # Save if output path provided
    if output_path:
        Path(output_path).parent.mkdir(parents=True, exist_ok=True)
        fig.savefig(output_path, dpi=150, bbox_inches="tight")
        logger.info(f"Saved plot to {output_path}")

    return fig


def plot_mask_overlay(
    image_data: Union[Any, np.ndarray],
    mask_data: Union[Any, np.ndarray],
    title: str = "Image with Mask Overlay",
    output_path: Optional[str] = None,
    figsize: Tuple[float, float] = (10, 8),
    mask_color: str = "red",
    mask_alpha: float = 0.3,
    log_scale: bool = False,
) -> plt.Figure:
    """
    Plot detector image with mask overlay.

    Args:
        image_data: Background image data
        mask_data: Boolean mask data (True = masked regions)
        title: Title for the plot
        output_path: Path to save the plot (optional)
        figsize: Figure size (width, height) in inches
        mask_color: Color for mask overlay
        mask_alpha: Transparency of mask overlay (0-1)
        log_scale: Whether to use logarithmic color scale for image

    Returns:
        matplotlib Figure object
    """
    # Convert to numpy arrays
    if hasattr(image_data, "as_numpy_array"):
        img_array = image_data.as_numpy_array()
    else:
        img_array = np.array(image_data)

    if hasattr(mask_data, "as_numpy_array"):
        mask_array = mask_data.as_numpy_array()
    else:
        mask_array = np.array(mask_data)

    # Handle 1D arrays
    if len(img_array.shape) == 1 and hasattr(image_data, "accessor"):
        accessor = image_data.accessor()
        height, width = accessor.all()
        img_array = img_array.reshape(height, width)
        mask_array = mask_array.reshape(height, width)

    fig, ax = plt.subplots(figsize=figsize)

    # Plot background image
    norm = None
    if log_scale:
        img_array = np.where(img_array <= 0, 1e-6, img_array)
        norm = LogNorm(vmin=img_array.min(), vmax=img_array.max())

    im = ax.imshow(img_array, cmap="gray", norm=norm, origin="lower", aspect="equal")

    # Overlay mask
    masked_regions = np.ma.masked_where(~mask_array, mask_array)
    ax.imshow(
        masked_regions,
        cmap=plt.cm.get_cmap(mask_color).with_extremes(under="none"),
        alpha=mask_alpha,
        origin="lower",
        aspect="equal",
    )

    # Add colorbar for image
    plt.colorbar(im, ax=ax, label="Intensity")

    # Set title and labels
    ax.set_title(title)
    ax.set_xlabel("Fast axis (pixels)")
    ax.set_ylabel("Slow axis (pixels)")

    # Save if output path provided
    if output_path:
        Path(output_path).parent.mkdir(parents=True, exist_ok=True)
        fig.savefig(output_path, dpi=150, bbox_inches="tight")
        logger.info(f"Saved plot to {output_path}")

    return fig


def plot_spot_overlay(
    image_data: Union[Any, np.ndarray],
    spot_positions: List[Tuple[float, float]],
    title: str = "Image with Spot Overlay",
    output_path: Optional[str] = None,
    figsize: Tuple[float, float] = (10, 8),
    spot_color: str = "red",
    spot_size: float = 20,
    log_scale: bool = False,
    predicted_positions: Optional[List[Tuple[float, float]]] = None,
    predicted_color: str = "blue",
    max_points: Optional[int] = None,
) -> plt.Figure:
    """
    Plot detector image with spot position overlays.

    Args:
        image_data: Background image data
        spot_positions: List of (x, y) pixel coordinates for observed spots
        title: Title for the plot
        output_path: Path to save the plot (optional)
        figsize: Figure size (width, height) in inches
        spot_color: Color for observed spot markers
        spot_size: Size of spot markers
        log_scale: Whether to use logarithmic color scale
        predicted_positions: Optional list of predicted spot positions
        predicted_color: Color for predicted spot markers
        max_points: Maximum number of points to plot for performance

    Returns:
        matplotlib Figure object
    """
    # Convert to numpy array
    if hasattr(image_data, "as_numpy_array"):
        img_array = image_data.as_numpy_array()
    else:
        img_array = np.array(image_data)

    # Handle 1D arrays
    if len(img_array.shape) == 1 and hasattr(image_data, "accessor"):
        accessor = image_data.accessor()
        height, width = accessor.all()
        img_array = img_array.reshape(height, width)

    fig, ax = plt.subplots(figsize=figsize)

    # Plot background image
    norm = None
    if log_scale:
        img_array = np.where(img_array <= 0, 1e-6, img_array)
        norm = LogNorm(vmin=img_array.min(), vmax=img_array.max())

    im = ax.imshow(img_array, cmap="gray", norm=norm, origin="lower", aspect="equal")

    # Apply subsampling to spot positions if necessary
    sampled_spots = spot_positions
    sampled_predicted = predicted_positions
    sampled_note = ""

    if max_points and spot_positions and len(spot_positions) > max_points:
        indices = np.random.choice(len(spot_positions), max_points, replace=False)
        sampled_spots = [spot_positions[i] for i in indices]
        sampled_note = f" (sampled {max_points} of {len(spot_positions)} spots)"

    if max_points and predicted_positions and len(predicted_positions) > max_points:
        pred_indices = np.random.choice(
            len(predicted_positions), max_points, replace=False
        )
        sampled_predicted = [predicted_positions[i] for i in pred_indices]
        if not sampled_note:  # Only add note if not already added for observed spots
            sampled_note = (
                f" (sampled {max_points} of {len(predicted_positions)} predicted spots)"
            )

    # Plot observed spots
    if sampled_spots:
        x_coords, y_coords = zip(*sampled_spots)
        ax.scatter(
            x_coords,
            y_coords,
            c=spot_color,
            s=spot_size,
            marker="o",
            alpha=0.7,
            label="Observed spots",
            edgecolors="white",
            linewidth=0.5,
        )

    # Plot predicted spots if provided
    if sampled_predicted:
        pred_x, pred_y = zip(*sampled_predicted)
        ax.scatter(
            pred_x,
            pred_y,
            c=predicted_color,
            s=spot_size,
            marker="x",
            alpha=0.7,
            label="Predicted spots",
            linewidth=1.5,
        )

    # Add colorbar for image
    plt.colorbar(im, ax=ax, label="Intensity")

    # Add legend if we have spots
    if spot_positions or predicted_positions:
        ax.legend()

    # Set title and labels
    ax.set_title(title + sampled_note)
    ax.set_xlabel("Fast axis (pixels)")
    ax.set_ylabel("Slow axis (pixels)")

    # Save if output path provided
    if output_path:
        Path(output_path).parent.mkdir(parents=True, exist_ok=True)
        fig.savefig(output_path, dpi=150, bbox_inches="tight")
        logger.info(f"Saved plot to {output_path}")

    return fig


def plot_multi_panel_comparison(
    images: List[Union[Any, np.ndarray]],
    titles: List[str],
    output_path: Optional[str] = None,
    figsize: Tuple[float, float] = (15, 5),
    log_scale: bool = False,
    cmap: str = "viridis",
) -> plt.Figure:
    """
    Plot multiple images side by side for comparison.

    Args:
        images: List of image arrays to plot
        titles: List of titles for each subplot
        output_path: Path to save the plot (optional)
        figsize: Figure size (width, height) in inches
        log_scale: Whether to use logarithmic color scale
        cmap: Colormap name

    Returns:
        matplotlib Figure object
    """
    n_images = len(images)
    if n_images != len(titles):
        raise ValueError("Number of images must match number of titles")

    fig, axes = plt.subplots(1, n_images, figsize=figsize)
    if n_images == 1:
        axes = [axes]

    for i, (image_data, title) in enumerate(zip(images, titles)):
        # Convert to numpy array
        if hasattr(image_data, "as_numpy_array"):
            img_array = image_data.as_numpy_array()
        else:
            img_array = np.array(image_data)

        # Handle 1D arrays
        if len(img_array.shape) == 1 and hasattr(image_data, "accessor"):
            accessor = image_data.accessor()
            height, width = accessor.all()
            img_array = img_array.reshape(height, width)

        # Set up color scaling
        norm = None
        if log_scale:
            img_array = np.where(img_array <= 0, 1e-6, img_array)
            norm = LogNorm(vmin=img_array.min(), vmax=img_array.max())

        # Plot image
        im = axes[i].imshow(
            img_array, cmap=cmap, norm=norm, origin="lower", aspect="equal"
        )

        # Add colorbar
        plt.colorbar(im, ax=axes[i])

        # Set title and labels
        axes[i].set_title(title)
        axes[i].set_xlabel("Fast axis (pixels)")
        axes[i].set_ylabel("Slow axis (pixels)")

    plt.tight_layout()

    # Save if output path provided
    if output_path:
        Path(output_path).parent.mkdir(parents=True, exist_ok=True)
        fig.savefig(output_path, dpi=150, bbox_inches="tight")
        logger.info(f"Saved plot to {output_path}")

    return fig


def ensure_output_dir(output_dir: str) -> Path:
    """
    Ensure output directory exists and return Path object.

    Args:
        output_dir: Output directory path

    Returns:
        Path object for the output directory
    """
    output_path = Path(output_dir)
    output_path.mkdir(parents=True, exist_ok=True)
    return output_path


def close_all_figures():
    """Close all matplotlib figures to free memory."""
    plt.close("all")


def plot_3d_grid_slice(
    grid_data_3d: np.ndarray,
    slice_dim_idx: int,
    slice_val_idx: int,
    title: str,
    output_path: str,
    cmap: str = "viridis",
    norm: Optional[Any] = None,
    xlabel: str = "H",
    ylabel: str = "K",
    aspect: str = "auto",
) -> plt.Figure:
    """
    Plot a 2D slice through a 3D grid.

    Args:
        grid_data_3d: 3D numpy array with shape (H, K, L)
        slice_dim_idx: Dimension to slice (0=H, 1=K, 2=L)
        slice_val_idx: Index along slice dimension
        title: Plot title
        output_path: Path to save the plot
        cmap: Colormap name
        norm: Color normalization (e.g., LogNorm)
        xlabel: X-axis label
        ylabel: Y-axis label
        aspect: Aspect ratio ('auto', 'equal')

    Returns:
        matplotlib Figure object
    """
    # Extract the 2D slice
    if slice_dim_idx == 0:  # H slice
        slice_data = grid_data_3d[slice_val_idx, :, :]
        xlabel = "K"
        ylabel = "L"
    elif slice_dim_idx == 1:  # K slice
        slice_data = grid_data_3d[:, slice_val_idx, :]
        xlabel = "H"
        ylabel = "L"
    elif slice_dim_idx == 2:  # L slice
        slice_data = grid_data_3d[:, :, slice_val_idx]
        xlabel = "H"
        ylabel = "K"
    else:
        raise ValueError("slice_dim_idx must be 0, 1, or 2")

    fig, ax = plt.subplots(figsize=(8, 6))

    # Create the image plot
    im = ax.imshow(
        slice_data,
        cmap=cmap,
        norm=norm,
        origin="lower",
        aspect=aspect,
        interpolation="nearest",
    )

    # Add colorbar
    plt.colorbar(im, ax=ax)

    # Set title and labels
    ax.set_title(title)
    ax.set_xlabel(xlabel)
    ax.set_ylabel(ylabel)

    # Save plot
    Path(output_path).parent.mkdir(parents=True, exist_ok=True)
    fig.savefig(output_path, dpi=150, bbox_inches="tight")
    logger.info(f"Saved 3D grid slice plot to {output_path}")

    return fig


def plot_radial_average(
    q_magnitudes: np.ndarray,
    intensities: np.ndarray,
    num_bins: int,
    title: str,
    output_path: str,
    sigmas: Optional[np.ndarray] = None,
) -> plt.Figure:
    """
    Plot radial average of intensities vs q-magnitude.

    Args:
        q_magnitudes: Array of q-magnitude values
        intensities: Array of intensity values
        num_bins: Number of bins for radial averaging
        title: Plot title
        output_path: Path to save the plot
        sigmas: Optional array of sigma values for error bars

    Returns:
        matplotlib Figure object
    """
    # Remove invalid data
    valid_mask = np.isfinite(q_magnitudes) & np.isfinite(intensities)
    q_valid = q_magnitudes[valid_mask]
    I_valid = intensities[valid_mask]
    
    if sigmas is not None:
        sigmas_valid = sigmas[valid_mask]
    else:
        sigmas_valid = None

    if len(q_valid) == 0:
        logger.warning("No valid data for radial average plot")
        fig, ax = plt.subplots(figsize=(10, 6))
        ax.text(0.5, 0.5, "No valid data", ha="center", va="center")
        ax.set_title(title)
        fig.savefig(output_path, dpi=150, bbox_inches="tight")
        return fig

    # Create bins
    q_min, q_max = np.min(q_valid), np.max(q_valid)
    if q_min == q_max:
        q_max = q_min + 1e-6  # Avoid zero width
    
    bin_edges = np.linspace(q_min, q_max, num_bins + 1)
    bin_centers = (bin_edges[:-1] + bin_edges[1:]) / 2

    # Bin the data
    bin_indices = np.digitize(q_valid, bin_edges) - 1
    bin_indices = np.clip(bin_indices, 0, num_bins - 1)

    mean_intensities = []
    std_intensities = []
    
    for i in range(num_bins):
        mask = bin_indices == i
        if np.any(mask):
            I_bin = I_valid[mask]
            mean_intensities.append(np.mean(I_bin))
            
            if sigmas_valid is not None:
                # Propagate uncertainties: std error of mean
                weights = 1.0 / (sigmas_valid[mask] ** 2)
                weighted_mean = np.average(I_bin, weights=weights)
                weighted_std = np.sqrt(1.0 / np.sum(weights))
                std_intensities.append(weighted_std)
            else:
                std_intensities.append(np.std(I_bin) / np.sqrt(len(I_bin)))
        else:
            mean_intensities.append(np.nan)
            std_intensities.append(np.nan)

    mean_intensities = np.array(mean_intensities)
    std_intensities = np.array(std_intensities)

    fig, ax = plt.subplots(figsize=(10, 6))

    # Plot with error bars if available
    if sigmas is not None:
        ax.errorbar(
            bin_centers,
            mean_intensities,
            yerr=std_intensities,
            fmt="o-",
            markersize=4,
            linewidth=1,
            capsize=3,
        )
    else:
        ax.plot(bin_centers, mean_intensities, "o-", markersize=4, linewidth=1)

    ax.set_xlabel("Q (Å⁻¹)")
    ax.set_ylabel("Mean Intensity")
    ax.set_title(title)
    ax.grid(True, alpha=0.3)

    # Save plot
    Path(output_path).parent.mkdir(parents=True, exist_ok=True)
    fig.savefig(output_path, dpi=150, bbox_inches="tight")
    logger.info(f"Saved radial average plot to {output_path}")

    return fig


def plot_parameter_vs_index(
    param_values: np.ndarray,
    index_values: np.ndarray,
    title: str,
    param_label: str,
    index_label: str,
    output_path: str,
) -> plt.Figure:
    """
    Plot parameter values vs index (e.g., scaling factors vs still index).

    Args:
        param_values: Array of parameter values
        index_values: Array of index values
        title: Plot title
        param_label: Y-axis label for parameter
        index_label: X-axis label for index
        output_path: Path to save the plot

    Returns:
        matplotlib Figure object
    """
    fig, ax = plt.subplots(figsize=(10, 6))

    ax.plot(index_values, param_values, "o-", markersize=4, linewidth=1)

    ax.set_xlabel(index_label)
    ax.set_ylabel(param_label)
    ax.set_title(title)
    ax.grid(True, alpha=0.3)

    # Add statistics
    mean_val = np.mean(param_values)
    std_val = np.std(param_values)
    ax.axhline(mean_val, color="red", linestyle="--", alpha=0.7, label=f"Mean: {mean_val:.3f}")
    ax.axhline(mean_val + std_val, color="orange", linestyle=":", alpha=0.7, label=f"±1σ: {std_val:.3f}")
    ax.axhline(mean_val - std_val, color="orange", linestyle=":", alpha=0.7)
    ax.legend()

    # Save plot
    Path(output_path).parent.mkdir(parents=True, exist_ok=True)
    fig.savefig(output_path, dpi=150, bbox_inches="tight")
    logger.info(f"Saved parameter vs index plot to {output_path}")

    return fig


def plot_smoother_curve(
    smoother_eval_func: callable,
    x_range: Tuple[float, float],
    num_points: int,
    title: str,
    output_path: str,
    control_points_x: Optional[np.ndarray] = None,
    control_points_y: Optional[np.ndarray] = None,
) -> plt.Figure:
    """
    Plot a smoother curve with optional control points.

    Args:
        smoother_eval_func: Function that evaluates the smoother at given x values
        x_range: Tuple of (x_min, x_max) for plotting range
        num_points: Number of points to evaluate the curve
        title: Plot title
        output_path: Path to save the plot
        control_points_x: Optional x-coordinates of control points
        control_points_y: Optional y-coordinates of control points

    Returns:
        matplotlib Figure object
    """
    # Generate evaluation points
    x_eval = np.linspace(x_range[0], x_range[1], num_points)
    
    try:
        y_eval = smoother_eval_func(x_eval)
    except Exception as e:
        logger.error(f"Failed to evaluate smoother function: {e}")
        # Create a fallback plot
        fig, ax = plt.subplots(figsize=(10, 6))
        ax.text(0.5, 0.5, f"Failed to evaluate smoother: {e}", ha="center", va="center")
        ax.set_title(title)
        fig.savefig(output_path, dpi=150, bbox_inches="tight")
        return fig

    fig, ax = plt.subplots(figsize=(10, 6))

    # Plot the smooth curve
    ax.plot(x_eval, y_eval, "-", linewidth=2, label="Smoother curve")

    # Plot control points if provided
    if control_points_x is not None and control_points_y is not None:
        ax.scatter(
            control_points_x,
            control_points_y,
            c="red",
            s=50,
            marker="o",
            label="Control points",
            zorder=5,
        )

    ax.set_xlabel("Q (Å⁻¹)")
    ax.set_ylabel("Correction Factor")
    ax.set_title(title)
    ax.grid(True, alpha=0.3)
    ax.legend()

    # Save plot
    Path(output_path).parent.mkdir(parents=True, exist_ok=True)
    fig.savefig(output_path, dpi=150, bbox_inches="tight")
    logger.info(f"Saved smoother curve plot to {output_path}")

    return fig


def setup_logging_for_plots():
    """Set up logging configuration for plotting scripts."""
    logging.basicConfig(
        level=logging.INFO,
        format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
    )
</file>

<file path="src/diffusepipe/crystallography/__init__.py">
"""Crystallography components for diffuse scattering pipeline."""

from .still_processing_and_validation import (
    StillProcessorAndValidatorComponent,
    ModelValidator,
    ValidationMetrics,
    create_default_config,
    create_default_extraction_config,
)

__all__ = [
    "StillProcessorAndValidatorComponent",
    "ModelValidator",
    "ValidationMetrics",
    "create_default_config",
    "create_default_extraction_config",
]
</file>

<file path="src/diffusepipe/orchestration/stills_pipeline_orchestrator_IDL.md">
// == BEGIN IDL ==
module src.diffusepipe.orchestration {

    # @depends_on_resource(type="ExternalTool:DIALS", purpose="Performing stills crystallographic processing via DIALS Python API (e.g., dials.stills_process)")
    # @depends_on_resource(type="FileSystem", purpose="Managing working directories, creating subdirectories, reading/writing logs and intermediate files")
    # @depends_on([src.diffusepipe.extraction.DataExtractor], [src.diffusepipe.diagnostics.ConsistencyChecker], [src.diffusepipe.diagnostics.QValueCalculator])
    # @depends_on_type(src.diffusepipe.types.StillsPipelineConfig)
    # @depends_on_type(src.diffusepipe.types.StillProcessingOutcome)
    # @depends_on_type(src.diffusepipe.types.ComponentInputFiles) // Used internally to prepare inputs for other components
    # @depends_on_type(src.diffusepipe.types.OperationOutcome)    // Used internally to interpret results from other components
    interface StillsPipelineOrchestrator {
        // Preconditions:
        // - Each path in `cbf_image_paths` must point to an existing, readable CBF file.
        // - `config` must be a valid `StillsPipelineConfig` object, including valid PHIL configuration for `dials.stills_process` within `config.dials_stills_process_config`.
        // - `root_output_directory` must be a path to a writable directory (it will be created if it doesn't exist).
        // - The DIALS Python environment must be correctly configured and accessible.
        // Postconditions:
        // - A subdirectory is created within `root_output_directory` for each successfully initiated CBF processing attempt.
        // - DIALS intermediate files (e.g., `integrated.expt`, `integrated.refl`, `shoeboxes.refl` if configured), extracted diffuse data (NPZ), and diagnostic outputs are stored within these respective subdirectories.
        // - A summary log file (e.g., `pipeline_summary.log`) is created in `root_output_directory` detailing the outcome for each image.
        // - Returns a list of `StillProcessingOutcome` objects, one for each input CBF file.
        // Behavior:
        // - Creates `root_output_directory` if it doesn't exist.
        // - Initializes a summary log file within `root_output_directory`.
        // - For each `cbf_path` in `cbf_image_paths`:
        //   1. Creates a unique working subdirectory inside `root_output_directory` (e.g., named using the CBF filename).
        //   2. Initializes a `StillProcessingOutcome` for the current image.
        //   3. **Data Type Detection (Module 1.S.0):**
        //      a. Determines the `processing_route` ("stills" or "sequence") based on the CBF header of `cbf_path` and `config.dials_stills_process_config.force_processing_mode`.
        //      b. Selects the appropriate DIALS adapter (`DIALSStillsProcessAdapter` or `DIALSSequenceProcessAdapter`) based on the `processing_route`.
        //   4. **DIALS Processing Stage (using the selected adapter):**
        //      a. Initializes the selected DIALS adapter with the PHIL parameters derived from `config.dials_stills_process_config`.
        //      b. The adapter imports the `cbf_path` into a DIALS `ExperimentList` (or handles equivalent for CLI).
        //      c. The adapter invokes its main processing method. This internally handles spot finding, indexing, refinement, and integration.
        //      d. The adapter logs relevant command-equivalent information, captures DIALS logs, and monitors for successful completion.
        //      e. If the DIALS adapter reports failure, sets `StillProcessingOutcome.status` to "FAILURE_DIALS", records details in `dials_outcome`, logs to summary, and proceeds to the next CBF file.
        //      f. If successful, the adapter retrieves the `integrated.expt` (containing `Crystal_i`) and `integrated.refl` (containing Bragg spots, partialities, and optionally shoeboxes) as DIALS Python objects.
        //   5. **Data Extraction Stage:**
        //      a. If all DIALS steps succeeded:
        //         i. Prepares `ComponentInputFiles` (paths to `cbf_path`, `indexed_refined_detector.expt`, `indexed_refined_detector.refl`, `bragg_mask.pickle`, and `config.extraction_config.external_pdb_path` if specified in `config`).
        //         ii. Defines `output_npz_path` within the working directory.
        //         iii. Calls `DataExtractor.extract_from_still(inputs, config.extraction_config, output_npz_path)`.
        //         iv. Updates `StillProcessingOutcome.extraction_outcome` and `StillProcessingOutcome.status`.
        //   6. **Diagnostics Stage (Conditional):**
        //      a. If extraction was successful (`extraction_outcome.status == "SUCCESS"`) and `config.run_consistency_checker` is true:
        //         i. Calls `ConsistencyChecker.check_q_consistency(inputs_for_consistency, config.extraction_config.verbose, working_directory)`.
        //         ii. Updates `StillProcessingOutcome.consistency_outcome`.
        //      b. If extraction was successful and `config.run_q_calculator` is true:
        //         i. Calls `QValueCalculator.calculate_q_map(inputs_for_qcalc, output_prefix_in_working_dir)`.
        //         ii. Updates `StillProcessingOutcome.q_calc_outcome`.
        //   7. Appends the final `StillProcessingOutcome` for the current image to the list and writes a summary to the main log.
        // - Returns the list of all `StillProcessingOutcome` objects.
        // @raises_error(condition="InvalidConfiguration", description="The provided `StillsPipelineConfig` is invalid or essential global settings are missing.")
        // @raises_error(condition="DIALSEnvironmentError", description="DIALS command-line tools are not found or the DIALS environment is not correctly sourced.")
        // @raises_error(condition="FileSystemError", description="Cannot create the `root_output_directory` or its subdirectories, or cannot write the summary log file.")
        list<src.diffusepipe.types.StillProcessingOutcome> process_stills_batch(
            list<string> cbf_image_paths,
            src.diffusepipe.types.StillsPipelineConfig config,
            string root_output_directory
        );
    }
}
// == END IDL ==
</file>

<file path="CLAUDE.md">
# CLAUDE.md

This file provides guidance to Claude Code (claude.ai/code) when working with code in this repository.

## Project Structure
- DiffusePipe is a Python project for processing crystallography data
- Core package is `src/diffusepipe/`
- Configuration files (PHIL files) are stored in `src/diffusepipe/config/`
- Uses DIALS crystallography software through CLI tools

## Important File Locations
- Main processing script: `/src/scripts/process_pipeline.sh`
- PHIL configuration files:
  - `/src/diffusepipe/config/find_spots.phil`
  - `/src/diffusepipe/config/refine_detector.phil`
- IDL specifications are in files named `*_IDL.md` next to the implementation files

## External Dependencies
- DIALS crystallography software (primarily accessed via its Python API, e.g., `dials.stills_process`)
- Python 3.10+
- PDB file for consistency checks (6o2h.pdb in the example)

## Documentation
- Main DIALS/CCTBX/DXTBX documentation: `libdocs/dials/DIALS_Python_API_Reference.md`
- For features not covered in documentation, explore source code under `./bio/` as last resort

## Testing Approach
- Use pytest for testing
- Strongly emphasize integration tests over unit tests
- Avoid mocks whenever possible - use real components instead
- Only mock external APIs with usage limits or services requiring complex infrastructure

## Code Style and Documentation
- Format using Black
- Lint with Ruff
- Google Style docstrings
- Follow PEP 8 naming conventions
- IDL files serve as the specification/contract for each component

# Input data Format 
The diffraction images are stored in .cbf format under ./747/. Example header:
<cbf header>
###CBF: VERSION 1.5, CBFlib v0.7.8 - PILATUS detectors

data_lys_nitr_10_1_0048

_array_data.header_convention "PILATUS_1.2"
_array_data.header_contents
;
# Detector: PILATUS3 6M, S/N 60-0127
# 2017-06-26T02:54:23.347
# Pixel_size 172e-6 m x 172e-6 m
# Silicon sensor, thickness 0.001000 m
# Exposure_time 0.0990000 s
# Exposure_period 0.1000000 s
# Tau = 0 s
# Count_cutoff 1009797 counts
# Threshold_setting: 6344 eV
# Gain_setting: autog (vrf = 1.000)
# N_excluded_pixels = 852
# Excluded_pixels: badpixel_mask.tif
# Flat_field: FF_p60-0127_E12688_T6344_vrf_m0p100.tif
# Trim_file: p60-0127_E12688_T6344.bin
# Image_path: /F1a/ando/20170624/lysozyme/nitrate/ubatch4/
# Ratecorr_lut_directory: ContinuousStandard_v1.1
# Retrigger_mode: 1
# Wavelength 0.97680 A
# Detector_distance 0.22982 m
# Beam_xy (1264.48, 1242.52) pixels
# Start_angle 254.7000 deg.
# Angle_increment 0.1000 deg.
# Phi 250.0000 deg.
# Oscillation_axis PHI
# N_oscillations 1
# Shutter_time 0.1000000 s
;

_array_data.data
;
--CIF-BINARY-FORMAT-SECTION--
Content-Type: application/octet-stream;
     conversions="x-CBF_BYTE_OFFSET"
Content-Transfer-Encoding: BINARY
X-Binary-Size: 6224641
X-Binary-ID: 1
X-Binary-Element-Type: "signed 32-bit integer"
X-Binary-Element-Byte-Order: LITTLE_ENDIAN
Content-MD5: v+hI0TIRqPDQ7ABGUxprUA==
X-Binary-Number-of-Elements: 6224001
X-Binary-Size-Fastest-Dimension: 2463
X-Binary-Size-Second-Dimension: 2527
X-Binary-Size-Padding: 4095
</cbf header>

## Common Tasks
- When running the process_pipeline.sh script, use absolute paths to PHIL files
- For any changes to the pipeline, consult the relevant IDL files first
- Always check the log files in the processing directories when debugging pipeline issues
- The script creates a summary log at the root directory (`dials_processing_summary.log`)

## Running the Pipeline
```bash
# Basic usage with a single CBF file
bash src/scripts/process_pipeline.sh path/to/image.cbf --external_pdb path/to/reference.pdb

# Processing multiple CBF files
bash src/scripts/process_pipeline.sh path/to/image1.cbf path/to/image2.cbf --external_pdb path/to/reference.pdb

# With verbose Python script output
bash src/scripts/process_pipeline.sh path/to/image.cbf --external_pdb path/to/reference.pdb --verbose

# Disabling diagnostic scripts
bash src/scripts/process_pipeline.sh path/to/image.cbf --external_pdb path/to/reference.pdb --run_diagnostics false
```

## Other bash commands:
# generating a task-aware context file:
repomix --include "**/*.md,**/*.py,**/*.phil" --ignore .aider.chat.history.md
llm -m gemini-2.5-pro-preview-06-05 <  prompt.txt > context_package.json
{ (echo "<prompt>"; cat .claude/commands/context.md; echo "</prompt>"); (echo "<planned task>"; cat task.txt; echo "</planned task>"); (echo "<code context>"; cat repomix-output.xml; echo "</code context>"); } > context_package.json
bash adaptive_substitute.sh context_package.json 10000 expanded_context.txt

## Development Process
1. Review IDL specification for the component to be modified
2. Implement changes following the IDL contract
3. Add integration tests with real components (minimize mocks)
4. Format code (Black) and run linter (Ruff)
5. Verify changes work with the pipeline script

## Debugging the Pipeline

For comprehensive debugging guidance, see `docs/06_DIALS_DEBUGGING_GUIDE.md`.

### Key Debugging Principles

**Test Suite Remediation Patterns:**
- Use `MagicMock` instead of `Mock` for DIALS objects that need magic method support (`__getitem__`, `__and__`, `__or__`)
- Replace complex mocking with real DIALS `flex` arrays for authentic integration testing
- Only mock external APIs with usage limits or complex infrastructure requirements

**Quick DIALS Troubleshooting:**
1. **Check CBF headers** for oscillation data vs stills (`Angle_increment` value)
2. **Compare with working DIALS logs** in existing processing directories
3. **Verify PHIL parameters** match working approach (see critical parameters in debugging guide)
4. **Test DIALS API imports** independently
5. **Use CLI-based adapters** as fallback for API issues

**Primary Validation Method:** Q-vector consistency checking (`|Δq|` typically < 0.01 Å⁻¹)

## Pipeline Steps and Expected Outputs
1. **DIALS Stills Processing (via `dials.stills_process` Python API Adapter)**
   - Input: CBF file, base geometry, PHIL configuration for `dials.stills_process`.
   - Process: The adapter uses `dials.stills_process.Processor` to internally perform import, spot finding, indexing, refinement, and integration (including partiality and optional shoebox generation).
   - Output: 
     - `Experiment_dials_i` (DIALS Python Experiment object containing refined crystal model).
     - `Reflections_dials_i` (DIALS Python reflection_table object with indexed spots, partialities, and optionally shoeboxes).
     - Corresponding DIALS output files (e.g., `integrated.expt`, `integrated.refl`) are saved to the working directory.
   - Log: The `dials.stills_process` tool typically produces its own comprehensive log. Individual step logs (like `dials.find_spots.log`) might also be generated by it if configured.

2. **Bragg Mask Generation (using `dials.generate_mask` or shoebox data)**
   - Input: `Experiment_dials_i` and `Reflections_dials_i` (and shoeboxes if that option is chosen).
   - Output: `BraggMask_2D_raw_i` (Python `flex.bool` mask object), and `bragg_mask.pickle` file.
   - Log: `dials.generate_mask.log` (if `dials.generate_mask` is used).

3. **Python Extraction (DataExtractor component)**
   - Input: Raw image data, `Experiment_dials_i`, `BraggMask_2D_raw_i`, static pixel mask, PDB file (optional).
   - Output: NPZ file with diffuse data (`q_vectors`, `intensities`, `variances`).
   - Log: `extract_diffuse_data.log` (or similar, managed by the Python component).
</file>

<file path="scripts/dev_workflows/run_phase2_e2e_visual_check.py">
#!/usr/bin/env python3
"""
End-to-End Visual Check Script for Phase 2 DiffusePipe Processing

This script orchestrates the complete Phase 1 (DIALS processing, masking) and 
Phase 2 (DataExtractor) pipeline for a single CBF image, then runs visual 
diagnostics to verify the correctness of the diffuse scattering extraction 
and correction processes.

The script provides an automated pathway to:
1. Process a CBF image through DIALS (import, find spots, index, refine)
2. Generate pixel masks (static + dynamic)
3. Generate Bragg masks (from spots or shoeboxes)
4. Combine masks into total diffuse mask
5. Extract diffuse scattering data with pixel corrections
6. Generate comprehensive visual diagnostics

This is particularly useful for:
- Validating Phase 2 implementation on real data
- Debugging extraction pipeline issues
- Generating reference outputs for testing
- Development workflow verification

Author: DiffusePipe
"""

import argparse
import json
import logging
import pickle
import subprocess
import sys
from pathlib import Path
from typing import Dict, Any, Optional

# Add project src to path for imports
sys.path.insert(0, str(Path(__file__).resolve().parent.parent.parent / "src"))

try:
    # DIALS/DXTBX imports
    from dxtbx.imageset import ImageSetFactory
    from dxtbx.model.experiment_list import ExperimentListFactory

    # Project component imports
    from diffusepipe.crystallography.still_processing_and_validation import (
        StillProcessorAndValidatorComponent,
        create_default_config,
        create_default_extraction_config,
    )
    from diffusepipe.types.types_IDL import (
        ExtractionConfig,
        ComponentInputFiles,
    )
    from diffusepipe.masking.pixel_mask_generator import (
        PixelMaskGenerator,
        create_default_static_params,
        create_default_dynamic_params,
    )
    from diffusepipe.masking.bragg_mask_generator import (
        BraggMaskGenerator,
        create_default_bragg_mask_config,
    )
    from diffusepipe.extraction.data_extractor import DataExtractor

except ImportError as e:
    print(f"Error importing required modules: {e}")
    print(
        "Please ensure DIALS is properly installed and the project is set up correctly."
    )
    sys.exit(1)

logger = logging.getLogger(__name__)


def parse_arguments() -> argparse.Namespace:
    """Parse command-line arguments."""
    parser = argparse.ArgumentParser(
        description="End-to-end Phase 2 visual check pipeline",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  # Basic usage with a CBF file
  python run_phase2_e2e_visual_check.py \\
    --cbf-image ../../747/lys_nitr_10_6_0491.cbf \\
    --output-base-dir ./e2e_outputs \\
    --pdb-path ../../6o2h.pdb

  # With custom configurations
  python run_phase2_e2e_visual_check.py \\
    --cbf-image image.cbf \\
    --output-base-dir ./outputs \\
    --dials-phil-path custom_dials.phil \\
    --static-mask-config '{"beamstop": {"type": "circle", "center_x": 1250, "center_y": 1250, "radius": 50}}' \\
    --bragg-mask-config '{"border": 3}' \\
    --extraction-config-json '{"pixel_step": 2}' \\
    --verbose

  # Using shoebox-based Bragg masking
  python run_phase2_e2e_visual_check.py \\
    --cbf-image image.cbf \\
    --output-base-dir ./outputs \\
    --use-bragg-mask-option-b \\
    --verbose
        """,
    )

    # Required arguments
    parser.add_argument(
        "--cbf-image", type=str, required=True, help="Path to input CBF image file"
    )
    parser.add_argument(
        "--output-base-dir",
        type=str,
        required=True,
        help="Base output directory (unique subdirectory will be created)",
    )

    # Optional DIALS/processing arguments
    parser.add_argument(
        "--dials-phil-path",
        type=str,
        help="Path to custom DIALS PHIL configuration file",
    )
    parser.add_argument(
        "--pdb-path", type=str, help="Path to external PDB file for validation"
    )

    # Masking configuration arguments
    parser.add_argument(
        "--static-mask-config",
        type=str,
        help="JSON string for static mask configuration (StaticMaskParams)",
    )
    parser.add_argument(
        "--bragg-mask-config", type=str, help="JSON string for Bragg mask configuration"
    )
    parser.add_argument(
        "--use-bragg-mask-option-b",
        action="store_true",
        help="Use shoebox-based Bragg masking (requires shoeboxes from DIALS)",
    )

    # Extraction configuration arguments
    parser.add_argument(
        "--extraction-config-json",
        type=str,
        help="JSON string for extraction configuration overrides",
    )
    parser.add_argument(
        "--pixel-step", type=int, help="Pixel sampling step size for extraction"
    )
    parser.add_argument(
        "--save-pixel-coords",
        action="store_true",
        default=True,
        help="Save original pixel coordinates in NPZ output (default: True)",
    )

    # General arguments
    parser.add_argument("--verbose", action="store_true", help="Enable verbose logging")

    return parser.parse_args()


def setup_logging(log_file_path: Path, verbose: bool = False) -> None:
    """Set up logging configuration."""
    log_level = logging.DEBUG if verbose else logging.INFO

    # Create formatter
    formatter = logging.Formatter(
        "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
    )

    # File handler
    file_handler = logging.FileHandler(log_file_path)
    file_handler.setLevel(log_level)
    file_handler.setFormatter(formatter)

    # Console handler
    console_handler = logging.StreamHandler()
    console_handler.setLevel(log_level)
    console_handler.setFormatter(formatter)

    # Configure root logger
    logging.basicConfig(level=log_level, handlers=[file_handler, console_handler])

    logger.info(f"Logging configured. Log file: {log_file_path}")


def parse_json_config(json_str: Optional[str], config_name: str) -> Dict[str, Any]:
    """Parse JSON configuration string."""
    if not json_str:
        return {}

    try:
        config = json.loads(json_str)
        logger.info(f"Parsed {config_name} configuration: {config}")
        return config
    except json.JSONDecodeError as e:
        logger.error(f"Failed to parse {config_name} JSON: {e}")
        raise


def extract_pdb_symmetry(pdb_path: str) -> tuple[Optional[str], Optional[str]]:
    """
    Extract unit cell and space group from PDB file.

    Returns:
        tuple[Optional[str], Optional[str]]: (unit_cell_string, space_group_string)
    """
    try:
        from iotbx import pdb

        pdb_input = pdb.input(file_name=pdb_path)
        crystal_symmetry = pdb_input.crystal_symmetry()

        if crystal_symmetry is None:
            logger.warning(f"No crystal symmetry found in PDB file: {pdb_path}")
            return None, None

        unit_cell = crystal_symmetry.unit_cell()
        space_group = crystal_symmetry.space_group()

        # Format unit cell parameters as comma-separated string
        uc_params = unit_cell.parameters()
        unit_cell_string = f"{uc_params[0]:.3f},{uc_params[1]:.3f},{uc_params[2]:.3f},{uc_params[3]:.2f},{uc_params[4]:.2f},{uc_params[5]:.2f}"

        # Get space group symbol
        space_group_string = space_group.type().lookup_symbol()

        logger.info(
            f"Extracted from PDB: unit_cell={unit_cell_string}, space_group={space_group_string}"
        )
        return unit_cell_string, space_group_string

    except Exception as e:
        logger.warning(f"Failed to extract symmetry from PDB file {pdb_path}: {e}")
        return None, None


def run_phase1_dials_processing(
    args: argparse.Namespace, image_output_dir: Path
) -> tuple[Path, Path]:
    """
    Run Phase 1 DIALS processing and validation.

    Returns:
        tuple[Path, Path]: Paths to experiment file and reflection file
    """
    logger.info("=== Phase 1: DIALS Processing and Validation ===")

    # Extract known symmetry from PDB if provided
    known_unit_cell = None
    known_space_group = None
    if args.pdb_path:
        known_unit_cell, known_space_group = extract_pdb_symmetry(args.pdb_path)

    # 2.1: DIALS Processing & Validation
    logger.info("Initializing DIALS processor...")
    processor = StillProcessorAndValidatorComponent()

    # Create DIALS configuration with known symmetry
    dials_config = create_default_config(
        known_unit_cell=known_unit_cell, known_space_group=known_space_group
    )
    if args.dials_phil_path:
        dials_config.stills_process_phil_path = args.dials_phil_path
    if args.use_bragg_mask_option_b:
        dials_config.output_shoeboxes = True

    # Create extraction config for validation
    extraction_config = create_default_extraction_config()

    logger.info(f"Processing CBF image: {args.cbf_image}")
    logger.info(f"Output directory: {image_output_dir}")

    # Process the still
    still_outcome = processor.process_and_validate_still(
        image_path=args.cbf_image,
        config=dials_config,
        extraction_config=extraction_config,
        external_pdb_path=args.pdb_path,
        output_dir=str(image_output_dir),
    )

    # Check outcome
    if still_outcome.status != "SUCCESS":
        logger.error(f"DIALS processing failed: {still_outcome.status}")
        logger.error(f"Error details: {still_outcome.message}")
        raise RuntimeError(f"Phase 1 DIALS processing failed: {still_outcome.message}")

    logger.info("DIALS processing completed successfully")

    # Identify output files
    expt_file = image_output_dir / "indexed_refined_detector.expt"
    refl_file = image_output_dir / "indexed_refined_detector.refl"

    # Verify files exist
    if not expt_file.exists():
        raise FileNotFoundError(f"Expected experiment file not found: {expt_file}")
    if not refl_file.exists():
        raise FileNotFoundError(f"Expected reflection file not found: {refl_file}")

    logger.info(f"Generated experiment file: {expt_file}")
    logger.info(f"Generated reflection file: {refl_file}")

    return expt_file, refl_file


def run_phase1_mask_generation(
    args: argparse.Namespace, image_output_dir: Path, expt_file: Path, refl_file: Path
) -> tuple[Path, Path, Path]:
    """
    Run Phase 1 mask generation (pixel, Bragg, and total masks).

    Returns:
        tuple[Path, Path, Path]: Paths to pixel mask, Bragg mask, and total diffuse mask
    """
    logger.info("=== Phase 1: Mask Generation ===")

    # Load experiment and reflection data
    logger.info("Loading experiment and reflection data...")
    experiments = ExperimentListFactory.from_json_file(str(expt_file))
    experiment = experiments[0]
    detector = experiment.detector

    # Load raw CBF image
    imageset = ImageSetFactory.new([args.cbf_image])[0]

    # 2.2: Pixel Mask Generation
    logger.info("Generating pixel masks...")
    pixel_generator = PixelMaskGenerator()

    # Parse static mask configuration
    static_config = parse_json_config(args.static_mask_config, "static mask")
    if static_config:
        # Update default with provided config
        static_params = create_default_static_params()
        for key, value in static_config.items():
            if hasattr(static_params, key):
                setattr(static_params, key, value)
    else:
        static_params = create_default_static_params()

    # Use default dynamic parameters
    dynamic_params = create_default_dynamic_params()

    # Generate combined pixel mask
    global_pixel_mask_tuple = pixel_generator.generate_combined_pixel_mask(
        detector=detector,
        static_params=static_params,
        representative_images=[imageset],
        dynamic_params=dynamic_params,
    )

    # Save pixel mask
    pixel_mask_path = image_output_dir / "global_pixel_mask.pickle"
    with open(pixel_mask_path, "wb") as f:
        pickle.dump(global_pixel_mask_tuple, f)
    logger.info(f"Saved pixel mask: {pixel_mask_path}")

    # 2.3: Bragg Mask Generation
    logger.info("Generating Bragg mask...")
    bragg_generator = BraggMaskGenerator()

    # Parse Bragg mask configuration
    bragg_config_overrides = parse_json_config(args.bragg_mask_config, "Bragg mask")
    bragg_config = create_default_bragg_mask_config()
    bragg_config.update(bragg_config_overrides)

    # Load reflection data
    from dials.array_family import flex

    reflections = flex.reflection_table.from_file(str(refl_file))

    # Generate Bragg mask
    if args.use_bragg_mask_option_b:
        logger.info("Using shoebox-based Bragg masking (Option B)")
        bragg_mask_tuple = bragg_generator.generate_bragg_mask_from_shoeboxes(
            reflections=reflections, detector=detector
        )
    else:
        logger.info("Using spot-based Bragg masking (Option A)")
        bragg_mask_tuple = bragg_generator.generate_bragg_mask_from_spots(
            experiment=experiment, reflections=reflections, config=bragg_config
        )

    # Save Bragg mask
    bragg_mask_path = image_output_dir / "bragg_mask.pickle"
    with open(bragg_mask_path, "wb") as f:
        pickle.dump(bragg_mask_tuple, f)
    logger.info(f"Saved Bragg mask: {bragg_mask_path}")

    # 2.4: Total Diffuse Mask Generation
    logger.info("Generating total diffuse mask...")
    total_diffuse_mask_tuple = bragg_generator.get_total_mask_for_still(
        bragg_mask=bragg_mask_tuple,
        global_pixel_mask=global_pixel_mask_tuple,
    )

    # Save total diffuse mask
    total_diffuse_mask_path = image_output_dir / "total_diffuse_mask.pickle"
    with open(total_diffuse_mask_path, "wb") as f:
        pickle.dump(total_diffuse_mask_tuple, f)
    logger.info(f"Saved total diffuse mask: {total_diffuse_mask_path}")

    return pixel_mask_path, bragg_mask_path, total_diffuse_mask_path


def run_phase2_data_extraction(
    args: argparse.Namespace,
    image_output_dir: Path,
    expt_file: Path,
    total_diffuse_mask_path: Path,
) -> Path:
    """
    Run Phase 2 data extraction.

    Returns:
        Path: Path to output NPZ file
    """
    logger.info("=== Phase 2: Data Extraction ===")

    # 3.1: Instantiate DataExtractor
    data_extractor = DataExtractor()

    # 3.2: Create ComponentInputFiles
    component_inputs = ComponentInputFiles(
        cbf_image_path=args.cbf_image,
        dials_expt_path=str(expt_file),
        bragg_mask_path=str(total_diffuse_mask_path),  # Use total diffuse mask
        external_pdb_path=args.pdb_path,
    )

    # 3.3: Create ExtractionConfig
    extraction_config = create_default_extraction_config()

    # Set pixel coordinate saving
    extraction_config.save_original_pixel_coordinates = args.save_pixel_coords

    # Apply JSON overrides if provided
    if args.extraction_config_json:
        json_overrides = parse_json_config(
            args.extraction_config_json, "extraction config"
        )
        # Update config with overrides
        config_dict = extraction_config.model_dump()
        config_dict.update(json_overrides)
        extraction_config = ExtractionConfig(**config_dict)

    # Apply pixel step if provided
    if args.pixel_step:
        extraction_config.pixel_step = args.pixel_step

    logger.info(f"Extraction configuration: {extraction_config}")

    # 3.4: Define output NPZ path
    output_npz_path = image_output_dir / "diffuse_data.npz"

    # 3.5: Extract diffuse data
    logger.info(f"Extracting diffuse data to: {output_npz_path}")
    data_extractor_outcome = data_extractor.extract_from_still(
        inputs=component_inputs,
        config=extraction_config,
        output_npz_path=str(output_npz_path),
    )

    # 3.6: Check outcome
    if data_extractor_outcome.status != "SUCCESS":
        logger.error(f"Data extraction failed: {data_extractor_outcome.status}")
        logger.error(f"Error details: {data_extractor_outcome.message}")
        raise RuntimeError(
            f"Phase 2 data extraction failed: {data_extractor_outcome.message}"
        )

    logger.info("Data extraction completed successfully")
    return output_npz_path


def run_visual_check(
    args: argparse.Namespace,
    image_output_dir: Path,
    expt_file: Path,
    total_diffuse_mask_path: Path,
    bragg_mask_path: Path,
    pixel_mask_path: Path,
    npz_file: Path,
) -> None:
    """
    Run visual diagnostics check.
    """
    logger.info("=== Phase 3: Visual Diagnostics ===")

    # 4.1: Construct command for check_diffuse_extraction.py
    diagnostics_dir = image_output_dir / "extraction_diagnostics"

    # Get the script path relative to the project root
    script_path = (
        Path(__file__).resolve().parent.parent
        / "visual_diagnostics"
        / "check_diffuse_extraction.py"
    )

    cmd = [
        "python",
        str(script_path),
        "--raw-image",
        args.cbf_image,
        "--expt",
        str(expt_file),
        "--total-mask",
        str(total_diffuse_mask_path),
        "--npz-file",
        str(npz_file),
        "--bragg-mask",
        str(bragg_mask_path),
        "--pixel-mask",
        str(pixel_mask_path),
        "--output-dir",
        str(diagnostics_dir),
    ]

    if args.verbose:
        cmd.append("--verbose")

    logger.info(f"Running visual diagnostics: {' '.join(cmd)}")

    # 4.2: Execute the command
    try:
        result = subprocess.run(
            cmd,
            capture_output=True,
            text=True,
            check=False,
            cwd=str(Path(__file__).resolve().parent.parent.parent),  # Project root
        )

        # 4.3: Log results
        if result.returncode == 0:
            logger.info("Visual diagnostics completed successfully")
        else:
            logger.error(
                f"Visual diagnostics failed with return code: {result.returncode}"
            )

        if result.stdout:
            logger.info("Visual diagnostics stdout:")
            for line in result.stdout.strip().split("\n"):
                logger.info(f"  {line}")

        if result.stderr:
            logger.error("Visual diagnostics stderr:")
            for line in result.stderr.strip().split("\n"):
                logger.error(f"  {line}")

        if result.returncode != 0:
            logger.warning("Visual diagnostics failed, but continuing...")

    except Exception as e:
        logger.error(f"Failed to run visual diagnostics: {e}")
        logger.warning("Continuing without visual diagnostics...")


def main():
    """Main orchestration function."""
    # Parse arguments
    args = parse_arguments()

    # Create unique output directory
    cbf_path = Path(args.cbf_image)
    output_base = Path(args.output_base_dir)
    image_output_dir = output_base / cbf_path.stem
    image_output_dir.mkdir(parents=True, exist_ok=True)

    # Setup logging
    log_file = image_output_dir / "e2e_visual_check.log"
    setup_logging(log_file, args.verbose)

    logger.info("Starting end-to-end Phase 2 visual check pipeline")
    logger.info(f"Input CBF: {args.cbf_image}")
    logger.info(f"Output directory: {image_output_dir}")

    try:
        # Phase 1: DIALS Processing
        expt_file, refl_file = run_phase1_dials_processing(args, image_output_dir)

        # Phase 1: Mask Generation
        pixel_mask_path, bragg_mask_path, total_diffuse_mask_path = (
            run_phase1_mask_generation(args, image_output_dir, expt_file, refl_file)
        )

        # Phase 2: Data Extraction
        npz_file = run_phase2_data_extraction(
            args, image_output_dir, expt_file, total_diffuse_mask_path
        )

        # Phase 3: Visual Diagnostics
        run_visual_check(
            args,
            image_output_dir,
            expt_file,
            total_diffuse_mask_path,
            bragg_mask_path,
            pixel_mask_path,
            npz_file,
        )

        logger.info("=== Pipeline Completed Successfully ===")
        logger.info(f"All outputs saved to: {image_output_dir}")
        logger.info("Generated files:")
        for file_path in sorted(image_output_dir.rglob("*")):
            if file_path.is_file():
                logger.info(f"  {file_path.relative_to(image_output_dir)}")

    except Exception as e:
        logger.error(f"Pipeline failed: {e}")
        logger.error("Check the log file for detailed error information")
        sys.exit(1)


if __name__ == "__main__":
    main()
</file>

<file path="src/diffusepipe/masking/pixel_mask_generator.py">
"""
Pixel mask generation for static and dynamic bad pixel identification.

This module implements Module 1.S.2 from the plan, providing functions to generate
global pixel masks based on detector properties and dynamic features observed
across representative still images.
"""

import logging
from typing import Dict, List, Optional, Tuple, Union
from dataclasses import dataclass

import numpy as np

from diffusepipe.exceptions import MaskGenerationError

# Import needed for patching in tests
try:
    from dials.array_family import flex
except ImportError:
    # This import might fail in testing environments without DIALS
    flex = None

logger = logging.getLogger(__name__)


@dataclass
class Circle:
    """Circular region definition."""

    center_x: float
    center_y: float
    radius: float


@dataclass
class Rectangle:
    """Rectangular region definition."""

    min_x: float
    max_x: float
    min_y: float
    max_y: float


@dataclass
class StaticMaskParams:
    """Parameters for static mask generation."""

    beamstop: Optional[Union[Circle, Rectangle]] = None
    untrusted_rects: Optional[List[Rectangle]] = None
    untrusted_panels: Optional[List[int]] = None


@dataclass
class DynamicMaskParams:
    """Parameters for dynamic mask generation."""

    hot_pixel_thresh: Optional[float] = None
    negative_pixel_tolerance: float = 0.0
    max_fraction_bad_pixels: float = 0.1


class PixelMaskGenerator:
    """
    Generator for static and dynamic pixel masks.

    This class implements the pixel masking logic described in Module 1.S.2,
    creating global masks based on detector properties and dynamic features.
    """

    def __init__(self):
        """Initialize the pixel mask generator."""
        pass

    def generate_combined_pixel_mask(
        self,
        detector: object,
        static_params: StaticMaskParams,
        representative_images: List[object],
        dynamic_params: DynamicMaskParams,
    ) -> Tuple[object, ...]:
        """
        Generate combined pixel mask from static and dynamic components.

        Args:
            detector: DIALS Detector object
            static_params: Parameters for static mask generation
            representative_images: List of ImageSet objects for dynamic analysis
            dynamic_params: Parameters for dynamic mask generation

        Returns:
            Tuple of flex.bool arrays, one per detector panel

        Raises:
            MaskGenerationError: When mask generation fails
        """
        try:
            logger.info("Starting combined pixel mask generation")

            # Generate static mask
            static_mask = self.generate_static_mask(detector, static_params)
            logger.info(f"Generated static mask for {len(static_mask)} panels")

            # Generate dynamic mask
            dynamic_mask = self.generate_dynamic_mask(
                detector, representative_images, dynamic_params
            )
            logger.info(f"Generated dynamic mask for {len(dynamic_mask)} panels")

            # Combine masks (logical AND)
            combined_mask = self._combine_masks(static_mask, dynamic_mask)
            logger.info("Combined static and dynamic masks")

            return combined_mask

        except Exception as e:
            raise MaskGenerationError(f"Failed to generate combined pixel mask: {e}")

    def generate_static_mask(
        self, detector: object, static_params: StaticMaskParams
    ) -> Tuple[object, ...]:
        """
        Generate static pixel mask based on detector properties.

        Args:
            detector: DIALS Detector object
            static_params: Parameters for static masking

        Returns:
            Tuple of flex.bool arrays, one per detector panel

        Raises:
            MaskGenerationError: When static mask generation fails
        """
        try:
            from dials.array_family import flex

            panel_masks = []

            for panel_idx, panel in enumerate(detector):
                logger.debug(f"Processing panel {panel_idx}")

                # Get panel dimensions
                panel_size = panel.get_image_size()
                height, width = (
                    panel_size[1],
                    panel_size[0],
                )  # Note: DIALS uses (fast, slow)

                # Initialize panel mask to True (good pixels)
                panel_mask = flex.bool(flex.grid(height, width), True)

                # Apply trusted range mask
                panel_mask = self._apply_trusted_range_mask(panel, panel_mask)

                # Apply beamstop mask
                if static_params.beamstop:
                    panel_mask = self._apply_beamstop_mask(
                        panel, panel_mask, static_params.beamstop
                    )

                # Apply untrusted rectangle masks
                if static_params.untrusted_rects:
                    panel_mask = self._apply_untrusted_rects_mask(
                        panel, panel_mask, static_params.untrusted_rects
                    )

                # Check if this panel should be entirely untrusted
                if (
                    static_params.untrusted_panels
                    and panel_idx in static_params.untrusted_panels
                ):
                    panel_mask = flex.bool(flex.grid(height, width), False)
                    logger.info(f"Panel {panel_idx} marked as entirely untrusted")

                panel_masks.append(panel_mask)

            return tuple(panel_masks)

        except Exception as e:
            raise MaskGenerationError(f"Failed to generate static mask: {e}")

    def generate_dynamic_mask(
        self,
        detector: object,
        representative_images: List[object],
        dynamic_params: DynamicMaskParams,
    ) -> Tuple[object, ...]:
        """
        Generate dynamic pixel mask based on analysis of representative images.

        Args:
            detector: DIALS Detector object
            representative_images: List of ImageSet objects to analyze
            dynamic_params: Parameters for dynamic masking

        Returns:
            Tuple of flex.bool arrays, one per detector panel

        Raises:
            MaskGenerationError: When dynamic mask generation fails
        """
        try:
            from dials.array_family import flex

            if not representative_images:
                logger.warning("No representative images provided for dynamic masking")
                # Return all-True masks
                panel_masks = []
                for panel in detector:
                    panel_size = panel.get_image_size()
                    height, width = panel_size[1], panel_size[0]
                    panel_mask = flex.bool(flex.grid(height, width), True)
                    panel_masks.append(panel_mask)
                return tuple(panel_masks)

            logger.info(f"Analyzing {len(representative_images)} representative images")

            # Initialize accumulators for each panel
            panel_stats = []
            for panel in detector:
                panel_size = panel.get_image_size()
                height, width = panel_size[1], panel_size[0]

                stats = {
                    "sum": np.zeros((height, width), dtype=np.float64),
                    "count": np.zeros((height, width), dtype=np.int32),
                    "min_val": np.full((height, width), np.inf, dtype=np.float64),
                    "max_val": np.full((height, width), -np.inf, dtype=np.float64),
                }
                panel_stats.append(stats)

            # Accumulate statistics across all representative images
            for img_idx, image_set in enumerate(representative_images):
                logger.debug(f"Processing representative image {img_idx + 1}")
                self._accumulate_image_stats(image_set, panel_stats)

            # Generate dynamic masks based on accumulated statistics
            panel_masks = []
            for panel_idx, stats in enumerate(panel_stats):
                panel_mask = self._generate_panel_dynamic_mask(
                    stats, dynamic_params, panel_idx
                )
                panel_masks.append(panel_mask)

            return tuple(panel_masks)

        except Exception as e:
            raise MaskGenerationError(f"Failed to generate dynamic mask: {e}")

    def _apply_trusted_range_mask(self, panel: object, panel_mask: object) -> object:
        """
        Apply trusted range mask using panel.get_trusted_range().

        Args:
            panel: DIALS Panel object
            panel_mask: Current panel mask (flex.bool)

        Returns:
            Updated panel mask
        """
        try:
            # Get trusted range for this panel
            trusted_range = panel.get_trusted_range()
            logger.debug(f"Panel trusted range: {trusted_range}")

            # For static masking, we use a representative raw data array
            # In practice, this would be applied per-image during processing
            # For now, just log the trusted range
            logger.debug(
                "Trusted range mask will be applied per-image during processing"
            )

            return panel_mask

        except Exception as e:
            logger.warning(f"Failed to apply trusted range mask: {e}")
            return panel_mask

    def _apply_beamstop_mask(
        self, panel: object, panel_mask: object, beamstop: Union[Circle, Rectangle]
    ) -> object:
        """
        Apply beamstop mask to panel.

        Args:
            panel: DIALS Panel object
            panel_mask: Current panel mask (flex.bool)
            beamstop: Beamstop geometry

        Returns:
            Updated panel mask with beamstop region masked
        """
        try:
            from dials.array_family import flex

            # Get panel dimensions
            panel_size = panel.get_image_size()
            height, width = panel_size[1], panel_size[0]

            # Create coordinate grids
            y_coords, x_coords = np.mgrid[0:height, 0:width]

            if isinstance(beamstop, Circle):
                # Circular beamstop
                distances_sq = (x_coords - beamstop.center_x) ** 2 + (
                    y_coords - beamstop.center_y
                ) ** 2
                beamstop_region = distances_sq <= (beamstop.radius**2)

            elif isinstance(beamstop, Rectangle):
                # Rectangular beamstop
                beamstop_region = (
                    (x_coords >= beamstop.min_x)
                    & (x_coords <= beamstop.max_x)
                    & (y_coords >= beamstop.min_y)
                    & (y_coords <= beamstop.max_y)
                )
            else:
                logger.warning(f"Unknown beamstop type: {type(beamstop)}")
                return panel_mask

            # Convert to flex.bool and apply
            beamstop_mask = flex.bool(beamstop_region.flatten())
            beamstop_mask.reshape(flex.grid(height, width))

            # Mask beamstop region (set to False)
            panel_mask = panel_mask & (~beamstop_mask)

            logger.debug(
                f"Applied beamstop mask, masked {beamstop_region.sum()} pixels"
            )

            return panel_mask

        except Exception as e:
            logger.warning(f"Failed to apply beamstop mask: {e}")
            return panel_mask

    def _apply_untrusted_rects_mask(
        self, panel: object, panel_mask: object, untrusted_rects: List[Rectangle]
    ) -> object:
        """
        Apply untrusted rectangle masks to panel.

        Args:
            panel: DIALS Panel object
            panel_mask: Current panel mask (flex.bool)
            untrusted_rects: List of rectangular regions to mask

        Returns:
            Updated panel mask with untrusted regions masked
        """
        try:
            from dials.array_family import flex

            # Get panel dimensions
            panel_size = panel.get_image_size()
            height, width = panel_size[1], panel_size[0]

            # Create coordinate grids
            y_coords, x_coords = np.mgrid[0:height, 0:width]

            total_masked = 0
            for rect in untrusted_rects:
                # Create mask for this rectangle
                rect_region = (
                    (x_coords >= rect.min_x)
                    & (x_coords <= rect.max_x)
                    & (y_coords >= rect.min_y)
                    & (y_coords <= rect.max_y)
                )

                # Convert to flex.bool and apply
                rect_mask = flex.bool(rect_region.flatten())
                rect_mask.reshape(flex.grid(height, width))

                # Mask this region (set to False)
                panel_mask = panel_mask & (~rect_mask)

                total_masked += rect_region.sum()

            logger.debug(
                f"Applied {len(untrusted_rects)} untrusted rectangles, "
                f"masked {total_masked} pixels"
            )

            return panel_mask

        except Exception as e:
            logger.warning(f"Failed to apply untrusted rectangles mask: {e}")
            return panel_mask

    def _accumulate_image_stats(
        self, image_set: object, panel_stats: List[Dict[str, np.ndarray]]
    ) -> None:
        """
        Accumulate statistics from a single image across all panels.

        Args:
            image_set: DIALS ImageSet object
            panel_stats: List of statistics dictionaries, one per panel
        """
        try:
            # Get raw data for this image
            raw_data_result = image_set.get_raw_data(
                0
            )  # First (and only) image in the set
            raw_data_iterable = (
                raw_data_result
                if isinstance(raw_data_result, tuple)
                else (raw_data_result,)
            )  # Ensure iterable

            for panel_idx, panel_data in enumerate(raw_data_iterable):
                if panel_idx >= len(panel_stats):
                    logger.warning(
                        f"Panel {panel_idx} exceeds expected number of panels"
                    )
                    continue

                # Convert flex array to numpy for vectorized operations
                # Convert flex array to numpy array properly to maintain 2D shape
                if hasattr(panel_data, "as_numpy_array"):
                    # DIALS flex array - use proper conversion method
                    panel_array = panel_data.as_numpy_array().astype(np.float64)
                else:
                    # Already a numpy array
                    panel_array = np.array(panel_data).astype(np.float64)

                stats = panel_stats[panel_idx]

                # Update statistics
                stats["sum"] += panel_array
                stats["count"] += 1
                stats["min_val"] = np.minimum(stats["min_val"], panel_array)
                stats["max_val"] = np.maximum(stats["max_val"], panel_array)

        except Exception as e:
            logger.warning(f"Failed to accumulate statistics from image: {e}")

    def _generate_panel_dynamic_mask(
        self,
        stats: Dict[str, np.ndarray],
        dynamic_params: DynamicMaskParams,
        panel_idx: int,
    ) -> object:
        """
        Generate dynamic mask for a single panel based on accumulated statistics.

        Args:
            stats: Accumulated statistics for this panel
            dynamic_params: Parameters for dynamic masking
            panel_idx: Panel index for logging

        Returns:
            Dynamic mask for this panel (flex.bool)
        """
        try:
            from dials.array_family import flex

            height, width = stats["sum"].shape

            # Initialize dynamic mask to True (good pixels)
            dynamic_mask = np.ones((height, width), dtype=bool)

            # Check for sufficient data
            valid_pixels = stats["count"] > 0
            if not np.any(valid_pixels):
                logger.warning(
                    f"Panel {panel_idx}: No valid pixel data for dynamic masking"
                )
                return flex.bool(dynamic_mask.flatten())

            # Calculate mean intensity where we have data
            mean_intensity = np.zeros_like(stats["sum"])
            mean_intensity[valid_pixels] = (
                stats["sum"][valid_pixels] / stats["count"][valid_pixels]
            )

            # Flag hot pixels
            if dynamic_params.hot_pixel_thresh is not None:
                hot_pixels = valid_pixels & (
                    stats["max_val"] > dynamic_params.hot_pixel_thresh
                )
                dynamic_mask &= ~hot_pixels
                hot_count = hot_pixels.sum()
                logger.debug(f"Panel {panel_idx}: Flagged {hot_count} hot pixels")

            # Flag negative pixels (accounting for tolerance)
            negative_pixels = valid_pixels & (
                stats["min_val"] < -abs(dynamic_params.negative_pixel_tolerance)
            )
            dynamic_mask &= ~negative_pixels
            negative_count = negative_pixels.sum()
            logger.debug(f"Panel {panel_idx}: Flagged {negative_count} negative pixels")

            # Check total fraction of bad pixels
            total_pixels = height * width
            good_pixels = dynamic_mask.sum()
            bad_fraction = 1.0 - (good_pixels / total_pixels)

            if bad_fraction > dynamic_params.max_fraction_bad_pixels:
                logger.warning(
                    f"Panel {panel_idx}: High fraction of bad pixels ({bad_fraction:.3f}), "
                    f"exceeds threshold {dynamic_params.max_fraction_bad_pixels}"
                )

            logger.info(
                f"Panel {panel_idx}: Dynamic mask flagged {total_pixels - good_pixels} "
                f"bad pixels ({bad_fraction:.3f} fraction)"
            )

            # Convert to flex.bool
            flex_mask = flex.bool(dynamic_mask.flatten())
            flex_mask.reshape(flex.grid(height, width))

            return flex_mask

        except Exception as e:
            logger.error(f"Failed to generate dynamic mask for panel {panel_idx}: {e}")
            # Return all-True mask as fallback
            from dials.array_family import flex

            height, width = stats["sum"].shape
            fallback_mask = flex.bool(flex.grid(height, width), True)
            return fallback_mask

    def _combine_masks(
        self, static_mask: Tuple[object, ...], dynamic_mask: Tuple[object, ...]
    ) -> Tuple[object, ...]:
        """
        Combine static and dynamic masks using logical AND.

        Args:
            static_mask: Tuple of static masks (one per panel)
            dynamic_mask: Tuple of dynamic masks (one per panel)

        Returns:
            Tuple of combined masks (one per panel)

        Raises:
            MaskGenerationError: When masks cannot be combined
        """
        try:
            if len(static_mask) != len(dynamic_mask):
                raise MaskGenerationError(
                    f"Static and dynamic masks have different panel counts: "
                    f"{len(static_mask)} vs {len(dynamic_mask)}"
                )

            combined_masks = []
            for panel_idx, (static_panel, dynamic_panel) in enumerate(
                zip(static_mask, dynamic_mask)
            ):
                # Combine using logical AND
                combined_panel = static_panel & dynamic_panel
                combined_masks.append(combined_panel)

                # Log statistics
                static_good = static_panel.count(True)
                dynamic_good = dynamic_panel.count(True)
                combined_good = combined_panel.count(True)
                total_pixels = len(combined_panel)

                logger.debug(
                    f"Panel {panel_idx}: Static={static_good}/{total_pixels}, "
                    f"Dynamic={dynamic_good}/{total_pixels}, "
                    f"Combined={combined_good}/{total_pixels}"
                )

            return tuple(combined_masks)

        except Exception as e:
            raise MaskGenerationError(f"Failed to combine masks: {e}")


# Convenience functions for creating mask parameters


def create_circular_beamstop(center_x: float, center_y: float, radius: float) -> Circle:
    """Create a circular beamstop definition."""
    return Circle(center_x=center_x, center_y=center_y, radius=radius)


def create_rectangular_beamstop(
    min_x: float, max_x: float, min_y: float, max_y: float
) -> Rectangle:
    """Create a rectangular beamstop definition."""
    return Rectangle(min_x=min_x, max_x=max_x, min_y=min_y, max_y=max_y)


def create_default_static_params() -> StaticMaskParams:
    """Create default static mask parameters."""
    return StaticMaskParams()


def create_default_dynamic_params() -> DynamicMaskParams:
    """Create default dynamic mask parameters."""
    return DynamicMaskParams(
        hot_pixel_thresh=1e6,  # Very high counts
        negative_pixel_tolerance=0.0,
        max_fraction_bad_pixels=0.1,
    )
</file>

<file path="checklists/phase1.md">
**Agent Task: Implement Phase 1 of `plan.md` for Stills Diffuse Scattering Pipeline**

**Overall Goal for Agent (Phase 1):** To implement the logic for processing individual stills to produce indexed crystal models, reflection lists with partialities, and comprehensive pixel masks ready for diffuse data extraction.

**Checklist Usage Instructions for Agent:** (Same as Phase 0)

1.  **Copy this entire checklist into your working memory or a dedicated scratchpad area.**
2.  **Context Priming:** Before starting a new phase or major module, carefully read all "Context Priming" points for that section.
3.  **Sequential Execution:** Address checklist items in the order presented.
4.  **Update State:** Use `[ ] Open`, `[P] In Progress`, `[D] Done`, `[B] Blocked`. Add a note for `[B]`.
5.  **Record Details:** File paths, decisions, sub-task breakdowns.
6.  **Iterative Review:** Periodically re-read completed sections and notes.
7.  **Save State:** Ensure this checklist with current states and notes is saved if pausing.

---

**Phase 1: Per-Still Geometry, Indexing, and Initial Masking**

| Item ID | Task Description                                                                | State | Details/Notes/Path                                                                                                                                                             |
| :------ | :------------------------------------------------------------------------------ | :---- | :----------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| **1.A** | **Context Priming (Phase 1)**                                                   | `[ ]` |                                                                                                                                                                              |
| 1.A.1   | Re-read `plan.md` Module 1.S.1: Per-Still Crystallographic Processing           | `[ ]` |                                                                                                                                                                              |
| 1.A.2   | Re-read `plan.md` Module 1.S.2: Static and Dynamic Pixel Mask Generation        | `[ ]` |                                                                                                                                                                              |
| 1.A.3   | Re-read `plan.md` Module 1.S.3: Combined Per-Still Bragg and Pixel Mask Gen     | `[ ]` |                                                                                                                                                                              |
| 1.A.4   | Review `src/diffusepipe/types/types_IDL.md` (esp. `DIALSStillsProcessConfig`, `ComponentInputFiles`, `OperationOutcome`). | `[ ]` |                                                                                                                                                                              |
| 1.A.5   | Review relevant adapters from Phase 0 (DIALSStillsProcessAdapter, DIALSGenerateMaskAdapter, DXTBXIOAdapter). | `[ ]` |                                                                                                                                                                              |
| 1.A.6   | Understand Goal: Implement modules for per-still DIALS processing, static/dynamic pixel masking, and Bragg/total mask generation. | `[ ]` |                                                                                                                                                                              |
|         |                                                                                 |       |                                                                                                                                                                              |
| **1.B** | **Module 1.S.1: Per-Still Crystallographic Processing and Model Validation**    | `[ ]` | *Note: Core logic is in `DIALSStillsProcessAdapter` (Phase 0.C.1). This section focuses on its usage context within a conceptual component and its higher-level testing.*          |
| **1.B.idl** | **Review/Confirm Conceptual Interface for a "StillProcessorAndValidatorComponent"** | `[ ]` | **Purpose:** This component orchestrates DIALS processing AND subsequent validation. <br>Input: `image_path: str`, `config: StillsPipelineConfig` (to access `DIALSStillsProcessConfig` and validation tolerances), `base_experiment_path?: str`, `external_pdb_path?: str`. <br>Output: `(Experiment_dials_i?, Reflections_dials_i?, validation_passed: bool, validation_metrics: dict, success_bool, logs_str)` or an `OperationOutcome`. <br>Behavior: Uses `DIALSStillsProcessAdapter`, then performs geometric and PDB consistency checks. <br>Path (Conceptual): `src/diffusepipe/crystallography/still_processing_and_validation.py` |
| 1.B.1   | Implementation of `StillProcessorAndValidatorComponent` (or equivalent logic) | `[ ]` | Path: `src/diffusepipe/crystallography/still_processing_and_validation.py` (New or renamed file) |
|         |                                                                                 |       |                                                                                                                                                                              |
| **1.B.V** | **Sub-Module 1.S.1.Validation: Geometric Model Consistency Checks Implementation** | `[ ]` | In `src/diffusepipe/crystallography/still_processing_and_validation.py` (or a dedicated `model_validator.py` called by it)                                                      |
| 1.B.V.1 | Implement PDB Consistency Check logic                                         | `[ ]` |                                                                                                                                                                              |
|         |   - Input: `Experiment_dials_i.crystal`, `external_pdb_path`, tolerances from config. | `[ ]` |                                                                                                                                                                              |
|         |   - Compare unit cell parameters (lengths, angles).                           | `[ ]` |                                                                                                                                                                              |
|         |   - Compare crystal orientation (A or U matrix).                              | `[ ]` |                                                                                                                                                                              |
| 1.B.V.2 | Implement Q-Vector Consistency Check logic                                  | `[ ]` |                                                                                                                                                                              |
|         |   - Input: `Experiment_dials_i`, `Reflections_dials_i`, `q_consistency_tolerance_angstrom_inv`. | `[ ]` |                                                                                                                                                                              |
|         |   - Loop subset of reflections: get `s1` vector, pixel coords (e.g., `xyzobs.px.value`). | `[ ]` |                                                                                                                                                                              |
|         |   - Calculate `q_model` (from `s1`, `s0`) and `q_observed` (from pixel coords and geometry). | `[ ]` |                                                                                                                                                                              |
|         |   - Compute `|Δq| = |q_model - q_observed|` and check against tolerance.     | `[ ]` |                                                                                                                                                                              |
| 1.B.V.3 | Implement Diagnostic Plot Generation for validation                           | `[ ]` | Similar to `consistency_checker.py` plots (q-diff histogram, q-scatter, heatmap). Save to working dir.                                                                       |
|         |                                                                                 |       |                                                                                                                                                                              |
| 1.B.2   | **Integration Test for StillProcessorAndValidatorComponent**           | `[ ]` | Path: `tests/crystallography/test_still_processor.py`                                                                                                                          |
| 1.B.2.a | Test Case: `test_process_single_still_successfully`                             | `[ ]` |                                                                                                                                                                              |
|         |   - Setup: Minimal CBF, `DIALSStillsProcessConfig`, Adapter instance          | `[ ]` | Test Data: `tests/data/minimal_still.cbf`, `tests/data/minimal_stills_process.phil`                                                                                              |
|         |   - Execution: Call the component/function wrapping `adapter.process_still(...)` | `[ ]` |                                                                                                                                                                              |
|         |   - Verification: Success status, valid `Experiment` & `reflection_table`       | `[ ]` |                                                                                                                                                                              |
|         |   - Verification: `"partiality"` column exists and has floats                   | `[ ]` |                                                                                                                                                                              |
|         |   - Verification: Assert validation_passed is True, Assert validation_metrics are reasonable. | `[ ]` |                                                                                                                                                                              |
| 1.B.2.b | Test Case: `test_process_still_indexing_failure`                                | `[ ]` |                                                                                                                                                                              |
|         |   - Setup: Image/config known to fail indexing                                  | `[ ]` | Test Data: `tests/data/unindexable_still.cbf` or specific PHIL.                                                                                                                |
|         |   - Execution: Call the component/function wrapping `adapter.process_still(...)` | `[ ]` |                                                                                                                                                                              |
|         |   - Verification: Failure status or `DIALSError` handled                        | `[ ]` |                                                                                                                                                                              |
| 1.B.2.c | Test Case: `test_validation_pdb_mismatch_cell`                          | `[ ]` |                                                                                                                                                                              |
|         |   - Setup: DIALS output with cell params differing from mock PDB outside tolerance. | `[ ]` |                                                                                                                                                                              |
|         |   - Execution: Call component.                                              | `[ ]` |                                                                                                                                                                              |
|         |   - Verification: Assert `validation_passed` is False, appropriate status/message. | `[ ]` |                                                                                                                                                                              |
| 1.B.2.d | Test Case: `test_validation_q_consistency_failure`                      | `[ ]` |                                                                                                                                                                              |
|         |   - Setup: Mock DIALS output where `q_model` and `q_observed` differ significantly. | `[ ]` |                                                                                                                                                                              |
|         |   - Execution: Call component.                                              | `[ ]` |                                                                                                                                                                              |
|         |   - Verification: Assert `validation_passed` is False, appropriate status/message. | `[ ]` |                                                                                                                                                                              |
| 1.B.2.e | Test Case: `test_validation_all_pass`                                     | `[ ]` |                                                                                                                                                                              |
|         |   - Setup: DIALS output and mock PDB that are consistent within tolerances. | `[ ]` |                                                                                                                                                                              |
|         |   - Execution: Call component.                                              | `[ ]` |                                                                                                                                                                              |
|         |   - Verification: Assert `validation_passed` is True.                       | `[ ]` |                                                                                                                                                                              |
|         |                                                                                 |       |                                                                                                                                                                              |
| **1.C** | **Module 1.S.2: Static and Dynamic Pixel Mask Generation**                      | `[ ]` |                                                                                                                                                                              |
| **1.C.idl** | **Define/Review Conceptual IDL for `PixelMaskGenerator`**                     | `[ ]` | **Purpose:** Define interface for generating `Mask_pixel`. <br>Path for IDL thoughts (e.g. in comments of Python file): `src/diffusepipe/masking/pixel_mask_generator.py` <br>Structs: `StaticMaskParams { beamstop?: Circle/Rect, untrusted_rects?: list<Rect>, untrusted_panels?: list<int> }`, `DynamicMaskParams { hot_pixel_thresh?: float }`. <br>Method: `generate_combined_pixel_mask(detector: Detector, static_params: StaticMaskParams, representative_images: list[ImageSet], dynamic_params: DynamicMaskParams) -> tuple[flex.bool, ...]`. <br>Errors: `MaskGenerationError`. |
| 1.C.1   | Implement `generate_static_mask` function                                     | `[ ]` | In `src/diffusepipe/masking/pixel_mask_generator.py` (Based on 1.C.idl)                                                                                                        |
| 1.C.1.a |   - `detector.get_trusted_range_mask()` application                             | `[ ]` |                                                                                                                                                                              |
| 1.C.1.b |   - Beamstop mask (circular/rectangular from `StaticMaskParams`)                | `[ ]` |                                                                                                                                                                              |
| 1.C.1.c |   - Untrusted panel/rectangle masks (from `StaticMaskParams`)                   | `[ ]` |                                                                                                                                                                              |
| 1.C.1.d |   - Combine static masks using `flex.bool` logic                                | `[ ]` |                                                                                                                                                                              |
| 1.C.2   | Implement `generate_dynamic_mask` function                                    | `[ ]` | In `src/diffusepipe/masking/pixel_mask_generator.py` (Based on 1.C.idl)                                                                                                        |
| 1.C.2.a |   - Iterate `representative_images` (list of `ImageSet`)                        | `[ ]` |                                                                                                                                                                              |
| 1.C.2.b |   - Identify anomalous pixels (vectorized) using `DynamicMaskParams`            | `[ ]` |                                                                                                                                                                              |
| 1.C.2.c |   - Combine per-image dynamic masks (logical OR)                                | `[ ]` |                                                                                                                                                                              |
| 1.C.3   | Implement `generate_combined_pixel_mask` function                             | `[ ]` | In `src/diffusepipe/masking/pixel_mask_generator.py`. Combines static & dynamic. (Based on 1.C.idl)                                                                            |
| 1.C.4   | **Unit Tests for Pixel Mask Generation**                                        | `[ ]` | Path: `tests/masking/test_pixel_mask_generator.py`                                                                                                                             |
| 1.C.4.a |   - Test `generate_static_mask` with mock `Detector` & `StaticMaskParams`       | `[ ]` |                                                                                                                                                                              |
| 1.C.4.b |   - Test `generate_dynamic_mask` with mock `ImageSet` data & `DynamicMaskParams`| `[ ]` |                                                                                                                                                                              |
| 1.C.4.c |   - Test `generate_combined_pixel_mask` for correct logical combination         | `[ ]` |                                                                                                                                                                              |
|         |                                                                                 |       |                                                                                                                                                                              |
| **1.D** | **Module 1.S.3: Combined Per-Still Bragg and Pixel Mask Generation**            | `[ ]` |                                                                                                                                                                              |
| **1.D.idl** | **Define/Review Conceptual IDL for `BraggMaskGenerator`**                     | `[ ]` | **Purpose:** Define interface for `BraggMask_2D_raw_i` and `Mask_total_2D_i`. <br>Path for IDL thoughts: `src/diffusepipe/masking/bragg_mask_generator.py` <br>Method (Option A): `generate_bragg_mask_from_spots(experiment: Experiment, reflections: reflection_table, adapter: DIALSGenerateMaskAdapter, config: dict) -> tuple[flex.bool, ...]`. <br>Method (Option B): `generate_bragg_mask_from_shoeboxes(reflections: reflection_table, detector: Detector) -> tuple[flex.bool, ...]`. <br>Method (Combine): `get_total_mask_for_still(bragg_mask: tuple[flex.bool,...], global_pixel_mask: tuple[flex.bool,...]) -> tuple[flex.bool,...]`. <br>Errors: `BraggMaskError`. |
| 1.D.1   | Implement Option A: `generate_bragg_mask_from_spots`                          | `[ ]` | In `src/diffusepipe/masking/bragg_mask_generator.py`. Calls `DIALSGenerateMaskAdapter`. (Based on 1.D.idl)                                                                      |
| 1.D.2   | Implement Option B: `generate_bragg_mask_from_shoeboxes`                      | `[ ]` | In `src/diffusepipe/masking/bragg_mask_generator.py`. (Based on 1.D.idl)                                                                                                     |
| 1.D.2.a |     - Initialize per-panel `flex.bool` masks to `False`.                        | `[ ]` |                                                                                                                                                                              |
| 1.D.2.b |     - Iterate shoeboxes from `reflection_table`.                                  | `[ ]` |                                                                                                                                                                              |
| 1.D.2.c |     - Identify `MaskCode.Foreground` / `MaskCode.Strong` pixels.                  | `[ ]` |                                                                                                                                                                              |
| 1.D.2.d |     - Project 3D mask to 2D panel coordinates & update panel mask.              | `[ ]` |                                                                                                                                                                              |
| 1.D.3   | Implement `get_total_mask_for_still` logic                                      | `[ ]` | In `src/diffusepipe/masking/bragg_mask_generator.py`. (Based on 1.D.idl)                                                                                                     |
| 1.D.4   | **Unit Tests for Bragg Mask Generation**                                        | `[ ]` | Path: `tests/masking/test_bragg_mask_generator.py`                                                                                                                             |
| 1.D.4.a |   - Test Option A: Mock `Experiment`, `reflection_table`, mock adapter.         | `[ ]` |                                                                                                                                                                              |
| 1.D.4.b |   - Test Option B: Create mock `reflection_table` with `Shoebox` objects.       | `[ ]` |                                                                                                                                                                              |
| 1.D.4.c |   - Test `get_total_mask_for_still`: Provide sample masks & verify logic.       | `[ ]` |                                                                                                                                                                              |
|         |                                                                                 |       |                                                                                                                                                                              |
| **1.E** | **Phase 1 Review & Next Steps**                                                 | `[ ]` |                                                                                                                                                                              |
| 1.E.1   | Self-Review: All Phase 1 items addressed? IDLs defined/reviewed?                | `[ ]` |                                                                                                                                                                              |
| 1.E.2   | Context Refresh: Re-read `plan.md` sections for Phase 2 (Data Extraction).      | `[ ]` |                                                                                                                                                                              |
| 1.E.3   | Decision: Proceed to Phase 2 Checklist.                                         | `[ ]` |                                                                                                                                                                              |

---

**Key Changes in this Phase 1 Checklist:**

1.  **IDL Definition/Review Steps (`.idl` suffixed items):** Added explicit steps (e.g., `1.B.idl`, `1.C.idl`, `1.D.idl`) to define or review the conceptual interface *before* Python implementation of the core logic for each module.
    *   These "IDL thoughts" are primarily for clarifying the contract of the Python functions/classes being developed for that module. They might live as comments in the Python files or in the agent's scratchpad initially.
    *   They ensure that the inputs, outputs, core behavior, and error conditions are considered upfront.
2.  **Specific File Paths:** Suggested more specific Python file names for the new modules (e.g., `pixel_mask_generator.py`, `bragg_mask_generator.py`) to better organize the masking logic.
3.  **Configuration Details:** Added notes on where configuration parameters (e.g., for static masks, dynamic mask thresholds) would come from or how they might be structured as inputs to the functions.
4.  **Clarity on "Component":** For 1.S.1, clarified that the "StillProcessorComponent" is more of a conceptual wrapper or a simple orchestrating function that uses the already-developed `DIALSStillsProcessAdapter`. No new complex IDL is needed if the adapter's interface is sufficient.
</file>

<file path="docs/00_START_HERE.md">
# Developer Orientation: Getting Started with DiffusePipe

**Welcome!** This document is your starting point for understanding how we build software in this project. Our approach emphasizes clear specifications and consistent implementation patterns. Please read this guide carefully before diving into the code.


**1. Core Philosophy: Interface Definition Language (IDL) as the Specification**

The cornerstone of our development process is the use of **Interface Definition Language (IDL)** files (e.g., `*_IDL.md` or similar, defining the contract for components).

*   **The IDL is the Contract:** Each IDL file defines a strict **contract** for a specific component (module or interface). It specifies *what* the component must do, its public interface, its expected behavior, its dependencies, and its error conditions. Implementations must not only match the public interface but also adhere to the behavioral descriptions, preconditions, postconditions, and specified data structures (like required dictionary keys) defined in the IDL.
*   **Specification, Not Implementation:** The IDL focuses on the **behavioral specification** and separates it from the *how* (the specific implementation details).
*   **Source of Truth:** When implementing a component, its corresponding IDL file is the authoritative source for its requirements.
*   **Bidirectional Goal:** We use IDLs both to *specify* new components before coding (IDL-to-Code) and to *document* the essential contract of existing components by abstracting away implementation details (Code-to-IDL).

> **Further Reading:** For the detailed syntax and rules for creating/reading IDL specifications, refer to `01_IDL_GUIDELINES.md`.

**2. Understanding the IDL Structure (`*_IDL.md` files)**

When you open an IDL file (e.g., `src/component_x/component_x_IDL.md`), you'll typically find:

*   **Module/Component Path:** Defines the logical grouping, corresponding to the code module path (e.g., `src.component.path`).
*   **Dependency Declarations (e.g., `@depends_on(...)`):** Declares dependencies on *other IDL-defined interfaces/modules* or *abstract external resources* (like `FileSystem`, `Database`, `ExternalAPI`). These signal required interactions.
*   **Interface/Class Definition:** Defines the contract for a specific class or a collection of related functions. The name typically matches the code class/module name.
*   **Method/Function Definitions (`returnType methodName(...)`)**:
    *   **Signature:** Specifies the method name, parameter names, parameter types (`string`, `int`, `list<Type>`, `dict<Key,Val>`, `optional Type`, `union<T1, T2>`, `object` for other interfaces), and the return type.
    *   **Documentation Block (CRITICAL - This IS the Spec!):**
        *   **`Preconditions:`**: What must be true *before* calling the method.
        *   **`Postconditions:`**: What is guaranteed *after* the method succeeds (return values, state changes).
        *   **`Behavior:`**: A description of the essential logic/algorithm the method performs and how it interacts with dependencies.
        *   **`Expected Data Format: { ... }`**: If a parameter or return type is a complex dictionary/object, its structure is defined here (e.g., using JSON-like notation or by referencing a defined `struct`).
        *   **`@raises_error(condition="ErrorCode", ...)` (or similar annotation):** Defines specific, contractual error conditions the method might signal.
    *   **`Invariants:`** (Optional): Properties of the component's state that should always hold true between method calls.
*   **Custom Data Structure Definitions (e.g., `struct StructName { ... }`):** Defines reusable, complex data structures. These might be defined within the IDL file itself or in a central types definition file (e.g., `ARCHITECTURE/types.md`) for globally shared types.
*   **Component Interactions Section (Optional):**
    *   Provides supplementary documentation about how the defined module or interface interacts with its dependencies for common or complex workflows.
    *   Often includes **sequence diagrams (e.g., using Mermaid syntax)** and textual explanations to clarify call chains and data flow between components.

**3. The Development Workflow: Implementing from IDL Specifications**

When assigned to implement or modify a component specified by an IDL (or tackling a new feature):

**Phase 0: Preparation & Understanding**

1.  **Define Task:** Clearly understand the goal (e.g., from an issue tracker, user story).
2.  **Locate/Review IDL:** Find the relevant IDL file(s). If modifying existing code without a complete IDL, consider generating/updating the IDL first (Code-to-IDL).
3.  **Understand Contract & Interactions:** Thoroughly read the IDL: purpose, dependencies, function/method signatures, and especially the documentation blocks (`Preconditions`, `Postconditions`, `Behavior`, `Expected Data Format`, error conditions). Look for any "Component Interactions" section. Pay close attention to dependency declarations.
4.  **Review Rules:** Briefly refresh understanding of key guidelines in `02_IMPLEMENTATION_RULES.md` (especially Testing) and `03_PROJECT_RULES.md`.
5.  **Outline Testing Strategy:** Based on the IDL dependencies and testing guidelines, outline the primary testing approach.
6.  **Setup Working Memory:** Update your working memory log (e.g., `TEMPLATES/WORKING_MEMORY_LOG_TEMPLATE.md`) with your "Current Task/Focus", initial "Testing Strategy Outline", and initial "Next Steps".

**Phase 1: Implementation & Testing ("Main Step")**

7.  **Refine Testing Strategy & Identify Mock Targets:** Solidify the testing approach. If using mocks, explicitly identify patch targets.
8.  **Create/Modify Files:** Ensure code file/directory structure matches the IDL module path.
9.  **Implement Structure & Dependencies:** Create Python classes/functions. Implement constructors, injecting dependencies specified in the IDL.
10. **Implement Functions/Methods:** Define signatures exactly matching the IDL. Implement logic described in `Behavior`. Use defined data models (e.g., Pydantic) for complex data. Fulfill `Postconditions`. Respect `Preconditions`. Implement error handling for specified conditions.
11. **Write Tests (Unit and/or Integration):** Implement tests verifying the implementation against the *entire* IDL contract. Follow Arrange-Act-Assert. Use fixtures.
12. **Log Progress:** Update your working memory log.

**Phase 2: Finalization & Sanity Checks ("Cleanup Step")**

13. **Format Code:** Run the project's code formatter.
14. **Lint Code:** Run the project's linter and address issues.
15. **Run All Tests:** Execute the full test suite.
16. **Perform Sanity Checks:** Self-review. Check against `02_IMPLEMENTATION_RULES.md` and `03_PROJECT_RULES.md`. Update directory structure docs if needed. Verify code still matches the IDL contract. Check related configuration files (e.g., test setups).
17. **Finalize Working Memory:** Update with final status and thoughts.

---

**4. Visual Diagnostics & Testing**

*   **Visual Diagnostics Guide:** See `docs/VISUAL_DIAGNOSTICS_GUIDE.md` for comprehensive documentation on visual verification tools for Phase 2 data extraction.
*   **End-to-End Testing:** Use `scripts/dev_workflows/run_phase2_e2e_visual_check.py` for complete pipeline verification.
*   **Diagnostic Tools:** Use `scripts/visual_diagnostics/check_diffuse_extraction.py` for detailed analysis of extraction outputs.
*   **DIALS Debugging:** See `docs/06_DIALS_DEBUGGING_GUIDE.md` for troubleshooting DIALS integration issues.

---

**5. Key Coding Standards**

*   **Python Version:** `3.10+`
*   **Formatting:** PEP 8 compliant, enforced by `Black` & `Ruff` (run `make format` / `make lint` or equivalent).
*   **Type Hinting:** **Mandatory** for all signatures. Use standard `typing` types. Be specific.
*   **Docstrings:** **Mandatory** for all modules, classes, functions, methods. Use **Google Style** docstrings.
*   **Imports:** Use **absolute imports** from `src` (or your project's source root). Group imports correctly. Top-level only unless exceptional reason exists (document it).
*   **Naming:** PEP 8 (snake_case for functions/variables, CamelCase for classes). Use descriptive names.
*   **Logging:** Use the standard `logging` module. Configure logging early in entry-point scripts as detailed in `02_IMPLEMENTATION_RULES.md`.

> **Further Reading:** See `02_IMPLEMENTATION_RULES.md` and `03_PROJECT_RULES.md` for complete details.

**5. Important Patterns & Principles**

*   **Parse, Don't Validate (e.g., with Pydantic):**
    *   **Concept:** Instead of validating raw data (like dicts) inside your logic, parse it into a well-defined model (e.g., a Pydantic `BaseModel`) at the component boundary.
    *   **Practice:** Define models for complex data structures. Use model validation methods to parse inputs.
    *   **Reference:** See relevant section in `02_IMPLEMENTATION_RULES.md`.
*   **Dependency Injection:**
    *   **Concept:** Components receive their dependencies via their constructor (`__init__`). Avoid global state or direct instantiation of dependencies within methods.
    *   **Practice:** Identify dependencies from IDLs. Add parameters to `__init__`. Store references as instance attributes.
*   **Separation of Atomic Units & Composition Logic:**
    *   **Concept:** Define small, focused, reusable units of functionality ("atomic units" or "primitives"). Use a separate composition layer or language (e.g., a DSL, Python scripting) to orchestrate these units into complex workflows.
    *   **Practice:** Ensure atomic units only use explicitly passed parameters. Ensure the composition layer handles all sequencing, conditionals, and loops.
*   **Interaction with External Services/APIs (e.g., LLMs, Databases):**
    *   **Standard:** Use a dedicated manager or bridge class to encapsulate interactions with significant external services.
    *   **Practice:** This manager class handles connection details, API call formatting, and basic response parsing. Other components use this manager rather than interacting directly with the external service's raw API.
    *   **Reference:** See relevant section in `02_IMPLEMENTATION_RULES.md` and project-specific library integration guides (e.g., in `LIBRARY_INTEGRATION/`).
*   **Integration with DIALS via Python API:**
    *   **Standard:** Interaction with DIALS crystallography software for initial image processing (spot finding, indexing, integration) is handled by a Python orchestrator (`StillsPipelineOrchestrator`). This orchestrator first determines the data type (true still or sequence data) by inspecting the CBF image header (Module 1.S.0).
    *   For **true still images** (Angle_increment = 0.0°), it uses the `dials.stills_process` Python API via a dedicated adapter (`DIALSStillsProcessAdapter`).
    *   For **sequence data** (Angle_increment > 0.0°), it uses a sequential DIALS command-line workflow (import → find_spots → index → integrate) via a separate adapter (`DIALSSequenceProcessAdapter`).
    *   Both adapters produce consistent output objects (`ExperimentList`, `reflection_table`) for downstream processing.
    *   **Practice:** The Python orchestrator configures and invokes the appropriate DIALS adapter based on data type. These adapters manage the internal DIALS workflow. Subsequent Python modules (`DataExtractor`, `ConsistencyChecker`, etc.) process and analyze the Python objects (e.g., DIALS `ExperimentList`, `reflection_table`) and files produced by this DIALS processing stage.
    *   **Geometric Corrections:** The `DataExtractor` leverages the robust DIALS `Corrections` API for Lorentz-Polarization and Quantum Efficiency corrections, while implementing custom calculations only for Solid Angle and Air Attenuation corrections specific to diffuse pixels.
    *   **Configuration:** PHIL files (e.g., in `src/diffusepipe/config/` or specified by the user) provide standardized parameter settings for `dials.stills_process`.
*   **Host Language Orchestration with DSL/Script Evaluation (if applicable):**
    *   **Concept:** For projects with an embedded Domain-Specific Language (DSL) or scripting capability, use the host language (e.g., Python) for complex data preparation. The DSL/script then focuses on orchestration, referencing data prepared by the host language.
    *   **Practice:** Python code prepares data, creates an environment for the DSL, binds data to variables, and then calls the DSL evaluator.
    *   **Reference:** See relevant section in `02_IMPLEMENTATION_RULES.md`.

**6. Testing Strategy**

*   **Framework:** `pytest`.
*   **Focus:** **Strongly emphasize Integration and Functional/End-to-End tests** over isolated unit tests. Components should be tested together to verify they work correctly according to their IDL contracts in realistic scenarios.
*   **Real Components Over Mocks:** **Avoid mocks whenever possible.** Use actual component implementations in tests to ensure behavior matches production. The goal is to verify real interactions, not theoretical ones.
*   **When to Use Real Components:**
    *   For all internal project components
    *   For file system operations (when reasonable)
    *   For database operations (using test databases)
    *   For any component where behavior can be realistically simulated
*   **Limited Mock Usage:** Only mock external services when absolutely necessary:
    *   Third-party APIs with usage limits or authentication requirements
    *   Services requiring complex infrastructure that can't be containerized
    *   Components with non-deterministic behavior that can't be controlled
*   **Error Path Testing:** Explicitly test error handling and propagation with real components.
*   **Fixtures:** Use testing framework fixtures for setup of test environments and dependencies.
*   **Structure:** Follow the `Arrange-Act-Assert` pattern. Mirror the `src` directory structure in `tests`.

> **Further Reading:** See relevant section in `02_IMPLEMENTATION_RULES.md`.

**7. Project Navigation (Actual Structure)**

*   **`src/` (source root):** Main application source code.
    *   `src/diffusepipe/`: Primary package
        *   `__init__.py`
        *   `adapters/`: DIALS/DXTBX API wrappers
            *   `dials_stills_process_adapter.py` and `*_IDL.md`
            *   `dials_sequence_process_adapter.py` and `*_IDL.md`
            *   `dials_generate_mask_adapter.py` and `*_IDL.md`
            *   `dxtbx_io_adapter.py` and `*_IDL.md`
        *   `config/`: Configuration files (PHIL files)
            *   `find_spots.phil`
            *   `refine_detector.phil`
            *   `sequence_*_default.phil` files
        *   `crystallography/`: Crystal model processing and validation
            *   `q_consistency_checker.py` and `*_IDL.md`
            *   `still_processing_and_validation.py` and `*_IDL.md`
        *   `diagnostics/`: Diagnostic tools
            *   `q_calculator.py` and `*_IDL.md`
        *   `extraction/`: Data extraction components
            *   `data_extractor.py` and `*_IDL.md`
        *   `masking/`: Pixel and Bragg mask generation
            *   `pixel_mask_generator.py` and `*_IDL.md`
            *   `bragg_mask_generator.py` and `*_IDL.md`
        *   `orchestration/`: Pipeline coordination
            *   `pipeline_orchestrator_IDL.md`
            *   `stills_pipeline_orchestrator_IDL.md`
        *   `types/`: Data type definitions
            *   `types_IDL.py` and `types_IDL.md`
        *   `utils/`: Utility functions
            *   `cbf_utils.py`
    *   `src/scripts/`: Processing scripts
        *   `process_pipeline.sh`: Main processing script
*   **`scripts/`**: Development and diagnostic scripts
    *   `scripts/dev_workflows/run_phase2_e2e_visual_check.py`: End-to-end pipeline verification
    *   `scripts/visual_diagnostics/check_diffuse_extraction.py`: Diagnostic plot generation
*   **`tests/`**: Test suite (integration-focused)
    *   Mirrors `src/` structure for easy navigation
    *   `tests/data/`: Test data files
*   **`libdocs/`**: External library documentation
    *   `libdocs/dials/`: DIALS/CCTBX/DXTBX documentation
*   **Project Configuration:**
    *   `CLAUDE.md`: Instructions for Claude AI assistant
    *   `plan.md`: Master technical implementation specification
    *   `pyproject.toml`: Python project configuration
*   **`docs/`**: All project documentation.
    *   `docs/00_START_HERE.md`: This file (developer onboarding guide)
    *   `docs/01_IDL_GUIDELINES.md`: IDL structure and syntax guidelines
    *   `docs/02_IMPLEMENTATION_RULES.md`: Code implementation standards
    *   `docs/03_PROJECT_RULES.md`: Project workflow and organization
    *   `docs/04_REFACTORING_GUIDE.md`: Guidelines for code refactoring
    *   `docs/05_DOCUMENTATION_GUIDE.md`: Documentation standards
    *   `docs/06_DIALS_DEBUGGING_GUIDE.md`: DIALS integration troubleshooting
    *   `docs/VISUAL_DIAGNOSTICS_GUIDE.md`: Visual verification tools for Phase 2
    *   `docs/LESSONS_LEARNED.md`: Project development insights
    *   `docs/ARCHITECTURE/`: Architecture documentation
        *   `adr/`: Architecture Decision Records
        *   `types.md`: Shared data structure definitions
    *   `docs/LIBRARY_INTEGRATION/`: External library integration guides
    *   `docs/TEMPLATES/`: Document templates
    *   `docs/WORKFLOWS/`: Workflow documentation
*   **`README.md`**: Top-level project overview.

**8. Development Workflow & Recommended Practices**

*   **Follow the IDL Specification:** Adhere strictly to the IDL for the component.
*   **Plan Your Tests:** Consider testing strategy *before* implementation.
*   **Use Working Memory:** Maintain a log of your progress (see `TEMPLATES/WORKING_MEMORY_LOG_TEMPLATE.md`).
*   **Be Aware of Existing Code & Configuration:** Consider impacts on related code and config files.
*   **Test Driven (where practical):** Write tests to verify against the IDL contract.
*   **Commit Often:** Small, logical commits with clear messages.
*   **Format and Lint:** Before committing.
*   **Ask Questions:** If unsure about requirements or design.

---

**9. External Dependencies**

*   **DIALS Crystallography Software:** The project relies heavily on DIALS for crystallographic data processing.
    *   Must be installed and available in the system PATH
    *   Commands used include: `dials.import`, `dials.find_spots`, `dials.index`, `dials.refine`, and `dials.generate_mask`
*   **PDB Files:** The project uses PDB files (e.g., `6o2h.pdb`) for crystallographic consistency checks.
    *   Used by the Python scripts to validate processing results against known structures
    *   Must be provided via the `--external_pdb` parameter to the processing script
*   **Python Dependencies:**
    *   `dxtbx`: DIALS Toolbox for image format reading
    *   `cctbx`: Computational Crystallography Toolbox
    *   `numpy`, `scipy`: Numerical processing
    *   `matplotlib`: Optional visualization
    *   `tqdm`: Progress bars

**10. Getting Started Checklist**

1.  [ ] Read this document (`00_START_HERE.md`).
2.  [ ] Read `01_IDL_GUIDELINES.md` to understand IDL guidelines.
3.  [ ] Read `02_IMPLEMENTATION_RULES.md` for detailed coding/testing rules.
4.  [ ] Read `03_PROJECT_RULES.md` for project structure and workflow.
5.  [ ] Review the main `README.md` and key architecture diagrams (e.g., in `docs/ARCHITECTURE/`).
6.  [ ] Set up your local development environment (`Python 3.10+`, DIALS, dependencies).
7.  [ ] Browse the `src/` directory and a few IDL files to see the structure.
8.  [ ] Try running the processing script with a test CBF file and PDB file.
9.  [ ] Ask questions!

Welcome aboard! By following these guidelines, we can build a robust, maintainable, and consistent system together.
</file>

<file path="docs/02_IMPLEMENTATION_RULES.md">
# Implementation Rules and Developer Guidelines

**1. Purpose**

This document outlines the standard conventions, patterns, and rules for implementing code (primarily Python, but principles can be adapted) within this project. Adhering to these guidelines ensures consistency, maintainability, testability, and portability across the codebase, especially when translating IDL specifications into concrete implementations.

**2. Core Principles**

*   **Consistency:** Code should look and feel consistent regardless of who wrote it. Follow established patterns and conventions.
*   **Clarity & Readability:** Prioritize clear, understandable code over overly clever or complex solutions. Code is read more often than it is written.
*   **Simplicity (KISS & YAGNI):** Implement the simplest solution that meets the current requirements. Avoid unnecessary complexity or features. (See `03_PROJECT_RULES.md` for details).
*   **Testability:** Design code with testing in mind. Use dependency injection and avoid tight coupling.
*   **Portability (Conceptual):** While implementing in a specific language (e.g., Python), aim for logic and structures that are conceptually portable to other common languages if future needs arise. Minimize reliance on highly language-specific idioms where simpler, universal constructs exist for core logic.
*   **Parse, Don't Validate:** Structure data transformations such that input is parsed into well-defined, type-safe structures upfront, minimizing the need for scattered validation checks later in the code. (See Section 5).

**3. Project Structure and Imports**

*   **Directory Structure:** Strictly follow the established directory structure outlined in `03_PROJECT_RULES.md`. Place new modules and files in their logical component directories.
*   **Import Conventions (Python Example):**
    *   Use **absolute imports** starting from the `src` directory (or your project's source root) for all internal project modules.
        ```python
        # Good (assuming 'src' is the root of your package)
        from src.components.my_component import MyClass
        from src.utils.errors import CustomError

        # Bad (Avoid relative imports that traverse too many levels or are ambiguous)
        # from ..utils.errors import CustomError
        ```
    *   Group imports in the standard order: standard library, third-party, project-specific.
    *   Place imports at the top of the module. Avoid imports inside functions/methods unless absolutely necessary for specific reasons (e.g., avoiding circular dependencies, optional heavy imports) and document the reason clearly.

**4. Safe Refactoring Practices**

*   **Incremental Approach:** When refactoring large methods or classes (>100 lines), work in small increments of 5-10 lines at a time. Never attempt to replace 300+ lines in a single operation.
*   **Syntax Verification:** After each change, verify file syntax immediately: `python -m py_compile file.py`. Do not proceed to the next change until syntax is confirmed correct.
*   **Frequent Commits:** Commit working code after each successful incremental change. This provides recovery points if later changes introduce issues.
*   **Extract First, Then Modify:** When moving code to new files, create and test the new file completely before modifying the original. Never perform both operations simultaneously.
*   **Verify Imports:** After any refactoring involving imports, test that all imports work before proceeding with method calls or class usage.
*   **Avoid Complex String Replacements:** For large method extractions, manual verification is safer than complex multi-line string matching operations.

**Critical Warning Signs:**
*   File "appears" to work but contains syntax errors in unused code paths
*   Duplicate method definitions with different signatures
*   Orphaned code blocks with incorrect indentation
*   Import statements without corresponding class usage

**Recovery Strategy:**
*   If refactoring corruption occurs, immediately revert to the last known good state using version control
*   Restart refactoring with smaller incremental changes
*   Use proper IDE/editor with syntax highlighting and error detection

**5. Coding Style and Formatting**

*   **Language Standards:** Strictly adhere to the idiomatic style guidelines for your project's primary language (e.g., PEP 8 for Python). Use linters and formatters.
*   **Type Hinting (Python Example):**
    *   **Mandatory (if language supports):** All function and method signatures (parameters and return types) **must** include type hints.
    *   Use appropriate types for optional parameters/nullable types.
    *   Use specific types rather than generic "any" types whenever possible.
    *   For complex dictionary structures passed as parameters (especially from IDL "Expected Data Format"), define a `TypedDict`, data class, or Pydantic model for clarity and validation.
*   **Docstrings/Comments:**
    *   **Mandatory:** All modules, classes, functions, and methods must have clear documentation (e.g., docstrings in Python).
    *   Use a consistent style (e.g., Google Style for Python docstrings).
    *   Clearly document parameters, return values, and any exceptions/errors raised.
    *   Explain the *purpose* and *behavior*, not just *what* the code does.
*   **Vectorization with NumPy:**
    *   **Principle:** For operations involving numerical data, especially on arrays or sequences of numbers (e.g., pixel data, lists of coordinates, q-vectors), prioritize vectorized operations using NumPy over explicit Python loops where feasible and sensible.
    *   **Benefits:** NumPy vectorization typically leads to more concise, readable, and significantly more performant code for numerical tasks.
    *   **Practice:** Identify opportunities to replace loops that perform element-wise arithmetic or apply mathematical functions to sequences with equivalent NumPy array operations.
        ```python
        # Less Preferred (Python loop)
        # result = []
        # for x, y in zip(list_a, list_b):
        #     result.append(x * y + 5)

        # Preferred (NumPy vectorization)
        # import numpy as np
        # array_a = np.array(list_a)
        # array_b = np.array(list_b)
        # result_array = array_a * array_b + 5
        ```
    *   **Consideration:** While vectorization is generally preferred for performance, ensure that its use does not unduly obfuscate the logic for very complex, non-standard operations. Balance performance with clarity. DIALS `flex` arrays also offer vectorized operations and should be used when interacting directly with DIALS data structures.

**5.1 Performance Optimization Strategies**

**Vectorization Success Patterns:**
*   **Vectorize by Default:** When in doubt, prefer vectorized implementations. Only use iterative approaches during initial prototyping or for very complex non-standard operations.
*   **Batch Processing:** Process coordinates, q-vectors, and corrections as arrays rather than individual elements. Group operations that can be parallelized.
*   **Memory Efficiency:** Minimize Python loop overhead through vectorized NumPy operations. Use array-based data structures over lists of individual objects.
*   **Algorithmic Validation:** Test vectorized implementations against known reference results or simplified test cases to ensure correctness.

**Performance Measurement Framework:**
*   **Structured Testing:** Document speedups with before/after timing measurements using consistent test conditions.
*   **Realistic Data Sizes:** Test with actual detector geometries and data volumes representative of production use.
*   **Performance Characterization:** Document specific improvements with quantified metrics (e.g., "2.4x speedup: 4.0s → 1.7s").
*   **Scalability Validation:** Ensure optimizations work across different data scales and don't degrade with larger inputs.

**Example Vectorization Pattern:**
```python
# Before: Python loop (slower)
result = []
for x, y in zip(list_a, list_b):
    result.append(complex_calculation(x, y))

# After: Vectorized operation (faster)
array_a = np.array(list_a)
array_b = np.array(list_b)
result_array = vectorized_complex_calculation(array_a, array_b)

# Verification: Prove equivalence (critical)
assert np.allclose(result, result_array, rtol=1e-15)
```

**Optimization Implementation Guidelines:**
*   **Profile First:** Identify actual bottlenecks through profiling before optimizing
*   **Vectorize First:** Implement vectorized solutions directly rather than iterative approaches
*   **Validate Results:** Ensure vectorized code produces correct results using reference data or simplified test cases
*   **Document Benefits:** Record specific performance improvements achieved
*   **Test Edge Cases:** Verify optimizations work correctly with boundary conditions

**5.2 Scientific Accuracy Implementation Guidelines**

**NIST Data Integration Standards:**
*   **Reference Data Sources:** Use tabulated NIST X-ray mass attenuation coefficients over rough approximations for critical calculations.
*   **Proper Atmospheric Composition:** Standard dry air by mass (N: 78.084%, O: 20.946%, Ar: 0.934%, C: 0.036%) with correct molar masses.
*   **Thermodynamic Accuracy:** Implement ideal gas law with configurable temperature and pressure parameters for environmental calculations.
*   **Energy Range Coverage:** Support wide energy ranges (e.g., 1-100 keV) with appropriate interpolation methods for accuracy.

**Validation Requirements:**
*   **Reference Value Testing:** Validate calculated values against NIST reference data within 1% tolerance where possible.
*   **Cross-Validation:** Compare against known theoretical values from authoritative scientific literature.
*   **Unit Conversion Precision:** Ensure proper dimensional consistency throughout calculations (e.g., mm to m conversions).
*   **Configurable Parameters:** Support variable environmental conditions (temperature, pressure, humidity) rather than hard-coded values.

**Scientific Enhancement Process:**
1. **Identify Approximations:** Locate rough approximations or "magic numbers" in existing code that could benefit from scientific accuracy.
2. **Research Authoritative Sources:** Find official data from NIST, IUCR, or other recognized scientific institutions.
3. **Implement Accurate Calculations:** Replace approximations with scientifically validated formulas and tabulated data.
4. **Create Validation Tests:** Develop comprehensive tests against reference data with appropriate tolerances.
5. **Document Scientific Basis:** Include references to data sources, formulas used, and any assumptions made.

**Example: Air Attenuation Correction Enhancement:**
```python
# Before: Rough approximation
air_correction = wavelength**3 * 0.001  # Magic number

# After: NIST-based calculation
def calculate_air_attenuation(wavelength_angstrom, path_length_mm, 
                            temperature_k=293.15, pressure_atm=1.0):
    """Calculate air attenuation using NIST mass attenuation data"""
    energy_kev = 12.398 / wavelength_angstrom
    mu_air = get_nist_mass_attenuation_coefficient(energy_kev)
    air_density = calculate_air_density(temperature_k, pressure_atm)
    path_length_m = path_length_mm / 1000.0
    return np.exp(-mu_air * air_density * path_length_m)
```

*   **Naming:** Follow language-standard naming conventions (e.g., snake_case for Python variables/functions, CamelCase for Python classes). Use descriptive names.

**5. Data Handling: Parse, Don't Validate (Leveraging Models like Pydantic)**

*   **Principle:** Instead of passing raw dictionaries or loosely typed data and validating fields throughout the code, parse external/untrusted data (e.g., API responses, configuration files, parameters described via IDL "Expected Data Format") into **well-defined data models** (e.g., Pydantic models in Python, or data classes/structs) at the boundaries of your system or component.
*   **Benefits:**
    *   **Upfront Validation:** Data validity is checked once during parsing/instantiation.
    *   **Type Safety:** Subsequent code operates on validated, type-hinted objects.
    *   **Reduced Boilerplate:** Eliminates repetitive validation checks.
    *   **Clear Data Contracts:** Models serve as clear definitions of expected data structures.
*   **Implementation (Python/Pydantic Example):**
    *   Define Pydantic `BaseModel` subclasses (or equivalent in your language) for structured data.
    *   Use these models in function signatures where appropriate.
    *   Parse incoming data using model validation methods (e.g., `YourModel.model_validate(data)`).
    *   Handle validation errors (e.g., `pydantic.ValidationError`) during parsing to manage invalid input gracefully.

    ```python
    from pydantic import BaseModel, ValidationError
    from typing import Optional

    # IDL: Expected Data Format: { "name": "string", "retries": "int" }
    class TaskParams(BaseModel):
        name: str
        retries: Optional[int] = 3 # Example with default

    def process_task(raw_params: dict):
        try:
            params = TaskParams.model_validate(raw_params)
            print(f"Processing task: {params.name} with {params.retries} retries")
            # ... logic using validated params ...
        except ValidationError as e:
            print(f"Invalid task parameters: {e}")
            # Handle error
    ```
*   **Understanding External Library Data Structures:** When your component consumes objects directly instantiated or returned by an external library, consult that library's documentation to understand their precise structure, attributes, and access methods. Do not assume a generic structure. This understanding should inform both implementation and test case design.
    *   **Action:** During preparation, if your component relies on specific object types from an external library, review the documentation for those object types.

**5.x Dependency Injection & Initialization**

*   **Constructor/Setter Injection:** Components MUST receive their runtime dependencies (other components, resources specified in IDL `@depends_on`) via their constructor or dedicated setter methods. Avoid complex internal logic within components to locate or instantiate their own major dependencies.
*   **Initialization Order:** In orchestrating components (like a main `Application` class), instantiate dependencies in the correct order *before* injecting them into dependent components that require them during their own initialization.
*   **Circular Dependencies:** Be vigilant for circular import dependencies. Minimize top-level imports in modules involved in complex interactions; prefer imports inside methods/functions where feasible. Use string type hints or forward references if supported by the language to break cycles needed only for type checking. If cycles are identified, prioritize refactoring.
*   **Test Instantiation:** Include tests verifying that components can be instantiated correctly with their required (real or mocked) dependencies.

**6. External Service Interaction (e.g., LLMs, APIs, Databases)**

*   **Standard:** Use dedicated "Manager" or "Bridge" classes to encapsulate interactions with significant external services or APIs.
*   **Implementation Pattern:**
    *   A Manager/Bridge class is responsible for handling connection details, API call formatting, authentication, and basic request/response processing for a specific external service.
    *   Other components in the system use this Manager/Bridge class rather than interacting directly with the external service's raw API or client library.
    *   The Manager/Bridge class itself might use a specific client library for the external service (e.g., `requests`, `boto3`, `pydantic-ai`).
*   **Structured Output/Input:** If the external service supports or requires structured data (e.g., JSON schemas for input/output), leverage this. If using Pydantic, you can define models for these schemas and use them in your Manager/Bridge.
    *   **Schema-to-Model Resolution (If applicable):** If task/service definitions include references to data schemas (e.g., by name or path), implement a helper function (e.g., `resolve_model_class`) to dynamically load the corresponding Pydantic model or data class.
*   **Reference:** Familiarize yourself with the chosen client libraries. Document key usage patterns or link to official documentation in `LIBRARY_INTEGRATION/`.
*   **Verify Library Usage:** **Crucially, when integrating *any* significant third-party library, carefully verify API usage** (function signatures, required arguments, expected data formats, object constructor parameters) against the library's official documentation for the specific version being used.
*   **Test Wrapper Interactions:** Manager/Bridge classes should have targeted integration tests verifying their interaction with the (mocked) external service endpoint.

**7. Testing Conventions**

*   **Framework:** Use a standard testing framework (e.g., `pytest` for Python).
*   **Prioritize Integration and Functional Tests:**
    *   **Core Principle:** Integration tests with real components should be the foundation of your testing strategy. Test components together as they would operate in production to verify their interactions fulfill IDL contracts.
    *   **Real-World Scenarios:** Design tests around realistic workflows that exercise multiple components working together.
    *   **No Mock Chains:** Never use chains of mocks where one mock returns another mock. This creates tests that pass but don't validate actual behavior.

*   **Avoiding Mocks - Preferred Alternatives:**
    *   **Use Real Components:** Whenever possible, instantiate and use actual component implementations in tests rather than mocks.
    *   **Test Databases:** Use ephemeral test databases (e.g., SQLite in-memory, containerized PostgreSQL) rather than mocking database interactions.
    *   **File System:** Use temporary directories and files rather than mocking file system operations.
    *   **Test Fixtures:** Create comprehensive fixtures that provide real test data and properly configured components.
    *   **Test Doubles:** When necessary, prefer simple stubs or fakes that implement the same interface as the real component but with simplified behavior, rather than mocks with complex expectations.

*   **Limited Mocking - Only When Necessary:**
    *   **External API Boundaries:** Mock third-party APIs with usage limits, authentication requirements, or that require complex infrastructure that can't be containerized.
    *   **Non-Deterministic Components:** Mock components whose behavior cannot be controlled deterministically in a test environment (e.g., random number generators, time-dependent operations).
    *   **Expensive Resources:** Mock resources that are prohibitively expensive to create for each test run and cannot be reasonably containerized.

*   **When Mocking Is Unavoidable:**
    *   **Patch Where It's Looked Up:** When using `patch` (e.g., `unittest.mock.patch`), the `target` string must be the path to the object *where it is looked up/imported*, not necessarily where it was defined.
    *   **Prefer Dependency Injection:** For classes using Dependency Injection, pass test doubles into the constructor during test setup rather than patching.
    *   **Ensure Type Fidelity:** When mocking external libraries, ensure mock return values match the **expected type** returned by the real library. Use real library types for mock data if possible.
    *   **Verify Critical Dependencies:** Ensure critical runtime dependencies are installed and importable in the test environment.
    *   **Test Boundaries Separately:** Test argument preparation logic for external API calls separately from the external call itself.
*   **Test Doubles:** Use appropriate test doubles (Stubs, Mocks, Fakes).
*   **Arrange-Act-Assert:** Structure tests clearly.
*   **Fixtures:** Use testing framework fixtures for setup.
*   **Markers:** Use markers to categorize tests (e.g., `@pytest.mark.integration`).
*   **Testing Error Conditions:**
    *   Verify overall failure status (e.g., `result_status == "FAILED"`).
    *   Prefer asserting error type/reason codes over exact message strings.
    *   Check key details in structured error objects.
    *   Use message substring checks sparingly.
    *   Test exception raising using appropriate framework mechanisms (e.g., `pytest.raises`).
    *   When asserting complex return structures (e.g., dictionaries from Pydantic models), be mindful of serialization effects and assert against the actual returned structure.
*   **Unit Test Complex Logic:** Complex internal algorithms or utility functions should have dedicated unit tests.
*   **Debugging Mock Failures & Test Failures:** Systematically inspect mock attributes, call logs, and actual vs. expected values when tests fail.
*   **Test Setup for Error Conditions:** Ensure tests for error handling satisfy preconditions up to the point where the error is expected.
*   **Testing Configurable Behavior and Constants:** Write assertions that test behavioral outcomes rather than being rigidly tied to exact constant values. Review tests when constants change.

**7.1 C++ Backend Object Compatibility Testing**

When testing components that interact with DIALS or other libraries with C++ backends, special considerations apply:

**DIALS C++ Integration Requirements:**
*   **Real Class Mocking:** Create actual Python classes that mimic C++ object interfaces instead of using MagicMock for constructors that will be passed to C++ code.
*   **Type Compatibility:** Ensure mock objects work with `isinstance()` checks and C++ type conversion requirements that occur during library calls.
*   **Constructor vs Method Patching:** Distinguish between patching methods (use `patch.object`) and classes (use proper mock classes with required magic methods).
*   **Magic Method Implementation:** Add proper `__len__`, `__getitem__`, `__iter__`, and other magic methods to mock classes as required by the actual usage patterns.

**Example C++ Compatible Mock Class:**
```python
class MockExperimentList:
    """Mock class compatible with DIALS C++ ExperimentList constructor"""
    def __init__(self, experiments=None):
        self.experiments = experiments or []
    
    def __len__(self):
        return len(self.experiments)
    
    def __getitem__(self, index):
        return self.experiments[index]
    
    def __iter__(self):
        return iter(self.experiments)
    
    # Required for isinstance() compatibility with C++ backends
```

**Common C++ Integration Errors to Avoid:**
*   `ExperimentList([MagicMock])` fails due to C++ backend requiring real Experiment objects
*   Mock objects lacking proper structure to work with `isinstance()` checks
*   Incomplete mock setup for vectorized operations in `dials.array_family.flex` modules
*   Wrong patch targets (patching module-level imports instead of actual method calls)

**Proven Fix Patterns:**
*   Use `patch.object(adapter, '_method_name')` for internal method mocking rather than module-level patching
*   Create comprehensive `flex` module mocks with proper `bool`, `grid`, and `int` mock setup
*   Test import error scenarios using `builtins.__import__` patching with proper `sys.modules` cleanup
*   Enhance reflections mocks with proper `__contains__` and `__getitem__` setup for real implementation compatibility

**7.2 Test Suite Remediation Methodology**

When facing systematic test failures, use this proven remediation strategy:

**Systematic Failure Analysis:**
*   **Failure Categorization:** Group test failures by root cause (API changes, mock strategy issues, assertion problems, import errors).
*   **Mock Evolution Priority:** Transition systematically: Mock → MagicMock → Real Components as appropriate for each test case.
*   **Realistic Bounds:** Update assertions to match actual system behavior and detector physics (e.g., solid angle corrections < 3e6, not arbitrary small values).
*   **Error Handling Enhancement:** Improve bounds checking and edge case handling throughout test implementations.

**Proven Remediation Process:**
1. **Categorize Failures:** Group similar failures by failure type (import errors, mock issues, assertion problems, API incompatibilities).
2. **Fix by Category:** Apply targeted fixes to each category rather than attempting wholesale changes.
3. **Real Component Integration:** Replace complex mocking with actual components where feasible for more authentic testing.
4. **Iterative Validation:** Test each fix independently before proceeding to ensure no regressions are introduced.
5. **Regression Prevention:** Ensure fixes don't break existing passing tests - run full test suite after each category fix.

**Test Authenticity Guidelines:**
*   **Real Components Over Mocks:** Use actual DIALS `flex` arrays and objects instead of complex mock hierarchies where possible.
*   **API Compatibility:** Fix DIALS import issues and method signatures systematically when library versions change.
*   **Magic Method Support:** Use `MagicMock` for objects requiring `__getitem__`, `__and__`, `__or__`, `__iter__` operations.
*   **Bounds Validation:** Use realistic tolerances based on actual detector geometries and physical correction factors.

**Example Successful Remediation Pattern:**
```python
# Before: Complex mock failing with AttributeError
mock_detector = Mock()
mock_detector.__getitem__ = Mock(side_effect=AttributeError)

# After: MagicMock with proper magic method support
mock_detector = MagicMock()
mock_detector.__getitem__.return_value = mock_panel
mock_detector.__iter__.return_value = iter([mock_panel])

# Best: Real component when feasible
real_detector_data = flex.bool(flex.grid(height, width), True)
```

**Success Metrics:**
*   **Quantified Improvement:** Track test pass rates (e.g., "64% reduction in failures: 22 → 8")
*   **Stability Validation:** Ensure remediated tests pass consistently across multiple runs
*   **Regression Monitoring:** Verify that fixes don't introduce new failure modes
*   **Integration Success:** Confirm that fixed tests validate actual component behavior, not just mock interactions

**8. Error Handling**

*   Use custom exception classes where appropriate for application-specific errors.
*   Catch specific exceptions rather than generic ones.
*   Provide informative error messages.
*   Format errors into a standard result structure (e.g., a `TaskResult`-like object with `status: "FAILED"`, details in `content`/`notes`) at appropriate boundaries.
*   Adhere to the project's [Error Handling Philosophy] (e.g., in `ARCHITECTURE/overview.md` or similar) regarding returning structured errors vs. raising exceptions.
*   **Consistent Error Formatting in Orchestrators:** Components orchestrating calls to other components MUST implement consistent error handling, using helpers to standardize FAILED result creation and populate structured error details.
*   **Defensive Handling of Returned Data Structures:** Use defensive checks (`isinstance()`, `dict.get()`) when processing complex data structures returned from other components or external sources.

**9. IDL to Code Implementation**

*   **Contractual Obligation:** The IDL file is the source of truth. The implementation **must** match interfaces, method signatures (including type hints), preconditions, postconditions, and described behavior precisely.
*   **Naming:** Code names should correspond directly to IDL names.
*   **Parameters & Return Types:** Must match the IDL. Use data models for complex "Expected Data Format" parameters/returns.
*   **Error Raising:** Implement error conditions described in the IDL.
*   **Dependencies:** Implement dependencies (e.g., from `@depends_on`) using constructor injection.

**10. Logging Conventions**

*   **Early Configuration:** Configure logging as the **very first step** in application entry-point scripts **before importing any application modules**.
    ```python
    # Example (Entry Point Script - Python)
    import logging
    # --- Logging Setup FIRST ---
    LOG_LEVEL = logging.DEBUG # Or get from args/env
    logging.basicConfig(level=LOG_LEVEL, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
    # --- Application Imports (After logging) ---
    # from src.main import Application
    ```
*   **Module Loggers (Python Example):** Use `logger = logging.getLogger(__name__)` at the top of each module.
*   **Setting Specific Levels (Python Example):** If needed, explicitly set levels for specific module loggers *after* `basicConfig` in your entry point: `logging.getLogger("src.noisy_module").setLevel(logging.WARNING)`.

**11. Utility Scripts**

*   **Path Setup:** Scripts outside main source/test directories MUST include robust path setup to reliably import project modules. Assume scripts might be run from different working directories; ideally, design them to be run from the project root.
*   **Execution Location:** Document the intended execution location.
*   **Environment Consistency:** Ensure scripts use the project's standard environment.

**12. Guidelines for DSLs or Complex Parsers/Evaluators (If Applicable)**

*   **Principle of Explicit Intent:** Ensure DSL syntax is unambiguous, especially for distinguishing executable code from literal data. Use quoting or specific data constructors for literal data.
*   **Separate Evaluation from Application:** Design core evaluation to determine an expression's *value*. Isolate logic that *applies* a function/operator to *already evaluated* arguments.
*   **Implement Robust and Explicit Dispatch Logic:** Clearly define dispatching rules for different language constructs and handle unrecognized/invalid constructs with specific errors.
*   **Validate Inputs at Key Boundaries (Defensive Programming):** Perform basic validation on inputs passed between internal evaluator functions.
*   **Ensure Contextual Error Reporting:** Error messages should pinpoint the semantic source of the error and include relevant context.
*   **Host Language Orchestration Pattern (Recommended Usage):**
    *   **Guideline:** Leverage the host language (e.g., Python) for complex data preparation before invoking a DSL evaluator.
    *   **Pattern:**
        1.  Prepare data in the host language.
        2.  Create DSL environment.
        3.  Bind prepared data to DSL variables.
        4.  Write focused DSL script referencing these variables.
        5.  Call DSL evaluator.
        6.  Process result in the host language.
    *   **Rationale:** Leverages host language strengths, keeps DSL focused on orchestration, simplifies DSL evaluator.

**13. Data Merging Conventions**

*   **Document Precedence:** Clearly document merging logic and precedence rules.
*   **Establish Conventions:** For common scenarios (e.g., merging status notes from called components with orchestrator notes), define a project convention (e.g., component's notes take precedence).
*   **Implement Correctly (Python Example for Dicts):**
    ```python
    # Correct precedence: component_data overwrites orchestrator_defaults
    final_data = orchestrator_defaults.copy()
    final_data.update(component_data)
    ```

**14. Documenting Component Interactions**

*   **Preferred Method:** Use a dedicated "Component Interactions" section within the component's IDL file (see `01_IDL_GUIDELINES.md`).
*   **Content:** Mermaid sequence diagrams and textual explanations for key scenarios.
*   **Maintenance:** Update this section when interaction patterns change significantly.

**15. DIALS Integration Best Practices**

*   **Data Type Detection (Module 1.S.0):** Always check CBF headers for `Angle_increment`. This value determines the processing pathway.
*   **Processing Mode Selection:**
    *   **True Stills Data (Angle_increment = 0.0°):** Use the `DIALSStillsProcessAdapter`, which wraps the `dials.stills_process` Python API. Ensure PHIL parameters are appropriate for stills.
    *   **Sequence Data (Angle_increment > 0.0°):** Use the `DIALSSequenceProcessAdapter`, which executes a sequential DIALS CLI workflow (`dials.import` → `dials.find_spots` → `dials.index` → `dials.integrate`). Use the critical PHIL parameters specified in `plan.md` (Section 0.6) for this route.
*   **Automatic Routing:** The `StillsPipelineOrchestrator` (or equivalent) should implement this data type detection and routing logic.
*   **Configuration:** Allow forcing a processing mode via configuration (`DIALSStillsProcessConfig.force_processing_mode`) to override auto-detection if necessary.
*   **Adapter Output Consistency:** Both adapters (`DIALSStillsProcessAdapter` and `DIALSSequenceProcessAdapter`) must return DIALS `Experiment` and `reflection_table` objects with a consistent structure for downstream modules.
*   **PHIL Parameter Validation:** For sequence processing, ensure the critical PHIL parameters (e.g., `spotfinder.filter.min_spot_size=3`, `indexing.method=fft3d`, `geometry.convert_sequences_to_stills=false`) are correctly applied by the `DIALSSequenceProcessAdapter`. Compare with working manual DIALS logs if issues arise.
*   **Validation Approach (Module 1.S.1.Validation):** The primary geometric validation method is Q-vector consistency (`q_model` vs. `q_observed`). Pixel-based validation is a simpler alternative or debug tool.
*   **Debugging Strategy:** When troubleshooting, first confirm the correct processing route was chosen. Then, compare DIALS logs from the failing adapter with logs from a manually executed, working DIALS workflow for that data type.

**16. Service/Plugin Registration and Naming (If Applicable)**

*   **Naming Constraints:** If registering callables (tools, plugins, services) that will be exposed to external systems (e.g., LLMs, other APIs), ensure their names conform to any constraints imposed by those external systems (e.g., regex for valid characters, length limits).
*   **Lookup and Invocation:** The key used for registration is typically the identifier used for lookup and invocation.
*   **Recommendation:** Prefer names valid for both internal use and external exposure to avoid complex mapping layers.
</file>

<file path="src/diffusepipe/adapters/dials_stills_process_adapter.py">
"""Adapter for dials.stills_process Python API."""

import logging
from pathlib import Path
from typing import Optional, Tuple, Any

from diffusepipe.exceptions import DIALSError, ConfigurationError, DataValidationError
from diffusepipe.types.types_IDL import DIALSStillsProcessConfig

# Imports needed for patching in tests
try:
    from libtbx.phil import parse
    from dxtbx.model.experiment_list import ExperimentListFactory
    from dials.command_line.stills_process import Processor, do_import
except ImportError:
    # These imports might fail in testing environments without DIALS
    parse = None
    ExperimentListFactory = None
    Processor = None
    do_import = None

logger = logging.getLogger(__name__)


class DIALSStillsProcessAdapter:
    """
    Thin wrapper around dials.stills_process.Processor for true still images.

    This adapter should only be used for CBF files with Angle_increment = 0.0°.
    For oscillation data (Angle_increment > 0.0°), use DIALSSequenceProcessAdapter instead.
    """

    def __init__(self):
        """Initialize the DIALS stills process adapter."""
        self._processor: Optional[Any] = None  # Type hint for Processor
        self._extracted_params: Optional[Any] = (
            None  # Type hint for extracted PHIL params
        )

    def process_still(
        self,
        image_path: str,
        config: DIALSStillsProcessConfig,
        base_expt_path: Optional[str] = None,
        output_dir_final: Optional[str] = None,
    ) -> Tuple[Optional[object], Optional[object], bool, str]:
        """
        Process a single still image using dials.stills_process.Processor.

        Args:
            image_path: Path to the CBF image file to process (must be true still)
            config: Configuration parameters for dials.stills_process
            base_expt_path: Optional path to base experiment file for geometry
            output_dir_final: Optional path to save final output files with consistent naming

        Returns:
            Tuple containing:
            - Experiment object (or None if failed)
            - Reflection table object (or None if failed)
            - Success boolean
            - Log messages string

        Raises:
            DIALSError: When DIALS operations fail
            ConfigurationError: When configuration is invalid
            DataValidationError: When partiality data is missing
        """
        log_messages = []

        try:
            # Validate inputs
            if not Path(image_path).exists():
                raise ConfigurationError(f"Image file does not exist: {image_path}")

            # Generate PHIL parameters for true stills processing
            logger.info("Generating PHIL parameters for stills processing...")
            self._extracted_params = self._generate_phil_parameters(
                config, output_dir_final
            )
            log_messages.append(f"Generated PHIL parameters for {image_path}")

            # Import DIALS components
            try:
                logger.info("Importing DIALS components...")
                from dials.command_line.stills_process import Processor
                from dials.command_line.stills_process import do_import

                logger.info("Successfully imported DIALS components")
            except ImportError as e:
                raise DIALSError(f"Failed to import DIALS components: {e}")

            # Step 1: Import experiments
            logger.info("Step 1: Importing experiments...")
            if base_expt_path and Path(base_expt_path).exists():
                from dxtbx.model.experiment_list import ExperimentListFactory

                experiments = ExperimentListFactory.from_json_file(base_expt_path)
                log_messages.append(f"Loaded base experiment from {base_expt_path}")
            else:
                experiments = do_import(image_path)
                log_messages.append(f"Imported experiment from {image_path}")

            if not experiments or len(experiments) == 0:
                raise DIALSError(f"Failed to import experiments from {image_path}")

            # Step 2: Initialize and run the Processor
            logger.info("Step 2: Initializing stills_process.Processor...")
            self._processor = Processor(params=self._extracted_params)

            # Step 3: Process experiments using the Processor
            logger.info(
                "Step 3: Processing experiments with stills_process.Processor..."
            )
            tag = Path(image_path).stem  # Use filename as tag
            self._processor.process_experiments(tag=tag, experiments=experiments)

            # Step 4: Extract results
            logger.info("Step 4: Extracting results...")
            integrated_experiments = self._processor.all_integrated_experiments
            integrated_reflections = self._processor.all_integrated_reflections

            if not integrated_experiments or len(integrated_experiments) == 0:
                raise DIALSError("stills_process produced no integrated experiments")

            if not integrated_reflections or len(integrated_reflections) == 0:
                raise DIALSError("stills_process produced no integrated reflections")

            log_messages.append("Completed DIALS stills processing")

            # Extract single experiment and reflections
            experiment = self._extract_experiment(integrated_experiments)
            reflections = self._extract_reflections(integrated_reflections)

            # Validate partiality column
            self._validate_partiality(reflections)
            log_messages.append("Validated partiality data")

            return experiment, reflections, True, "\n".join(log_messages)

        except Exception as e:
            error_msg = f"DIALS stills processing failed: {e}"
            log_messages.append(error_msg)
            logger.error(error_msg)
            logger.error(f"Exception type: {type(e).__name__}")
            logger.error(f"Exception details: {str(e)}")

            if isinstance(e, (DIALSError, ConfigurationError, DataValidationError)):
                raise
            else:
                raise DIALSError(error_msg) from e

    def _generate_phil_parameters(
        self, config: DIALSStillsProcessConfig, output_dir_final: Optional[str] = None
    ) -> Any:
        """
        Generate PHIL parameters for true stills processing.

        This method generates parameters appropriate for dials.stills_process,
        which is designed for true still images (Angle_increment = 0.0°).

        Args:
            config: Configuration object containing DIALS parameters

        Returns:
            Extracted PHIL parameter object for dials.stills_process

        Raises:
            ConfigurationError: When PHIL generation fails
        """
        try:
            from dials.command_line.stills_process import (
                phil_scope as stills_phil_scope,
            )
            from libtbx.phil import parse
            from cctbx import uctbx, sgtbx

            # Start with DIALS stills_process master scope
            working_phil = stills_phil_scope

            # Apply base PHIL file if provided
            if config.stills_process_phil_path:
                phil_path = Path(config.stills_process_phil_path)
                if not phil_path.exists():
                    raise ConfigurationError(
                        f"PHIL file not found: {config.stills_process_phil_path}"
                    )

                with open(phil_path, "r") as f:
                    phil_content = f.read()
                user_phil = parse(phil_content)
                working_phil = working_phil.fetch(user_phil)

            # Extract params object to apply direct overrides
            params = working_phil.extract()

            # Set output directory and prefix for consistent naming
            if output_dir_final:
                logger.info(f"Setting output directory: {output_dir_final}")
                params.output.output_dir = output_dir_final
                params.output.prefix = "indexed_refined_detector"

            # Apply configuration overrides for stills processing
            if config.known_unit_cell:
                logger.info(f"Adding known unit cell: {config.known_unit_cell}")
                params.indexing.known_symmetry.unit_cell = uctbx.unit_cell(
                    config.known_unit_cell
                )

            if config.known_space_group:
                logger.info(f"Adding known space group: {config.known_space_group}")
                try:
                    params.indexing.known_symmetry.space_group = sgtbx.space_group_info(
                        config.known_space_group
                    ).group()
                except Exception as e:
                    raise ConfigurationError(
                        f"Invalid space group string: {config.known_space_group} - {e}"
                    )

            if config.spotfinder_threshold_algorithm:
                params.spotfinder.threshold.algorithm = (
                    config.spotfinder_threshold_algorithm
                )

            if config.min_spot_area is not None:
                params.spotfinder.filter.min_spot_size = config.min_spot_area

            if config.output_shoeboxes is not None:
                params.output.shoeboxes = config.output_shoeboxes

            if config.calculate_partiality is not None:
                # Note: estimate_partiality parameter removed from current DIALS stills_process PHIL
                # Partiality calculation may be handled automatically or via different parameters
                logger.info(
                    f"Partiality calculation requested: {config.calculate_partiality} (parameter no longer available in DIALS PHIL)"
                )

            # Store the extracted params
            self._extracted_params = params

            # Debug: log key parameters
            logger.debug(f"Unit cell: {params.indexing.known_symmetry.unit_cell}")
            logger.debug(f"Space group: {params.indexing.known_symmetry.space_group}")
            logger.debug(
                f"Spotfinder algorithm: {params.spotfinder.threshold.algorithm}"
            )
            logger.debug(f"Min spot size: {params.spotfinder.filter.min_spot_size}")
            logger.debug(f"Output shoeboxes: {params.output.shoeboxes}")
            # Note: estimate_partiality parameter no longer available in DIALS PHIL

            return params

        except Exception as e:
            raise ConfigurationError(f"Failed to generate PHIL parameters: {e}")

    def _extract_experiment(self, integrated_experiments: object) -> Optional[object]:
        """
        Extract single experiment from integrated experiments.

        Args:
            integrated_experiments: Result from stills processing

        Returns:
            Single Experiment object or None
        """
        if integrated_experiments and len(integrated_experiments) > 0:
            return integrated_experiments[0]
        return None

    def _extract_reflections(self, integrated_reflections: object) -> Optional[object]:
        """
        Extract reflection table from integrated reflections.

        Args:
            integrated_reflections: Result from stills processing

        Returns:
            Reflection table object or None
        """
        return integrated_reflections

    def _validate_partiality(self, reflections: Optional[object]) -> None:
        """
        Validate that reflection table contains partiality column.

        Args:
            reflections: Reflection table to validate

        Raises:
            DataValidationError: When partiality column is missing
        """
        if reflections is None:
            return

        try:
            # Check if this is a DIALS reflection table with has_key method
            if not hasattr(reflections, "has_key"):
                logger.warning(
                    "Could not validate partiality column (possibly mock object or unexpected type)"
                )
                return

            # Check for partiality column
            if not reflections.has_key("partiality"):
                raise DataValidationError(
                    "Reflection table missing required 'partiality' column"
                )

            # Validate partiality values are reasonable
            partialities = reflections["partiality"]
            if len(partialities) == 0:
                logger.warning("No reflections with partiality values found")
            else:
                logger.info(
                    f"Found {len(partialities)} reflections with partiality values"
                )

        except AttributeError:
            # In case of mock objects during testing
            logger.warning(
                "Could not validate partiality column (possibly mock object)"
            )
            pass
</file>

<file path="src/diffusepipe/crystallography/still_processing_and_validation.py">
"""
Still processor and validation component for orchestrating per‑still DIALS
processing and geometric validation.

This module provides a higher‑level interface for processing individual still
images using a DIALS adapter and performing geometric model validation.
"""

from __future__ import annotations

import logging
from pathlib import Path
from typing import Any, Dict, Optional, Tuple

import numpy as np  # Used only for simple statistics in the plot helper

from diffusepipe.adapters.dials_sequence_process_adapter import (
    DIALSSequenceProcessAdapter,
)
from diffusepipe.adapters.dials_stills_process_adapter import (
    DIALSStillsProcessAdapter,
)
from diffusepipe.utils.cbf_utils import get_angle_increment_from_cbf
from diffusepipe.crystallography.q_consistency_checker import QConsistencyChecker
from diffusepipe.types.types_IDL import (
    DIALSStillsProcessConfig,
    ExtractionConfig,
    OperationOutcome,
)

logger = logging.getLogger(__name__)


# -----------------------------------------------------------------------------
#                               DATA CLASSES
# -----------------------------------------------------------------------------
class ValidationMetrics:
    """Container for validation metrics and results."""

    def __init__(self) -> None:
        self.pdb_cell_passed: Optional[bool] = None
        self.pdb_orientation_passed: Optional[bool] = None
        self.q_consistency_passed: Optional[bool] = None

        self.mean_delta_q_mag: Optional[float] = None
        self.max_delta_q_mag: Optional[float] = None
        self.median_delta_q_mag: Optional[float] = None
        self.misorientation_angle_vs_pdb: Optional[float] = None
        self.num_reflections_tested: int = 0

    # ------------------------------------------------------------------
    def to_dict(self) -> Dict[str, Any]:
        """Serialise metrics so they can be placed into an OperationOutcome."""

        return {
            "pdb_cell_passed": self.pdb_cell_passed,
            "pdb_orientation_passed": self.pdb_orientation_passed,
            "q_consistency_passed": self.q_consistency_passed,
            "mean_delta_q_mag": self.mean_delta_q_mag,
            "max_delta_q_mag": self.max_delta_q_mag,
            "median_delta_q_mag": self.median_delta_q_mag,
            "misorientation_angle_vs_pdb": self.misorientation_angle_vs_pdb,
            "num_reflections_tested": self.num_reflections_tested,
        }


# -----------------------------------------------------------------------------
#                           MODEL VALIDATOR CLASS
# -----------------------------------------------------------------------------
class ModelValidator:
    """Performs geometry checks: PDB match and Q‑vector consistency."""

    def __init__(self) -> None:  # noqa: D401
        self.q_checker = QConsistencyChecker()

    # ------------------------------------------------------------------
    def validate_geometry(
        self,
        experiment: object,
        reflections: object,
        *,
        external_pdb_path: Optional[str] = None,
        extraction_config: Optional[ExtractionConfig] = None,
        output_dir: Optional[str] = None,
    ) -> Tuple[bool, ValidationMetrics]:
        """Run all validation sub‑checks and collate the results. Primary internal check is Q-vector consistency."""

        metrics = ValidationMetrics()

        # ---------------- PDB check ----------------
        cell_len_tol = extraction_config.cell_length_tol if extraction_config else 0.02
        cell_ang_tol = extraction_config.cell_angle_tol if extraction_config else 2.0
        orient_tol_deg = (
            extraction_config.orient_tolerance_deg if extraction_config else 5.0
        )

        if external_pdb_path and Path(external_pdb_path).exists():
            (
                metrics.pdb_cell_passed,
                metrics.pdb_orientation_passed,
                metrics.misorientation_angle_vs_pdb,
            ) = self._check_pdb_consistency(
                experiment,
                external_pdb_path,
                cell_len_tol,
                cell_ang_tol,
                orient_tol_deg,
            )

        # --------------- Q‑vector check -------------
        q_tol = (
            extraction_config.q_consistency_tolerance_angstrom_inv
            if extraction_config
            else 0.01
        )
        metrics.q_consistency_passed, stats = self._check_q_consistency(
            experiment, reflections, q_tol
        )
        metrics.mean_delta_q_mag = stats.get("mean")
        metrics.max_delta_q_mag = stats.get("max")
        metrics.median_delta_q_mag = stats.get("median")
        metrics.num_reflections_tested = stats.get("count", 0)

        # --------------- Overall pass/fail ----------
        validation_passed = (
            metrics.q_consistency_passed is True
            and metrics.num_reflections_tested > 0  # NEW – guard against empty check
            and (metrics.pdb_cell_passed is not False)
            and (metrics.pdb_orientation_passed is not False)
        )
        logger.info("Geometric validation result: %s", validation_passed)
        return validation_passed, metrics

    # ------------------------------------------------------------------
    @staticmethod
    def _check_pdb_consistency(
        experiment: object,
        pdb_path: str,
        cell_length_tol: float,
        cell_angle_tol: float,
        orient_tolerance_deg: float,
    ) -> Tuple[bool, bool, Optional[float]]:
        """Stub PDB comparison – always passes until fully implemented."""

        logger.info("Checking PDB consistency against %s", pdb_path)

        try:
            # Step 1: Load PDB crystal symmetry
            from iotbx import pdb
            from scitbx import matrix

            pdb_input = pdb.input(file_name=pdb_path)
            pdb_crystal_symmetry = pdb_input.crystal_symmetry()

            if pdb_crystal_symmetry is None:
                logger.warning(
                    "PDB file %s has no crystal symmetry information, skipping PDB consistency check",
                    pdb_path,
                )
                return True, True, None  # Pass if PDB lacks symmetry

            pdb_uc = pdb_crystal_symmetry.unit_cell()
            pdb_sg = pdb_crystal_symmetry.space_group()

            # Step 2: Get experiment crystal symmetry
            exp_crystal = experiment.crystal
            exp_uc = exp_crystal.get_unit_cell()
            exp_sg = exp_crystal.get_space_group()

            # Step 3: Compare unit cells
            cell_passed = exp_uc.is_similar_to(
                pdb_uc,
                relative_length_tolerance=cell_length_tol,
                absolute_angle_tolerance=cell_angle_tol,
            )

            # Log comparison details
            logger.info("Unit cell comparison:")
            logger.info("  Experiment: %s", exp_uc.parameters())
            logger.info("  PDB: %s", pdb_uc.parameters())
            logger.info(
                "  Cell similarity: %s (tol: length=%g, angle=%g°)",
                cell_passed,
                cell_length_tol,
                cell_angle_tol,
            )

            # Step 4: Compare space groups (informational)
            if exp_sg.type().number() != pdb_sg.type().number():
                logger.warning(
                    "Space group mismatch: experiment=%s, PDB=%s",
                    exp_sg.type().lookup_symbol(),
                    pdb_sg.type().lookup_symbol(),
                )

            # Step 5: Compare orientations
            A_dials = matrix.sqr(exp_crystal.get_A())
            B_pdb = matrix.sqr(pdb_uc.fractionalization_matrix()).transpose().inverse()

            # For PDB, assume conventional orientation (U_pdb = Identity), so A_pdb_ref = B_pdb
            misorientation_deg = ModelValidator._calculate_misorientation_static(
                A_dials, B_pdb
            )
            orientation_passed = misorientation_deg <= orient_tolerance_deg

            logger.info("Orientation comparison:")
            logger.info(
                "  Misorientation angle: %.2f° (tolerance: %.2f°)",
                misorientation_deg,
                orient_tolerance_deg,
            )
            logger.info("  Orientation similarity: %s", orientation_passed)

            return cell_passed, orientation_passed, misorientation_deg

        except Exception as e:
            logger.error("PDB consistency check failed: %s", e)
            return False, False, None

    # ------------------------------------------------------------------
    @staticmethod
    def _calculate_misorientation_static(
        A1_matrix_sqr: Any, A2_matrix_sqr: Any
    ) -> float:
        """Calculate misorientation angle between two A-matrices (UB matrices)."""

        from scitbx import matrix

        def angle_between_orientations(
            a_mat: "matrix.sqr", b_mat: "matrix.sqr"
        ) -> float:
            try:
                a_inv = a_mat.inverse()
            except RuntimeError:  # Singular matrix
                return 180.0
            r_ab = b_mat * a_inv
            trace_r = r_ab.trace()
            cos_angle = (trace_r - 1.0) / 2.0
            cos_angle_clipped = np.clip(cos_angle, -1.0, 1.0)  # Handle precision errors
            angle_rad = np.arccos(cos_angle_clipped)
            return np.degrees(angle_rad)

        # Compare A1 with A2 and A1 with -A2 (handles potential hand inversion)
        mis_direct = angle_between_orientations(A1_matrix_sqr, A2_matrix_sqr)
        A2_inverted_hand = matrix.sqr([-x for x in A2_matrix_sqr.elems])
        mis_inverted = angle_between_orientations(A1_matrix_sqr, A2_inverted_hand)

        return min(mis_direct, mis_inverted)

    # ------------------------------------------------------------------
    def _check_q_consistency(
        self, experiment: object, reflections: object, tolerance: float
    ) -> Tuple[bool, Dict[str, float]]:
        """Perform Q-vector consistency check by comparing model-derived q-vectors with those recalculated from observed pixel positions. Delegates to QConsistencyChecker."""
        return self.q_checker.check_q_consistency(experiment, reflections, tolerance)


# -----------------------------------------------------------------------------
#          ORCHESTRATOR (PROCESSING + VALIDATION) – PUBLIC ENTRY POINT
# -----------------------------------------------------------------------------
class StillProcessorAndValidatorComponent:
    """Run DIALS processing and then the geometry checks in one call."""

    def __init__(self) -> None:  # noqa: D401
        self.stills_adapter = DIALSStillsProcessAdapter()
        self.sequence_adapter = DIALSSequenceProcessAdapter()
        self.validator = ModelValidator()

    # ------------------------------------------------------------------
    def _determine_processing_route(
        self, image_path: str, config: DIALSStillsProcessConfig
    ) -> Tuple[str, object]:
        """
        Determine which adapter to use based on CBF data type detection.

        Implements Module 1.S.0: CBF Data Type Detection and Processing Route Selection.

        Args:
            image_path: Path to CBF image file
            config: Configuration that may override auto-detection

        Returns:
            Tuple of (processing_route, selected_adapter)
            where processing_route is "stills" or "sequence"
        """
        # Step 1: Check for force override
        if config.force_processing_mode:
            if config.force_processing_mode.lower() == "stills":
                logger.info(f"Force override: using stills processing for {image_path}")
                return "stills", self.stills_adapter
            elif config.force_processing_mode.lower() == "sequence":
                logger.info(
                    f"Force override: using sequence processing for {image_path}"
                )
                return "sequence", self.sequence_adapter
            else:
                logger.warning(
                    f"Invalid force_processing_mode: {config.force_processing_mode}, falling back to auto-detection"
                )

        # Step 2: Auto-detect from CBF header
        try:
            angle_increment = get_angle_increment_from_cbf(image_path)

            if angle_increment is not None:
                if angle_increment == 0.0:
                    logger.info(
                        f"Auto-detected stills data (Angle_increment=0.0°) for {image_path}"
                    )
                    return "stills", self.stills_adapter
                elif angle_increment > 0.0:
                    logger.info(
                        f"Auto-detected sequence data (Angle_increment={angle_increment}°) for {image_path}"
                    )
                    return "sequence", self.sequence_adapter
                else:
                    logger.warning(
                        f"Unexpected negative Angle_increment ({angle_increment}°), defaulting to sequence processing"
                    )
                    return "sequence", self.sequence_adapter
            else:
                logger.warning(
                    f"Could not determine Angle_increment from {image_path}, defaulting to sequence processing (safer)"
                )
                return "sequence", self.sequence_adapter

        except Exception as e:
            logger.warning(
                f"CBF header parsing failed for {image_path}: {e}, defaulting to sequence processing"
            )
            return "sequence", self.sequence_adapter

    # ------------------------------------------------------------------
    def process_and_validate_still(
        self,
        *,
        image_path: str,
        config: DIALSStillsProcessConfig,
        extraction_config: ExtractionConfig,
        base_experiment_path: Optional[str] = None,
        external_pdb_path: Optional[str] = None,
        output_dir: Optional[str] = None,
    ) -> OperationOutcome:
        """Helper that chains adapter + validator and packs an OperationOutcome."""

        logger.info("Processing still image: %s", image_path)

        # -------------------- Module 1.S.0: CBF Data Type Detection and Processing Route Selection ---------------------
        processing_route, selected_adapter = self._determine_processing_route(
            image_path, config
        )
        logger.info(f"Selected processing route: {processing_route}")

        # -------------------- DIALS processing ---------------------
        try:
            exp, refl, success, log = selected_adapter.process_still(
                image_path=image_path,
                config=config,
                base_expt_path=base_experiment_path,
                output_dir_final=output_dir,
            )
        except Exception as exc:
            logger.error("DIALS processing raised: %s", exc)
            return OperationOutcome(
                status="FAILURE_DIALS_PROCESSING",
                message=str(exc),
                error_code="DIALS_PROCESSING_EXCEPTION",
                output_artifacts={},
            )

        if not success or exp is None or refl is None:
            return OperationOutcome(
                status="FAILURE_DIALS_PROCESSING",
                message="DIALS processing failed – see log_messages",
                error_code="DIALS_PROCESSING_FAILED",
                output_artifacts={"log_messages": log},
            )

        # -------------------- Validation ---------------------------
        passed, metrics = self.validator.validate_geometry(
            experiment=exp,
            reflections=refl,
            external_pdb_path=external_pdb_path,
            extraction_config=extraction_config,
            output_dir=output_dir,
        )

        outcome_status = "SUCCESS" if passed else "FAILURE_GEOMETRY_VALIDATION"
        return OperationOutcome(
            status=outcome_status,
            message="Processed and validated" if passed else "Validation failed",
            error_code=None if passed else "GEOMETRY_VALIDATION_FAILED",
            output_artifacts={
                "experiment": exp,
                "reflections": refl,
                "validation_passed": passed,
                "validation_metrics": metrics.to_dict(),
                "processing_route_used": processing_route,  # Include routing information
                "log_messages": log,
            },
        )

    # ------------------------------------------------------------------
    # Backward‑compat shortcut -----------------------------------------
    def process_still(
        self,
        image_path: str,
        config: DIALSStillsProcessConfig,
        base_experiment_path: Optional[str] = None,
        output_dir: Optional[str] = None,
    ) -> OperationOutcome:  # noqa: D401 – keep old name
        """Legacy API: just run DIALS without validation."""
        # Use routing logic even for legacy API
        processing_route, selected_adapter = self._determine_processing_route(
            image_path, config
        )
        logger.info(f"Legacy API: Selected processing route: {processing_route}")

        exp, refl, success, log = selected_adapter.process_still(
            image_path=image_path,
            config=config,
            base_expt_path=base_experiment_path,
            output_dir_final=output_dir,
        )
        if success and exp is not None and refl is not None:
            return OperationOutcome(
                status="SUCCESS",
                message="DIALS processing only (legacy path)",
                error_code=None,
                output_artifacts={
                    "experiment": exp,
                    "reflections": refl,
                    "processing_route_used": processing_route,  # Include routing information
                    "log_messages": log,
                },
            )
        return OperationOutcome(
            status="FAILURE",
            message="DIALS processing failed (legacy path)",
            error_code="DIALS_PROCESSING_FAILED",
            output_artifacts={
                "processing_route_used": processing_route,  # Include routing information even for failures
                "log_messages": log,
            },
        )


# -----------------------------------------------------------------------------
#                           CONFIGURATION HELPERS
# -----------------------------------------------------------------------------
def create_default_config(
    phil_path: Optional[str] = None,
    enable_partiality: bool = True,
    enable_shoeboxes: bool = False,
    known_unit_cell: Optional[str] = None,
    known_space_group: Optional[str] = None,
) -> DIALSStillsProcessConfig:
    """Create a default DIALS stills process configuration."""

    return DIALSStillsProcessConfig(
        stills_process_phil_path=phil_path,
        force_processing_mode=None,  # Default to auto-detection
        calculate_partiality=enable_partiality,
        output_shoeboxes=enable_shoeboxes,
        known_unit_cell=known_unit_cell,
        known_space_group=known_space_group,
        spotfinder_threshold_algorithm="dispersion",
        min_spot_area=3,
    )


def create_default_extraction_config() -> ExtractionConfig:
    """Create a default extraction configuration with reasonable validation tolerances."""

    return ExtractionConfig(
        gain=1.0,
        cell_length_tol=0.02,  # 2%
        cell_angle_tol=2.0,  # 2 degrees
        orient_tolerance_deg=5.0,  # 5 degrees
        q_consistency_tolerance_angstrom_inv=0.01,  # 0.01 Å⁻¹
        pixel_step=1,
        lp_correction_enabled=False,
        plot_diagnostics=True,
        verbose=False,
    )


# -----------------------------------------------------------------------------
# Back-compat shim — legacy code and tests expect this symbol to exist
# -----------------------------------------------------------------------------
class StillProcessorComponent(StillProcessorAndValidatorComponent):
    """
    Legacy alias preserved for external code that still does:

        from diffusepipe.crystallography.still_processing_and_validation \
            import StillProcessorComponent
    """

    # No extra behaviour; everything lives in the parent class.
    pass
</file>

<file path="src/diffusepipe/adapters/dials_sequence_process_adapter.py">
"""Adapter for DIALS sequential processing workflow (import→find_spots→index→integrate)."""

import logging
import shutil
import tempfile
import subprocess
from pathlib import Path
from typing import Optional, Tuple, Dict, List

from diffusepipe.exceptions import DIALSError, ConfigurationError, DataValidationError
from diffusepipe.types.types_IDL import DIALSStillsProcessConfig

logger = logging.getLogger(__name__)


class DIALSSequenceProcessAdapter:
    """
    Adapter for DIALS sequential processing workflow.

    This adapter replicates the successful manual workflow:
    1. dials.import
    2. dials.find_spots
    3. dials.index with known_symmetry
    4. dials.integrate

    This approach works for 0.1° oscillation images that stills_process fails on.
    """

    def __init__(self):
        """Initialize the DIALS sequence process adapter."""
        # Get the path to config files (relative to this package)
        self.config_dir = Path(__file__).parent.parent / "config"

        # Define base PHIL file paths for each step
        self.base_phil_files = {
            "import": self.config_dir / "sequence_import_default.phil",
            "find_spots": self.config_dir / "sequence_find_spots_default.phil",
            "index": self.config_dir / "sequence_index_default.phil",
            "integrate": self.config_dir / "sequence_integrate_default.phil",
        }

    def _load_and_merge_phil_parameters(
        self,
        step: str,
        config: DIALSStillsProcessConfig,
        runtime_overrides: Optional[Dict[str, str]] = None,
    ) -> List[str]:
        """
        Load base PHIL file and merge with configuration overrides.

        Args:
            step: DIALS step name ("import", "find_spots", "index", "integrate")
            config: Configuration object with potential overrides
            runtime_overrides: Additional runtime parameter overrides

        Returns:
            List of command-line parameter strings for the DIALS command
        """
        try:
            from libtbx.phil import parse
        except ImportError:
            logger.warning("libtbx.phil not available, using hardcoded parameters")
            return self._get_fallback_parameters(step, config, runtime_overrides)

        # Start with base PHIL file if it exists
        phil_parameters = []
        base_file = self.base_phil_files.get(step)

        if base_file and base_file.exists():
            try:
                # Parse base PHIL file
                with open(base_file, "r") as f:
                    base_phil_content = f.read()
                parse(base_phil_content)  # Validate PHIL syntax

                # Base PHIL file loaded successfully
                logger.debug(f"Loaded base PHIL parameters for {step} from {base_file}")

            except Exception as e:
                logger.warning(f"Failed to load base PHIL file {base_file}: {e}")
                return self._get_fallback_parameters(step, config, runtime_overrides)
        else:
            logger.warning(f"Base PHIL file not found for {step}: {base_file}")
            return self._get_fallback_parameters(step, config, runtime_overrides)

        # Apply sequence processing overrides from config
        if config.sequence_processing_phil_overrides:
            for override in config.sequence_processing_phil_overrides:
                phil_parameters.append(override)
                logger.debug(f"Applied sequence override: {override}")

        # Apply runtime overrides (like input/output file names)
        if runtime_overrides:
            for param, value in runtime_overrides.items():
                phil_param = f"{param}={value}"
                phil_parameters.append(phil_param)
                logger.debug(f"Applied runtime override: {phil_param}")

        return phil_parameters

    def _get_fallback_parameters(
        self,
        step: str,
        config: DIALSStillsProcessConfig,
        runtime_overrides: Optional[Dict[str, str]] = None,
    ) -> List[str]:
        """
        Fallback method to provide hardcoded critical parameters when PHIL loading fails.

        This preserves the original behavior with hardcoded critical parameters.
        """
        parameters = []

        if step == "find_spots":
            parameters.extend(
                [
                    "spotfinder.filter.min_spot_size=3",  # Critical: not default 2
                    "spotfinder.threshold.algorithm=dispersion",  # Critical: not default
                ]
            )

            # Handle config overrides with warnings for critical parameters
            if (
                config.spotfinder_threshold_algorithm
                and config.spotfinder_threshold_algorithm != "dispersion"
            ):
                logger.warning(
                    f"Overriding critical sequence parameter: spotfinder.threshold.algorithm={config.spotfinder_threshold_algorithm} (recommended: dispersion)"
                )
                parameters[-1] = (
                    f"spotfinder.threshold.algorithm={config.spotfinder_threshold_algorithm}"
                )

        elif step == "index":
            parameters.extend(
                [
                    "indexing.method=fft3d",  # Critical: not fft1d
                    "geometry.convert_sequences_to_stills=false",  # Critical: preserve oscillation
                ]
            )

            # Add known symmetry parameters
            if config.known_space_group:
                parameters.append(
                    f'indexing.known_symmetry.space_group="{config.known_space_group}"'
                )
            if config.known_unit_cell:
                parameters.append(
                    f"indexing.known_symmetry.unit_cell={config.known_unit_cell}"
                )
                # Fix unit cell during refinement to preserve PDB reference
                parameters.append("refinement.parameterisation.crystal.fix=cell")

        elif step == "integrate":
            parameters.extend(
                [
                    "geometry.convert_sequences_to_stills=false",  # Consistency
                    "integration.summation.estimate_partiality=true",  # For validation
                ]
            )

        # Apply runtime overrides
        if runtime_overrides:
            for param, value in runtime_overrides.items():
                parameters.append(f"{param}={value}")

        return parameters

    def process_still(
        self,
        image_path: str,
        config: DIALSStillsProcessConfig,
        base_expt_path: Optional[str] = None,
        output_dir_final: Optional[str] = None,
    ) -> Tuple[Optional[object], Optional[object], bool, str]:
        """
        Process a still/sequence image using DIALS sequential workflow.

        Args:
            image_path: Path to the CBF image file to process
            config: Configuration parameters for DIALS processing
            base_expt_path: Optional path to base experiment file for geometry (ignored)
            output_dir_final: Optional path to save final output files with consistent naming

        Returns:
            Tuple containing:
            - Experiment object (or None if failed)
            - Reflection table object (or None if failed)
            - Success boolean
            - Log messages string

        Raises:
            DIALSError: When DIALS operations fail
            ConfigurationError: When configuration is invalid
            DataValidationError: When partiality data is missing
        """
        log_messages = []

        try:
            # Validate inputs
            if not Path(image_path).exists():
                raise ConfigurationError(f"Image file does not exist: {image_path}")

            # Use temporary directory for processing
            with tempfile.TemporaryDirectory() as temp_dir:
                temp_path = Path(temp_dir)
                original_cwd = Path.cwd()

                # Convert to absolute paths before changing directories
                abs_image_path = Path(image_path).resolve()
                abs_output_dir_final = (
                    Path(output_dir_final).resolve() if output_dir_final else None
                )

                try:
                    # Change to temp directory for processing
                    import os

                    os.chdir(temp_path)

                    # Step 1: Import
                    logger.info("Step 1: Running dials.import")
                    self._run_dials_import(str(abs_image_path))
                    log_messages.append("Completed dials.import")

                    if not Path("imported.expt").exists():
                        raise DIALSError("dials.import failed to create imported.expt")

                    # Step 2: Find spots
                    logger.info("Step 2: Running dials.find_spots")
                    self._run_dials_find_spots(config)
                    log_messages.append("Completed dials.find_spots")

                    if not Path("strong.refl").exists():
                        raise DIALSError(
                            "dials.find_spots failed to create strong.refl"
                        )

                    # Step 3: Index
                    logger.info("Step 3: Running dials.index")
                    self._run_dials_index(config)
                    log_messages.append("Completed dials.index")

                    if not Path("indexed.expt").exists():
                        raise DIALSError("dials.index failed to create indexed.expt")

                    # Step 4: Integrate
                    logger.info("Step 4: Running dials.integrate")
                    self._run_dials_integrate(config)
                    log_messages.append("Completed dials.integrate")

                    if not Path("integrated.expt").exists():
                        raise DIALSError(
                            "dials.integrate failed to create integrated.expt"
                        )

                    # Load results using DIALS Python API
                    logger.info("Loading results with DIALS Python API")
                    experiment, reflections = self._load_results()

                    # Validate partiality column
                    self._validate_partiality(reflections)
                    log_messages.append("Validated partiality data")

                    # Copy output files to final directory if specified
                    if abs_output_dir_final:
                        self._copy_outputs_to_final_directory(
                            temp_path, str(abs_output_dir_final), log_messages
                        )

                    return experiment, reflections, True, "\n".join(log_messages)

                finally:
                    os.chdir(original_cwd)

        except Exception as e:
            error_msg = f"DIALS sequential processing failed: {e}"
            log_messages.append(error_msg)
            logger.error(error_msg)
            logger.error(f"Exception type: {type(e).__name__}")
            logger.error(f"Exception details: {str(e)}")

            if isinstance(e, (DIALSError, ConfigurationError, DataValidationError)):
                raise
            else:
                raise DIALSError(error_msg) from e

    def _run_dials_import(self, image_path: str) -> subprocess.CompletedProcess:
        """Run dials.import step using PHIL files."""
        # For import, we mainly need to set the output file
        runtime_overrides = {
            "output.experiments": "imported.expt",
            "output.log": "dials.import.log",
        }

        # Use empty config since import step doesn't typically need config overrides
        empty_config = DIALSStillsProcessConfig()

        phil_params = self._load_and_merge_phil_parameters(
            "import", empty_config, runtime_overrides
        )

        # Build command
        cmd = ["dials.import", image_path] + phil_params

        logger.info(f"Running dials.import with parameters: {phil_params}")
        result = subprocess.run(cmd, capture_output=True, text=True)

        if result.returncode != 0:
            raise DIALSError(f"dials.import failed: {result.stderr}")

        logger.debug(f"dials.import stdout: {result.stdout}")
        return result

    def _run_dials_find_spots(
        self, config: DIALSStillsProcessConfig
    ) -> subprocess.CompletedProcess:
        """Run dials.find_spots step using PHIL files and configuration overrides."""
        # Load parameters from PHIL file and merge with config
        runtime_overrides = {
            "output.reflections": "strong.refl",
            "output.log": "dials.find_spots.log",
        }

        phil_params = self._load_and_merge_phil_parameters(
            "find_spots", config, runtime_overrides
        )

        # Build command
        cmd = ["dials.find_spots", "imported.expt"] + phil_params

        logger.info(f"Running dials.find_spots with parameters: {phil_params}")
        result = subprocess.run(cmd, capture_output=True, text=True)

        if result.returncode != 0:
            raise DIALSError(f"dials.find_spots failed: {result.stderr}")

        logger.debug(f"dials.find_spots stdout: {result.stdout}")
        return result

    def _run_dials_index(
        self, config: DIALSStillsProcessConfig
    ) -> subprocess.CompletedProcess:
        """Run dials.index step using PHIL files and configuration overrides."""
        # Load parameters from PHIL file and merge with config
        runtime_overrides = {
            "output.experiments": "indexed.expt",
            "output.reflections": "indexed.refl",
            "output.log": "dials.index.log",
        }

        # Add known symmetry from config to runtime overrides
        if config.known_space_group:
            runtime_overrides["indexing.known_symmetry.space_group"] = (
                f'"{config.known_space_group}"'
            )
        if config.known_unit_cell:
            runtime_overrides["indexing.known_symmetry.unit_cell"] = (
                config.known_unit_cell
            )
            # Fix unit cell during refinement to preserve PDB reference
            runtime_overrides["refinement.parameterisation.crystal.fix"] = "cell"

        phil_params = self._load_and_merge_phil_parameters(
            "index", config, runtime_overrides
        )

        # Build command
        cmd = ["dials.index", "imported.expt", "strong.refl"] + phil_params

        logger.info(f"Running dials.index with parameters: {phil_params}")
        result = subprocess.run(cmd, capture_output=True, text=True)

        if result.returncode != 0:
            raise DIALSError(f"dials.index failed: {result.stderr}")

        logger.debug(f"dials.index stdout: {result.stdout}")
        return result

    def _run_dials_integrate(
        self, config: DIALSStillsProcessConfig
    ) -> subprocess.CompletedProcess:
        """Run dials.integrate step using PHIL files and configuration overrides."""
        # Load parameters from PHIL file and merge with config
        runtime_overrides = {
            "output.experiments": "integrated.expt",
            "output.reflections": "integrated.refl",
            "output.log": "dials.integrate.log",
        }

        phil_params = self._load_and_merge_phil_parameters(
            "integrate", config, runtime_overrides
        )

        # Build command
        cmd = ["dials.integrate", "indexed.expt", "indexed.refl"] + phil_params

        logger.info(f"Running dials.integrate with parameters: {phil_params}")
        result = subprocess.run(cmd, capture_output=True, text=True)

        if result.returncode != 0:
            raise DIALSError(f"dials.integrate failed: {result.stderr}")

        logger.debug(f"dials.integrate stdout: {result.stdout}")
        return result

    def _load_results(self) -> Tuple[object, object]:
        """Load experiment and reflection results using DIALS Python API."""
        try:
            from dxtbx.model.experiment_list import ExperimentListFactory
            from dials.array_family import flex

            # Load experiment
            experiments = ExperimentListFactory.from_json_file("integrated.expt")
            if len(experiments) == 0:
                raise DIALSError("No experiments in integrated.expt")

            experiment = experiments[0]

            # Load reflections
            reflections = flex.reflection_table.from_file("integrated.refl")

            logger.info(f"Loaded experiment and {len(reflections)} reflections")
            return experiment, reflections

        except Exception as e:
            raise DIALSError(f"Failed to load DIALS results: {e}")

    def _validate_partiality(self, reflections: Optional[object]) -> None:
        """
        Validate that reflection table contains partiality column.

        Args:
            reflections: Reflection table to validate

        Raises:
            DataValidationError: When partiality column is missing
        """
        if reflections is None:
            return

        try:
            # Check for partiality column
            if not hasattr(reflections, "has_key") or not reflections.has_key(
                "partiality"
            ):
                raise DataValidationError(
                    "Reflection table missing required 'partiality' column"
                )

            # Validate partiality values are reasonable
            partialities = reflections["partiality"]
            if len(partialities) == 0:
                logger.warning("No reflections with partiality values found")
            else:
                logger.info(
                    f"Found {len(partialities)} reflections with partiality values"
                )

        except AttributeError:
            # In case of mock objects during testing
            logger.warning(
                "Could not validate partiality column (possibly mock object)"
            )
            pass

    def _copy_outputs_to_final_directory(
        self, temp_path: Path, output_dir_final: str, log_messages: list
    ) -> None:
        """
        Copy DIALS output files to final directory with consistent naming.

        Args:
            temp_path: Path to temporary directory containing DIALS outputs
            output_dir_final: Final output directory path
            log_messages: List to append log messages to
        """
        try:
            # Ensure output directory exists (use absolute path)
            output_dir = Path(output_dir_final).resolve()
            output_dir.mkdir(parents=True, exist_ok=True)

            # Define source and target paths (use absolute paths)
            source_expt = temp_path.resolve() / "integrated.expt"
            source_refl = temp_path.resolve() / "integrated.refl"
            target_expt = output_dir / "indexed_refined_detector.expt"
            target_refl = output_dir / "indexed_refined_detector.refl"

            # Copy files with consistent naming
            if source_expt.exists():
                shutil.copy2(source_expt, target_expt)
                logger.info(f"Copied {source_expt} -> {target_expt}")
                log_messages.append(f"Saved experiment file: {target_expt}")
            else:
                logger.warning(f"Source experiment file not found: {source_expt}")

            if source_refl.exists():
                shutil.copy2(source_refl, target_refl)
                logger.info(f"Copied {source_refl} -> {target_refl}")
                log_messages.append(f"Saved reflection file: {target_refl}")
            else:
                logger.warning(f"Source reflection file not found: {source_refl}")

        except Exception as e:
            error_msg = (
                f"Failed to copy outputs to final directory {output_dir_final}: {e}"
            )
            logger.error(error_msg)
            log_messages.append(error_msg)
</file>

<file path="README.md">
# Crystal Diffuse Scattering Pipeline

This project does pixel-level analysis of crystallographic diffuse scattering in a scripted workflow capable of handling both true still images and sequence data.

The output will be a merged 3D diffuse scattering map.

## Methodology 

The approach follows Meisburger's published work. We use standard libraries wherever possible. Certain things -- especially background estimation -- have been adapted to work with stills data. 


## Features

*   **Dual Processing Modes**: Automatically detects whether input CBF files contain true stills (`Angle_increment = 0.0°`) or oscillation data (`Angle_increment > 0.0°`) and routes them to the appropriate DIALS processing workflow.
*   **DIALS Integration**: Uses DIALS for spot finding, auto-indexing, geometric refinement, and integration.
*   **Masking**: Generates static (beamstop, detector gaps), dynamic (hot/bad pixels), and per-still Bragg masks to isolate the diffuse signal.
*   **corrections**: Applies a series of physical corrections to each diffuse pixel:
    *   Lorentz-Polarization (LP) and Quantum Efficiency (QE) via DIALS.
    *   Custom Solid Angle ($\Omega$) and Air Attenuation corrections.
*   **Relative Scaling and Merging**: A custom scaling model, built on the DIALS framework, places all datasets on a common relative scale before merging them into a final 3D map via inverse-variance weighting.
*   **Visual Diagnostics**: Includes scripts for visual verification of each processing step.

## Project Structure

```
├── src/diffusepipe/           # Main Python package
│   ├── adapters/              # Wrappers for DIALS/DXTBX APIs
│   ├── config/                # Default DIALS PHIL configuration files
│   ├── crystallography/       # Crystal model processing and validation
│   ├── diagnostics/           # Q-vector calculation and consistency checking
│   ├── extraction/            # Diffuse data extraction and correction
│   ├── masking/               # Pixel and Bragg mask generation
│   ├── merging/               # Voxel data merging
│   ├── orchestration/         # Pipeline coordination
│   ├── scaling/               # Relative scaling model and components
│   └── types/                 # Pydantic data models for configuration and outcomes
├── docs/                      # Comprehensive project documentation
├── scripts/                   # Development and visual diagnostic scripts
├── tests/                     # Test suite (integration-focused)
└── libdocs/                   # Internal documentation for key libraries (e.g., DIALS)
```

## Getting Started

### Prerequisites

*   Python 3.10+
*   DIALS, cctbx and dxtbx
*   Required Python packages (see `pyproject.toml` or `requirements.txt`)

### End-to-End Visual Check

The quickest way to see the full pipeline in action is to use the end-to-end visual check script. This script processes a single CBF image from raw data through all masking and extraction steps, and generates a comprehensive set of diagnostic plots. Phase 3 (voxelization) is complete but not tested. You can run phase 2 end to end:

```bash
python scripts/dev_workflows/run_phase2_e2e_visual_check.py \
  --cbf-image /path/to/your/image.cbf \
  --output-base-dir ./e2e_outputs \
  --pdb-path /path/to/reference.pdb
```

See the **[Visual Diagnostics Guide](docs/VISUAL_DIAGNOSTICS_GUIDE.md)**.

## Pipeline Workflow

The processing is divided into three main phases:

### Phase 1: Per-Still Processing & Masking
The goal of this phase is to process each raw detector image to obtain a validated crystallographic model and a comprehensive mask that isolates the diffuse scattering signal.

*   **Data Type Detection:** The pipeline first inspects the CBF header to determine if the image is a true still or part of an oscillation sequence. It then automatically routes the data to the appropriate DIALS processing adapter.
*   **Crystallographic Processing:** DIALS is used to perform spot-finding, auto-indexing, and geometric refinement. This step yields a precise crystal orientation matrix ($\mathbf{U}_i$) and unit cell for each image.
*   **Geometric Validation:** The quality of the crystal model is verified using a Q-vector consistency check, which ensures that the model accurately maps between reciprocal space and detector space.
*   **Mask Generation:** A total mask is created for each still by combining a global bad-pixel mask (for detector gaps, shadows, etc.) with a per-still Bragg mask that covers the regions of intense Bragg diffraction for that specific orientation.

### Phase 2: Diffuse Intensity Extraction & Correction
This phase iterates through the valid pixels identified in Phase 1 and applies a series of physical corrections to obtain accurate intensity measurements.

*   **Data Extraction:** For every unmasked pixel in an image, its raw intensity is read, and its position is used to calculate the corresponding scattering vector ($\mathbf{q}$).
*   **Physical Corrections:** The intensity of each pixel is corrected for a series of experimental and geometric effects. The full correction is applied multiplicatively, combining four key factors:
    1.  **Lorentz-Polarization (LP):** Accounts for polarization and the geometry of diffraction.
    2.  **Quantum Efficiency (QE):** Account for detector sensitivity.
    3.  **Solid Angle ($\Omega$):** Normalizes intensity by the solid angle subtended by the pixel.
    4.  **Air Attenuation:** Corrects for X-ray absorption by air between the sample and detector.
*   **Output:** The result of this phase is a list of corrected `{q-vector, intensity, sigma}` data points for each individual still image.

### Phase 3: Voxelization, Scaling & Merging
This phase combines the processed data from all individual stills into a single, self-consistent 3D diffuse scattering map.

*   **Global Grid Definition:** A common 3D grid in reciprocal space is defined by first calculating an average crystal model from all successfully processed stills. This ensures all data can be mapped to a consistent reference frame.
*   **Voxel Accumulation:** The corrected diffuse data points from every still are binned into this global 3D grid. For memory efficiency with large datasets, this process is handled by a `VoxelAccumulator` that can use an HDF5 file as a backend.
*   **Relative Scaling:** A custom scaling model is refined to correct for inter-image variations ( changes in beam intensity or illuminated crystal volume). This model determines a multiplicative scale factor for each still, placing all datasets on a common relative scale.
*   **Merging:** The final scaled observations are merged. Within each voxel of the 3D grid, all contributing intensity measurements are combined using an inverse-variance weighted average to produce the final merged intensity and its associated error.

## Next Steps: Phase 4 (Pending)

The final phase of the pipeline will focus on placing the merged data onto an absolute scale and preparing it for scientific interpretation.

*   **Absolute Scaling:** The relatively-scaled diffuse map will be converted to absolute units ( electron units per unit cell). This will be achieved by matching the total experimental scattering (diffuse + Bragg) to the total theoretical scattering from a known unit cell composition, using the Krogh-Moe/Norman summation method.
*   **Incoherent Subtraction:** The theoretical incoherent (Compton) scattering background will be calculated from the sample composition and subtracted from the absolute-scaled map.
*   **Final Output:** The result will be the final, absolutely-scaled 3D diffuse scattering map, ready for analysis.

## Implementation and Library usage
We combine custom code with the DIALS/CCTBX/DXTBX ecosystem:

| Phase | Module / Step | Description | Primary Logic | Core Toolkit / API |
| :---- | :--- | :---------- | :--- | :--- |
| **Phase 1** | **Data Type Detection** | Determines if data is stills or sequence from CBF header. | **Custom** | `dxtbx.load` |
| | **Crystallographic Processing** | Spot finding, indexing, and geometric refinement. | **DIALS** | `dials.stills_process` / DIALS CLI |
| | **Geometric Validation** | Q-vector consistency check ($\mathbf{q}_{\text{model}}$ vs $\mathbf{q}_{\text{observed}}$). | **Custom** | `dxtbx.model` |
| | **Static/Dynamic Masking** | Creates masks for bad pixels, gaps, and beamstop. | **Custom** | `dxtbx.model.Detector` |
| | **Bragg Mask Generation** | Creates masks to exclude Bragg peak regions for each still. | **DIALS / Custom** | `dials.util.masking` or Custom shoebox logic |
| **Phase 2** | **Q-Vector Calculation** | Computes scattering vector $\mathbf{q}$ for each diffuse pixel. | **Custom** | `dxtbx.model` |
| | **LP & QE Corrections** | Applies Lorentz-Polarization & Quantum Efficiency factors. | **DIALS** | `dials.algorithms.integration.Corrections` |
| | **Solid Angle ($\Omega$) Correction** | Applies solid angle correction factor to pixel intensities. | **Custom** | `dxtbx.model.Panel` |
| | **Air Attenuation Correction** | Applies air absorption correction using Beer-Lambert law. | **Custom** | `cctbx.eltbx` |
| **Phase 3** | **Global Grid Definition** | Averages crystal models to define a common 3D grid. | **Custom** | `cctbx.uctbx` & `dxtbx.model` |
| | **Voxel Accumulation** | Bins corrected diffuse pixels into the global grid. | **Custom** | `h5py` & `cctbx.sgtbx` |
| | **Relative Scaling Model** | Defines the mathematical model for inter-still scaling. | **Custom** | `dials.algorithms.scaling.model` |
| | **Scaling Parameter Refinement** | Refines scale factors to minimize intensity discrepancies. | **Custom** | `scitbx.lstbx` |
| | **Data Merging** | Performs inverse-variance weighted merge of scaled data. | **Custom** | `numpy` |
| **Phase 4** | **Absolute Scale Calculation** | Determines absolute scale factor via Krogh-Moe summation. | *(Pending)* | `cctbx.eltbx` |
| *(Pending)* | **Incoherent Subtraction** | Calculates and subtracts Compton scattering background. | *(Pending)* | `cctbx.eltbx` |
</file>

<file path="src/diffusepipe/types/types_IDL.md">
// == BEGIN IDL ==
module src.diffusepipe.types {

    // Configuration for DIALS stills_process Python API execution by the orchestrator
    // Behavior: Defines parameters to configure dials.stills_process.
    struct DIALSStillsProcessConfig {
        // Preconditions: Path to an existing, readable PHIL file containing comprehensive
        // parameters for dials.stills_process. This is the primary way to configure it.
        stills_process_phil_path: optional string;

        // Behavior: Overrides automatic CBF data type detection. Valid values: 'stills', 'sequence', or null for auto-detection.
        // When null, the system will analyze CBF headers to determine the appropriate processing pathway.
        force_processing_mode: optional string;

        // Behavior: A list of PHIL parameter strings (e.g., 'parameter=value') to be applied specifically when the sequence processing route is chosen.
        // These overrides are applied *after* any base PHIL file for sequence processing and can fine-tune sequence-specific steps.
        sequence_processing_phil_overrides: optional list<string>;

        // Behavior: If true (default), automatic CBF data type detection (Module 1.S.0) is performed.
        // If false, the system might default to a specific processing mode or require `force_processing_mode` to be set.
        data_type_detection_enabled: optional boolean; // Default to True in implementation

        // Behavior: Known unit cell for indexing, e.g., "a,b,c,alpha,beta,gamma".
        // Overrides or supplements PHIL file if provided.
        known_unit_cell: optional string;

        // Behavior: Known space group for indexing, e.g., "P1", "C2".
        // Overrides or supplements PHIL file if provided.
        known_space_group: optional string;

        // Behavior: Spot finding algorithm, e.g., "dispersion".
        // Overrides or supplements PHIL file if provided.
        spotfinder_threshold_algorithm: optional string;

        // Behavior: Minimum spot area for spot finding.
        // Overrides or supplements PHIL file if provided.
        min_spot_area: optional int;

        // Behavior: If true, ensures shoeboxes are saved by dials.stills_process (needed for some Bragg mask generation methods).
        // Overrides or supplements PHIL file if provided.
        output_shoeboxes: optional boolean;

        // Behavior: If true, ensures partialities are calculated and output by dials.stills_process.
        // Note: While partialities are still useful for DIALS integration quality assessment,
        // they will NOT be used as quantitative divisors in this pipeline's scaling (due to
        // inaccuracies for true stills). Instead, P_spot serves as a threshold filter.
        // Overrides or supplements PHIL file if provided.
        calculate_partiality: optional boolean; // Should default to true in implementation

        // Add other critical, frequently tuned dials.stills_process parameters here as needed.
        // The adapter layer will be responsible for merging these with the PHIL file content.
    }

    // Parameters for the DataExtractor component
    // Behavior: Defines all settings controlling the diffuse data extraction process from a single image.
    struct ExtractionConfig {
        // Behavior: Low-resolution limit (maximum d-spacing in Angstroms). Data beyond this (smaller |q|) is excluded.
        min_res: optional float;

        // Behavior: High-resolution limit (minimum d-spacing in Angstroms). Data beyond this (larger |q|) is excluded.
        max_res: optional float;

        // Behavior: Minimum pixel intensity (after gain, corrections, background subtraction) to be included.
        min_intensity: optional float;

        // Behavior: Maximum pixel intensity (after gain, corrections, background subtraction) to be included (e.g., to filter saturated pixels).
        max_intensity: optional float;

        // Preconditions: Must be a positive float.
        // Behavior: Detector gain factor applied to raw pixel intensities.
        gain: float;

        // Preconditions: Must be a non-negative float (e.g., 0.01 for 1% tolerance).
        // Behavior: Fractional tolerance for comparing DIALS-derived cell lengths with an external PDB reference.
        cell_length_tol: float;

        // Preconditions: Must be a non-negative float (e.g., 0.1 for 0.1 degrees tolerance).
        // Behavior: Tolerance in degrees for comparing DIALS-derived cell angles with an external PDB reference.
        cell_angle_tol: float;

        // Preconditions: Must be a non-negative float.
        // Behavior: Tolerance in degrees for comparing DIALS-derived crystal orientation with an external PDB reference.
        orient_tolerance_deg: float;

        // Preconditions: Must be a positive float (e.g., 0.01 for 0.01 Å⁻¹ tolerance).
        // Behavior: Tolerance in Å⁻¹ for q-vector consistency checks in geometric model validation.
        // Used in Module 1.S.1.Validation to compare |q_model - q_observed|.
        q_consistency_tolerance_angstrom_inv: float;


        // Preconditions: Must be a positive integer.
        // Behavior: Process every Nth pixel (e.g., 1 for all pixels, 2 for every other).
        pixel_step: int;

        // Behavior: If true, Lorentz-Polarization correction is applied using DIALS Corrections API.
        // This leverages the robust, well-tested dials.algorithms.integration.Corrections class.
        lp_correction_enabled: boolean;

        // Behavior: Path to a pre-processed background image/map (e.g., NPZ or image format) to be subtracted pixel-wise.
        // This takes precedence over `subtract_constant_background_value` if both are provided.
        subtract_measured_background_path: optional string;

        // Behavior: A constant value to be subtracted from all pixels if `subtract_measured_background_path` is not used.
        subtract_constant_background_value: optional float;

        // Behavior: If true, diagnostic plots (e.g., q-distribution, intensity histograms) are generated.
        plot_diagnostics: boolean;

        // Behavior: If true, enables verbose logging output during extraction.
        verbose: boolean;
    }

    // Configuration for the relative scaling model (future DiffuseDataMerger component)
    // Behavior: Controls the complexity and parameters of the custom scaling model used in Module 3.S.3.
    struct RelativeScalingConfig {
        // Behavior: If true, refines a per-still (or per-group) overall multiplicative scale factor.
        // This is part of the initial default multiplicative-only model.
        refine_per_still_scale: boolean; // Should default to true

        // Behavior: If true, refines a 1D resolution-dependent multiplicative scale factor.
        // This is optionally part of the initial default multiplicative-only model.
        refine_resolution_scale_multiplicative: boolean; // Should default to false initially

        // Behavior: Number of bins for resolution-dependent scaling if enabled.
        resolution_scale_bins: optional int;

        // Behavior: If true, refines additive offset components (e.g., background terms).
        // CRITICAL: This **must** be `false` for the initial v1 implementation to avoid parameter
        // correlation issues and simplify error propagation. Enabling this requires careful
        // validation and updates to error propagation logic (see plan.md, Module 3.S.4).
        // Only enable in future versions after the multiplicative model is stable and residuals clearly justify it.
        refine_additive_offset: boolean; // Should default to false and be hard-coded to false in v1.

        // Behavior: Minimum P_spot threshold for including Bragg reflections in reference generation.
        // Reflections below this threshold are excluded to avoid poor-quality data.
        min_partiality_threshold: float; // Should default to 0.1
    }

    // Overall pipeline configuration for processing stills
    // Behavior: Encapsulates all settings for the StillsPipelineOrchestrator.
    struct StillsPipelineConfig {
        dials_stills_process_config: DIALSStillsProcessConfig; // Updated
        extraction_config: ExtractionConfig;
        relative_scaling_config: RelativeScalingConfig; // Configuration for future scaling component
        run_consistency_checker: boolean; // If true, ConsistencyChecker is run after successful extraction.
        run_q_calculator: boolean;      // If true, QValueCalculator is run after successful extraction.
    }

    // Represents a set of related input file paths for a component
    // Behavior: Standardized way to pass file dependencies to components. Fields are optional to allow flexibility for different components.
    struct ComponentInputFiles {
        // Behavior: Path to the primary CBF image file being processed.
        cbf_image_path: optional string;

        // Behavior: Path to the DIALS experiment list JSON file (.expt) corresponding to the cbf_image_path.
        dials_expt_path: optional string;

        // Behavior: Path to the DIALS reflection table file (.refl) corresponding to the cbf_image_path.
        dials_refl_path: optional string;

        // Behavior: Path to the DIALS-generated Bragg mask pickle file.
        bragg_mask_path: optional string;

        // Behavior: Path to an external PDB file used for consistency checks (e.g., unit cell, orientation).
        external_pdb_path: optional string;
    }

    // Generic outcome for operations within components
    // Behavior: Standardized return type for component methods indicating success/failure and providing details.
    struct OperationOutcome {
        // Preconditions: Must be one of "SUCCESS", "FAILURE", "WARNING".
        status: string;
        message: optional string; // Human-readable message about the outcome.
        error_code: optional string; // A machine-readable code for specific error types.
        // Behavior: A map where keys are artifact names (e.g., "npz_file", "consistency_plot") and values are their file paths.
        output_artifacts: optional map<string, string>;
    }

    // Outcome for processing a single still image through the main pipeline
    // Behavior: Summarizes the results of all processing stages for a single input CBF image.
    struct StillProcessingOutcome {
        input_cbf_path: string; // Path to the original CBF file.
        // Preconditions: Must be one of "SUCCESS_ALL", "SUCCESS_DIALS_ONLY", "SUCCESS_EXTRACTION_ONLY", "FAILURE_DIALS", "FAILURE_EXTRACTION", "FAILURE_DIAGNOSTICS".
        status: string;
        message: optional string; // Overall message for this image's processing.
        working_directory: string; // Path to the dedicated working directory.
        dials_outcome: OperationOutcome; // Outcome of the DIALS processing steps.
        extraction_outcome: OperationOutcome; // Outcome of the DataExtractor.
        consistency_outcome: optional OperationOutcome; // Outcome of the ConsistencyChecker, if run.
        q_calc_outcome: optional OperationOutcome;      // Outcome of the QValueCalculator, if run.
    }
}
// == END IDL ==
</file>

<file path="src/diffusepipe/types/types_IDL.py">
"""
Python implementation of types defined in types_IDL.md.

This module implements Pydantic models corresponding to the IDL struct definitions
for type safety and validation throughout the pipeline.
"""

from typing import Dict, Optional, Any, List
from pydantic import BaseModel, Field


class DIALSStillsProcessConfig(BaseModel):
    """Configuration for DIALS stills_process Python API execution by the orchestrator."""

    stills_process_phil_path: Optional[str] = Field(
        None,
        description="Path to an existing, readable PHIL file containing comprehensive parameters for dials.stills_process",
    )
    force_processing_mode: Optional[str] = Field(
        None,
        description="Overrides automatic CBF data type detection. Valid values: 'stills', 'sequence', or None for auto-detection",
    )
    sequence_processing_phil_overrides: Optional[List[str]] = Field(
        None,
        description="A list of PHIL parameter strings to be applied specifically for sequence processing.",
    )
    data_type_detection_enabled: Optional[bool] = Field(
        True,  # Default to True
        description="If true, automatic CBF data type detection is performed.",
    )
    known_unit_cell: Optional[str] = Field(
        None, description="Known unit cell for indexing, e.g., 'a,b,c,alpha,beta,gamma'"
    )
    known_space_group: Optional[str] = Field(
        None, description="Known space group for indexing, e.g., 'P1', 'C2'"
    )
    spotfinder_threshold_algorithm: Optional[str] = Field(
        None, description="Spot finding algorithm, e.g., 'dispersion'"
    )
    min_spot_area: Optional[int] = Field(
        None, description="Minimum spot area for spot finding"
    )
    output_shoeboxes: Optional[bool] = Field(
        None, description="If true, ensures shoeboxes are saved by dials.stills_process"
    )
    calculate_partiality: Optional[bool] = Field(
        True,
        description="If true, ensures partialities are calculated and output by dials.stills_process",
    )


class ExtractionConfig(BaseModel):
    """Parameters for the DataExtractor component."""

    min_res: Optional[float] = Field(
        None, description="Low-resolution limit (maximum d-spacing in Angstroms)"
    )
    max_res: Optional[float] = Field(
        None, description="High-resolution limit (minimum d-spacing in Angstroms)"
    )
    min_intensity: Optional[float] = Field(
        None,
        description="Minimum pixel intensity (after gain, corrections, background subtraction) to be included",
    )
    max_intensity: Optional[float] = Field(
        None,
        description="Maximum pixel intensity (after gain, corrections, background subtraction) to be included",
    )
    gain: float = Field(
        description="Detector gain factor applied to raw pixel intensities"
    )
    cell_length_tol: float = Field(
        description="Fractional tolerance for comparing DIALS-derived cell lengths with an external PDB reference"
    )
    cell_angle_tol: float = Field(
        description="Tolerance in degrees for comparing DIALS-derived cell angles with an external PDB reference"
    )
    orient_tolerance_deg: float = Field(
        description="Tolerance in degrees for comparing DIALS-derived crystal orientation with an external PDB reference"
    )
    q_consistency_tolerance_angstrom_inv: float = Field(
        description="Tolerance in Å⁻¹ for q-vector consistency checks in geometric model validation"
    )
    pixel_step: int = Field(
        description="Process every Nth pixel (e.g., 1 for all pixels, 2 for every other)"
    )
    lp_correction_enabled: bool = Field(
        description="If true, Lorentz-Polarization correction is applied using DIALS Corrections API"
    )
    subtract_measured_background_path: Optional[str] = Field(
        None,
        description="Path to a pre-processed background image/map to be subtracted pixel-wise",
    )
    subtract_constant_background_value: Optional[float] = Field(
        None, description="A constant value to be subtracted from all pixels"
    )
    plot_diagnostics: bool = Field(
        description="If true, diagnostic plots are generated"
    )
    verbose: bool = Field(
        description="If true, enables verbose logging output during extraction"
    )
    air_temperature_k: Optional[float] = Field(
        293.15, description="Air temperature in Kelvin for air attenuation correction (default: 20°C)"
    )
    air_pressure_atm: Optional[float] = Field(
        1.0, description="Air pressure in atmospheres for air attenuation correction (default: 1 atm)"
    )
    save_original_pixel_coordinates: bool = Field(
        True, description="If true, saves original panel IDs and pixel coordinates in NPZ output for visual diagnostics"
    )
    external_pdb_path: Optional[str] = Field(
        None, description="Path to an external PDB file used for consistency checks"
    )


class RelativeScalingConfig(BaseModel):
    """Configuration for the relative scaling model."""

    refine_per_still_scale: bool = Field(
        True,
        description="If true, refines a per-still (or per-group) overall multiplicative scale factor",
    )
    refine_resolution_scale_multiplicative: bool = Field(
        False,
        description="If true, refines a 1D resolution-dependent multiplicative scale factor",
    )
    resolution_scale_bins: Optional[int] = Field(
        None, description="Number of bins for resolution-dependent scaling if enabled"
    )
    refine_additive_offset: bool = Field(
        False,
        description="If true, refines additive offset components (e.g., background terms)",
    )
    min_partiality_threshold: float = Field(
        0.1,
        description="Minimum P_spot threshold for including Bragg reflections in reference generation",
    )
    # Phase 3 additional configuration
    grid_config: Optional[Dict[str, Any]] = Field(
        None,
        description="Configuration for GlobalVoxelGrid (d_min_target, d_max_target, ndiv_h/k/l)",
    )
    voxel_accumulator_backend: str = Field(
        "memory",
        description="Backend for VoxelAccumulator: 'memory' or 'hdf5'",
    )
    refinement_config: Optional[Dict[str, Any]] = Field(
        None,
        description="Configuration for scaling refinement (max_iterations, convergence_tolerance)",
    )
    merge_config: Optional[Dict[str, Any]] = Field(
        None,
        description="Configuration for data merging (outlier_rejection, minimum_observations)",
    )


class StillsPipelineConfig(BaseModel):
    """Overall pipeline configuration for processing stills."""

    dials_stills_process_config: DIALSStillsProcessConfig
    extraction_config: ExtractionConfig
    relative_scaling_config: Optional[RelativeScalingConfig] = Field(
        None,
        description="Configuration for Phase 3 relative scaling. If None, Phase 3 is skipped"
    )
    run_consistency_checker: bool = Field(
        description="If true, ConsistencyChecker is run after successful extraction"
    )
    run_q_calculator: bool = Field(
        description="If true, QValueCalculator is run after successful extraction"
    )


class ComponentInputFiles(BaseModel):
    """Represents a set of related input file paths for a component."""

    cbf_image_path: Optional[str] = Field(
        None, description="Path to the primary CBF image file being processed"
    )
    dials_expt_path: Optional[str] = Field(
        None, description="Path to the DIALS experiment list JSON file (.expt)"
    )
    dials_refl_path: Optional[str] = Field(
        None, description="Path to the DIALS reflection table file (.refl)"
    )
    bragg_mask_path: Optional[str] = Field(
        None, description="Path to the DIALS-generated Bragg mask pickle file"
    )
    external_pdb_path: Optional[str] = Field(
        None, description="Path to an external PDB file used for consistency checks"
    )


class OperationOutcome(BaseModel):
    """Generic outcome for operations within components."""

    status: str = Field(description="Must be one of 'SUCCESS', 'FAILURE', 'WARNING'")
    message: Optional[str] = Field(
        None, description="Human-readable message about the outcome"
    )
    error_code: Optional[str] = Field(
        None, description="A machine-readable code for specific error types"
    )
    output_artifacts: Optional[Dict[str, Any]] = Field(
        None,
        description="A map where keys are artifact names and values are their file paths or objects",
    )


class StillProcessingOutcome(BaseModel):
    """Outcome for processing a single still image through the main pipeline."""

    input_cbf_path: str = Field(description="Path to the original CBF file")
    status: str = Field(
        description="Must be one of 'SUCCESS_ALL', 'SUCCESS_DIALS_ONLY', 'SUCCESS_EXTRACTION_ONLY', 'FAILURE_DIALS', 'FAILURE_EXTRACTION', 'FAILURE_DIAGNOSTICS'"
    )
    message: Optional[str] = Field(
        None, description="Overall message for this image's processing"
    )
    working_directory: str = Field(
        description="Path to the dedicated working directory"
    )
    dials_outcome: OperationOutcome = Field(
        description="Outcome of the DIALS processing steps"
    )
    extraction_outcome: OperationOutcome = Field(
        description="Outcome of the DataExtractor"
    )
    consistency_outcome: Optional[OperationOutcome] = Field(
        None, description="Outcome of the ConsistencyChecker, if run"
    )
    q_calc_outcome: Optional[OperationOutcome] = Field(
        None, description="Outcome of the QValueCalculator, if run"
    )
</file>

<file path="plan.md">
**`plan.md`**

**Stills Diffuse Scattering Processing Plan (DIALS-Integrated with Integrated Testing)**

**Nomenclature:**

*   `I_raw(px, py, i)`: Raw intensity at detector pixel `(px, py)` for still image `i`. Accessed via the `get_raw_data(0)` method of the `dxtbx.imageset.ImageSet` object corresponding to still `i`.
*   `t_exp(i)`: Exposure time for still `i`. Accessed via the `get_scan().get_exposure_times()[0]` method of the `dxtbx.imageset.ImageSet` object for still `i`, or from image header metadata if a scan object is not present.
*   `Experiment_dials_i`: A `dxtbx.model.experiment_list.ExperimentList` object (typically containing a single `Experiment`) for still `i`, output from `dials.stills_process`. It contains the unique crystal model `Crystal_i` (a `dxtbx.model.Crystal` object).
*   `Reflections_dials_i`: A `dials.array_family.flex.reflection_table` for still `i`, output from `dials.stills_process`. It contains indexed Bragg spots, including partiality values in a column named `"partiality"`.
*   `Mask_pixel`: A global static bad pixel mask (e.g., beamstop, bad pixels, panel gaps), represented as a tuple/list of `dials.array_family.flex.bool` arrays, one per detector panel.
*   `BraggMask_2D_raw_i(px, py)`: A boolean mask for still `i` excluding its own Bragg peak regions, generated from `dials.stills_process` outputs (e.g., via `dials.generate_mask`). Represented as a tuple/list of `dials.array_family.flex.bool` arrays.
*   `Mask_total_2D_i(px, py)`: The combined mask for still `i`, calculated as `Mask_pixel AND (NOT BraggMask_2D_raw_i)`.
*   `v`: Voxel index in the 3D reciprocal space grid defined in Phase 3.
*   `p`: Detector pixel index, typically a tuple `(panel_index, slow_scan_pixel_coord, fast_scan_pixel_coord)`.
*   `|q|`: Magnitude of the scattering vector `q_vector`, calculated as `q_vector.length()`.
*   `H,K,L`: Integer Miller indices, often stored as `dials.array_family.flex.miller_index`.
*   `h,k,l`: Continuous (fractional) Miller indices, resulting from transforming `q_vector` using `Crystal_i.get_A().inverse()` or `Crystal_avg_ref.get_A().inverse()`.
*   `i`: Still image index or a unique identifier for a still image.
*   `P_spot`: Partiality of an observed Bragg reflection, obtained from the `"partiality"` column of `Reflections_dials_i`.

---
**0. Testing Principles and Conventions**

*   **0.1 Granular Testing:** Each significant computational module or processing step defined in this plan will have corresponding tests to verify its correctness and expected outputs based on controlled inputs.
*   **0.2 Input Data Strategy:**
    *   Initial pipeline steps consuming raw still images (Module 1.S.1) will be tested using a small, curated set of real representative still image files (e.g., CBF format). For continuous integration (CI), tiny image stubs (e.g., minimal valid CBF files representing a few detector tiles) or runtime-generated simulated HDF5/CBF-like images will be used to manage repository size and avoid licensing issues.
    *   Subsequent modules will primarily be tested using the serialized output files (e.g., per-still `.expt` files, `.refl` tables, `.pickle` mask files from DIALS; NPZ files from Python components) generated from successfully running tests of preceding modules. This promotes end-to-end validation of data flow and component integration.
    *   For highly focused unit tests of specific algorithms or utility functions *within* a module, inputs may be programmatically constructed in test code if it enhances clarity or allows for more precise control over edge cases.
*   **0.3 Test Data Management:** All test input files (curated CBFs, serialized DIALS outputs, reference outputs, tiny image stubs) will be stored in a dedicated directory within the test suite (e.g., `tests/data/`).
*   **0.4 Verification:** Assertions will check the correctness of calculations, data transformations, and the structure/content of output data structures against pre-calculated expected values or known properties. For modules producing file outputs, tests may compare against reference output files or check key statistics.
*   **0.5 Scope:** Testing will focus on the logic implemented within this project. The internal correctness of external tools like DIALS is assumed, but their integration and invocation by this project (via the adapter layer) will be tested.
*   **0.7 Correction-factor Sign Convention (NEW):**  
    *   **All per-pixel corrections are expressed as *multiplicative* factors**.  
    *   Any divisor returned by an external API (e.g., `Corrections.lp()`) is **immediately inverted** once and stored as a multiplier.  
    *   A helper in `diffusepipe.corrections` (`apply_corrections(raw_I, lp_mult, qe_mult, sa_mult, air_mult)`) centralises this logic and is covered by a regression test using an analytic pixel at 45 °.  The rest of the plan therefore speaks only of multipliers.
*   **0.6 Adapter Layer:** External DIALS/CCTBX/DXTBX Python API calls (e.g., to `dials.stills_process` Python components, scaling framework components, CCTBX utilities) **shall be wrapped** in a thin, project-specific adapter layer residing within the `diffusepipe` package. This adapter layer will be unit-tested against expected behavior based on DIALS/CCTBX documentation and observed outputs, and the main pipeline logic will call these adapters. This localizes changes if the external API evolves and simplifies mocking for higher-level tests.

**0.6 Adapter Layer Enhancement for Dual Processing Modes:**
External DIALS processing **shall be wrapped** in two complementary adapter implementations:

*   **`DIALSStillsProcessAdapter`:** Wraps `dials.stills_process` Python API for true still images (Angle_increment = 0.0°).
*   **`DIALSSequenceProcessAdapter`:** Implements CLI-based sequential workflow for oscillation data (Angle_increment > 0.0°).

Both adapters **must** produce identical output interfaces (`Experiment` and `reflection_table` objects) to ensure downstream compatibility. The choice between adapters is determined by Module 1.S.0 data type detection.

**Critical PHIL Parameters for Sequence Processing (to be used by `DIALSSequenceProcessAdapter`):**
*   `spotfinder.filter.min_spot_size=3` (not default 2)
*   `spotfinder.threshold.algorithm=dispersion` (not default)
*   `indexing.method=fft3d` (not fft1d)
*   `geometry.convert_sequences_to_stills=false` (preserve oscillation)

**0.7 Geometric Validation Strategy Decision:**
The project follows **Q-vector consistency checking as the primary geometric validation method** for Module 1.S.1.Validation. This approach compares `q_model` (derived from DIALS-refined crystal models and Miller indices) with `q_observed` (recalculated from observed pixel coordinates) using the tolerance `q_consistency_tolerance_angstrom_inv`. 

Pixel-based position validation (`|observed_px - calculated_px|`) serves as a secondary diagnostic tool or simpler fallback method when Q-vector validation proves persistently problematic, but is **not** the primary planned validation approach. This decision ensures robust geometric validation while maintaining compatibility with crystallographic conventions.

---

**0.X. Intermediate Quality Control (QC) Checkpoints**

To facilitate early error detection and build confidence in the pipeline's outputs, specific QC metrics and reports will be generated by the `StillsPipelineOrchestrator` after each major data processing phase. These may include simple plots (e.g., saved as PNGs) and text summaries.

*   **After Module 1.S.1 (Per-Still Crystallographic Processing):**
    *   **Metrics/Reports:**
        *   Indexing success rate (percentage of stills successfully indexed).
        *   Distribution of refined unit cell parameters (a, b, c, alpha, beta, gamma) across all indexed stills (histograms, mean, stddev).
        *   Distribution of the number of indexed spots per still.
        *   Distribution of `P_spot` (partiality) values from `Reflections_dials_i`.
        *   RMSD of spot predictions if refinement was performed.
    *   **Purpose:** Assess overall success of DIALS processing, identify problematic stills or batches, evaluate consistency of crystal models.

*   **After Module 2.S.2 (Per-Still Diffuse Intensity Extraction & Correction):**
    *   **Metrics/Reports (for a subset of representative stills):**
        *   Histograms of raw pixel intensities vs. corrected intensities.
        *   2D heatmaps of `TotalCorrection_mult(p)` on the detector.
        *   Q-space coverage plots (e.g., projection of `q_vector` for accepted pixels) for a few sample stills.
        *   Number of pixels passing all filters vs. total unmasked pixels.
    *   **Purpose:** Verify correction factors are reasonable, check filter effectiveness, inspect q-space sampling per still.

*   **After Module 3.S.4 (Merging Relatively Scaled Data into Voxels):**
    *   **Metrics/Reports:**
        *   Overall R-factor or residual statistics from the relative scaling (Module 3.S.3).
        *   Plot of refined scale factors (e.g., `b_i` values vs. `p_order(i)` if applicable).
        *   Redundancy (number of observations per voxel) map/histogram for the `GlobalVoxelGrid`.
        *   Mean intensity vs. resolution for `I_merged_relative`.
    *   **Purpose:** Assess quality and stability of relative scaling, examine data completeness and consistency in the merged dataset.

*   **After Module 4.S.1 (Absolute Scaling and Incoherent Subtraction):**
    *   **Metrics/Reports:**
        *   The determined `Scale_Abs` factor.
        *   Plot of radially averaged experimental total scattering vs. theoretical total scattering (used for determining `Scale_Abs`).
        *   Plot of final `I_abs_diffuse` vs. `|q|`, compared with theoretical coherent scattering `F_calc_sq_voxel`.
        *   Wilson plot of scaled Bragg intensities.
    *   **Purpose:** Validate absolute scale, check appropriateness of incoherent subtraction, overall sanity check of final diffuse map.

Initially, these QC reports (plots and text summaries saved to the output directory) are intended for user review and manual assessment of the pipeline's performance and the quality of intermediate data at each stage. Automatic pipeline interruption or decision-making based on QC metrics can be considered as a future enhancement if specific, robust thresholds and criteria can be defined.

---

**Phase 1: Per-Still Geometry, Indexing, and Initial Masking**

**Module 1.S.0: CBF Data Type Detection and Processing Route Selection**
*   **Action:** Analyze CBF file headers to determine if data is true stills (Angle_increment = 0.0°) or sequence data (Angle_increment > 0.0°), then route to appropriate processing pathway.
*   **Input (per still `i`):**
    *   File path to raw CBF image `i`.
    *   Configuration `config.dials_stills_process_config.force_processing_mode` (optional, to override detection).
*   **Process:**
    1.  If `force_processing_mode` is set ("stills" or "sequence"), use that.
    2.  Else, parse CBF header of image `i` to extract `Angle_increment` value.
        *   This requires a utility function (e.g., in `diffusepipe.utils.cbf_utils`) that can robustly read CBF headers (e.g., using `dxtbx.load` or minimal parsing).
    3.  Determine data type:
        *   IF `Angle_increment` is present and `Angle_increment == 0.0`: Route to stills processing pathway (`DIALSStillsProcessAdapter`).
        *   IF `Angle_increment` is present and `Angle_increment > 0.0`: Route to sequence processing pathway (`DIALSSequenceProcessAdapter`).
        *   IF `Angle_increment` is not found or ambiguous (and not overridden): Default to sequence processing pathway (safer) and log a warning.
    4.  Log the determined `data_type` and `processing_route` for debugging.
*   **Output (per still `i`):**
    *   `processing_route`: String ("stills" or "sequence") indicating which adapter to use.
*   **Testing:**
    *   **Input:** Sample CBF files with `Angle_increment = 0.0`, `Angle_increment > 0.0`, and missing `Angle_increment`.
    *   **Verification:** Assert correct `processing_route` determination. Test override logic.

**Module 1.S.1: Per-Still Crystallographic Processing and Model Validation**
*   **Action:** For each input image `i`, use the appropriate DIALS processing pathway (determined by Module 1.S.0) to perform spot finding, indexing, optional geometric refinement, and integrate Bragg reflections. This process determines the crystal orientation `U_i`, unit cell `Cell_i`, and reflection partialities `P_spot` for each image.
*   **Input (per still `i` or a small batch of stills):**
    *   File path(s) to raw still image(s).
    *   Base experimental geometry information (`DE_base`), potentially provided as a reference DIALS `.expt` file or constructed programmatically. This includes common detector and source models.
    *   Configuration for DIALS processing (e.g., an instance of `DIALSStillsProcessConfig` from `types_IDL.md`). The adapter layer will translate this into PHIL parameters for the selected DIALS workflow. This configuration must ensure:
        *   Calculation and output of reflection partialities (critical).
        *   Saving of shoeboxes if Option B in Module 1.S.3 (Bragg mask from shoeboxes) is chosen.
        *   Appropriate spot finding, indexing, refinement, and integration strategies for the specific still dataset.
        *   Error handling settings (e.g., `squash_errors = False` for debugging).
    *   Known unit cell parameters and space group information can be part of `DIALSStillsProcessConfig` and passed as hints to `dials.stills_process`.
*   **Process (Orchestrated per still `i` or small batch by the `StillsPipelineOrchestrator` component, which uses Module 1.S.0 to select and call the appropriate adapter):**
    To efficiently process datasets containing numerous still images, the `StillsPipelineOrchestrator` **shall implement parallel execution** for this module. This will typically involve distributing the processing of individual stills (or small, independent chunks of stills) across multiple CPU cores, for example, using Python's `multiprocessing.Pool`. Each worker process will execute the full adapter logic for its assigned still(s). The orchestrator will be responsible for managing this parallel execution, collecting results (including success/failure status and paths to key output files written to still-specific working directories), and aggregating logs or error messages.
    The subsequent numbered steps describe the logic performed *within each parallel worker* or for each still:

    **Route A: True Stills Processing (Angle_increment = 0.0°):**
    1.  The `DIALSStillsProcessAdapter` initializes a `dials.command_line.stills_process.Processor` instance. It constructs the necessary PHIL parameters from the input `DIALSStillsProcessConfig`.
    2.  The adapter calls `dials.command_line.stills_process.do_import()` (or equivalent logic within the `Processor`) using the image file path and base geometry to create an initial `dxtbx.model.experiment_list.ExperimentList` for the still.
    3.  The adapter invokes the main processing method of the `Processor` instance (e.g., `processor.process_experiments()`) on the imported experiment(s). This step internally handles spot finding, indexing, refinement, and integration.
    4.  The adapter collects the output `integrated_experiments` and `integrated_reflections` from the `Processor` instance.

    **Route B: Sequence Processing (Angle_increment > 0.0°):**
    1.  The `DIALSSequenceProcessAdapter` is used, which implements a CLI-based sequential workflow.
    2.  Execute `dials.import` with sequence-appropriate parameters.
    3.  Execute `dials.find_spots` with critical PHIL parameters:
        *   `spotfinder.filter.min_spot_size=3`
        *   `spotfinder.threshold.algorithm=dispersion`
    4.  Execute `dials.index` with parameters:
        *   `indexing.method=fft3d`
        *   `geometry.convert_sequences_to_stills=false`
        *   (Known unit cell and space group from `DIALSStillsProcessConfig` are applied here).
    5.  Execute `dials.integrate` with sequence integration parameters.
    6.  The adapter loads the output experiment and reflection objects from the files generated by the DIALS CLI tools.

    **Common Continuation (after Route A or Route B):**
    5.  If the selected DIALS processing adapter reports success, proceed to Sub-Module 1.S.1.Validation. If validation fails, set `StillProcessingOutcome.status` to "FAILURE_GEOMETRY_VALIDATION", record details, log to summary, and proceed to the next CBF file.
    6.  If successful, retrieve `Experiment_dials_i` and `Reflections_dials_i` objects. These objects **must** have an identical structure regardless of the processing route taken, ensuring downstream compatibility.
*   **Output (for each successfully processed still `i`):**
    *   `Experiment_dials_i`: The `dxtbx.model.Experiment` object from `integrated_experiments` corresponding to still `i`, containing the refined `Crystal_i`.
    *   `Reflections_dials_i`: A `dials.array_family.flex.reflection_table` (selected from `integrated_reflections` by experiment `id` if `dials.stills_process` outputs a composite table for a batch) containing indexed Bragg spots for still `i`. This table **must** include a column named `"partiality"` containing `P_spot` values. **Note:** The quantitative reliability of `P_spot` values from `dials.stills_process` for true still images requires careful validation. See Module 3.S.3 for strategies on its use in scaling.
    *   (If configured in `dials.stills_process`) Shoebox data associated with `Reflections_dials_i`, stored within the reflection table or as separate files, if needed for Bragg mask generation (Option B in Module 1.S.3).
*   **Consistency Check:** Successful execution of `dials.stills_process` for the still (indicated by the adapter). Validity of the output `Crystal_i` model. Presence and reasonableness of values in the `"partiality"` column of `Reflections_dials_i`.

*   **Sub-Module 1.S.1.Validation: Geometric Model Consistency Checks**
    *   **Action:** Immediately after successful DIALS processing for still `i`, perform geometric consistency checks on the generated `Experiment_dials_i` and `Reflections_dials_i`.
    *   **Input (per still `i`):**
        *   `Experiment_dials_i` (from main 1.S.1 output).
        *   `Reflections_dials_i` (from main 1.S.1 output).
        *   `external_pdb_path` (if provided in the global pipeline `config.extraction_config.external_pdb_path`).
        *   Configuration for tolerances (e.g., `config.extraction_config.cell_length_tol`, `config.extraction_config.cell_angle_tol`, `config.extraction_config.orient_tolerance_deg`, and `config.extraction_config.q_consistency_tolerance_angstrom_inv` for Q-vector validation).
    *   **Process:**
        1.  **PDB Consistency Checks (if `external_pdb_path` provided):**
            a.  Compare unit cell parameters (lengths and angles) from `Experiment_dials_i.crystal` against the reference PDB, using `config.extraction_config.cell_length_tol` and `config.extraction_config.cell_angle_tol`.
            b.  Compare crystal orientation (`Experiment_dials_i.crystal.get_A()`) against the reference PDB (potentially by comparing U matrices after aligning B matrices, or by comparing the A matrix to a conventionally set PDB A matrix), using `config.extraction_config.orient_tolerance_deg`.
            c.  If any PDB consistency check fails, flag this still as failing validation.
        2.  **Internal Q-Vector Consistency Check:**
            a.  For a representative subset of indexed reflections in `Reflections_dials_i` (e.g., up to 500 randomly selected reflections):
                i.  **Calculate `q_model`**: This is typically derived from the reflection's `s1` vector (from `Reflections_dials_i`) and the beam's `s0` vector (from `Experiment_dials_i.beam`), such that `q_model = s1 - s0`. The `s1` vector in `Reflections_dials_i` is calculated by DIALS based on the refined crystal model and Miller index.
                ii. **Calculate `q_observed`**:
                    1.  Obtain the observed pixel centroid coordinates (e.g., from `xyzobs.px.value` or `xyzcal.px` in `Reflections_dials_i`).
                    2.  Convert these pixel coordinates to laboratory frame coordinates using `Experiment_dials_i.detector[panel_id].get_pixel_lab_coord()`.
                    3.  From the lab coordinates and `Experiment_dials_i.beam.get_s0()`, calculate the scattered beam vector `s1_observed`.
                    4.  Compute `q_observed = s1_observed - Experiment_dials_i.beam.get_s0()`.
                iii. Calculate the difference vector `Δq = q_model - q_observed` and its magnitude `|Δq|`.
            b.  Calculate statistics on these `|Δq|` values (mean, median, max, count).
            c.  If the mean `|Δq|` exceeds `config.extraction_config.q_consistency_tolerance_angstrom_inv` OR the max `|Δq|` exceeds `(config.extraction_config.q_consistency_tolerance_angstrom_inv * 5)`, flag this still as failing validation.
        3.  **Diagnostic Plot Generation:** Generate and save diagnostic plots similar to those from the original `consistency_checker.py` (q-difference histogram, q-magnitude scatter, q-difference heatmap on detector).
    *   **Output (per still `i`):**
        *   `validation_passed_flag`: Boolean.
        *   Diagnostic metrics (e.g., mean `|Δq|`, max `|Δq|`, misorientation_angle_vs_pdb).
        *   Paths to saved diagnostic plots.
        *   `processing_route_used`: String indicating whether stills or sequence processing was used
    *   **Consequence of Failure:** If `validation_passed_flag` is false, the `StillsPipelineOrchestrator` should mark this still appropriately (e.g., `StillProcessingOutcome.status = "FAILURE_GEOMETRY_VALIDATION"`) and skip subsequent processing steps for this still (i.e., skip Module 1.S.3 and Phase 2).

*   **Testing (Module 1.S.1 - including Validation):**
    *   (Existing tests for DIALS processing adapter remain)
    *   **Testing for Dual Processing Mode Support:**
        *   **Data Type Detection Testing:**
            *   **Input:** CBF files with known `Angle_increment` values (0.0°, 0.1°, 0.5°)
            *   **Verification:** Assert correct routing to stills vs sequence processing
            
        *   **Sequence Processing Adapter Testing:**
            *   **Input:** CBF file with oscillation data, sequence processing configuration
            *   **Execution:** Call `DIALSSequenceProcessAdapter.process_still()`
            *   **Verification:** Assert successful processing and correct output object types
            
        *   **Processing Route Integration Testing:**
            *   **Input:** Mixed dataset with both stills and sequence CBF files
            *   **Verification:** Assert each file is processed with correct adapter and produces valid results
            
        *   **PHIL Parameter Validation Testing:**
            *   **Input:** Sequence data with incorrect PHIL parameters (default values)
            *   **Verification:** Assert processing failure, then success with correct parameters
    *   **Testing for Sub-Module 1.S.1.Validation:**
        *   **Input:** Sample `Experiment_dials_i`, `Reflections_dials_i`, (optional) mock PDB data, and tolerance configurations.
        *   **Execution:** Call the Python function(s) implementing the validation logic.
        *   **Verification (PDB Checks):**
            *   Assert correct pass/fail status when cell parameters are within/outside tolerance of mock PDB.
            *   Assert correct pass/fail status when orientation is within/outside tolerance of mock PDB.
        *   **Verification (Q-Vector Consistency):**
            *   Assert correct calculation of `q_model` and `q_observed` for sample reflections.
            *   Assert correct pass/fail status based on `|Δq|` against `q_consistency_tolerance_angstrom_inv`.
        *   **Verification (Plots):** Check that plot files are generated if requested (existence check, not necessarily content validation in unit tests).

**Module 1.S.2: Static and Dynamic Pixel Mask Generation**
*   **Action:** Create a 2D detector mask `Mask_pixel` based on detector properties, known bad regions (beamstop, panel gaps), and potentially dynamic features (hot/cold/negative pixels) observed across a representative subset of, or all, input stills. This mask is considered global for the dataset.
*   **Input:**
    *   `DE_base.detector`: The base `dxtbx.model.Detector` object.
    *   A representative subset of (or all) raw still images `I_raw(px, py, i)` for dynamic masking.
    *   Configuration parameters defining static masked regions (e.g., beamstop coordinates, untrusted panel/pixel lists).
*   **Process:**
    1.  Generate `Mask_static_panel(px,py)` for each panel using `panel.get_trusted_range_mask(raw_data_for_panel)` (using data from one representative still for the trusted range check if not all stills are processed for this), and by applying masks for beamstop, panel gaps, and user-defined untrusted regions. These are combined using logical AND operations on `flex.bool` arrays.
    2.  Generate `Mask_dynamic_panel(px,py)`: Iterate through the chosen subset of (or all) raw still images. For each pixel, identify if it consistently exhibits anomalous values (e.g., negative counts after pedestal correction, excessively high counts not associated with Bragg peaks). Combine these per-image dynamic masks using logical OR to create a final `Mask_dynamic_panel`.
    3.  `Mask_pixel_panel(px,py) = Mask_static_panel(px,py) AND Mask_dynamic_panel(px,py)`.
        *   **Implementation Note:** All per-pixel operations for dynamic mask generation (e.g., identifying negative, hot, or anomalous pixels across the subset of images) **must be implemented using vectorized array operations** (e.g., leveraging NumPy and DIALS `flex` array capabilities). Python `for` loops iterating over individual pixel indices for full-detector operations are to be avoided due to performance infeasibility.
*   **Output:** `Mask_pixel`: A tuple/list of 2D `dials.array_family.flex.bool` arrays, one per detector panel.
*   **Testing (Module 1.S.2):**
    *   **Input:** Programmatically constructed `dxtbx.model.Detector` object, sample `flex.int` or `flex.double` arrays representing raw pixel data for several "stills" with known anomalous pixels. Static mask configurations.
    *   **Execution:** Call the Python function for static and dynamic mask generation.
    *   **Verification:** Assert that the output `Mask_pixel` correctly identifies known bad regions based on static configurations and correctly flags anomalous pixels based on the dynamic analysis across the provided sample "stills".

**Module 1.S.3: Combined Per-Still Bragg and Pixel Mask Generation**
*   **Action:** For each successfully processed still `i`, generate its specific Bragg peak mask (`BraggMask_2D_raw_i`) using the outputs from Module 1.S.1. Then, combine this with the global `Mask_pixel` (from Module 1.S.2) to produce the final mask (`Mask_total_2D_i`) for diffuse data extraction for that still.
*   **Input (per still `i`):**
    *   `Experiment_dials_i` and `Reflections_dials_i` (from Module 1.S.1).
    *   (If Option B is chosen) Shoebox data associated with `Reflections_dials_i`.
    *   `Mask_pixel` (from Module 1.S.2).
    *   Configuration parameters for `dials.generate_mask` (if Option A is chosen) or for custom mask generation logic from shoeboxes.
*   **Process (per still `i`):**
    1.  **Generate `BraggMask_2D_raw_i`:**
        *   **Option A (Recommended):** Execute `dials.generate_mask` (via its Python API adapter) using `Experiment_dials_i` and `Reflections_dials_i` as input. Configure `dials.generate_mask` to create a mask that covers the regions occupied by these indexed Bragg spots.
        *   **Option B (Alternative, if shoeboxes are preferred):** If `dials.stills_process` was configured to save shoeboxes with foreground/background mask information, extract the foreground mask for each reflection in `Reflections_dials_i` and project/combine these onto a 2D per-panel mask to form `BraggMask_2D_raw_i`. This projection should use pixels flagged with `dials.algorithms.shoebox.MaskCode.Foreground` and/or `dials.algorithms.shoebox.MaskCode.Strong` to define Bragg regions, rather than relying on potentially noisy `P_spot` values for this masking purpose.
    2.  Combine with static/dynamic pixel mask: `Mask_total_2D_i(px,py) = Mask_pixel(px,py) AND (NOT BraggMask_2D_raw_i(px,py))`. The logical NOT inverts `BraggMask_2D_raw_i` so that Bragg regions are `false`.
        **Data Flow for Masks:**
            1.  The `StillsPipelineOrchestrator` maintains the global `Mask_pixel` (from Module 1.S.2).
            2.  For the current still `i`, the `StillsPipelineOrchestrator` generates a temporary `BraggMask_2D_raw_i` (using logic from this module).
            3.  The `StillsPipelineOrchestrator` then computes the temporary `Mask_total_2D_i = Mask_pixel AND (NOT BraggMask_2D_raw_i)`.
            4.  This `Mask_total_2D_i` object (e.g., tuple of `flex.bool` arrays) is passed directly as an argument to the `DataExtractor` (for Modules 2.S.1 & 2.S.2) along with the raw image data for still `i`.
            5.  After the `DataExtractor` has processed still `i`, the temporary `BraggMask_2D_raw_i` and `Mask_total_2D_i` for that still can be discarded by the `StillsPipelineOrchestrator` to conserve memory.
            This streamed approach ensures that only one (or a small batch) of complete per-still total masks needs to be in memory at any given time.
*   **Output (per still `i`):** `Mask_total_2D_i` (a tuple/list of `dials.array_family.flex.bool` arrays, one per panel).
*   **Testing (Module 1.S.3):**
    *   **Input:** Sample `Experiment_dials_i`, `Reflections_dials_i` (with known spot positions), and `Mask_pixel`. Configuration for mask generation.
    *   **Execution:** Call the adapter for `dials.generate_mask` or the shoebox processing logic, then combine the resulting Bragg mask with `Mask_pixel`.
    *   **Verification:** Assert that `Mask_total_2D_i` correctly excludes the known Bragg peak regions for that specific still's orientation and reflection list, and also incorporates the exclusions from `Mask_pixel`.

---

**Phase 2: Per-Still Diffuse Intensity Extraction and Pixel-Level Correction**
*(Corresponds to the `DataExtractor` IDL interface)*

**Note on Configuration:** The existing `cell_length_tol`, `cell_angle_tol`, and `orient_tolerance_deg` tolerances in ExtractionConfig are now used by Module 1.S.1.Validation instead of directly by the DataExtractor. A new field `q_consistency_tolerance_angstrom_inv` may need to be added to the configuration for the |Δq| tolerance in the validation step.

**Module 2.S.1: Pixel-Based Diffuse Data Extraction & Q-Calculation**
*   **Action:** For each still `i`, iterate through its detector pixels. If a pixel is deemed suitable for diffuse scattering analysis (i.e., it passes `Mask_total_2D_i`), extract its raw intensity and calculate its corresponding q-vector using the specific geometry of still `i`.
*   **Input (per still `i`):**
    *   Raw image data for still `i` (accessible via an `dxtbx.imageset.ImageSet` object for still `i`).
    *   `Experiment_dials_i` (containing `Detector_i`, `Beam_i`, and `Crystal_i` from Module 1.S.1).
    *   `Mask_total_2D_i` (from Module 1.S.3).
    *   Pixel step configuration (e.g., process every Nth pixel).
*   **Process (for each pixel `p=(panel_idx, py, px)` on still `i`, respecting pixel step):**
    1.  If `Mask_total_2D_i[panel_idx](py, px)` is `true`:
        a.  `I_raw_val = ImageSet_i.get_raw_data(0)[panel_idx][py, px]`. (The `0` index assumes the ImageSet for a still contains one image).
        b.  Calculate `q_vector(p)` using `Experiment_dials_i.detector[panel_idx]` and `Experiment_dials_i.beam`. The crystal model `Experiment_dials_i.crystal` is implicitly used by these detector/beam models if they were refined together. If `Experiment_dials_i.crystal.num_scan_points > 1` (unusual for a single still from `stills_process` but possible if it represents a micro-sweep), ensure `get_A_at_scan_point(0)` is used.
        c.  Store the tuple: `(q_vector(p), I_raw_val, panel_idx, px, py, still_identifier=i)`.
*   **Output (per still `i`):**
    *   `RawDiffusePixelList_i`: A list of tuples, where each tuple is `(q_vector, raw_intensity, original_panel_index, original_fast_pixel_coord, original_slow_pixel_coord, still_identifier)`. `q_vector` is a `scitbx.matrix.col` object.
    *   **Implementation Note on Performance:** If iterating through the *unmasked* pixels (after `Mask_total_2D_i` is applied) still results in a very large number of pixels per still, direct Python looping might be a bottleneck. In such cases, the implementation should first identify the coordinates of all `True` pixels in `Mask_total_2D_i` (e.g., `np.where(mask_numpy)`). These coordinates can then be passed as batches to efficient DXTBX/CCTBX functions for q-vector calculation (e.g., `panel.get_lab_coord_multiple()`).
*   **Testing (Module 2.S.1):**
    *   **Input:** Sample still image data (e.g., a small `flex` array), a corresponding `Experiment_dials_i` object, and a `Mask_total_2D_i`.
    *   **Execution:** Call the Python function(s) implementing this module.
    *   **Verification:** Assert that `RawDiffusePixelList_i` contains the correct q-vectors (calculated using `scitbx.matrix.col`) and raw intensity values for the expected unmasked pixels.

**Module 2.S.2: Pixel-Based Correction Factor Application**
*   **Action:** For each diffuse pixel extracted in Module 2.S.1, calculate and apply all relevant geometric and experimental correction factors to its intensity, and propagate uncertainties.
*   **Input (per still `i`):**
    *   `RawDiffusePixelList_i` (from Module 2.S.1).
    *   `Experiment_dials_i` (for geometry models: `Detector_i`, `Beam_i`, `Goniometer_i` if relevant).
    *   `t_exp(i)` (exposure time for still `i`).
    *   Detector gain value.
    *   Configuration parameters for all applicable corrections (e.g., detector thickness/material for efficiency, air path details if not derived from geometry).
    *   Background subtraction parameters (e.g., path to a pre-processed background map for still `i` or a relevant group, or a constant background value).
    *   Configuration for resolution and intensity filters.
*   **Process (for each entry `(q_vec, I_raw_val, panel_idx, px, py, still_id)` in `RawDiffusePixelList_i`):**
    1.  **Calculate Per-Pixel Correction Factors `TotalCorrection_mult(p)` Using DIALS Corrections API:**
        *   All individual correction factors **shall be converted to and combined as multipliers** to form `TotalCorrection_mult(p)`. The adapter layer or calculation logic for each specific correction type is responsible for this conversion.
        *   **DIALS Corrections Object Setup:** For each still `i`, the `DataExtractor` (or its adapter layer) will instantiate a `dials.algorithms.integration.Corrections` object using `Experiment_dials_i.beam`, `Experiment_dials_i.goniometer` (if present), and `Experiment_dials_i.detector`. This object provides robust, well-tested implementations for standard geometric corrections.
        *   **For the array of accepted diffuse pixel observations** (defined by their s1 vectors and panel IDs for still `i`):
            *   **Data Preparation for DIALS API:**
                *   From the `M` accepted diffuse pixel observations for still `i`, extract their calculated `s1` vectors into a `flex.vec3_double` array (`s1_flex_array`) of length `M`.
                *   Extract their `original_panel_index` values into a `flex.size_t` array (`panel_indices_flex_array`) of length `M`.
            *   **Lorentz-Polarization (LP) Correction:** `LP_divisors_array = corrections_obj.lp(s1_flex_array)`. Convert to multiplier: `LP_mult_array = 1.0 / LP_divisors_array`.
            *   **Detector Quantum Efficiency (QE) Correction:** `QE_multipliers_array = corrections_obj.qe(s1_flex_array, panel_indices_flex_array)`. Use directly: `QE_mult_array = QE_multipliers_array`.
            *   These `LP_mult_array` and `QE_mult_array` are `flex.double` arrays of length `M`, providing the correction factors for each of the `M` diffuse pixels.
            *   **Note:** The DIALS `Corrections` object handles complex effects like parallax internally for LP and QE corrections.
        *   **Custom Corrections (Not Available from DIALS Corrections for Arbitrary Diffuse Pixels):**
            *   **Solid Angle Correction:** The plan must retain a custom (but carefully validated) calculation for `SolidAngle_divisor(p)` based on `Experiment_dials_i.detector[panel_idx]` geometry and pixel coordinates, since diffuse pixels are not necessarily at Bragg positions where DIALS corrections are typically applied. Convert to multiplier: `SA_mult(p) = 1.0 / SolidAngle_divisor(p)`.
            *   **Air Attenuation Correction:** A custom calculation for `AirAttenuation_divisor(p)` (subsequently inverted to `Air_mult(p)`) will be implemented. This calculation must:
                i.  Determine the path length of the X-ray from the sample to the pixel `p`.
                ii. Use the X-ray wavelength (from `Experiment_dials_i.beam`) to determine the X-ray energy.
                iii. Calculate the linear attenuation coefficient of air (`μ_air`) at this energy. This **must not** use simple heuristics like λ³ scaling, especially for a wide range of energies. Instead, it should be based on the sum of mass attenuation coefficients for the primary constituents of air (e.g., N, O, Ar) at the given X-ray energy, multiplied by their respective partial densities. Mass attenuation coefficients should be sourced from standard databases (e.g., NIST, or via a library like `xraylib` or `XrayDB` if such a dependency is acceptable, or from pre-tabulated values within the project for key energies if a library is too heavy). The calculation should account for air density based on standard temperature and pressure, or allow these as configurable parameters if significant variations are expected.
                iv. Apply the Beer-Lambert law: `Attenuation = exp(-μ_air * path_length)`. The `AirAttenuation_divisor(p)` would be this `Attenuation` factor (since intensity is reduced by this factor), and `Air_mult(p) = 1.0 / Attenuation`.
        *   **Final Assembly:** `TotalCorrection_mult(p) = LP_mult(p) × QE_mult(p) × SA_mult(p) × Air_mult(p)`.  
          All four terms are already in multiplier form because of rule 0.7.
        *   **Regression Test Requirement:** A regression test must be implemented that, for a synthetic experiment and a few selected pixel positions (including off-Bragg positions), compares the individual `LP_divisor` and `QE_multiplier` values obtained from the DIALS `Corrections` adapter against trusted reference values or a separate, careful analytic calculation for those specific points. The custom Solid Angle and Air Attenuation calculations must also have dedicated unit tests with known geometric configurations.
    2.  **Background Subtraction:**
        *   If a background map relevant to still `i` is provided: Load map, retrieve value `I_bkg_map(p)` for the current pixel, and subtract it from `I_raw_val`. The variance of the background map value `Var_bkg(p)` must also be retrieved.
        *   If a constant background value `BG_const` is configured: `I_raw_val_bg_sub = I_raw_val - BG_const`. `Var_bkg(p)` is typically 0 unless `BG_const` has an associated uncertainty.
        *   If no background subtraction: `I_raw_val_bg_sub = I_raw_val`. `Var_bkg(p)` is 0.
    3.  **Apply Gain and Exposure Time Normalization:**
        *   `I_processed_per_sec = I_raw_val_bg_sub / t_exp(i)`.
        *   `Var_photon_initial = I_raw_val / gain` (variance of the original raw count before background subtraction, assuming Poisson statistics).
        *   `Var_processed_per_sec = (Var_photon_initial + Var_bkg(p)) / (t_exp(i))^2`.
    4.  **Apply Total Multiplicative Correction:**
        *   `I_corrected_val = I_processed_per_sec * TotalCorrection_mult(p)`.
    5.  **Error Propagation:**
        *   `Var_corrected_val = Var_processed_per_sec * (TotalCorrection_mult(p))^2`. This assumes `TotalCorrection_mult(p)` has negligible uncertainty. If uncertainties of correction factors are significant and known, they should be propagated here. This simplification (ignoring correction factor uncertainties) must be documented.
        *   `Sigma_corrected_val = sqrt(Var_corrected_val)`.
    6.  **Resolution and Intensity Filtering:**
        *   Calculate d-spacing from `q_vec.length()`.
        *   If `I_corrected_val` or its d-spacing fall outside the configured minimum/maximum intensity or resolution limits, discard this pixel observation.
*   **Output (per still `i`):**
    *   `CorrectedDiffusePixelList_i`: A list of tuples `(q_vector, I_corrected, Sigma_corrected, q_x, q_y, q_z, original_panel_index, original_fast_pixel_coord, original_slow_pixel_coord, still_identifier)` for all pixels that passed all filters. `q_x, q_y, q_z` are components of `q_vector`.
*   **Testing (Module 2.S.2):**
    *   **Input:** Sample `RawDiffusePixelList_i`, a corresponding `Experiment_dials_i` object, exposure time, gain, and configurations for all corrections and filters.
    *   **Execution:** Call the Python function(s) implementing this correction module.
    *   **Verification:** Assert that `I_corrected` and `Sigma_corrected` in `CorrectedDiffusePixelList_i` match expected values based on known correction formulas and inputs. Verify adherence to the "all multipliers" convention for corrections. Test filter logic.
    *   **DIALS Corrections Regression Test:** For a synthetic experiment, verify that `apply_corrections()` recovers the analytic intensity of a 45 ° pixel to <1 % once LP has been inverted.
    *   **Custom Corrections Unit Tests:** The custom Solid Angle and Air Attenuation calculations must have dedicated unit tests with known geometric configurations and expected theoretical values.
    *   **Validation Test for Combined Corrections:** Include a specific unit test that provides synthetic pixel data with known, simple geometric properties. This test should calculate `TotalCorrection_mult(p)` using the implemented logic (including calls to adapters for DIALS corrections and custom calculations) and assert that the final combined multiplicative factor matches a pre-calculated, theoretically correct value. This validates both the individual correction factor calculations/conversions and their final combination.

---

**Implementation Note on Efficient Pixel Data Handling (Relevant to Modules 2.S.1, 2.S.2, and 3.S.2)**

While the plan may describe intermediate outputs like `RawDiffusePixelList_i` or `CorrectedDiffusePixelList_i` as "lists of tuples" for conceptual clarity of per-pixel transformations, the actual implementation **must** prioritize memory efficiency and processing speed by using array-based data structures and vectorized operations.

1.  **Per-Still Data Storage (Output of Module 2.S.2):**
    *   Instead of a Python list of individual pixel tuples for each still, the data for accepted diffuse observations from still `i` (i.e., the conceptual `CorrectedDiffusePixelList_i`) should be stored as a set of synchronized, contiguous arrays. For example:
        *   `final_q_vectors_still_i`: An array of shape (M, 3) for q-vectors (where M is the number of accepted pixels for still `i`).
        *   `final_I_corrected_still_i`: An array of shape (M) for corrected intensities.
        *   `final_Sigma_corrected_still_i`: An array of shape (M) for corresponding sigmas.
        *   (Optional but recommended) `final_original_pixel_coords_still_i`: An array of shape (M, 3) storing `(panel_idx, fast_coord, slow_coord)`.
        *   (Optional but recommended) `final_q_components_still_i`: An array of shape (M,3) for `(qx, qy, qz)`.
    *   These arrays should be NumPy arrays or DIALS `flex` arrays.

2.  **Vectorized Processing (Modules 2.S.1, 2.S.2):**
    *   All steps involving per-pixel calculations—such as applying masks, calculating q-vectors for multiple pixels, applying geometric/experimental corrections, and filtering by intensity or resolution—**must be implemented using vectorized operations** on these arrays.
    *   For example, in Module 2.S.1, after applying `Mask_total_2D_i` to a panel's raw data, obtain the coordinates of accepted pixels (e.g., via `np.where` or `flex.bool.iselection()`) and pass these coordinate arrays to batch-capable functions like `panel.get_lab_coord_multiple()` for efficient q-vector calculation.
    *   Similarly, in Module 2.S.2, all correction factors should be calculated as arrays corresponding to the accepted pixels, and then applied simultaneously through array arithmetic.

3.  **Data Handling for Binning and Scaling (Modules 3.S.2 and 3.S.3):**
    *   The per-still array sets (output of Module 2.S.2, as described in point 1 of this note) are the input to Module 3.S.2 (Binning).
    *   **For a first-pass implementation, the `BinnedPixelData_Global` structure created in Module 3.S.2 will be assumed to be manageable in memory.** This structure will map each `voxel_idx` in the `GlobalVoxelGrid` to a collection of all `(I_corrected, Sigma_corrected, still_id, ...other_relevant_per_observation_data...)` tuples/objects that fall into that voxel from across all processed stills.
    *   This in-memory `BinnedPixelData_Global` will then be directly used by Module 3.S.3 (Relative Scaling) for reference generation and residual calculation.
    *   **Future Optimization:** If memory limitations are encountered with very large datasets (i.e., the total number of accepted diffuse pixel observations across all stills is too large to hold all associated `I_corrected, Sigma_corrected, still_id` information in RAM, even when grouped by voxel), this strategy will need to be revisited. Future optimizations might include:
        *   Saving per-still accepted pixel arrays to disk and reading them sequentially during scaling.
        *   Implementing more complex streamed or on-disk aggregation for `BinnedPixelData_Global` if direct in-memory lists per voxel become too large.
    *   The initial implementation should proceed with the assumption of in-memory viability for `BinnedPixelData_Global` to simplify the first pass of development for the scaling algorithms. Performance and memory profiling will guide the need for these future optimizations.

This array-centric approach is critical for achieving acceptable performance and managing memory effectively for large-scale still diffuse scattering datasets.

---

**Phase 3: Voxelization, Relative Scaling, and Merging of Diffuse Data**

**Module 3.S.1: Global Voxel Grid Definition**
*   **Action:** Define a common 3D reciprocal space grid that will be used for merging diffuse scattering data from all processed still images.
*   **Input:**
    *   A collection of all `Experiment_dials_i` objects (specifically their `Crystal_i` models) from Phase 1.
    *   A collection of all `CorrectedDiffusePixelList_i` (from Phase 2) to determine the overall q-space coverage and thus the HKL range.
    *   Configuration parameters for the grid: target resolution limits (`d_min_target`, `d_max_target`), number of divisions per reciprocal lattice unit (e.g., `ndiv_h, ndiv_k, ndiv_l`), and optionally a specific reference unit cell if not averaging.
*   **Process:**
    1.  Determine an average reference crystal model (`Crystal_avg_ref`) from all `Crystal_i` models.
       After averaging, compute the rms Δhkl for a sample of Bragg reflections transformed with `(A_avg_ref)⁻¹`. If this `rms Δhkl` exceeds a threshold (e.g., 0.1), a prominent warning should be logged, indicating potential smearing in the final merged map due to significant crystal variability relative to the average model. For the v1 pipeline, this check is diagnostic only, and the direct transformation using `(A_avg_ref)⁻¹ · q_lab_i` (as described in Module 3.S.2) will still be used regardless of this RMS value.
        This involves:
        a.  **Average Unit Cell (`UC_avg`):** Robustly average all `Crystal_i.get_unit_cell()` parameters using CCTBX utilities to obtain `UC_avg`. This defines the reciprocal metric tensor `B_avg_ref = (UC_avg.fractionalization_matrix())^-T`.
        b.  **Average Orientation Matrix (`U_avg_ref`):**
            i.  Extract `U_i = Crystal_i.get_U()` for each still `i`.
            ii. **Pre-check for Orientation Spread:**
                - Select an initial reference orientation, e.g., `U_ref_for_check = U_0` (the first `U_i` in the list).
                - For every other `U_i`, calculate the misorientation angle (in degrees) between `U_i` and `U_ref_for_check` (e.g., from the trace of the rotation matrix `U_i · U_ref_for_checkᵀ`).
                - Calculate the Root Mean Square (RMS) of these misorientation angles.
                - If this RMS misorientation exceeds a threshold (e.g., 3-5 degrees), issue a prominent warning in the log. This warning should state that the significant spread in crystal orientations may lead to a `U_avg_ref` that is not highly representative and could result in smearing of features in the final merged diffuse map. For the v1 pipeline, averaging will still proceed. (Future versions might consider clustering or per-observation orientation adjustments if this spread is problematic).
            iii. Average all `U_i` matrices using a robust method suitable for rotation matrices (e.g., conversion to quaternions, averaging of quaternions using a method like Slerp or Nlerp if applicable for multiple matrices, and conversion back to a matrix; or averaging of small rotation vectors if deviations are confirmed to be minimal by the pre-check). The chosen averaging method must be documented in the implementation.
        c.  The final setting matrix for the grid is `A_avg_ref = U_avg_ref * B_avg_ref`. This matrix defines the grid's orientation and cell in the common laboratory frame.
    2.  Using `Crystal_avg_ref`, transform all `q_vector` components from all `CorrectedDiffusePixelList_i` to fractional Miller indices `(h,k,l)` to determine the overall minimum and maximum H, K, L ranges covered by the data, considering the target resolution limits.
    3.  Define the `GlobalVoxelGrid` object. This object will store:
        *   `Crystal_avg_ref` (the `dxtbx.model.Crystal` object used for HKL transformations to/from the grid).
        *   The integer HKL range of the grid.
        *   The number of subdivisions per unit cell (`ndiv_h, ndiv_k, ndiv_l`).
        *   Methods for converting `(h,k,l)` to `voxel_idx` and vice-versa.
*   **Output:** `GlobalVoxelGrid` (a single grid definition object for the entire dataset).
*   **Testing (Module 3.S.1):**
    *   **Input:** A list of minimal `Experiment_dials_i` objects (programmatically constructed, containing `Crystal_i` models) and a sample `CorrectedDiffusePixelList_i` (with `scitbx.matrix.col` q-vectors) to define HKL ranges. Grid definition parameters.
    *   **Execution:** Call the grid definition function.
    *   **Verification:** Assert that the `GlobalVoxelGrid` object has the correct reference crystal model, HKL range, and voxel divisions based on the inputs.

**Module 3.S.2: Binning Corrected Diffuse Pixels into Global Voxel Grid**
*   **Action:** Assign each corrected diffuse pixel observation from all stills to its corresponding voxel in the `GlobalVoxelGrid`, applying Laue group symmetry. Implement streamed voxel accumulation to manage memory.
*   **Input:**
    *   A collection of all `CorrectedDiffusePixelList_i` from Phase 2.
    *   `GlobalVoxelGrid` (from Module 3.S.1, containing `Crystal_avg_ref`).
    *   Laue group symmetry information (e.g., space group string or `cctbx.sgtbx.space_group` object).
*   **Process (Memory Management: This process must use streamed voxel accumulation, e.g., Welford's algorithm or similar, to update voxel statistics incrementally without holding all pixel data in memory simultaneously):**
    0.  **Initialize `VoxelAccumulator`:** An instance of the `VoxelAccumulator` class (defined with an HDF5 backend using `h5py` and `zstd` compression) is created. This class will manage the storage of all corrected diffuse pixel observations.  
    1.  Initialise either an in-memory accumulator (small jobs) **or** an on-disk `VoxelAccumulator` (large jobs) that keeps only one voxel’s running stats in RAM at a time.
        a.  Transform `q_vec` (which is `q_lab_i`, the lab-frame q-vector for still `i`) to fractional Miller indices `hkl_frac` in the coordinate system of the `GlobalVoxelGrid` (defined by `Crystal_avg_ref` and its setting matrix `A_avg_ref`):
            `hkl_frac = (A_avg_ref)⁻¹ · q_lab_i`
            **Note on Transformation:** This direct transformation maps the lab-frame q-vector from an individual still `i` into the fractional HKL coordinate system of the average reference crystal. The `rms Δhkl` check performed in Module 3.S.1 serves as a diagnostic for dataset homogeneity; if high, it indicates potential smearing in the merged map due to crystal variability, but the transformation formula itself remains this direct mapping for the initial pipeline version.
        b.  Map `(h,k,l)` to the asymmetric unit of the specified Laue group using CCTBX symmetry functions (e.g., via an adapter for `space_group.info().map_to_asu()`), obtaining `(h_asu, k_asu, l_asu)`.
        c.  Determine the `voxel_idx` in `GlobalVoxelGrid` corresponding to `(h_asu, k_asu, l_asu)`.
        d.  Update the accumulation data structure for `voxel_idx` with `(I_corr, Sigma_corr, still_id)`. If using Welford's, update mean, M2, and count for that `still_id` within that voxel, or for the voxel globally if scales are to be applied before final merge.
*   **Output:**
    *   The `VoxelAccumulator` instance, now populated with all observations in its HDF5 backend. The `BinnedPixelData_Global` (a dictionary mapping `voxel_idx` to a list of `(I_obs, Sigma_obs, still_id, q_lab_i)` observations) will be retrieved from this accumulator by Module 3.S.3 (Relative Scaling) using a method like `VoxelAccumulator.get_binned_data_for_scaling()`.
    *   `ScalingModel_initial_list`: A list of initial scaling model parameter objects, one for each still `i` (or group of stills if a grouping strategy is employed). Parameters typically initialized to unity scale and zero offset.
*   **Testing (Module 3.S.2):**
    *   **Input:** A sample `CorrectedDiffusePixelList_i` (with known `q_vector`s and values), a `GlobalVoxelGrid` object, and Laue group information.
    *   **Execution:** Call the binning and accumulation function.
    *   **Verification:** Assert that observations are correctly assigned to `voxel_idx` after HKL transformation and ASU mapping. Verify that accumulated statistics (if using Welford's) are correct for sample voxels.

**Module 3.S.3: Relative Scaling of Binned Observations (Custom Implementation using DIALS Components)**
*   **Action:** Iteratively refine scaling model parameters for each still `i` (or group of stills), using a custom-defined diffuse scaling model. This model will be built using DIALS/CCTBX components for parameterization (e.g., `SingleScaleFactor`, `GaussianSmoother`) and refinement (e.g., DIALS parameter manager, minimizers like Levenberg-Marquardt).
*   **Input:**
    *   `BinnedPixelData_Global` (from Module 3.S.2).
    *   `ScalingModel_initial_list` (from Module 3.S.2).
    *   Configuration for the custom diffuse scaling model. **Note on Iterative Model Development and Configuration:**
        1.  **Initial v1 Model (parameter-guarded):**  
            *   Exactly **one** free parameter per still: global multiplier `b_i`.  
            *   Optional **single 1-D resolution smoother** with ≤ 5 control points shared by all stills (`a(|q|)`), enabled only if `enable_res_smoother = True` in config.  
            *   **Panel, spatial or additive terms are *hard-disabled* in v1.** Attempting to enable them raises a configuration error.  
            *   A global constant `MAX_FREE_PARAMS = 5 + N_stills` is enforced; CI fails if exceeded.
        2.  **Developer-Led Model Evolution:** The process of adding more complexity to the active scaling model is primarily a developer-led activity during pipeline validation and refinement:
            *   After successfully running and testing the pipeline with the initial simple model, analyze scaling residuals for systematic trends (e.g., versus detector position `px,py`, panel ID, or the still ordering parameter `p_order(i)`).
            *   If significant, smooth, and interpretable trends are observed, the developer will incrementally enable and configure more complex components (e.g., `d_i(panel)` if strong panel-to-panel variations persist; `a_i(px,py,p_order(i))` using `GaussianSmoother2D/3D` if warranted by clear spatial residuals).
            *   Each newly activated component and its parameterization (e.g., number of bins, smoother parameters) must be validated for stability and its impact on improving the fit and data quality.
            *   **Critical for Additive Terms:** Additive offset components (like `c_i(|q|)` for background) should only be introduced after a stable multiplicative scaling solution is achieved, and ideally with strong physical justification or clear evidence from residuals. If both multiplicative and additive terms are refined simultaneously in later iterations, careful attention must be paid to parameter constraints, regularization, and potential correlations, as these terms can trade off leading to non-unique solutions.
        3.  **User-Facing Configuration:** The `advanced_components` section is ignored until a residual plot proves need; enabling it requires `--experimental-scale-model` CLI flag.
    *   The implementation will use DIALS components like `dials.algorithms.scaling.model.components.scale_components.ScaleComponentBase`, `SingleScaleFactor`, and `GaussianSmoother1D/2D/3D` as appropriate for the chosen parameterizations.
    *   Definition and source of the still ordering parameter `p_order(i)` must be configurable if components using it (like smoothers dependent on `p_order(i)`) are activated.
    *   A collection of all `Reflections_dials_i` (from Module 1.S.1), containing `I_bragg_obs_spot` and the `"partiality"` column (`P_spot`), for use in Bragg-based reference generation. **Critical Partiality Handling Strategy:** 
        *   **Partiality handling strategy (revised):**  
            *   `P_spot` **is *only* used as a quality flag**.  We keep reflections with `P_spot ≥ P_min_thresh` (default 0.1) but we **never divide by it** at any later stage.  
            *   Absolute scale will be obtained from Wilson statistics on merged Bragg data rather than from partiality-corrected intensities.  
            *   Unit tests formerly referring to “divide by P_spot” are removed; new tests assert that intensity values are unchanged after the quality filter.
*   **Process (Iterative, using a minimal‐parameter v1 model):**
    1.  **Parameter Management Setup:** Initialize DIALS's active parameter manager with all refineable parameters from all `ScalingModel_i` components. Set up any restraints (e.g., for smoothness of Gaussian smoother parameters).
    2.  **Iterative Refinement Loop:**
        a.  **Reference Generation:**
            *   **Bragg Reference (if used):** For each unique HKL (mapped to ASU), calculate a reference intensity `I_bragg_merged_ref(HKL_asu)`. This is a weighted average of `(I_bragg_obs_spot)` from all contributing stills (that passed the `P_spot >= P_min_thresh` filter), where each term is divided by the current estimate of its still's multiplicative scale factors (from the current `ScalingModel_i`). `P_spot` (from the `"partiality"` column) is used here only for the initial quality filtering, not as a divisor.
            *   **Diffuse Reference:** For each `voxel_idx`, calculate a reference intensity `I_diffuse_merged_ref(voxel_idx)`. This is a weighted average of `(I_diffuse_obs_for_still_i - current_additive_offset_for_still_i)` from all contributing stills, where each term is divided by the current estimate of its still's multiplicative scale factors.
        b.  **Scaling Model Parameter Refinement:**
            *   Instantiate the overall custom `DiffuseScalingModel` which aggregates all per-still/group `ScalingModel_i` components.
            *   Use a DIALS-compatible minimizer (e.g., Levenberg-Marquardt or Gauss-Newton, accessed via the adapter layer) with the custom `DiffuseScalingTarget` function.
            *   The `DiffuseScalingTarget` function calculates residuals for each observation in `BinnedPixelData_Global`. For an observation `I_obs` from still `i` in voxel `v`:
                `Residual_v,i = ( (I_obs - c_i_model_value) / (b_i_model_value * d_i_model_value * a_i_model_value) ) - I_reference(v)`.
                (The exact form depends on how `a,b,c,d` components are defined as additive or multiplicative).
            *   The minimizer adjusts the refineable parameters (e.g., control points of Gaussian smoothers, single scale factors) to minimize the sum of squared, weighted residuals.
        c.  **Convergence Check:** Evaluate convergence criteria (e.g., change in R-factor, change in parameters). If not converged, repeat from step 2a.
*   **Output:** `ScalingModel_refined_list` (containing the refined parameters for the custom `DiffuseScalingModel` components for each still or group).
*   **Testing (Module 3.S.3):**
    *   **Input:** `BinnedPixelData_Global`, `Reflections_dials_i` (with `"partiality"` column), initial scaling model parameters, and configurations for the custom scaling model components (including functional forms and parameterization strategies).
    *   **Execution:** Run the custom relative scaling procedure.
    *   **Verification:** Test with synthetic `BinnedPixelData_Global` where known scale factors, offsets, and sensitivity variations have been applied to different "stills." Assert that the refined parameters in `ScalingModel_refined_list` recover these known variations. Verify correct use of `P_spot` from the `"partiality"` column in Bragg reference generation. Check for convergence of the minimizer and reasonable residual values.

**Module 3.S.4: Merging Relatively Scaled Data into Voxels**
*   **Action:** Apply the final refined scaling model parameters (from `ScalingModel_refined_list`) to all observations in `BinnedPixelData_Global` and merge them within each voxel of the `GlobalVoxelGrid`.
*   **Input:**
    *   `BinnedPixelData_Global` (containing per-voxel lists of `(I_corr, Sigma_corr, still_id)`).
    *   `ScalingModel_refined_list` (containing parameters for the custom `DiffuseScalingModel` components for each still/group).
    *   `GlobalVoxelGrid`.
*   **Process:**
    1.  For each observation `(I_corr, Sigma_corr, still_id, ...)` associated with a `voxel_idx` in `BinnedPixelData_Global`:
        a.  Retrieve the refined `ScalingModel_i` (or its components) for the corresponding `still_id`.
        b.  Calculate the total multiplicative scale `M_i` and total additive offset `C_i` for this observation using the refined model parameters and the observation's specific properties (e.g., `|q|`, `px,py`, `panel_idx`, `p_order(i)`).
        c.  Apply the scaling: `I_final_relative = (I_corr - C_i) / M_i`.
        d.  Propagate uncertainties: `Sigma_final_relative = Sigma_corr / abs(M_i)`.
            **(Note: This formula is valid for the v1 scaling model where the additive offset `C_i` is zero. If future versions refine `C_i` with non-zero uncertainty `Var(C_i)`, the formula must be updated to `Sigma_final_relative = sqrt(Sigma_corr² + Var_C_i) / abs(M_i)`, requiring `Var(C_i)` to be estimated from the scaling model refinement.)**
            **Implementation Note for v1:** The code applying the scaling model (Module 3.S.4) should confirm that the additive component `C_i` derived from the `ScalingModel_refined_list` is indeed effectively zero (e.g., `abs(C_i) < 1e-9`) before using this simplified error propagation formula, especially if the scaling model structure could theoretically produce a non-zero `C_i` even if `refine_additive_offset` was `False`. This can be achieved with an assertion or a conditional check.
    2.  For each `voxel_idx` in `GlobalVoxelGrid`:
        a.  Collect all `I_final_relative` and `Sigma_final_relative` values from observations binned to this `voxel_idx`.
        b.  Perform a weighted merge (typically inverse variance weighting: `weight = 1 / Sigma_final_relative^2`) to calculate `I_merged_relative(voxel_idx)` and `Sigma_merged_relative(voxel_idx)`.
        c.  Store `num_observations_in_voxel(voxel_idx)`.
        d.  Calculate `q_center_x, q_center_y, q_center_z` and `|q|_center` for the voxel using `GlobalVoxelGrid.Crystal_avg_ref`.
*   **Output:** `VoxelData_relative`: A data structure (e.g., a `flex.reflection_table` or NumPy structured array) where each row represents a voxel and contains `(voxel_idx, H_center, K_center, L_center, q_center_x, q_center_y, q_center_z, |q|_center, I_merged_relative, Sigma_merged_relative, num_observations_in_voxel)`.
*   **Testing (Module 3.S.4):**
    *   **Input:** Sample `BinnedPixelData_Global`, a `ScalingModel_refined_list` with known refined parameters, and `GlobalVoxelGrid`.
    *   **Execution:** Call the merging function.
    *   **Verification:** Assert that `I_merged_relative` and `Sigma_merged_relative` in `VoxelData_relative` are correct based on the applied scales and weighted averaging logic.

---

**Phase 4: Absolute Scaling**

**Module 4.S.1: Absolute Scaling and Incoherent Scattering Subtraction**
*   **Action:** Convert the relatively scaled diffuse map (`VoxelData_relative`) to absolute units (e.g., electron units per unit cell) and subtract the theoretically calculated incoherent scattering contribution.
*   **Input:**
    *   `VoxelData_relative` (from Module 3.S.4).
    *   `unitCellInventory`: The complete atomic composition of the unit cell (e.g., a dictionary of element symbols to counts).
    *   `GlobalVoxelGrid.Crystal_avg_ref` (the `dxtbx.model.Crystal` object defining the unit cell and symmetry for theoretical calculations).
    *   Merged, relatively-scaled Bragg intensities **with no partiality correction**.  Scale derivation will follow a Wilson-style fit of ⟨|F|²⟩ vs resolution; reflections fail the quality filter if `P_spot < P_min_thresh`.
*   **Process:**
    1.  **Theoretical Scattering Calculation:**
        *   For each unique element in `unitCellInventory`, obtain its atomic form factor `f0(element, |q|)` and its **full incoherent (Compton) scattering cross-section** `S_incoh(element, |q|)`. This calculation will be performed via an adapter layer for CCTBX scattering utilities. **Note on Q-Range for Incoherent Scattering:** The adapter layer is responsible for ensuring the accuracy of `S_incoh(element, |q|)` across the entire q-range relevant to the diffuse data. This typically involves a hybrid strategy:
            1.  Utilize reliable tabulated data from CCTBX (e.g., IT1992 data accessible via `cctbx.eltbx.sasaki.table().incoherent()`) for q-ranges where these tables are accurate and valid (generally up to `sin(θ)/λ \approx 2.0 Å⁻¹` (corresponding to `|q| = 4π sin(θ)/λ \approx 25 Å⁻¹`)).
            2.  For `|q|` values exceeding the valid range of the tabulated data, the adapter must supplement or switch to an appropriate analytical formulation for high-q X-ray incoherent scattering. Priority should be given to using routines from `cctbx.eltbx.formulae` if they provide accurate high-q incoherent cross-sections. If not, the adapter should implement or utilize a well-established formula such as the relativistic Klein-Nishina formula.
            The source (table/formula) and any approximations used for calculating `S_incoh` across different q-ranges must be clearly documented within the adapter's implementation.
        *   For each voxel in `VoxelData_relative` (with its `|q|_center`):
            *   Calculate `F_calc_sq_voxel = sum_atoms_in_UC (f0(atom, |q|_center)^2)`.
            *   Calculate `I_incoherent_theoretical_voxel = sum_atoms_in_UC (S_incoh(atom, |q|_center))`.
    2.  **Absolute Scale Factor Determination (`Scale_Abs`):**
        *   The primary method for determining `Scale_Abs` will be the Krogh-Moe/Norman summation method. This involves scaling the total experimental scattering (radially averaged `I_merged_diffuse_relative` from `VoxelData_relative` + radially averaged `I_bragg_final_relative` (merged Bragg intensities)) to match the theoretical total scattering (`I_coh_UC(s) + I_incoh_UC(s)`).
        *   The `I_bragg_final_relative` values used in this sum are those obtained after relative scaling (Module 3.S.4 output) and must *not* have been divided by `P_spot` (partiality). They should, however, have passed the `P_spot >= P_min_thresh` quality filter during their initial processing and selection for relative scaling.
        *   (Optional Diagnostic) A secondary `Scale_Abs_Wilson` can be determined for diagnostic purposes by performing a Wilson plot on the merged Bragg data. For this diagnostic, one might consider using only reflections with intrinsically high partiality (e.g., `P_spot >= 0.95` from the original `Reflections_dials_i`, if available and deemed reliable enough for this specific check) to approximate fully recorded reflections for the Wilson plot. This approach avoids re-introducing `P_spot` as a divisor and maintains consistency with the primary data handling strategy where `P_spot` is a quality filter. This `Scale_Abs_Wilson` is for comparison and validation, not for scaling the diffuse data unless the Krogh-Moe method proves problematic.
    3.  **Apply Absolute Scale and Subtract Incoherent Scattering:**
        *   For each voxel in `VoxelData_relative` (solid-angle formula cross-validated against DIALS at θ = 0°, 30°, 60°):
            *   `I_abs_diffuse_voxel = VoxelData_relative.I_merged_relative(voxel) * Scale_Abs - I_incoherent_theoretical_voxel`.
            *   `Sigma_abs_diffuse_voxel = VoxelData_relative.Sigma_merged_relative(voxel) * Scale_Abs` (simplification, assumes `Scale_Abs` and `I_incoherent_theoretical_voxel` have negligible uncertainty relative to `Sigma_merged_relative`). This simplification must be documented.
*   **Output:**
    *   `VoxelData_absolute`: The final 3D diffuse scattering map on an absolute scale, with incoherent scattering removed. Structure similar to `VoxelData_relative` but with `I_abs_diffuse` and `Sigma_abs_diffuse`.
    *   `ScalingModel_final_list`: The per-still/group scaling models from `ScalingModel_refined_list` with the `Scale_Abs` factor incorporated into their overall scale components, and the `I_incoherent_theoretical` (as a function of `|q|`) incorporated into their additive offset components.
*   **Testing (Module 4.S.1):**
    *   **Input:** Sample `VoxelData_relative`, known `unitCellInventory`, `Crystal_avg_ref`, and a set of correctly prepared (scaled and partiality-corrected) Bragg intensities with known absolute scale.
    *   **Execution:** Call the absolute scaling and incoherent subtraction function.
    *   **Verification:** Assert that the calculated `Scale_Abs` is correct. Assert that `I_abs_diffuse` in `VoxelData_absolute` matches expected values after scaling and subtraction of incoherent scattering (calculated using full tabulated cross-sections).
</file>

</files>
</code context>
